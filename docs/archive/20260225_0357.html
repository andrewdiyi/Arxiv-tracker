<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-25 03:57</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260225_0357</div>
    <div class="row"><div class="card">
<div class="title">Continuum Robot State Estimation with Actuation Uncertainty</div>
<div class="meta-line">Authors: James M. Ferguson, Alan Kuntz, Tucker Hermans</div>
<div class="meta-line">First: 2026-01-08T01:53:42+00:00 · Latest: 2026-02-23T18:57:12+00:00</div>
<div class="meta-line">Comments: Public preprint for IEEE RAL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04493v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.04493v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuum robots are flexible, thin manipulators capable of navigating confined or delicate environments making them well suited for surgical applications. Previous approaches to continuum robot state estimation typically rely on simplified, deterministic actuation models. In contrast, our method jointly estimates robot shape, external loads, internal stresses, and actuation inputs. We adopt a discrete Cosserat rod formulation and show that, when paired with a midpoint integration rule, it achieves high numerical accuracy with relatively few state nodes. This discretization naturally induces a factor-graph structure for sparse nonlinear optimization on SE(3). We extend the formulation with actuation factors for tendon-driven robots and combine multiple rod graphs for parallel continuum robots with closed-loop topologies. By explicitly including actuation variables in the state, the linearized system can be reused to extract manipulator Jacobians, which we leverage in performing trajectory tracking. Finally, we validate the approach experimentally on a surgical concentric tube robot. Overall, our approach enables principled, real-time estimation across multiple continuum robot architectures, accounting for actuation uncertainty and providing direct access to manipulator Jacobians.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续体机器人状态估计中的驱动不确定性</div>
<div class="mono" style="margin-top:8px">连续体机器人是灵活、纤细的操作器，能够导航狭窄或精细的环境，使其非常适合手术应用。先前的连续体机器人状态估计方法通常依赖于简化的确定性驱动模型。相比之下，我们的方法联合估计了机器人形状、外部载荷、内部应力和驱动输入。我们采用离散柯西尔杆形式，并表明，当与中点积分规则配对时，它能够以相对较少的状态节点实现高数值精度。这种离散化自然地诱导了SE(3)上的稀疏非线性优化的因子图结构。我们通过引入驱动因子扩展了该形式，适用于腱驱动的机器人，并结合多个杆图来处理具有闭环拓扑的并行连续体机器人。通过明确包含驱动变量在状态中，线性化系统可以重用以提取操作器雅可比矩阵，我们利用这些雅可比矩阵进行轨迹跟踪。最后，我们在手术同心管机器人上进行了实验验证。总体而言，我们的方法能够针对多种连续体机器人架构进行原理性的实时估计，考虑了驱动不确定性，并直接提供了操作器雅可比矩阵。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of accurately estimating the state of continuum robots, which are used in surgical applications due to their flexibility and ability to navigate delicate environments. The authors propose a method that jointly estimates the robot&#x27;s shape, external loads, internal stresses, and actuation inputs using a discrete Cosserat rod formulation. This approach, when combined with a midpoint integration rule, achieves high numerical accuracy with a small number of state nodes. The method is validated experimentally on a surgical concentric tube robot, demonstrating its effectiveness in real-time estimation across various continuum robot architectures while accounting for actuation uncertainty and providing direct access to manipulator Jacobians.</div>
<div class="mono" style="margin-top:8px">该论文旨在准确估计用于外科手术的柔性连续机器人状态。作者提出了一种方法，该方法联合估计机器人的形状、外部载荷、内部应力和驱动装置输入，使用离散的柯西尔杆形式化方法。结合中点积分规则，该方法能够以较少的状态节点实现高数值精度。该方法在手术同心管机器人上进行了实验验证，展示了其在各种连续机器人架构中的实时估计效果，同时考虑了驱动装置的不确定性，并直接提供了操作器雅可比矩阵。</div>
</details>
</div>
<div class="card">
<div class="title">Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation</div>
<div class="meta-line">Authors: Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</div>
<div class="meta-line">First: 2025-05-22T11:37:39+00:00 · Latest: 2026-02-23T18:46:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16547v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.16547v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>寻找果实：零样本模拟到现实的强化学习在遮挡感知植物操作中的应用</div>
<div class="mono" style="margin-top:8px">在开放环境中进行自主收获面临复杂的操作问题。在大多数情况下，自主系统必须处理显著的遮挡，并在存在大量结构不确定性（每株植物都不同）的情况下进行交互。感知和建模不确定性使得设计可靠的收获操作控制器具有挑战性，导致部署时性能不佳。我们提出了一种模拟到现实的强化学习（RL）框架，用于遮挡感知的植物操作，其中策略完全在模拟中学习以重新定位茎和叶子以揭示目标果实。在我们提出的方法中，我们将高层的运动规划与低层的顺应控制解耦，简化了模拟到现实的转移。这种分解使得学习到的策略能够在具有不同刚度和形态的多种植物之间泛化。在多个真实世界的植物设置实验中，我们的系统在揭示目标果实方面取得了高达86.7%的成功率，展示了对遮挡变化和结构不确定性具有鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenges of autonomous harvesting in open environments, where significant occlusion and structural uncertainties complicate plant manipulation. The authors propose a sim2real reinforcement learning framework that learns a policy in simulation to reposition stems and leaves, revealing target fruits. By decoupling kinematic planning from compliant control, the system generalizes well across different plants. Experiments show up to 86.7% success in exposing target fruits, indicating robustness to occlusion and structural variations.</div>
<div class="mono" style="margin-top:8px">研究针对开放环境下的自主收获问题，重点关注由于显著遮挡和结构不确定性带来的复杂操作挑战。作者提出了一种模拟到现实的强化学习（RL）框架，用于重新定位茎叶以揭示目标果实。通过将高层运动规划与低层顺应控制分离，系统能够在不同植物之间泛化。实验结果显示，在多个真实植物设置中，系统在揭示目标果实方面高达86.7%的成功率，展示了对遮挡和结构变化的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning</div>
<div class="meta-line">Authors: Jiahui Fu, Junyu Nan, Lingfeng Sun, Hongyu Li, Jianing Qian, Jennifer L. Barry, Kris Kitani, George Konidaris</div>
<div class="meta-line">First: 2026-02-23T18:35:18+00:00 · Latest: 2026-02-23T18:35:18+00:00</div>
<div class="meta-line">Comments: 25 pages, 15 figures. Project webpage: https://nova-plan.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20119v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nova-plan.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NovaPlan：通过闭环视频语言规划实现零样本长时程操作</div>
<div class="mono" style="margin-top:8px">解决长时程任务需要机器人将高层次语义推理与低层次物理交互相结合。尽管视觉-语言模型（VLM）和视频生成模型可以分解任务并想象结果，但它们往往缺乏实现世界执行所需的物理基础。我们提出了NovaPlan，这是一种分层框架，将闭环VLM和视频规划与几何上接地的机器人执行统一起来，以实现零样本长时程操作。在高层次上，VLM规划器将任务分解为子目标，并在闭环中监控机器人执行，使系统能够通过自主重新规划从单步失败中恢复。为了计算低层次的机器人动作，我们从生成的视频中提取并利用与任务相关的对象关键点和人类手部姿态作为运动学先验，并采用切换机制选择更好的一个作为机器人动作的参考，即使在严重遮挡或深度不准确的情况下也能保持稳定的执行。我们在三个长时程任务和功能性操作基准（FMB）上展示了NovaPlan的有效性。我们的结果表明，NovaPlan可以在没有任何先验演示或训练的情况下执行复杂的装配任务并表现出灵巧的错误恢复行为。项目页面：https://nova-plan.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">NovaPlan is a hierarchical framework that integrates closed-loop vision-language planning and geometrically grounded robot execution for zero-shot long-horizon manipulation. It decomposes tasks into sub-goals and monitors robot execution, allowing for autonomous re-planning. To compute low-level actions, it uses task-relevant object keypoints and human hand poses as kinematic priors, switching between them based on their quality. NovaPlan demonstrates effectiveness in complex assembly tasks and error recovery without prior training or demonstrations.</div>
<div class="mono" style="margin-top:8px">NovaPlan 是一个层次框架，将视觉语言模型和视频规划与几何上接地的机器人执行相结合，用于零样本长时程操作。它使用闭环 VLM 计划器分解任务并监控机器人执行，以实现自主重新规划。为了计算低级动作，它从生成的视频中提取和利用物体关键点和人类手部姿势，并采用切换机制选择最佳参考用于机器人动作。NovaPlan 在复杂装配任务和错误恢复中表现出色，无需任何先验演示或训练。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network</div>
<div class="meta-line">Authors: Cristian Manca, Christian Scano, Giorgio Piras, Fabio Brau, Maura Pintor, Battista Biggio</div>
<div class="meta-line">First: 2026-02-03T14:50:19+00:00 · Latest: 2026-02-23T17:35:26+00:00</div>
<div class="meta-line">Comments: ITASEC-2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03596v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03596v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers. In this work, we study the problem of detecting 5G attacks \textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services. We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE-5GC：面向5G核心网络异常检测评估的安全指南</div>
<div class="mono" style="margin-top:8px">基于机器学习的异常检测系统在5G核心网络中越来越多地被用于监控复杂的高流量。然而，大多数现有方法都是在独立同分布（IID）数据可用且不存在适应性攻击者的假设下进行评估的，而在实际运行环境中这些假设很少成立。本文研究了在野外检测5G攻击的问题，重点关注实际部署环境。我们提出了5G核心网络异常检测评估的安全指南（SAGE-5GC），该指南基于领域知识并考虑潜在的对抗性威胁。使用一个真实的5G核心数据集，我们首先训练了几种异常检测器，并评估它们在标准5GC控制平面网络服务（基于PFCP）的典型网络攻击下的基线性能。然后，我们将评估扩展到对抗性环境，在这种环境中，攻击者试图通过操纵网络流量的可观察特征来逃避检测，同时保持恶意流量的预期功能。从一组可控特征开始，我们通过随机扰动分析模型的敏感性和对抗鲁棒性。最后，我们引入了一种基于遗传算法的实用优化策略，该策略仅作用于攻击者可控的特征，无需了解底层检测模型。实验结果表明，对抗性构造的攻击可以显著降低检测性能，强调了在野外部署的5G网络中需要稳健的安全意识评估方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the evaluation of anomaly detection systems in 5G Core networks by proposing SAGE-5GC, a set of security-aware guidelines. The study uses a realistic 5G Core dataset to train and evaluate several anomaly detectors against standard cyberattacks and under adversarial conditions. Key findings show that adversarial attacks can significantly reduce detection performance, highlighting the necessity for robust evaluation methods in operational environments.</div>
<div class="mono" style="margin-top:8px">研究旨在在现实条件下评估5G核心网络中异常检测系统的有效性，解决现有方法的局限性。研究提出了SAGE-5GC安全意识评估指南，并使用真实的5G核心数据集评估了多个异常检测器。研究结果表明，对抗性攻击可以显著降低检测性能，强调了5G网络中需要具备鲁棒性的评估方法的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">KINESIS: Motion Imitation for Human Musculoskeletal Locomotion</div>
<div class="meta-line">Authors: Merkourios Simos, Alberto Silvio Chiappa, Alexander Mathis</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2025-03-18T18:37:49+00:00 · Latest: 2026-02-23T17:30:07+00:00</div>
<div class="meta-line">Comments: Accepted to ICRA. Here we include an appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.14637v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.14637v2">PDF</a> · <a href="https://github.com/amathislab/Kinesis">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do humans move? Advances in reinforcement learning (RL) have produced impressive results in capturing human motion using physics-based humanoid control. However, torque-controlled humanoids fail to model key aspects of human motor control such as biomechanical joint constraints \&amp; non-linear and overactuated musculotendon control. We present KINESIS, a model-free motion imitation framework that tackles these challenges. KINESIS is trained on 1.8 hours of locomotion data and achieves strong motion imitation performance on unseen trajectories. Through a negative mining approach, KINESIS learns robust locomotion priors that we leverage to deploy the policy on several downstream tasks such as text-to-control, target point reaching, and football penalty kicks. Importantly, KINESIS learns to generate muscle activity patterns that correlate well with human EMG activity. We show that these results scale seamlessly across biomechanical model complexity, demonstrating control of up to 290 muscles. Overall, the physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control. Code, videos and benchmarks are available at https://github.com/amathislab/Kinesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KINESIS：模仿人类肌肉骨骼运动的运动模仿</div>
<div class="mono" style="margin-top:8px">人类是如何移动的？强化学习（RL）的进步在使用基于物理的人形控制捕捉人类运动方面取得了令人印象深刻的成果。然而，力矩控制的人形机器人无法模拟人类运动控制的关键方面，如生物力学关节约束和非线性、过驱动的肌肉肌腱控制。我们提出了KINESIS，一种无需模型的运动模仿框架，以应对这些挑战。KINESIS 在 1.8 小时的运动数据上进行训练，并在未见过的轨迹上实现了强大的运动模仿性能。通过负样本挖掘方法，KINESIS 学习到稳健的运动先验，我们利用这些先验将策略部署到多个下游任务，如文本到控制、目标点到达和足球点球。重要的是，KINESIS 学会生成与人类肌电图活动相关性良好的肌肉活动模式。我们展示了这些结果在生物力学模型复杂性方面的无缝扩展，展示了对多达 290 块肌肉的控制。总体而言，生理上的合理性使 KINESIS 成为解决人类运动控制中具有挑战性问题的有前途的模型。代码、视频和基准测试可在 https://github.com/amathislab/Kinesis/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">KINESIS is a model-free motion imitation framework that addresses the limitations of torque-controlled humanoids in capturing human locomotion. Trained on 1.8 hours of locomotion data, KINESIS achieves strong performance on unseen trajectories and learns robust locomotion priors, enabling it to perform various downstream tasks like text-to-control and football penalty kicks. Notably, KINESIS generates muscle activity patterns that closely correlate with human EMG activity, demonstrating its physiological plausibility and effectiveness across different biomechanical models.</div>
<div class="mono" style="margin-top:8px">KINESIS 是一种无需模型的运动模仿框架，解决了扭矩控制的人形机器人在捕捉人类运动时的局限性。通过训练1.8小时的运动数据，KINESIS 在未见过的轨迹上表现出强大的性能，并学习到稳健的运动先验，使其能够执行各种下游任务，如文本到控制和足球点球。值得注意的是，KINESIS 生成的肌肉活动模式与人类的EMG活动高度相关，展示了其生理上的合理性，并在不同生物力学模型中表现出有效性。</div>
</details>
</div>
<div class="card">
<div class="title">AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation</div>
<div class="meta-line">Authors: Ge Yuan, Qiyuan Qiao, Jing Zhang, Dong Xu</div>
<div class="meta-line">First: 2026-02-23T17:12:25+00:00 · Latest: 2026-02-23T17:12:25+00:00</div>
<div class="meta-line">Comments: Homepage: https://AdaWorldPolicy.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20057v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20057v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://AdaWorldPolicy.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaWorldPolicy：基于世界模型的扩散策略及其在线自适应学习在机器人操作中的应用</div>
<div class="mono" style="margin-top:8px">有效的机器人操作需要能够预测物理结果并适应现实环境的策略。在本工作中，我们提出了一种统一框架——基于世界模型的扩散策略及其在线自适应学习（AdaWorldPolicy），以在最少的人为干预下增强动态条件下的机器人操作。我们的核心见解是，世界模型提供了强大的监督信号，可以在动态环境中实现在线自适应学习，同时可以通过力-扭矩反馈来缓解动态力的变化。我们的AdaWorldPolicy集成了一个世界模型、一个动作专家和一个力预测器——所有这些都作为相互连接的Flow Matching Diffusion Transformers (DiT)实现。它们通过多模态自注意力层相互连接，从而实现深层特征交换以进行联合学习，同时保持各自的独特模块化特性。我们还提出了一种新的在线自适应学习（AdaOL）策略，该策略动态地在动作生成模式和未来想象模式之间切换，以驱动所有三个模块的反应性更新。这创建了一个强大的闭环机制，可以在最少的开销下适应视觉和物理域的变化。在一系列模拟和真实机器人基准测试中，我们的AdaWorldPolicy实现了最先进的性能，并具有动态适应能力以应对离分布场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AdaWorldPolicy is a unified framework that enhances robotic manipulation in dynamic environments through online adaptive learning. It integrates a world model, an action expert, and a force predictor using Flow Matching Diffusion Transformers, and employs a novel Online Adaptive Learning strategy to switch between action generation and future imagination modes. This approach achieves state-of-the-art performance across various simulated and real-robot benchmarks, demonstrating strong adaptability to out-of-distribution scenarios.</div>
<div class="mono" style="margin-top:8px">AdaWorldPolicy 是一个统一框架，通过在线自适应学习增强机器人在动态环境中的操作。它整合了世界模型、动作专家和力预测器，使用 Flow Matching Diffusion Transformers，并采用了一种新的在线自适应学习策略，在动作生成和未来想象模式之间切换。该方法在各种模拟和真实机器人基准测试中表现出色，展示了强大的适应出分布场景的能力。</div>
</details>
</div>
<div class="card">
<div class="title">RobPI: Robust Private Inference against Malicious Client</div>
<div class="meta-line">Authors: Jiaqi Xue, Mengxin Zheng, Qian Lou</div>
<div class="meta-line">First: 2026-02-23T14:58:08+00:00 · Latest: 2026-02-23T14:58:08+00:00</div>
<div class="meta-line">Comments: Accepted by SaTML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19918v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19918v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RobPI：针对恶意客户端的稳健私有推理</div>
<div class="mono" style="margin-top:8px">各种应用中机器学习推理的广泛应用引发了隐私担忧。为应对这一问题，已经创建了私有推理（PI）协议，允许各方在不泄露敏感数据的情况下进行推理。尽管在PI的效率方面取得了近期进展，但大多数当前方法都假设半诚实威胁模型，即数据所有者诚实并遵守协议。然而，在现实中，数据所有者可能有不同的动机并以不可预测的方式行事，使这一假设变得不切实际。为了展示恶意客户端如何破坏半诚实模型，我们首先设计了一种针对多种最新私有推理协议的推理操纵攻击。这种攻击允许恶意客户端以3到8倍少的查询次数修改模型输出。受攻击的启发，我们提出了并实现了RobPI，这是一种能够抵御恶意客户端的稳健和抗毁私有推理协议。RobPI 结合了一种独特的加密协议，通过将加密兼容的噪声编织到私有推理的logits和特征中，增强了安全性，从而有效地抵御恶意客户端攻击。我们在各种神经网络和数据集上的广泛实验表明，RobPI 将攻击成功率降低了约91.9%，并使恶意客户端攻击所需的查询次数增加了超过10倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the privacy concerns in machine learning inference by proposing RobPI, a robust private inference protocol designed to counteract malicious clients. It introduces an attack that demonstrates the vulnerabilities of existing semi-honest models, reducing the number of queries needed to manipulate model outputs by 3x to 8x. RobPI integrates a cryptographic protocol that adds noise to logits and features, significantly reducing the attack success rate and increasing the query requirements for malicious clients.</div>
<div class="mono" style="margin-top:8px">论文提出了一种名为RobPI的鲁棒私有推理协议，旨在应对恶意客户端。该协议通过在logits和特征中添加噪声来增强安全性。实验结果显示，RobPI将攻击成功率降低了91.9%，并将恶意客户端所需的查询次数提高了超过10倍。</div>
</details>
</div>
<div class="card">
<div class="title">Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation</div>
<div class="meta-line">Authors: Niklas Grambow, Lisa-Marie Fenner, Felipe Kempkes, Philip Hotz, Dingyuan Wan, Jörg Krüger, Kevin Haninger</div>
<div class="meta-line">First: 2025-09-30T14:22:45+00:00 · Latest: 2026-02-23T14:52:42+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 4 tables, the paper has been accepted for publication in the IEEE Robotics and Automation Letters</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26308v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.26308v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution states in robot manipulation often lead to unpredictable robot behavior or task failure, limiting success rates and increasing risk of damage. Anomaly detection (AD) can identify deviations from expected patterns in data, which can be used to trigger failsafe behaviors and recovery strategies. Prior work has applied data-driven AD on time series data for specific robotic tasks, however the transferability of an AD approach between different robot control strategies and task types has not been shown. Leveraging time series data, such as force/torque signals, allows to directly capture robot-environment interactions, crucial for manipulation and online failure detection. As robotic tasks can have widely signal characteristics and requirements, AD methods which can be applied in the same way to a wide range of tasks is needed, ideally with good data efficiency. We examine three industrial robotic tasks, robotic cabling, screwing, and sanding, each with multi-modal time series data and several anomalies. Several autoencoderbased methods are compared, and we evaluate the generalization across different robotic tasks and control methods (diffusion policy-, position-, and impedance-controlled). This allows us to validate the integration of AD in complex tasks involving tighter tolerances and variation from both the robot and its environment. Additionally, we evaluate data efficiency, detection latency, and task characteristics which support robust detection. The results indicate reliable detection with AUROC exceeding 0.96 in failures in the cabling and screwing task, such as incorrect or misaligned parts and obstructed targets. In the polishing task, only severe failures were reliably detected, while more subtle failure types remained undetected.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人装配、拧紧和操作中通用故障监控的异常检测</div>
<div class="mono" style="margin-top:8px">机器人操作中的离群状态通常会导致不可预测的机器人行为或任务失败，限制了成功率并增加了损坏风险。异常检测（AD）可以识别数据中的异常模式偏差，用于触发安全行为和恢复策略。先前的工作已经在特定机器人任务的时间序列数据上应用了数据驱动的AD方法，但不同机器人控制策略和任务类型之间AD方法的可移植性尚未得到证明。利用时间序列数据，如力/扭矩信号，可以直接捕捉机器人-环境交互，这对于操作和在线故障检测至关重要。由于机器人任务可能具有广泛的时间序列特征和要求，需要一种可以在多种任务中以相同方式应用的AD方法，最好具有良好的数据效率。我们研究了三种工业机器人任务：电缆装配、拧紧和打磨，每个任务都有多模态时间序列数据和多种异常。比较了几种基于自编码器的方法，并评估了不同机器人任务和控制方法（扩散策略、位置控制和阻抗控制）之间的泛化能力。这使我们能够验证AD在涉及更严格公差和来自机器人及其环境的变异的复杂任务中的集成。此外，我们还评估了数据效率、检测延迟和任务特征，以支持稳健的检测。结果表明，在电缆装配和拧紧任务中的故障检测中，AUROC超过0.96，如错误或错位的部件和被阻挡的目标。在打磨任务中，仅能可靠地检测严重故障，而更微妙的故障类型仍未被检测到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the reliability of robotic assembly tasks by detecting anomalies in multi-modal time series data such as force/torque signals. The study compares several autoencoder-based methods for anomaly detection across three industrial robotic tasks: cabling, screwing, and sanding. The results show reliable detection with an AUROC exceeding 0.96 for failures in cabling and screwing tasks, while only severe failures were detected in the sanding task due to its more subtle nature.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过异常检测提高机器人操作任务的成功率和安全性。通过在编接、拧螺丝和打磨三个工业机器人任务中比较基于自编码器的方法，研究显示在编接和拧螺丝任务中，对于错误或对齐不当的部件，异常检测的AUROC超过0.96。然而，在打磨任务中仅能检测到严重故障，表明在检测更细微的问题方面存在局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform</div>
<div class="meta-line">Authors: Stefan Fabian, Aljoscha Schmidt, Jonas Süß, Dishant, Aum Oza, Oskar von Stryk</div>
<div class="meta-line">First: 2026-02-23T14:38:23+00:00 · Latest: 2026-02-23T14:38:23+00:00</div>
<div class="meta-line">Comments: https://github.com/tu-darmstadt-ros-pkg/athena</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19898v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19898v1">PDF</a> · <a href="https://github.com/tu-darmstadt-ros-pkg/athena">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In disaster response and situation assessment, robots have great potential in reducing the risks to the safety and health of first responders. As the situations encountered and the required capabilities of the robots deployed in such missions differ wildly and are often not known in advance, heterogeneous fleets of robots are needed to cover a wide range of mission requirements. While UAVs can quickly survey the mission environment, their ability to carry heavy payloads such as sensors and manipulators is limited. UGVs can carry required payloads to assess and manipulate the mission environment, but need to be able to deal with difficult and unstructured terrain such as rubble and stairs. The ability of tracked platforms with articulated arms (flippers) to reconfigure their geometry makes them particularly effective for navigating challenging terrain. In this paper, we present Athena, an open-hardware rescue ground robot research platform with four individually reconfigurable flippers and a reliable low-cost remote emergency stop (E-Stop) solution. A novel mounting solution using an industrial PU belt and tooth inserts allows the replacement and testing of different track profiles. The manipulator with a maximum reach of 1.54m can be used to operate doors, valves, and other objects of interest. Full CAD &amp; PCB files, as well as all low-level software, are released as open-source contributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Athena：一种自主开源硬件跟踪救援机器人平台</div>
<div class="mono" style="margin-top:8px">在灾害响应和情况评估中，机器人在降低一线救援人员安全和健康风险方面具有巨大潜力。由于此类任务中遇到的情况和部署的机器人所需能力差异巨大且往往无法提前预知，因此需要异构机器人队列来覆盖广泛的任务需求。虽然无人机可以迅速对任务环境进行调查，但它们携带重型传感器和操作装置的能力有限。地面无人车可以携带所需设备来评估和操作任务环境，但需要能够应对诸如废墟和楼梯等复杂和未结构化的地形。具有可重构几何形状的履带平台（带有翻转臂）的能力使它们特别适用于穿越具有挑战性的地形。在本文中，我们介绍了Athena，一种具有四个可独立重构翻转臂和可靠的低成本远程紧急停止（E-Stop）解决方案的开源救援地面机器人研究平台。一种新颖的安装解决方案使用工业PU带和齿状插入件，允许更换和测试不同的履带配置。最大伸展长度为1.54米的操作装置可用于操作门、阀门和其他感兴趣的物体。完整的CAD和PCB文件以及所有低级软件均作为开源贡献发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Athena is an autonomous open-hardware rescue robot designed to navigate challenging terrains and perform rescue tasks. The robot features four reconfigurable flippers and a reliable remote emergency stop system. Key experimental findings include the robot&#x27;s ability to reconfigure its geometry for navigating difficult terrain and its manipulator with a 1.54m reach for operating doors and valves. Full CAD and PCB files are open-sourced.</div>
<div class="mono" style="margin-top:8px">Athena 是一种自主的开源硬件救援地面机器人，旨在应对复杂地形并执行环境调查和操作任务。它配备了四个可重新配置的翻转臂和可靠的远程紧急停止系统。通过一种新型的安装解决方案支持可互换的履带，并包括一个最大伸展长度为1.54米的操作臂。主要发现包括机器人在处理不规则地形方面的有效性及其在各种灾难响应场景中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling</div>
<div class="meta-line">Authors: Yirui Sun, Guangyu Zhuge, Keliang Liu, Jie Gu, Zhihao xia, Qionglin Ren, Chunxu tian, Zhongxue Ga</div>
<div class="meta-line">First: 2026-02-23T12:12:51+00:00 · Latest: 2026-02-23T12:12:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19764v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19764v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过深度多感官融合和稀疏专家缩放实现灵巧的实体操作</div>
<div class="mono" style="margin-top:8px">实现灵巧的实体操作需要深度整合异构多模态感官输入。然而，当前以视觉为中心的方法往往忽视了复杂任务中力和几何反馈的关键性。本文提出了一种DeMUSE框架，利用扩散变换器整合RGB、深度和6轴力信号，形成统一的序列流。采用自适应模态特定归一化（AdaMN）重新校准模态感知特征，缓解表示不平衡并协调多感官信号的异构分布。为了实现高效的扩展，架构使用共享专家的稀疏混合专家（MoE），增加物理先验模型容量的同时保持实时控制所需的低推理延迟。联合去噪目标同步合成环境演化和动作序列，确保物理一致性。在模拟和实际试验中，DeMUSE的成功率分别为83.2%和72.5%，展示了最先进的性能，验证了深度多感官整合对于复杂物理交互的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for integrating various sensory inputs to achieve dexterous manipulation. It introduces DeMUSE, a framework that uses a Diffusion Transformer to fuse RGB, depth, and force data. The model employs Adaptive Modality-specific Normalization to balance different sensory inputs and a Sparse Mixture-of-Experts to maintain efficiency. Experiments show DeMUSE achieves success rates of 83.2% and 72.5% in simulations and real-world tests, highlighting the importance of multi-sensory integration for complex physical tasks.</div>
<div class="mono" style="margin-top:8px">本文旨在通过整合多种感官输入来实现灵巧的操纵，特别强调力和几何反馈的重要性。该文提出了DeMUSE框架，使用扩散变换器结合RGB、深度和力数据。模型采用自适应模态特定归一化来平衡不同感官输入，并使用稀疏混合专家机制以保持实时性能。实验结果显示，在模拟和真实世界试验中的成功率分别为83.2%和72.5%，验证了深度多感官整合对于复杂物理交互的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Universal Pose Pretraining for Generalizable Vision-Language-Action Policies</div>
<div class="meta-line">Authors: Haitao Lin, Hanyang Yu, Jingshun Huang, He Zhang, Yonggen Ling, Ping Tan, Xiangyang Xue, Yanwei Fu</div>
<div class="meta-line">First: 2026-02-23T11:00:08+00:00 · Latest: 2026-02-23T11:00:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.
  To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.
  Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用姿态预训练以实现通用化的视觉-语言-行动策略</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-行动（VLA）模型常常因为将高层次感知与稀疏的、特定于身体的行动监督纠缠在一起而遭受特征坍塌和低训练效率的问题。由于这些模型通常依赖于优化用于视觉问答（VQA）的VLM主干网络，它们在语义识别方面表现出色，但往往忽略了决定不同行动模式的微妙的3D状态变化。
  为了解决这些不一致，我们提出了Pose-VLA，这是一种解耦的范式，将VLA训练分为一个预训练阶段，用于在统一的相机中心空间中提取通用的3D空间先验，以及一个后训练阶段，用于在特定于机器人行动空间内高效地进行身体对齐。通过引入离散的姿态标记作为通用表示，Pose-VLA无缝地将来自多种3D数据集的空间定位与来自机器人演示的几何级轨迹相结合。我们的框架遵循两阶段预训练管道，通过姿态建立基本的空间定位，然后通过轨迹监督进行运动对齐。
  广泛的评估表明，Pose-VLA在RoboTwin 2.0上达到了79.5%的平均成功率，并在LIBERO上表现出竞争力，得分为96.0%。现实世界的实验进一步展示了仅使用每任务100个演示即可实现跨多种物体的稳健泛化，验证了我们预训练范式的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing Vision-Language-Action (VLA) models, which suffer from feature collapse and low training efficiency due to the entanglement of high-level perception with sparse action supervision. It proposes Pose-VLA, a decoupled paradigm that pre-trains models to extract universal 3D spatial priors and then aligns them with specific robot actions. Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments show robust generalization across diverse objects using only 100 demonstrations per task.</div>
<div class="mono" style="margin-top:8px">论文针对现有Vision-Language-Action (VLA)模型存在的特征塌陷和低训练效率问题，这些问题源于高层感知与稀疏动作监督的纠缠。提出了Pose-VLA，一种分阶段预训练模型以提取通用的3D空间先验并随后与特定机器人动作对齐的范式。Pose-VLA在RoboTwin 2.0上达到了79.5%的平均成功率，在LIBERO上表现也相当出色，达到96.0%。实验证明，仅使用每任务100个演示，模型就能在多种物体上实现稳健的泛化。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array</div>
<div class="meta-line">Authors: Bailey Dacre, Rodrigo Moreno, Jørn Lambertsen, Kasper Stoy, Andrés Faíña</div>
<div class="meta-line">First: 2026-02-23T09:54:32+00:00 · Latest: 2026-02-23T09:54:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19653v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19653v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distributed Manipulator Systems, composed of arrays of robotic actuators necessitate dense actuator arrays to effectively manipulate small objects. This paper presents a system composed of modular 3-DoF robotic tiles interconnected by a compliant surface layer, forming a continuous, controllable manipulation surface. The compliant layer permits increased actuator spacing without compromising object manipulation capabilities, significantly reducing actuator density while maintaining robust control, even for smaller objects. We characterize the coupled workspace of the array and develop a manipulation strategy capable of translating objects to arbitrary positions within an N X N array. The approach is validated experimentally using a minimal 2 X 2 prototype, demonstrating the successful manipulation of objects with varied shapes and sizes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用互联执行器阵列的可扩展低密度分布式操作</div>
<div class="mono" style="margin-top:8px">分布式操作器系统由机器人执行器阵列组成，需要密集的执行器阵列才能有效地操作小型物体。本文介绍了一种由模块化3-DoF机器人瓷砖组成且通过柔顺表面层互联的系统，形成一个连续可控的操作表面。柔顺层允许增加执行器间距而不影响物体操作能力，显著降低执行器密度同时保持稳健控制，即使对于较小的物体也是如此。我们表征了阵列的耦合工作空间并开发了一种操作策略，能够在N X N阵列内将物体移至任意位置。该方法通过使用最小的2 X 2原型进行实验验证，成功地操作了不同形状和大小的物体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of manipulating small objects with distributed robotic systems by introducing a system of modular 3-DoF robotic tiles interconnected by a compliant layer. This design allows for increased actuator spacing while maintaining effective manipulation, reducing actuator density. The system is validated through a 2 X 2 prototype, successfully demonstrating the manipulation of various object shapes and sizes.</div>
<div class="mono" style="margin-top:8px">本文提出了一种由模块化3-DoF机械臂单元和弹性表面层组成的分布式操纵系统，以解决小物体的操纵问题。该设计通过增加执行器间距同时保持有效的操纵能力，减少了执行器密度。通过使用2 X 2原型进行实验验证，展示了该系统能够成功操纵各种形状和大小的物体，并能够将物体移动到N X N阵列内的任意位置。</div>
</details>
</div>
<div class="card">
<div class="title">Compositional Planning with Jumpy World Models</div>
<div class="meta-line">Authors: Jesse Farebrother, Matteo Pirotta, Andrea Tirinzoni, Marc G. Bellemare, Alessandro Lazaric, Ahmed Touati</div>
<div class="meta-line">First: 2026-02-23T09:22:21+00:00 · Latest: 2026-02-23T09:22:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于跳跃世界模型的组合理式规划</div>
<div class="mono" style="margin-top:8px">时间抽象的规划能力是智能决策的核心。我们研究的是能够将预训练策略作为时间扩展动作进行组合的代理，从而解决单一策略无法解决的复杂任务。然而，由于长期预测中的累积误差使得通过策略序列化来估计访问分布变得困难，使得组合理式规划难以实现。受arXiv:2206.08736中引入的几何策略组合框架的启发，我们通过学习多步动力学的预测模型——所谓的跳跃世界模型——以离策略方式捕捉预训练策略在多个时间尺度上引起的状态占位，来应对这些挑战。基于arXiv:2503.09817中的时间差分流，我们通过引入一种新颖的一致性目标来增强这些模型，该目标在不同时间尺度上对齐预测，从而提高长期预测的准确性。我们进一步展示了如何结合这些生成的预测来估计执行任意策略序列在不同时间尺度上的价值。实验中，我们发现，使用跳跃世界模型进行组合理式规划在一系列复杂的操作和导航任务上显著提高了零样本性能，平均而言，在长期任务上相对于使用原始动作进行规划的性能提高了200%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable agents to plan using temporal abstractions by composing pre-trained policies, which is crucial for solving complex tasks. To address the challenge of compounding errors in long-horizon predictions, the study introduces jumpy world models that learn multi-step dynamics and align predictions across timescales. The method significantly improves zero-shot performance on manipulation and navigation tasks, achieving a 200% relative improvement over planning with primitive actions on long-horizon tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过组合预训练策略来使代理使用时间抽象进行规划，这对于解决复杂任务至关重要。方法包括学习跳跃世界模型来预测多步动态，并在不同时间尺度上对预测进行对齐，从而提高长期预测准确性。关键实验发现是，使用跳跃世界模型进行组合规划显著提高了在操作和导航任务上的零样本性能，相对于使用原始动作进行长期任务规划，平均提高了200%的相对性能。</div>
</details>
</div>
<div class="card">
<div class="title">ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training</div>
<div class="meta-line">Authors: Rushuai Yang, Hecheng Wang, Chiming Liu, Xiaohan Yan, Yunlong Wang, Xuan Du, Shuoyu Yue, Yongcheng Liu, Chuheng Zhang, Lizhe Qi, Yi Chen, Wei Shan, Maoqing Yao</div>
<div class="meta-line">First: 2026-02-13T07:46:37+00:00 · Latest: 2026-02-23T08:56:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12691v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12691v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALOE：视觉-语言-动作模型后训练的行动级离策略评估</div>
<div class="mono" style="margin-top:8px">我们研究如何通过在线强化学习（RL）提高现实世界中的大型基础视觉-语言-动作（VLA）系统。这一过程的核心是价值函数，它为从经验中引导VLA学习提供学习信号。实践中，价值函数是从来自不同数据源的轨迹片段中估计出来的，包括历史策略和间歇的人工干预。从混合数据中估计当前行为质量的价值函数本质上是一个离策略评估问题。然而，先前的工作通常采用保守的策略内估计方法以确保稳定性，这避免了直接评估当前高容量策略，并限制了学习效果。在本文中，我们提出了ALOE，一种针对VLA后训练的行动级离策略评估框架。ALOE 使用基于片段的时间差自助法来评估单个行动序列，而不是预测最终任务结果。这种设计在稀疏奖励下提高了对关键行动片段的有效归因，并支持稳定策略改进。我们在三个实际操作任务上评估了该方法，包括智能手机包装（高精度任务）、洗衣折叠（长时程可变形物体任务）和双臂拾取放置（涉及多对象感知）。在所有任务中，ALOE 提高了学习效率，同时没有牺牲执行速度，表明离策略RL可以以可靠的方式重新引入现实世界中的VLA后训练。有关视频和额外材料请参见我们的项目网站。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance large vision-language-action (VLA) systems through online reinforcement learning in real-world settings by addressing the off-policy evaluation problem. ALOE, an action-level off-policy evaluation framework, uses chunking-based temporal-difference bootstrapping to evaluate individual action sequences, improving credit assignment and supporting stable policy improvement. Experiments on three real-world tasks demonstrate that ALOE enhances learning efficiency without affecting execution speed, enabling reliable off-policy RL for VLA post-training.</div>
<div class="mono" style="margin-top:8px">研究旨在通过在线强化学习提升大型视觉-语言-行动系统在真实环境中的表现，重点解决离策评估问题。提出了ALOE，一种基于片段的时间差递推的行动级离策评估框架，以改进信用分配并支持稳定的策略改进。在智能手机打包、洗衣折叠和双臂取放等任务上的实验表明，ALOE 提高了学习效率而不影响执行速度，使离策RL能够可靠地用于真实世界的VLA后训练。</div>
</details>
</div>
<div class="card">
<div class="title">Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight</div>
<div class="meta-line">Authors: Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng</div>
<div class="meta-line">First: 2025-11-20T09:30:23+00:00 · Latest: 2026-02-23T08:44:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16175v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16175v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>螳螂：一种具有解耦视觉前瞻性的多功能视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">近期视觉-语言-动作（VLA）模型的发展表明，视觉信号可以有效补充稀疏的动作监督。然而，让VLA直接预测高维视觉状态会分散模型能力并导致高昂的训练成本，而将视觉状态压缩为更紧凑的监督信号则不可避免地会带来信息瓶颈。此外，现有方法往往由于忽视语言监督而表现出较差的理解和推理能力。本文介绍了一种名为螳螂的新框架，该框架具有解耦视觉前瞻（DVF）以解决这些问题。具体而言，螳螂通过元查询和扩散变换器（DiT）头部的结合，将视觉前瞻预测与主干分离。通过残差连接将当前视觉状态提供给DiT，简单的下一个状态预测目标使元查询能够自动捕捉描述视觉轨迹的潜在动作，从而增强显式动作的学习。解耦减轻了VLA主干的负担，使其能够通过语言监督保持理解和推理能力。实验上，预训练于人类操作视频、机器人演示和图像-文本对，经过微调后，螳螂在LIBERO基准测试中达到了96.7%的成功率，超越了强大的基线模型，同时表现出较高的收敛速度。实际世界评估表明，螳螂在指令遵循能力、对未见过的指令的泛化能力和推理能力方面优于开源VLA模型π_{0.5}。代码和权重已发布以支持开源社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mantis is a novel Vision-Language-Action model that addresses the limitations of existing methods by introducing Disentangled Visual Foresight (DVF). It decouples visual foresight prediction from the backbone using meta queries and a diffusion Transformer (DiT) head, enabling the model to learn explicit actions more effectively. Empirically, Mantis achieves a 96.7% success rate on the LIBERO benchmark after fine-tuning, outperforming powerful baselines and demonstrating high convergence speed. In real-world evaluations, Mantis surpasses π_{0.5} in instruction-following, generalization to unseen instructions, and reasoning ability.</div>
<div class="mono" style="margin-top:8px">Mantis 是一种新颖的视觉-语言-动作模型，旨在解决使用视觉信号预测高维视觉状态和忽视语言监督的问题。它引入了解耦视觉前瞻预测的 Disentangled Visual Foresight (DVF)，通过元查询和扩散 Transformer (DiT) 头来实现。Mantis 细分后在 LIBERO 基准测试中微调后达到 96.7% 的成功率，超越了强大的基线模型，并展示了快速收敛速度。在实际世界评估中，Mantis 在指令跟随、对未见过的指令的泛化能力和推理能力方面优于领先的开源 VLA 模型 π_{0.5}。</div>
</details>
</div>
<div class="card">
<div class="title">Switching Among Feedback-Linearizing Output Sets (Melds): Dwell-Time and Compatibility Guarantees</div>
<div class="meta-line">Authors: Mirko Mizzoni, Pieter van Goor, Barbara Bazzana, Antonio Franchi</div>
<div class="meta-line">First: 2025-10-20T11:35:01+00:00 · Latest: 2026-02-23T08:30:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17448v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17448v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study switching among multiple square selections of output functions (melds) drawn from a deck of candidate outputs for nonlinear systems that are static feedback linearizable via outputs. Fixing an operating point, each meld induces a distinct feedback-linearizing coordinate chart defined on a common neighborhood. Switching between melds therefore produces state-dependent coordinate mismatches that are not captured by classical switched-system analyses. We quantify this effect through Lipschitz bounds on the cross-chart maps over a compact safe set and introduce a reference-compatibility constant that measures mismatch among reference families across melds. We derive an explicit dwell-time condition depending on controller decay rates and the compatibility constant, that guarantees exponential decay of the active-output tracking errors between switches, seamless tracking of outputs shared by consecutive melds, and uniform boundedness of the state error within the safe set. A planar 3R manipulator illustrates the results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过输出集（Melds）切换的反馈线性化输出集切换：停留时间和兼容性保证</div>
<div class="mono" style="margin-top:8px">我们研究了非线性系统在通过输出进行静态反馈线性化时，从候选输出的牌组中选择多个正方形输出函数（melds）之间的切换。固定一个操作点，每个meld诱导出一个在公共邻域上定义的不同反馈线性化坐标图。meld之间的切换因此会产生状态依赖的坐标不匹配，这在经典的切换系统分析中并未捕捉到。我们通过在紧凑的安全集上对跨图映射的Lipschitz界来量化这种效应，并引入了一个参考兼容性常数，该常数衡量meld之间参考族之间的不匹配程度。我们推导出一个显式的停留时间条件，该条件依赖于控制器衰减率和兼容性常数，以保证在切换之间活动输出跟踪误差的指数衰减、连续meld共享输出的无缝跟踪以及状态误差在安全集内的均匀有界。一个平面3R机械臂说明了这些结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates switching among multiple output selections (melds) for nonlinear systems that are feedback linearizable via outputs. It introduces a reference-compatibility constant to measure mismatches among reference families across melds and derives a dwell-time condition that ensures exponential decay of tracking errors and seamless tracking of shared outputs. A 3R manipulator is used to illustrate these results.</div>
<div class="mono" style="margin-top:8px">本文研究了非线性系统通过输出反馈线性化时，切换多个输出选择（melds）的情况。通过Lipschitz界量化了状态相关的坐标不匹配，并引入了参考兼容性常数。研究推导出一个停留时间条件，确保切换时跟踪误差的指数衰减、共享输出的无缝跟踪，并保持状态误差在安全集内的有界性。一个3R机械臂示例说明了这些结果。</div>
</details>
</div>
<div class="card">
<div class="title">Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement</div>
<div class="meta-line">Authors: Minku Kim, Kuan-Chia Chen, Aayam Shrestha, Li Fuxin, Stefan Lee, Alan Fern</div>
<div class="meta-line">First: 2026-02-14T19:11:02+00:00 · Latest: 2026-02-23T07:14:01+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures, Project page: https://osudrl.github.io/Humanoid_Hanoi/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13850v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.13850v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://osudrl.github.io/Humanoid_Hanoi/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce Humanoid Hanoi, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines. Project page: https://osudrl.github.io/Humanoid_Hanoi/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人哈诺伊塔：基于技能的整体身体控制研究</div>
<div class="mono" style="margin-top:8px">我们研究了一种基于技能的框架，用于类人机器人盒子重排，通过在任务级别按顺序使用可重用技能实现长期执行。在我们的架构中，所有技能通过一个共享的任务无关的整体身体控制器（WBC）执行，提供了一致的闭环接口用于技能组合，与使用每个技能单独低级控制器的非共享设计不同。我们发现，简单地重复使用相同的预训练WBC在长期执行中会降低鲁棒性，因为新技能及其组合会诱导状态和命令分布的变化。我们通过一个简单的数据聚合程序解决了这个问题，该程序通过在域随机化下闭环技能执行的回放来增强共享-WBC的训练。为了评估该方法，我们引入了类人哈诺伊塔，一个长期目标的塔式重排基准，我们在模拟和Digit V3类人机器人上报告了结果，展示了在长时间范围内的完全自主重排，并量化了共享-WBC方法相对于非共享基线的优势。项目页面：https://osudrl.github.io/Humanoid_Hanoi/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores a skill-based framework for humanoid box rearrangement, utilizing a shared whole-body controller (WBC) to enable long-term execution by sequencing reusable skills. The approach contrasts with non-shared designs that use separate low-level controllers for each skill. The research addresses the reduced robustness of the shared WBC over long horizons by introducing a data aggregation procedure. The study evaluates the framework through the Humanoid Hanoi benchmark, demonstrating autonomous rearrangement over extended horizons and highlighting the benefits of the shared-WBC approach compared to non-shared baselines on the Digit V3 humanoid robot.</div>
<div class="mono" style="margin-top:8px">该论文探讨了一种基于技能的方法，通过使用共享的整体身体控制器（WBC）来实现长时间执行，并通过序列化可重用的技能来完成木箱重新排列。作者发现，重新使用相同的预训练WBC会随着时间的推移降低鲁棒性，因为状态和命令分布发生了变化。他们通过在域随机化下进行闭环技能执行的数据来解决这一问题。该方法通过Humanoid Hanoi基准进行评估，展示了长时间自主重新排列的成功，并强调了共享-WBC方法相较于非共享基线的优势。</div>
</details>
</div>
<div class="card">
<div class="title">TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models</div>
<div class="meta-line">Authors: Hokyun Im, Euijin Jeong, Andrey Kolobov, Jianlong Fu, Youngwoon Lee</div>
<div class="meta-line">Venue: ICLR 2026 Poster</div>
<div class="meta-line">First: 2025-11-07T14:37:07+00:00 · Latest: 2026-02-23T04:57:06+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026 (Poster). Project webpage : https://jellyho.github.io/TwinVLA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05275v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05275v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jellyho.github.io/TwinVLA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language-action models (VLAs) trained on large-scale robotic datasets have demonstrated strong performance on manipulation tasks, including bimanual tasks. However, because most public datasets focus on single-arm demonstrations, adapting VLAs for bimanual tasks typically requires substantial additional bimanual data and fine-tuning. To address this challenge, we introduce TwinVLA, a modular framework that composes two copies of a pretrained single-arm VLA into a coordinated bimanual VLA. Unlike monolithic cross-embodiment models trained on mixtures of single-arm and bimanual data, TwinVLA improves both data efficiency and performance by composing pretrained single-arm policies. Across diverse bimanual tasks in real-world and simulation settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model without requiring any bimanual pretraining. Furthermore, it narrows the gap to state-of-the-art model $π_0$, which relies on extensive proprietary bimanual data and compute cost. These results establish our modular composition approach as a data-efficient and scalable path toward high-performance bimanual manipulation, leveraging public single-arm data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">TwinVLA is a modular framework that combines two pretrained single-arm vision-language-action models to perform bimanual manipulation tasks efficiently. It outperforms a comparably-sized monolithic model without requiring bimanual pretraining data, demonstrating data efficiency and scalability for high-performance bimanual manipulation.</div>
<div class="mono" style="margin-top:8px">TwinVLA 是一个模块化框架，使用两个预训练的单臂视觉-语言-行动模型来执行双臂操作任务，比单一模型更高效。它不需要额外的双臂数据或微调，并在现实世界和模拟环境中多种双臂任务上优于一个同等规模的单一模型。TwinVLA 还缩小了与依赖大量专有数据和高计算成本的最新模型之间的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization</div>
<div class="meta-line">Authors: Yanting Yang, Shenyuan Gao, Qingwen Bu, Li Chen, Dimitris N. Metaxas</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-22T22:53:16+00:00 · Latest: 2026-02-22T22:53:16+00:00</div>
<div class="meta-line">Comments: ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19372v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19372v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving complex, long-horizon robotic manipulation tasks requires a deep understanding of physical interactions, reasoning about their long-term consequences, and precise high-level planning. Vision-Language Models (VLMs) offer a general perceive-reason-act framework for this goal. However, previous approaches using reflective planning to guide VLMs in correcting actions encounter significant limitations. These methods rely on inefficient and often inaccurate implicit learning of state-values from noisy foresight predictions, evaluate only a single greedy future, and suffer from substantial inference latency. To address these limitations, we propose a novel test-time computation framework that decouples state evaluation from action generation. This provides a more direct and fine-grained supervisory signal for robust decision-making. Our method explicitly models the advantage of an action plan, quantified by its reduction in distance to the goal, and uses a scalable critic to estimate. To address the stochastic nature of single-trajectory evaluation, we employ beam search to explore multiple future paths and aggregate them during decoding to model their expected long-term returns, leading to more robust action generation. Additionally, we introduce a lightweight, confidence-based trigger that allows for early exit when direct predictions are reliable, invoking reflection only when necessary. Extensive experiments on diverse, unseen multi-stage robotic manipulation tasks demonstrate a 24.6% improvement in success rate over state-of-the-art baselines, while significantly reducing inference time by 56.5%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>看得更远，决策更明智：基于价值引导的多路径反射用于VLM策略优化</div>
<div class="mono" style="margin-top:8px">解决复杂的长期机器人操作任务需要深刻理解物理交互，推理其长期后果，并进行精确的高层规划。视觉语言模型（VLMs）提供了一种通用的感知-推理-行动框架来实现这一目标。然而，之前使用反思规划来引导VLMs纠正行动的方法遇到了显著的局限性。这些方法依赖于从嘈杂的前瞻性预测中不高效且往往不准确的隐式学习状态值，仅评估单一贪婪的未来，并遭受显著的推理延迟。为了解决这些局限性，我们提出了一种新颖的测试时计算框架，将状态评估与动作生成解耦。这为稳健决策提供了更直接和精细的监督信号。我们的方法明确地建模了行动方案的优势，通过其对目标距离的减少来量化，并使用可扩展的评论者来估计。为了解决单轨迹评估的随机性，我们采用束搜索探索多个未来路径，并在解码过程中将它们聚合起来以建模其预期的长期回报，从而实现更稳健的动作生成。此外，我们引入了一种轻量级、基于置信度的触发器，允许在直接预测可靠时提前退出，仅在必要时触发反思。在多种未见过的多阶段机器人操作任务上的广泛实验表明，与最先进的基线相比，成功率提高了24.6%，同时推理时间减少了56.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of previous approaches using reflective planning to guide Vision-Language Models (VLMs) in robotic manipulation tasks. It proposes a novel test-time computation framework that decouples state evaluation from action generation, using a scalable critic to estimate the advantage of action plans. The method employs beam search to explore multiple future paths and aggregate their expected long-term returns, and introduces a lightweight trigger for early reflection. Experiments show a 24.6% improvement in success rate and a 56.5% reduction in inference time compared to state-of-the-art baselines.</div>
<div class="mono" style="margin-top:8px">本文解决了之前使用反射规划来指导视觉-语言模型（VLMs）在机器人操作任务中的局限性。它提出了一种新的框架，将状态评估与动作生成解耦，使用可扩展的评论家来估计动作计划的优势，并采用束搜索来探索多个未来路径。实验结果显示，与最先进的方法相比，成功率提高了24.6%，推理时间减少了56.5%。</div>
</details>
</div>
<div class="card">
<div class="title">MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations</div>
<div class="meta-line">Authors: Nilay Yilmaz, Maitreya Patel, Naga Sai Abhiram Kusumba, Yixuan He, Yezhou Yang</div>
<div class="meta-line">First: 2026-02-22T22:05:11+00:00 · Latest: 2026-02-22T22:05:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19357v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. To explore whether state-of-the-art Vision-Language Models (VLMs) exhibit this ability, we develop MentalBlackboard, an open-ended spatial visualization benchmark for Paper Folding and Hole Punching tests within two core tasks: prediction and planning. Our prediction experiments reveal that models struggle with applying symmetrical transformations, even when they predict the sequence of unfolding steps correctly. Also, rotations introduce a significant challenge to the physical situational awareness for models. The planning task reveals limitations of models in analyzing symmetrical relationships and in implementing the multi-stage symmetry process, with Claude Opus 4.1 achieving the highest planning score at an accuracy of 10\%. The top-performing model, o3, attains a peak performance of 71.6\% on the generalization task, which does not require spatial visualization but transfers spatial data; however, it achieves only 25\% accuracy on text-based prediction tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MentalBlackboard：通过数学变换评估空间可视化能力</div>
<div class="mono" style="margin-top:8px">空间可视化是指在心中想象、变换和操作物体及其动作的空间特征的能力。这种智能是人类认知的一部分，其中行动和感知在心理层面相互连接。为了探索最先进的视觉-语言模型（VLMs）是否具备这种能力，我们开发了MentalBlackboard，这是一个开放性的空间可视化基准，用于纸张折叠和打孔测试，包含两个核心任务：预测和规划。我们的预测实验表明，模型在应用对称变换时存在困难，即使它们正确预测了展开步骤的顺序。此外，旋转对模型的物理情境意识提出了重大挑战。规划任务揭示了模型在分析对称关系和实施多阶段对称过程方面的局限性，Claude Opus 4.1在规划任务中得分最高，准确率为10%。表现最佳的模型o3在泛化任务中达到71.6%的峰值性能，该任务不需要空间可视化，但涉及空间数据的转移；然而，在基于文本的预测任务中，它仅达到25%的准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to evaluate whether state-of-the-art Vision-Language Models (VLMs) possess the ability of spatial visualization through MentalBlackboard, a benchmark for Paper Folding and Hole Punching tasks. The experiments show that models have difficulty with symmetrical transformations and rotations, indicating limitations in physical situational awareness. The top-performing model, o3, achieves 71.6% accuracy on a generalization task but only 25% on text-based prediction tasks requiring spatial visualization.</div>
<div class="mono" style="margin-top:8px">研究旨在通过MentalBlackboard基准，评估最先进的视觉-语言模型（VLMs）是否具备空间可视化的能力，该基准包含纸折叠和孔穿刺任务。实验表明，模型在对称变换和旋转方面存在困难，显示出物理情境意识的局限性。表现最佳的模型o3在一般化任务上达到71.6%的准确率，但在需要空间可视化能力的文本预测任务上仅达到25%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Design and Control of Modular Magnetic Millirobots for Multimodal Locomotion and Shape Reconfiguration</div>
<div class="meta-line">Authors: Erik Garcia Oyono, Jialin Lin, Dandan Zhang</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-22T21:25:05+00:00 · Latest: 2026-02-22T21:25:05+00:00</div>
<div class="meta-line">Comments: Accepted by 2026 ICRA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19346v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modular small-scale robots offer the potential for on-demand assembly and disassembly, enabling task-specific adaptation in dynamic and constrained environments. However, existing modular magnetic platforms often depend on workspace collisions for reconfiguration, employ bulky three-dimensional electromagnetic systems, and lack robust single-module control, which limits their applicability in biomedical settings. In this work, we present a modular magnetic millirobotic platform comprising three cube-shaped modules with embedded permanent magnets, each designed for a distinct functional role: a free module that supports self-assembly and reconfiguration, a fixed module that enables flip-and-walk locomotion, and a gripper module for cargo manipulation. Locomotion and reconfiguration are actuated by programmable combinations of time-varying two-dimensional uniform and gradient magnetic field inputs. Experiments demonstrate closed-loop navigation using real-time vision feedback and A* path planning, establishing robust single-module control capabilities. Beyond locomotion, the system achieves self-assembly, multimodal transformations, and disassembly at low field strengths. Chain-to-gripper transformations succeeded in 90% of trials, while chain-to-square transformations were less consistent, underscoring the role of module geometry in reconfiguration reliability. These results establish a versatile modular robotic platform capable of multimodal behavior and robust control, suggesting a promising pathway toward scalable and adaptive task execution in confined environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模块化磁性微机器人设计与控制以实现多模式运动和形状重构</div>
<div class="mono" style="margin-top:8px">模块化的小型机器人具有按需组装和拆卸的潜力，能够在动态和受限环境中实现任务特定的适应。然而，现有的模块化磁性平台通常依赖于工作空间碰撞进行重构，采用体积较大的三维电磁系统，并缺乏单模块的稳健控制，这限制了它们在生物医学环境中的应用。在本研究中，我们提出了一种由三个立方体模块组成的小型模块化磁性微机器人平台，每个模块都嵌入了永久磁铁，并设计了不同的功能角色：一个自由模块支持自我组装和重构，一个固定模块实现翻转行走运动，一个夹持模块用于货物操作。运动和重构通过编程的二维均匀和梯度磁场输入实现。实验表明，该系统能够使用实时视觉反馈和A*路径规划实现闭环导航，并建立了单模块控制能力。除了运动之外，该系统在低磁场强度下实现了自我组装、多模式转换和拆卸。90%的链到夹持器转换成功，而链到正方形的转换则不够一致，突显了模块几何形状在重构可靠性中的作用。这些结果建立了一个多功能的模块化机器人平台，能够实现多模式行为和稳健控制，表明了一条在受限环境中实现可扩展和自适应任务执行的有希望的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces a modular magnetic millirobotic platform designed for on-demand assembly and reconfiguration, featuring three cube-shaped modules with distinct functions. The robots are actuated by programmable magnetic fields, enabling closed-loop navigation and robust single-module control. Experiments show successful self-assembly, multimodal transformations, and disassembly, with 90% success rate in chain-to-gripper transformations and less consistent chain-to-square transformations, highlighting the importance of module geometry in reconfiguration reliability.</div>
<div class="mono" style="margin-top:8px">该研究介绍了一种模块化磁性毫米级机器人平台，旨在实现按需组装和拆卸，以适应受限环境中的特定任务。该平台由具有不同功能的三个立方体模块组成，通过可编程的磁场输入实现移动和重构。关键实验结果包括单模块控制的鲁棒性、成功实现自我组装和多模态转换，以及链状结构到夹爪的转换成功率高达90%，突显了该平台的多功能性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics</div>
<div class="meta-line">Authors: Shirui Chen, Cole Harrison, Ying-Chun Lee, Angela Jin Yang, Zhongzheng Ren, Lillian J. Ratliff, Jiafei Duan, Dieter Fox, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-22T19:25:48+00:00 · Latest: 2026-02-22T19:25:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19313v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19313v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM&#x27;s internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TOPReward: 代币概率作为隐藏的零样本奖励用于机器人技术</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作（VLA）模型在预训练方面取得了快速进展，但在强化学习（RL）中的进步仍受到现实世界设置中低样本效率和稀疏奖励的阻碍。开发可泛化的过程奖励模型对于提供必要的细粒度反馈以弥合这一差距至关重要，但现有的时序价值函数往往无法泛化到其训练领域之外。我们引入了TOPReward，这是一种新颖的概率基础时序价值函数，它利用预训练视频视觉-语言模型（VLM）的潜在世界知识来估计机器人任务进度。与之前的方法直接提示VLM输出进度值不同，TOPReward直接从VLM的内部代币概率中提取任务进度，避免了数值表示错误。在Qwen3-VL上的零样本评估中，TOPReward在130多个不同的真实世界任务和多个机器人平台上（例如Franka、YAM、SO-100/101）实现了0.947的平均价值顺序相关性（VOC），显著优于在相同开源模型上达到接近零相关性的最新GVL基线。我们进一步证明，TOPReward作为一种多功能工具，适用于下游应用，包括成功检测和奖励对齐的行为克隆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of low sample efficiency and sparse rewards in robotics reinforcement learning by introducing TOPReward, a novel temporal value function that uses token probabilities from pretrained Vision-Language Models to estimate task progress. TOPReward outperforms existing methods, achieving a mean Value-Order Correlation of 0.947 across 130+ real-world tasks, significantly better than the state-of-the-art GVL baseline. It also shows versatility in applications like success detection and reward-aligned behavior cloning.</div>
<div class="mono" style="margin-top:8px">研究通过引入TOPReward，一种利用预训练视频视觉语言模型的潜在知识来概率性地估计任务进度的新时序价值函数，解决了机器人强化学习中样本效率低和奖励稀疏的问题。TOPReward在多种真实世界任务和机器人平台上的零样本评估中实现了0.947的平均值-顺序相关性，显著优于最先进的GVL基线。</div>
</details>
</div>
<div class="card">
<div class="title">3D Shape Control of Extensible Multi-Section Soft Continuum Robots via Visual Servoing</div>
<div class="meta-line">Authors: Abhinav Gandhi, Shou-Shan Chiang, Cagdas D. Onal, Berk Calli</div>
<div class="meta-line">First: 2026-02-22T17:06:16+00:00 · Latest: 2026-02-22T17:06:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19273v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19273v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a novel vision-based control algorithm for regulating the whole body shape of extensible multisection soft continuum manipulators. Contrary to existing vision-based control algorithms in the literature that regulate the robot&#x27;s end effector pose, our proposed control algorithm regulates the robot&#x27;s whole body configuration, enabling us to leverage its kinematic redundancy. Additionally, our model-based 2.5D shape visual servoing provides globally stable asymptotic convergence in the robot&#x27;s 3D workspace compared to the closest works in the literature that report local minima. Unlike existing visual servoing algorithms in the literature, our approach does not require information from proprioceptive sensors, making it suitable for continuum manipulators without such capabilities. Instead, robot state is estimated from images acquired by an external camera that observes the robot&#x27;s whole body shape and is also utilized to close the shape control loop. Traditionally, visual servoing schemes require an image of the robot at its reference pose to generate the reference features. In this work, we utilize an inverse kinematics solver to generate reference features for the desired robot configuration and do not require images of the robot at the reference. Experiments are performed on a multisection continuum manipulator demonstrating the controller&#x27;s capability to regulate the robot&#x27;s whole body shape while precisely positioning the robot&#x27;s end effector. Results validate our controller&#x27;s ability to regulate the shape of continuum robots while demonstrating a smooth transient response and a steady-state error within 1 mm. Proof-of-concept object manipulation experiments including stacking, pouring, and pulling tasks are performed to demonstrate our controller&#x27;s applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展多节软连续机器人基于视觉伺服的三维形状控制</div>
<div class="mono" style="margin-top:8px">在本文中，我们提出了一种基于视觉的新型控制算法，用于调节可扩展多节软连续操作器的整体身体形状。与文献中现有的基于视觉的控制算法调节机器人末端执行器姿态不同，我们提出的控制算法调节机器人的整体身体构型，使我们能够利用其运动学冗余度。此外，基于模型的2.5维形状视觉伺服在机器人的3D工作空间中提供了全局稳定的渐近收敛，而文献中最近的工作仅报告了局部极小值。与文献中现有的视觉伺服算法不同，我们的方法不需要来自本体感受传感器的信息，使其适用于没有此类功能的连续操作器。相反，机器人状态是从外部相机拍摄的机器人整体形状图像中估计出来的，并且也被用于关闭形状控制回路。传统上，视觉伺服方案需要机器人参考姿态的一张图像来生成参考特征。在本文中，我们利用逆运动学求解器为期望的机器人构型生成参考特征，并不需要机器人在参考姿态下的图像。在多节连续操作器上进行了实验，展示了控制器调节机器人整体身体形状并精确定位机器人末端执行器的能力。结果验证了控制器调节连续机器人形状的能力，同时展示了平滑的瞬态响应和稳态误差在1毫米以内。进行了概念验证的物体操作实验，包括堆叠、倾倒和拉取任务，以展示控制器的应用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a vision-based control algorithm for regulating the entire shape of extensible multi-section soft continuum robots, which differs from existing methods that focus on the end effector pose. The proposed 2.5D shape visual servoing algorithm ensures global stability and asymptotic convergence in the robot&#x27;s 3D workspace. Experiments on a multi-section continuum manipulator show that the controller can precisely position the end effector while regulating the robot&#x27;s shape, with a steady-state error within 1 mm. Additionally, the approach does not require proprioceptive sensors, making it suitable for robots lacking such sensors.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于视觉的控制算法，用于调节可伸展多节软连续机器人的整体形状，不同于现有方法主要关注末端执行器的姿态。所提出的2.5D形状视觉伺服算法在机器人的3D工作空间中实现了全局稳定的渐近收敛。实验结果表明，该控制器可以精确地定位末端执行器，并且在稳态时的误差在1毫米以内。此外，该控制器还可以执行堆叠、倒水和拉取等物体操作任务，具有平滑的瞬态响应。</div>
</details>
</div>
<div class="card">
<div class="title">The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption</div>
<div class="meta-line">Authors: Timothy Duggan, Pierrick Lorang, Hong Lu, Matthias Scheutz</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-22T16:22:06+00:00 · Latest: 2026-02-22T16:22:06+00:00</div>
<div class="meta-line">Comments: Accepted at the 2026 IEEE International Conference on Robotics &amp; Automation (ICRA 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19260v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://price-is-not-right.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently been proposed as a pathway toward generalist robotic policies capable of interpreting natural language and visual inputs to generate manipulation actions. However, their effectiveness and efficiency on structured, long-horizon manipulation tasks remain unclear. In this work, we present a head-to-head empirical comparison between a fine-tuned open-weight VLA model π0 and a neuro-symbolic architecture that combines PDDL-based symbolic planning with learned low-level control. We evaluate both approaches on structured variants of the Towers of Hanoi manipulation task in simulation while measuring both task performance and energy consumption during training and execution. On the 3-block task, the neuro-symbolic model achieves 95% success compared to 34% for the best-performing VLA. The neuro-symbolic model also generalizes to an unseen 4-block variant (78% success), whereas both VLAs fail to complete the task. During training, VLA fine-tuning consumes nearly two orders of magnitude more energy than the neuro-symbolic approach. These results highlight important trade-offs between end-to-end foundation-model approaches and structured reasoning architectures for long-horizon robotic manipulation, emphasizing the role of explicit symbolic structure in improving reliability, data efficiency, and energy efficiency. Code and models are available at https://price-is-not-right.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>价格不是一切：神经符号方法在具有显著较低能耗的结构化长时操纵任务中优于VLA</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型最近被提出作为通才机器人策略的途径，能够解释自然语言和视觉输入以生成操纵动作。然而，它们在结构化、长时操纵任务上的有效性与效率尚不明确。在本研究中，我们对微调的开放权重VLA模型π0和结合PDDL基于的符号规划与学习到的低级控制的神经符号架构进行了直接对比。我们在模拟中评估了两种方法在结构化的汉诺塔操纵任务上的表现，并测量了训练和执行期间的任务性能和能耗。在三块任务中，神经符号模型的成功率为95%，而最佳VLA模型的成功率为34%。神经符号模型还能够泛化到未见过的四块变体（成功率为78%），而两种VLA均未能完成任务。在训练期间，VLA微调的能耗比神经符号方法高近两个数量级。这些结果突显了端到端基础模型方法与结构化推理架构之间在长时机器人操纵中的重要权衡，强调了显式符号结构在提高可靠性和数据效率以及能耗效率方面的作用。代码和模型可在https://price-is-not-right.github.io 获取</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study compares a fine-tuned Vision-Language-Action (VLA) model and a neuro-symbolic architecture on structured long-horizon manipulation tasks. The neuro-symbolic model, which combines symbolic planning with learned control, outperforms the VLA model in terms of task success and energy efficiency. On a 3-block task, the neuro-symbolic model achieved 95% success, compared to 34% for the VLA model. The neuro-symbolic model also generalized better to a 4-block variant, while both VLAs failed. During training, the VLA model consumed nearly two orders of magnitude more energy than the neuro-symbolic model, highlighting the importance of explicit symbolic structure for long-horizon robotic manipulation.</div>
<div class="mono" style="margin-top:8px">该研究将细调的Vision-Language-Action (VLA)模型与神经符号架构在结构化的长期操作任务上进行了对比。神经符号模型在任务成功率和泛化能力上优于VLA，分别在3块任务上达到95%的成功率，在4块任务上达到78%的成功率，而VLA无法完成任务。此外，神经符号模型在训练和执行过程中消耗的能量显著较少，突显了显式符号结构在长期机器人操作中的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Controlled Face Manipulation and Synthesis for Data Augmentation</div>
<div class="meta-line">Authors: Joris Kirchner, Amogh Gudi, Marian Bittner, Chirag Raman</div>
<div class="meta-line">First: 2026-02-22T15:03:06+00:00 · Latest: 2026-02-22T15:03:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19219v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning vision models excel with abundant supervision, but many applications face label scarcity and class imbalance. Controllable image editing can augment scarce labeled data, yet edits often introduce artifacts and entangle non-target attributes. We study this in facial expression analysis, targeting Action Unit (AU) manipulation where annotation is costly and AU co-activation drives entanglement. We present a facial manipulation method that operates in the semantic latent space of a pre-trained face generator (Diffusion Autoencoder). Using lightweight linear models, we reduce entanglement of semantic features via (i) dependency-aware conditioning that accounts for AU co-activation, and (ii) orthogonal projection that removes nuisance attribute directions (e.g., glasses), together with an expression neutralization step to enable absolute AU edit. We use these edits to balance AU occurrence by editing labeled faces and to diversify identities/demographics via controlled synthesis. Augmenting AU detector training with the generated data improves accuracy and yields more disentangled predictions with fewer co-activation shortcuts, outperforming alternative data-efficient training strategies and suggesting improvements similar to what would require substantially more labeled data in our learning-curve analysis. Compared to prior methods, our edits are stronger, produce fewer artifacts, and preserve identity better.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>受控面部操控与合成以数据增强</div>
<div class="mono" style="margin-top:8px">深度学习视觉模型在大量监督下表现出色，但许多应用面临标签稀缺和类别不平衡的问题。可控图像编辑可以增强稀缺的标注数据，然而编辑往往引入伪影并纠缠非目标属性。我们研究了这一问题在面部表情分析中的应用，目标是动作单元（AU）操控，其中标注成本高且AU共激活导致纠缠。我们提出了一种在预训练面部生成器（扩散自编码器）的语义潜在空间中操作的面部操控方法。通过使用轻量级线性模型，我们通过（i）依赖感知条件，考虑AU共激活，和（ii）正交投影去除无关属性方向（例如眼镜），以及一个表情中性化步骤，来减少语义特征的纠缠，从而实现绝对AU编辑。我们使用这些编辑来平衡AU出现频率，通过编辑标注的面部图像，并通过受控合成来多样化身份/人口统计特征。将生成的数据用于AU检测器训练，可以提高准确性并产生更少共激活捷径的去纠缠预测，优于其他数据高效训练策略，表明在我们的学习曲线分析中，这类似于需要大量更多标注数据才能达到的效果。与先前方法相比，我们的编辑更强，产生的伪影更少，且更好地保留了身份。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of label scarcity in deep learning vision models by proposing a method for controlled facial expression manipulation. The method operates in the semantic latent space of a pre-trained face generator and uses lightweight linear models to reduce attribute entanglement. Key findings include improved accuracy and more disentangled predictions when augmenting AU detector training with generated data, outperforming other data-efficient strategies. This approach also better preserves identity and produces fewer artifacts compared to previous methods.</div>
<div class="mono" style="margin-top:8px">论文通过提出一种可控面部表情操纵方法来应对深度学习视觉模型中的标签稀缺问题。该方法在预训练的面部生成器的语义潜空间中操作，并使用轻量级线性模型来减少属性纠缠。主要发现包括：通过使用生成的数据增强AU检测器训练，可以提高准确性和更少的共激活捷径，从而获得更解耦的预测，优于其他数据高效策略。此外，这种方法还能更好地保留身份并减少伪影。</div>
</details>
</div>
<div class="card">
<div class="title">An Interpretable Data-Driven Model of the Flight Dynamics of Hawks</div>
<div class="meta-line">Authors: Lydia France, Karl Lapo, J. Nathan Kutz</div>
<div class="meta-line">First: 2026-02-22T13:58:03+00:00 · Latest: 2026-02-22T13:58:03+00:00</div>
<div class="meta-line">Comments: 16 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite significant analysis of bird flight, generative physics models for flight dynamics do not currently exist. Yet the underlying mechanisms responsible for various flight manoeuvres are important for understanding how agile flight can be accomplished. Even in a simple flight, multiple objectives are at play, complicating analysis of the overall flight mechanism. Using the data-driven method of dynamic mode decomposition (DMD) on motion capture recordings of hawks, we show that multiple behavioral states such as flapping, turning, landing, and gliding, can be modeled by simple and interpretable modal structures (i.e. the underlying wing-tail shape) which can be linearly combined to reproduce the experimental flight observations. Moreover, the DMD model can be used to extrapolate naturalistic flapping. Flight is highly individual, with differences in style across the hawks, but we find they share a common set of dynamic modes. The DMD model is a direct fit to data, unlike traditional models constructed from physics principles which can rarely be tested on real data and whose assumptions are typically invalid in real flight. The DMD approach gives a highly accurate reconstruction of the flight dynamics with only three parameters needed to characterize flapping, and a fourth to integrate turning manoeuvres. The DMD analysis further shows that the underlying mechanism of flight, much like simplest walking models, displays a parametric coupling between dominant modes suggesting efficiency for locomotion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鹰的飞行动力学可解释数据驱动模型</div>
<div class="mono" style="margin-top:8px">尽管对鸟类飞行进行了大量分析，但目前尚不存在生成物理模型来描述飞行动力学。然而，各种飞行机动背后的机制对于理解如何实现敏捷飞行至关重要。即使在简单的飞行中，也有多个目标在起作用，这使得对整体飞行机制的分析变得复杂。通过在鹰的运动捕捉记录上使用动态模式分解（DMD）的数据驱动方法，我们展示了多种行为状态，如拍打、转弯、着陆和滑翔，可以通过简单的可解释模态结构（即基础翼尾形状）建模，并且这些模态可以线性组合以重现实验飞行观察。此外，DMD模型可以用于外推自然拍打。飞行高度个体化，不同鹰的风格存在差异，但它们共享一组动态模态。DMD模型直接拟合数据，不同于基于物理原理的传统模型，这些模型很少能在实际数据上进行测试，其假设在实际飞行中通常无效。DMD方法仅需三个参数即可高度准确地重建拍打的飞行动力学，并且需要第四个参数来整合转弯机动。DMD分析进一步表明，飞行的基本机制，就像最简单的行走模型一样，显示了主导模态之间的参数耦合，这表明其在运动中的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to develop a generative physics model for bird flight dynamics, which is currently lacking. By applying dynamic mode decomposition (DMD) to motion capture data of hawks, the researchers identified simple and interpretable modal structures that can model various flight behaviors such as flapping, turning, landing, and gliding. The DMD model accurately reconstructs experimental flight observations with only four parameters, demonstrating a high degree of accuracy and interpretability. The model also reveals a parametric coupling between dominant modes, suggesting efficiency in locomotion. Unlike traditional models based on physics principles, the DMD approach is directly fitted to data and can be tested on real flight scenarios, providing a more accurate representation of flight dynamics.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过动态模式分解（DMD）对猎鹰的飞行动力学进行建模，使用运动捕捉数据。研究结果显示，通过简单的可解释模态结构可以模拟多种行为状态，如拍打、转弯、降落和滑翔，并且这些模态可以线性组合以重现实验飞行观察结果。DMD模型只需要四个参数即可准确重构飞行动力学，并且展示了主导模态之间的参数耦合，表明其在运动中的高效性。与传统模型不同，DMD方法直接拟合数据，可以在真实飞行数据上进行测试，提供了一个更准确且可解释的猎鹰飞行动力学模型。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Prompt Guided Unified Pushing Policy</div>
<div class="meta-line">Authors: Hieu Bui, Ziyan Gao, Yuya Hosoda, Joo-Ho Lee</div>
<div class="meta-line">First: 2026-02-22T13:48:38+00:00 · Latest: 2026-02-22T13:48:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19193v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19193v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As one of the simplest non-prehensile manipulation skills, pushing has been widely studied as an effective means to rearrange objects. Existing approaches, however, typically rely on multi-step push plans composed of pre-defined pushing primitives with limited application scopes, which restrict their efficiency and versatility across different scenarios. In this work, we propose a unified pushing policy that incorporates a lightweight prompting mechanism into a flow matching policy to guide the generation of reactive, multimodal pushing actions. The visual prompt can be specified by a high-level planner, enabling the reuse of the pushing policy across a wide range of planning problems. Experimental results demonstrate that the proposed unified pushing policy not only outperforms existing baselines but also effectively serves as a low-level primitive within a VLM-guided planning framework to solve table-cleaning tasks efficiently.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉提示引导的统一推动物策</div>
<div class="mono" style="margin-top:8px">作为一种最简单的非抓握操作技能，推动物已被广泛研究作为有效手段重新排列物体。现有方法通常依赖于由预定义推动物基础组成的多步推动物计划，这限制了它们在不同场景中的效率和灵活性。在本文中，我们提出了一种统一的推动物策，将轻量级提示机制融入到流匹配策略中，以引导生成反应性和多模态推动物作。视觉提示可以由高级规划者指定，使推动物策在广泛的规划问题中得到重用。实验结果表明，所提出的统一推动物策不仅优于现有基线，而且作为VLM引导规划框架中的低级基础操作，能够高效地解决桌面清洁任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of existing pushing policies by proposing a unified pushing policy that integrates a visual prompting mechanism into a flow matching policy. This approach allows for the generation of reactive and multimodal pushing actions, enhancing the policy&#x27;s efficiency and versatility. The experimental results show that the proposed policy outperforms existing methods and can be effectively used as a low-level primitive within a vision-language model-guided planning framework to solve table-cleaning tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了一种统一的推动物策，将视觉提示机制融入流匹配策略中，以生成反应性和多模态的推动物作，从而提高策略的效率和灵活性。实验结果表明，所提出的方法不仅优于现有方法，还可以作为视觉语言模型引导规划框架中的低级原语来高效解决桌面清理任务。</div>
</details>
</div>
<div class="card">
<div class="title">Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation</div>
<div class="meta-line">Authors: Thanh Nguyen Canh, Thanh-Tuan Tran, Haolan Zhang, Ziyan Gao, Nak Young Chong, Xiem HoangVan</div>
<div class="meta-line">First: 2026-02-22T13:26:27+00:00 · Latest: 2026-02-22T13:26:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19184v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19184v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://thanhnguyencanh.github.io/LfD4hri">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from Demonstration (LfD) offers a promising paradigm for robot skill acquisition. Recent approaches attempt to extract manipulation commands directly from video demonstrations, yet face two critical challenges: (1) general video captioning models prioritize global scene features over task-relevant objects, producing descriptions unsuitable for precise robotic execution, and (2) end-to-end architectures coupling visual understanding with policy learning require extensive paired datasets and struggle to generalize across objects and scenarios. To address these limitations, we propose a novel ``Human-to-Robot&#x27;&#x27; imitation learning pipeline that enables robots to acquire manipulation skills directly from unstructured video demonstrations, inspired by the human ability to learn by watching and imitating. Our key innovation is a modular framework that decouples the learning process into two distinct stages: (1) Video Understanding, which combines Temporal Shift Modules (TSM) with Vision-Language Models (VLMs) to extract actions and identify interacted objects, and (2) Robot Imitation, which employs TD3-based deep reinforcement learning to execute the demonstrated manipulations. We validated our approach in PyBullet simulation environments with a UR5e manipulator and in a real-world experiment with a UF850 manipulator across four fundamental actions: reach, pick, move, and put. For video understanding, our method achieves 89.97% action classification accuracy and BLEU-4 scores of 0.351 on standard objects and 0.265 on novel objects, representing improvements of 76.4% and 128.4% over the best baseline, respectively. For robot manipulation, our framework achieves an average success rate of 87.5% across all actions, with 100% success on reaching tasks and up to 90% on complex pick-and-place operations. The project website is available at https://thanhnguyencanh.github.io/LfD4hri.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类到机器人交互：从视频演示学习机器人模仿</div>
<div class="mono" style="margin-top:8px">演示学习（LfD）为机器人的技能获取提供了有希望的范式。最近的方法试图直接从视频演示中提取操作命令，但面临两个关键挑战：（1）通用视频描述模型优先考虑全局场景特征而非任务相关对象，产生的描述不适合精确的机器人执行；（2）将视觉理解与策略学习耦合的端到端架构需要大量配对数据集，并且难以在不同对象和场景之间泛化。为了解决这些限制，我们提出了一种新颖的“人类到机器人”模仿学习管道，使机器人能够直接从未结构化的视频演示中获取操作技能，灵感来源于人类通过观看和模仿学习的能力。我们的关键创新是一种模块化框架，将学习过程分解为两个不同的阶段：（1）视频理解，结合时空移位模块（TSM）与视觉-语言模型（VLM）以提取动作并识别交互对象；（2）机器人模仿，采用基于TD3的深度强化学习执行演示的操作。我们在PyBullet仿真环境中使用UR5e操作器验证了我们的方法，并在真实世界实验中使用UF850操作器进行了四类基本操作（接近、拾取、移动和放置）的测试。在视频理解方面，我们的方法在标准对象上的动作分类准确率为89.97%，在新型对象上的BLEU-4得分为0.351，0.265，分别比最佳基线提高了76.4%和128.4%。在机器人操作方面，我们的框架在所有操作上的平均成功率达到了87.5%，接近100%的成功率在接近任务上，最高可达90%的复杂拾取和放置操作。项目网站可访问 https://thanhnguyencanh.github.io/LfD4hri/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges in Learning from Demonstration (LfD) for robot manipulation by proposing a novel Human-to-Robot imitation learning pipeline. The method decouples the learning process into Video Understanding and Robot Imitation stages. For Video Understanding, it uses Temporal Shift Modules (TSM) and Vision-Language Models (VLMs) to extract actions and identify interacted objects, achieving high accuracy and BLEU-4 scores. For Robot Imitation, it employs TD3-based deep reinforcement learning. Experiments in simulation and real-world settings show the method&#x27;s effectiveness, with an 89.97% action classification accuracy, 87.5% success rate across all actions, and up to 90% success on complex pick-and-place operations.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新的‘人类到机器人’模仿学习管道，以解决机器人操作中的学习从演示（LfD）挑战。该方法将学习过程分为视频理解和机器人模仿两个阶段。在视频理解阶段，使用时空移位模块和视觉-语言模型来分类动作和识别交互对象，实现了高准确率和BLEU分数。在机器人模仿阶段，使用基于TD3的深度强化学习来执行演示的操纵。该方法在UR5e和UF850操纵器的仿真和真实世界实验中得到了验证，成功率达到87.5%，在复杂任务上的成功率高达90%。</div>
</details>
</div>
<div class="card">
<div class="title">Impact-Robust Posture Optimization for Aerial Manipulation</div>
<div class="meta-line">Authors: Amr Afifi, Ahmad Gazar, Javier Alonso-Mora, Paolo Robuffo Giordano, Antonio Franchi</div>
<div class="meta-line">First: 2026-02-14T13:16:46+00:00 · Latest: 2026-02-22T10:55:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13762v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13762v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot&#x27;s state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot&#x27;s configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot&#x27;s state w.r.t. nominal TSID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>冲击鲁棒姿态优化在空中操作中的应用</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的方法，用于优化具有冗余自由度的力矩控制机器人在冲击期间的姿态，以提高其鲁棒性。使用刚性冲击模型作为配置依赖度量的基础，该度量量化了冲击前后速度的变化。通过寻找使上述度量最小化的配置（姿态），可以显著减少机器人在冲击期间的状态和输入命令的峰值，从而提高安全性和鲁棒性。识别冲击鲁棒姿态的问题被表述为上述度量的最小-最大优化问题。为克服该问题的实时不可行性，我们将该问题重新表述为基于梯度的运动任务，该任务逐步引导机器人向使所提度量最小化的配置移动。该任务嵌入在任务空间逆动力学（TSID）全身控制器中，使其能够与其他控制目标无缝集成。该方法应用于执行重复点接触任务的具有冗余自由度的空中操作器。我们在现实物理模拟器中测试了该方法，并将其与标准TSID进行了比较。与标准TSID相比，该方法使冲击后的机器人配置峰值降低了最多51%，并且成功避免了执行器饱和。此外，我们通过在四足机器人和人形机器人上进行额外的数值模拟，强调了运动学冗余对于冲击鲁棒性的重要性，结果显示与标准TSID相比，机器人状态冲击后的峰值降低了最多45%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a method to optimize the posture of torque-controlled robots to enhance robustness during impacts. By minimizing a configuration-dependent metric that quantifies velocity variations before and after impacts, the method reduces spikes in the robot&#x27;s state and input commands, improving safety. The optimization problem is solved using a gradient-based motion task embedded in a task-space inverse dynamics controller, which is tested on a kinematically redundant aerial manipulator. The results show a significant reduction (up to 51%) in post-impact spikes and avoidance of actuator saturation, with additional simulations on quadruped and humanoid robots further validating the importance of kinematic redundancy for impact robustness.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种优化扭矩控制机器人姿态的方法，以增强其在冲击中的鲁棒性。通过最小化一个基于配置的度量，该度量量化了冲击前后速度的变化，从而减少机器人状态和输入命令的峰值，提高安全性。优化问题被重新表述为基于梯度的运动任务，并嵌入到任务空间逆动力学控制器中，允许实时实现。实验结果表明，峰值减少幅度最高可达51%，并且避免了执行器饱和。此外，对四足机器人和人形机器人的额外模拟进一步突显了运动冗余度对冲击鲁棒性的重要性，与标准控制方法相比，峰值减少幅度最高可达45%。</div>
</details>
</div>
<div class="card">
<div class="title">Design, Locomotion, and Control of Amphibious Robots: Recent Advances</div>
<div class="meta-line">Authors: Yi Jin, Chang Liu, Roger D. Quinn, Robert J. Wood, C. Chase Cao</div>
<div class="meta-line">First: 2026-02-22T07:18:02+00:00 · Latest: 2026-02-22T07:18:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.19077v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.19077v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Amphibious robots, operating seamlessly across land and water, are advancing applications in conservation, disaster response, and defense. Their performance depends on locomotion mechanisms, actuation technologies, and sensor-control integration. This review highlights recent progress in these areas, examining movement strategies, material-based actuators, and control systems for autonomy and adaptability. Challenges and opportunities are outlined to guide future research toward more efficient, resilient, and multifunctional amphibious robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>水陆两用机器人设计、运动与控制：近期进展</div>
<div class="mono" style="margin-top:8px">水陆两用机器人能够在陆地和水域之间无缝操作，其应用正在环保、灾害响应和国防领域不断拓展。其性能取决于运动机制、驱动技术和传感器-控制集成。本文综述了这些领域的最新进展，探讨了运动策略、基于材料的驱动器和自主性与适应性的控制系统。概述了挑战与机遇，以指导未来研究，开发更高效、更坚固和多功能的水陆两用机器人。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review article discusses the advancement of amphibious robots designed to operate on both land and water, focusing on their locomotion mechanisms, actuation technologies, and sensor-control integration. The study examines recent progress in movement strategies, material-based actuators, and control systems to enhance autonomy and adaptability. Key findings highlight the need for more efficient and resilient designs to meet the demands of conservation, disaster response, and defense applications.</div>
<div class="mono" style="margin-top:8px">这篇综述关注能够在陆地和水域之间自由行动的机器人技术进步，这些机器人在保护、灾害响应和国防等领域有着广泛应用。研究探讨了在运动策略、基于材料的驱动技术和传感器-控制集成方面的最新进展。主要发现包括改进的运动策略、基于材料的驱动器和增强自主性和适应性的控制系统。还讨论了未来研究面临的挑战和机遇，以推动更高效、更坚固和多功能的两栖机器人发展。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0337.html">20260223_0337</a>
<a href="archive/20260222_0338.html">20260222_0338</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
