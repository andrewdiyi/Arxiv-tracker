<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-03-01 03:37</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260301_0337</div>
    <div class="row"><div class="card">
<div class="title">DropVLA: An Action-Level Backdoor Attack on Vision--Language--Action Models</div>
<div class="meta-line">Authors: Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang</div>
<div class="meta-line">First: 2025-10-13T02:45:48+00:00 · Latest: 2026-02-26T18:32:27+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 tables, 3 figures. Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10932v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10932v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models map multimodal perception and language instructions to executable robot actions, making them particularly vulnerable to behavioral backdoor manipulation: a hidden trigger introduced during training can induce unintended physical actions while nominal task performance remains intact. Prior work on VLA backdoors primarily studies untargeted attacks or task-level hijacking, leaving fine-grained control over individual actions largely unexplored. In this work, we present DropVLA, an action-level backdoor attack that forces a reusable action primitive (e.g., open_gripper) to execute at attacker-chosen decision points under a realistic pipeline-black-box setting with limited data-poisoning access, using a window-consistent relabeling scheme for chunked fine-tuning. On OpenVLA-7B evaluated with LIBERO, vision-only poisoning achieves 98.67%-99.83% attack success rate (ASR) with only 0.31% poisoned episodes while preserving 98.50%-99.17% clean-task retention, and successfully triggers the targeted action within 25 control steps at 500 Hz (0.05 s). Text-only triggers are unstable at low poisoning budgets, and combining text with vision provides no consistent ASR improvement over vision-only attacks. The backdoor remains robust to moderate trigger variations and transfers across evaluation suites (96.27%, 99.09%), whereas text-only largely fails (0.72%). We further validate physical-world feasibility on a 7-DoF Franka arm with pi0-fast, demonstrating non-trivial attack efficacy under camera-relative motion that induces image-plane trigger drift. These results reveal that VLA models can be covertly steered at the granularity of safety-critical actions with minimal poisoning and without observable degradation of nominal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DropVLA：视觉-语言-行动模型中的行动级后门攻击</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型将多模态感知和语言指令映射为可执行的机器人动作，使其特别容易受到行为后门操纵：在训练期间引入的隐藏触发器可以在不影响名义任务性能的情况下诱导意外的物理动作。先前对VLA后门的研究主要集中在无目标攻击或任务级劫持上，而对个体动作的精细控制尚未得到充分探索。在本文中，我们提出了DropVLA，这是一种行动级后门攻击，它在有限的数据污染访问和现实的管道黑盒设置下，通过窗口一致的重新标记方案进行分块微调，迫使可重用的动作原语（例如，open_gripper）在攻击者选择的决策点执行。在使用LIBERO评估的OpenVLA-7B上，仅通过视觉污染，攻击成功率（ASR）达到98.67%-99.83%，污染的剧集比例仅为0.31%，同时保持98.50%-99.17%的任务清洁保留率，并在25个控制步骤内（500 Hz，0.05秒）成功触发目标动作。仅文本触发在低污染预算下不稳定，结合文本与视觉并不能在视觉污染攻击上提供一致的ASR改进。后门对适度的触发器变化具有鲁棒性，并在评估套件之间转移（96.27%，99.09%），而仅文本则大多失败（0.72%）。我们进一步在7自由度的Franka手臂上使用pi0-fast验证了物理世界的可行性，展示了在相机相对运动下诱导图像平面触发漂移的非平凡攻击效果。这些结果表明，VLA模型可以在最小的污染和无明显名义性能退化的情况下，以安全关键动作的粒度被隐蔽地引导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DropVLA is an action-level backdoor attack on VLA models that forces a specific action primitive to execute at chosen decision points with limited data-poisoning access. Vision-only poisoning achieves high attack success rates while preserving task performance, and the backdoor remains robust to trigger variations and transfers across different evaluation suites. Physical-world validation on a 7-DoF Franka arm confirms the attack&#x27;s efficacy under image-plane trigger drift.</div>
<div class="mono" style="margin-top:8px">DropVLA 是一种针对 VLA 模型的动作级后门攻击，能够在有限的数据污染访问下，使特定的动作原语在选定的决策点执行。仅通过视觉污染即可实现高攻击成功率，同时保持任务性能。该后门对触发器的微小变化具有鲁棒性，并且可以在不同的评估套件之间进行转移。物理世界验证在 7 自由度的 Franka 手臂上证实了攻击的有效性，即使在图像平面触发器漂移的情况下也能实现攻击。</div>
</details>
</div>
<div class="card">
<div class="title">Physics Informed Viscous Value Representations</div>
<div class="meta-line">Authors: Hrishikesh Viswanath, Juanwu Lu, S. Talha Bukhari, Damon Conover, Ziran Wang, Aniket Bera</div>
<div class="meta-line">First: 2026-02-26T17:53:46+00:00 · Latest: 2026-02-26T17:53:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23280v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23280v1">PDF</a> · <a href="https://github.com/HrishikeshVish/phys-fk-value-GCRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理知情的粘性值表示</div>
<div class="mono" style="margin-top:8px">离线目标条件强化学习（GCRL）从静态预先收集的数据集中学习目标条件策略。然而，由于状态-动作空间覆盖有限，准确的价值估计仍然是一个挑战。最近的物理知情方法通过在偏微分方程（PDE）上定义的正则化来对价值函数施加物理和几何约束，例如伊孔诺方程。然而，这些形式在复杂、高维环境中往往难以良好定义。在本文中，我们提出了一种基于哈密尔顿-雅可比-贝尔曼（HJB）方程粘性解的物理知情正则化。通过提供基于物理的归纳偏置，我们的方法将学习过程与最优控制理论联系起来，在价值迭代期间显式地正则化和限制更新。此外，我们利用费曼-卡茨定理将PDE解重新表述为期望，使目标的可计算蒙特卡洛估计避免了高阶梯度中的数值不稳定性。实验表明，我们的方法提高了几何一致性，使其广泛适用于导航和高维、复杂的操作任务。开源代码可在https://github.com/HrishikeshVish/phys-fk-value-GCRL/获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of accurate value estimation in offline goal-conditioned reinforcement learning by proposing a physics-informed regularization based on the viscosity solution of the Hamilton-Jacobi-Bellman equation. The method leverages optimal control theory to provide a physics-based inductive bias, bounding updates during value iterations and avoiding numerical instability. Experiments show that this approach improves geometric consistency, making it suitable for navigation and complex manipulation tasks in high-dimensional environments.</div>
<div class="mono" style="margin-top:8px">该研究通过提出基于Hamilton-Jacobi-Bellman方程粘性解的物理正则化方法，解决了离线目标条件强化学习中准确的价值估计问题。该方法利用Feynman-Kac定理实现可计算的蒙特卡洛估计，避免数值不稳定。实验表明，该方法在导航和高维复杂操作任务中具有更好的几何一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Dyslexify: A Mechanistic Defense Against Typographic Attacks in CLIP</div>
<div class="meta-line">Authors: Lorenz Hufe, Constantin Venhoff, Erblina Purelku, Maximilian Dreyer, Sebastian Lapuschkin, Wojciech Samek</div>
<div class="meta-line">First: 2025-08-28T09:08:30+00:00 · Latest: 2026-02-26T17:33:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20570v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.20570v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model&#x27;s layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce Dyslexify - a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, dyslexify improves performance by up to 22.06% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%, and demonstrate its utility in a medical foundation model for skin lesion diagnosis. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dyslexify：CLIP对抗 typographic 攻击的机制性防御</div>
<div class="mono" style="margin-top:8px">typographic 攻击通过在图像中注入文本来利用多模态系统，导致目标错误分类、恶意内容生成，甚至视觉语言模型的逃逸。在本研究中，我们分析了CLIP视觉编码器在typographic 攻击下的行为，发现模型后半部分层中的专门注意头因果地提取并传递typographic 信息至cls标记。基于这些见解，我们引入了Dyslexify——一种通过选择性地消除typographic 电路（由注意头组成）来防御CLIP模型的对抗方法，无需微调。Dyslexify在ImageNet-100的typographic 变体上提高了高达22.06%的性能，同时将标准ImageNet-100的准确性降低了不到1%，并在皮肤病变诊断的医学基础模型中展示了其效用。值得注意的是，我们的无需训练的方法在当前依赖于微调的最先进的typographic 防御方法中仍具有竞争力。为此，我们发布了一组dyslexic CLIP模型，这些模型在对抗typographic 攻击方面具有显著的鲁棒性。这些模型适合作为广泛的安全关键应用的即插即用替代品，其中基于文本的操纵风险超过了文本识别的实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address typographic attacks on multi-modal systems, particularly on CLIP models, by analyzing how CLIP vision encoders process typographic information. The method, Dyslexify, selectively ablates specific attention heads to defend against these attacks without requiring fine-tuning. The approach improves performance by up to 22.06% on a typographic variant of ImageNet-100 while maintaining standard accuracy and demonstrating utility in medical applications. Dyslexify is competitive with state-of-the-art defenses that rely on fine-tuning and provides robust, drop-in replacements for safety-critical applications.</div>
<div class="mono" style="margin-top:8px">研究旨在通过分析CLIP视觉编码器如何处理字体信息来应对多模态系统中的字体攻击。方法Dyslexify选择性地移除特定的注意力头以防御这些攻击，且无需微调。该方法在ImageNet-100的字体变体上可提高高达22.06%的性能，同时保持标准准确性，并在医疗应用中显示出实用性。Dyslexify与依赖微调的最新防御方法具有竞争力，并提供适用于各种安全关键应用的稳健替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly</div>
<div class="meta-line">Authors: Yijie Guo, Iretiayo Akinola, Lars Johannsmeier, Hugo Hadfield, Abhishek Gupta, Yashraj Narang</div>
<div class="meta-line">First: 2026-02-26T17:26:13+00:00 · Latest: 2026-02-26T17:26:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23253v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23253v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARR：基于模拟的装配策略与异构现实残差</div>
<div class="mono" style="margin-top:8px">机器人装配因其对精确、接触丰富的操作要求而一直是一个长期挑战。尽管基于模拟的学习已经使稳健的装配策略得以发展，但在实际应用中其性能往往会因模拟与现实之间的差距而下降。相反，基于现实世界的强化学习（RL）方法避免了模拟与现实之间的差距，但需要大量的人工监督且缺乏对环境变化的泛化能力。在本研究中，我们提出了一种结合模拟训练的基础策略和现实世界残差策略的混合方法，以高效地适应现实世界的差异。基础策略在模拟中使用低级状态观察和密集奖励进行训练，提供初始行为的强大先验。残差策略在现实世界中使用视觉观察和稀疏奖励进行学习，以补偿动力学和传感器噪声的差异。广泛的现实世界实验表明，我们的方法SPARR在多种两部分装配任务中实现了近乎完美的成功率。与最先进的零样本模拟到现实的方法相比，SPARR将成功率提高了38.4%，并将循环时间减少了29.7%。此外，SPARR不需要人工专业知识，而最先进的基于现实世界的RL方法则高度依赖于人工监督。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robotic assembly by proposing SPARR, a hybrid approach that combines a simulation-trained base policy with a real-world residual policy. The base policy, trained in simulation, provides strong priors for initial behavior, while the residual policy, learned in the real world, compensates for discrepancies in dynamics and sensor noise. Experimental results show that SPARR achieves near-perfect success rates in diverse two-part assembly tasks, improving success rates by 38.4% and reducing cycle time by 29.7% compared to state-of-the-art zero-shot sim-to-real methods.</div>
<div class="mono" style="margin-top:8px">本文提出了一种结合仿真训练基础策略和现实世界残差策略的混合方法SPARR，以应对机器人装配的挑战。基础策略在仿真中使用密集奖励进行训练，提供初始行为的良好先验，而残差策略在现实世界中使用稀疏奖励进行学习，以适应现实世界的差异。实验结果表明，SPARR在各种装配任务中实现了近乎完美的成功率，与最先进的仿真到现实的方法相比，在成功率上提高了38.4%，在循环时间上减少了29.7%，并且不需要人工监督。</div>
</details>
</div>
<div class="card">
<div class="title">InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation</div>
<div class="meta-line">Authors: Jiahao Liu, Cui Wenbo, Haoran Li, Dongbin Zhao</div>
<div class="meta-line">First: 2026-02-26T14:03:58+00:00 · Latest: 2026-02-26T14:03:58+00:00</div>
<div class="meta-line">Comments: 16 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23024v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InCoM：意图驱动的感知与结构化协调在全身移动操作中的应用</div>
<div class="mono" style="margin-top:8px">全身移动操作是通用机器人代理的基本能力，需要协调控制移动基座和操作臂，并在动态变化的视角下保持鲁棒的感知。然而，现有方法面临两个关键挑战：基座和臂部动作之间的强耦合使全身控制优化复杂化，且在移动操作过程中视角变化时感知注意力分配往往不佳。我们提出InCoM，一种意图驱动的感知与结构化协调框架，用于全身移动操作。InCoM推断潜在的运动意图，动态重新加权多尺度感知特征，实现阶段适应的感知注意力分配。为了支持鲁棒的跨模态感知，InCoM进一步引入了一种几何语义结构对齐机制，增强多模态对应关系。在控制方面，我们设计了一个解耦协调流匹配动作解码器，明确建模基座-臂部协调动作生成，缓解由控制耦合引起的优化难题。在没有访问特权感知信息的情况下，InCoM在三个ManiSkill-HAB场景中的成功率分别比最先进的方法高出28.2%、26.1%和23.6%，展示了其在全身移动操作中的强大效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">InCoM is an intent-driven perception and structured coordination framework for whole-body mobile manipulation. It addresses the challenges of coordinated control and perceptual attention by inferring latent motion intent and incorporating a geometric-semantic structured alignment mechanism. InCoM also decouples coordinated base-arm action generation, improving control optimization. Experimental results show that InCoM outperforms state-of-the-art methods by 28.2%, 26.1%, and 23.6% in success rate across three ManiSkill-HAB scenarios, highlighting its effectiveness for whole-body mobile manipulation.</div>
<div class="mono" style="margin-top:8px">论文提出了InCoM框架，旨在解决整体移动操作中的协调控制和感知注意力挑战。InCoM通过意图驱动的感知和结构化协调动态重新加权感知特征，并增强多模态对应关系。此外，它还解耦了基座和手臂的动作，以缓解控制耦合问题。实验结果表明，InCoM在三个ManiSkill-HAB场景中的成功率上分别优于最先进的方法28.2%、26.1%和23.6%。</div>
</details>
</div>
<div class="card">
<div class="title">DigiArm: An Anthropomorphic 3D-Printed Prosthetic Hand with Enhanced Dexterity for Typing Tasks</div>
<div class="meta-line">Authors: Dean Zadok, Tom Naamani, Yuval Bar-Ratson, Elisha Barash, Oren Salzman, Alon Wolf, Alex M. Bronstein, Nili Krausz</div>
<div class="meta-line">First: 2026-02-26T13:55:05+00:00 · Latest: 2026-02-26T13:55:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23017v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand. Current control systems for prosthetic hands are often limited to grasping, and commercial prosthetic hands lack the precision needed for dexterous manipulation or applications that require fine finger motions. Thus, there is a critical need for accessible and replicable prosthetic designs that enable individuals to interact with electronic devices and perform precise finger pressing, such as keyboard typing or piano playing, while preserving current prosthetic capabilities. This paper presents a low-cost, lightweight, 3D-printed robotic prosthetic hand, specifically engineered for enhanced dexterity with electronic devices such as a computer keyboard or piano, as well as general object manipulation. The robotic hand features a mechanism to adjust finger abduction/adduction spacing, a 2-D wrist with the inclusion of controlled ulnar/radial deviation optimized for typing, and control of independent finger pressing. We conducted a study to demonstrate how participants can use the robotic hand to perform keyboard typing and piano playing in real time, with different levels of finger and wrist motion. This supports the notion that our proposed design can allow for the execution of key typing motions more effectively than before, aiming to enhance the functionality of prosthetic hands.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DigiArm：一种增强灵巧度的3D打印仿人假手，适用于打字任务</div>
<div class="mono" style="margin-top:8px">尽管最近取得了进展，现有的假肢仍然无法复制人类手部的灵巧度和直观控制能力。当前假手的控制系统通常仅限于抓握，而商用假手缺乏进行精细操作或需要细小手指动作的应用所需的精确度。因此，迫切需要可访问且可复制的假肢设计，使个体能够与电子设备互动并执行精确的手指按压，如键盘打字或钢琴演奏，同时保留现有的假肢功能。本文介绍了一种低成本、轻量级的3D打印机器人假手，专门设计用于增强与电子设备（如计算机键盘或钢琴）以及一般物体操作的灵巧度。该机器人手具有调节手指展收间距的机制，具有优化的2D手腕，包括控制尺偏/桡偏，适用于打字，并可独立控制手指按压。我们进行了一项研究，以展示参与者如何实时使用机器人手进行键盘打字和钢琴演奏，不同手指和手腕运动水平。这支持了我们的设计可以更有效地执行关键打字动作的观点，旨在增强假手的功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the dexterity of prosthetic hands for tasks like typing and piano playing. The method involves designing a low-cost, 3D-printed robotic hand with adjustable finger spacing and a controlled wrist mechanism. Key findings show that participants could perform keyboard typing and piano playing more effectively with this design compared to existing prosthetic hands, highlighting its potential to enhance the functionality of prosthetic limbs.</div>
<div class="mono" style="margin-top:8px">研究旨在提高假肢手的灵巧性和直观控制能力，特别是对于需要精细手指动作的任务，如打字和钢琴演奏。研究介绍了DigiArm，这是一种低成本的3D打印机器人假肢手，具有可调节的手指间距和可控的腕部运动。参与者能够使用DigiArm进行实时键盘打字和钢琴演奏，展示了与现有假肢手相比的增强功能。</div>
</details>
</div>
<div class="card">
<div class="title">A Perspective on Open Challenges in Deformable Object Manipulation</div>
<div class="meta-line">Authors: Ryan Paul McKennaa, John Oyekan</div>
<div class="meta-line">First: 2026-02-26T13:39:30+00:00 · Latest: 2026-02-26T13:39:30+00:00</div>
<div class="meta-line">Comments: 28 pages, 7 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22998v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22998v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deformable object manipulation (DOM) represents a critical challenge in robotics, with applications spanning healthcare, manufacturing, food processing, and beyond. Unlike rigid objects, deformable objects exhibit infinite dimensionality, dynamic shape changes, and complex interactions with their environment, posing significant hurdles for perception, modeling, and control. This paper reviews the state of the art in DOM, focusing on key challenges such as occlusion handling, task generalization, and scalable, real-time solutions. It highlights advancements in multimodal perception systems, including the integration of multi-camera setups, active vision, and tactile sensing, which collectively address occlusion and improve adaptability in unstructured environments. Cutting-edge developments in physically informed reinforcement learning (RL) and differentiable simulations are explored, showcasing their impact on efficiency, precision, and scalability. The review also emphasizes the potential of simulated expert demonstrations and generative neural networks to standardize task specifications and bridge the simulation-to-reality gap. Finally, future directions are proposed, including the adoption of graph neural networks for high-level decision-making and the creation of comprehensive datasets to enhance DOM&#x27;s real-world applicability. By addressing these challenges, DOM research can pave the way for versatile robotic systems capable of handling diverse and dynamic tasks with deformable objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于可变形物体操作中开放挑战的视角</div>
<div class="mono" style="margin-top:8px">可变形物体操作（DOM）是机器人技术中的一个关键挑战，其应用范围从医疗保健、制造业、食品加工到更广泛的领域。与刚性物体不同，可变形物体表现出无限的维度、动态的形状变化以及与环境的复杂相互作用，这给感知、建模和控制带来了重大障碍。本文回顾了DOM的最新进展，重点关注遮挡处理、任务泛化和可扩展的实时解决方案等关键挑战。文章强调了多模态感知系统的进步，包括多摄像头设置、主动视觉和触觉传感的集成，这些共同解决了遮挡问题并提高了在非结构化环境中的适应性。文章还探讨了基于物理的强化学习（RL）和可微分模拟的最新进展，展示了它们在效率、精确性和可扩展性方面的影响力。回顾还强调了模拟专家演示和生成神经网络的潜力，以标准化任务规范并弥合模拟与现实之间的差距。最后，提出了未来方向，包括采用图神经网络进行高层次决策和创建综合数据集以增强DOM在现实世界中的适用性。通过解决这些挑战，DOM研究可以为能够处理各种动态任务的多功能机器人系统铺平道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the critical challenge of deformable object manipulation (DOM) in robotics, focusing on key challenges like occlusion handling and task generalization. It reviews advancements in multimodal perception systems and physically informed reinforcement learning, highlighting their role in improving efficiency and scalability. The study also discusses the potential of generative neural networks and simulated expert demonstrations to standardize task specifications and bridge the simulation-to-reality gap, suggesting future research directions involving graph neural networks and comprehensive datasets.</div>
<div class="mono" style="margin-top:8px">本文探讨了机器人领域中变形物体操作（DOM）的关键挑战，重点关注遮挡处理和任务泛化等问题。它回顾了多模态感知系统和物理启发式强化学习的进步，展示了它们在效率和可扩展性方面的影响力。研究还强调了模拟专家演示和生成神经网络在提高任务规范性和弥合仿真与现实差距方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy</div>
<div class="meta-line">Authors: Yuan Tang, Bruno V. Adorno, Brendan A. McGrath, Andrew Weightman</div>
<div class="meta-line">First: 2026-02-26T12:47:04+00:00 · Latest: 2026-02-26T12:47:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22952v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22952v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Percutaneous dilatational tracheostomy (PDT) is frequently performed on patients in intensive care units for prolonged mechanical ventilation. The needle puncture, as the most critical step of PDT, could lead to adverse consequences such as major bleeding and posterior tracheal wall perforation if performed inaccurately. Current practices of PDT puncture are all performed manually with no navigation assistance, which leads to large position and angular errors (5 mm and 30 degree). To improve the accuracy and reduce the difficulty of the PDT procedure, we propose a system that automates the needle insertion using a velocity-controlled robotic manipulator. Guided using pose data from two electromagnetic sensors, one at the needle tip and the other inside the trachea, the robotic system uses an adaptive constrained controller to adapt the uncertain kinematic parameters online and avoid collisions with the patient&#x27;s body and tissues near the target. Simulations were performed to validate the controller&#x27;s implementation, and then four hundred PDT punctures were performed on a mannequin to evaluate the position and angular accuracy. The absolute median puncture position error was 1.7 mm (IQR: 1.9 mm) and midline deviation was 4.13 degree (IQR: 4.55 degree), measured by the sensor inside the trachea. The small deviations from the nominal puncture in a simulated experimental setup and formal guarantees of collision-free insertions suggest the feasibility of the robotic PDT puncture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动机器人针刺用于经皮扩张气管切开术</div>
<div class="mono" style="margin-top:8px">经皮扩张气管切开术（PDT）常在重症监护病房中为长时间机械通气的患者进行。针刺作为PDT中最关键的步骤，如果操作不准确，可能会导致严重的出血和后气管壁穿孔等不良后果。目前的PDT针刺操作都是手动进行，没有导航辅助，导致位置和角度误差较大（5毫米和30度）。为了提高PDT操作的准确性和降低难度，我们提出了一种使用速度控制机器人操作器自动进行针刺的系统。该机器人系统通过两个电磁传感器提供的姿态数据进行引导，一个位于针尖，另一个位于气管内，使用自适应约束控制器在线适应不确定的运动参数，避免与患者身体和目标附近的组织发生碰撞。进行了仿真验证控制器的实现，并在人体模型上进行了四百次PDT针刺操作以评估位置和角度精度。通过气管内的传感器测量，绝对中位针刺位置误差为1.7毫米（四分位距：1.9毫米），中线偏移为4.13度（四分位距：4.55度）。模拟实验设置中的小偏差和碰撞自由插入的正式保证表明机器人PDT针刺的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the critical need for improved accuracy in percutaneous dilatational tracheostomy (PDT) by proposing an automated robotic system. The system uses a velocity-controlled robotic manipulator guided by electromagnetic sensors to perform needle puncture, reducing position and angular errors to 1.7 mm and 4.13 degrees, respectively. Simulations and experiments on a mannequin validated the system&#x27;s effectiveness and collision-free operation.</div>
<div class="mono" style="margin-top:8px">论文提出了一种自动机器人系统，以提高PDT中的针刺准确性。该系统使用一个速度控制的机器人 manipulator，并通过电磁传感器的姿态数据进行引导，以实现精确的针刺插入。在人模上的实验结果显示，针刺位置误差的中位数为1.7毫米，中线偏移为4.13度，显著减少了与手动操作相比的误差。</div>
</details>
</div>
<div class="card">
<div class="title">DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation</div>
<div class="meta-line">Authors: Zebin Yang, Yijiahao Qi, Tong Xie, Bo Yu, Shaoshan Liu, Meng Li</div>
<div class="meta-line">First: 2026-02-26T11:34:36+00:00 · Latest: 2026-02-26T11:34:36+00:00</div>
<div class="meta-line">Comments: DAC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22896v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22896v1">PDF</a> · <a href="https://github.com/PKU-SEC-Lab/DYSL_VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model&#x27;s reasoning with a vision model&#x27;s 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action&#x27;s importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DySL-VLA：基于动态-静态层跳过的高效视觉-语言-行动模型推理框架用于机器人操作</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型通过将语言模型的推理与视觉模型的3D理解相结合，在机器人操作等任务中取得了显著的成功。然而，它们的高计算成本仍然是实现实时性能所需的实际应用中的主要障碍。我们观察到任务中的动作具有不同的重要性：关键步骤需要高精度，而不太重要的步骤可以容忍更大的变化。利用这一洞察，我们提出了一种名为DySL-VLA的新框架，通过根据每个动作的重要性动态跳过VLA层来解决计算成本问题。DySL-VLA将其层分为两类：信息层，始终执行；增量层，可以有选择地跳过。为了在不牺牲准确性的前提下智能地跳过层，我们发明了一种先验-后跳机制来确定何时开始跳过层。我们还提出了一种跳层感知的两阶段知识蒸馏算法，以高效地将标准VLA训练成DySL-VLA。我们的实验表明，DySL-VLA在Calvin数据集上的成功长度比Deer-VLA提高了2.1%，同时将可训练参数减少了85.7倍，并且相对于RoboFlamingo基线在等精度下提供了3.75倍的速度提升。我们的代码可在https://github.com/PKU-SEC-Lab/DYSL_VLA上获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DySL-VLA proposes a dynamic layer-skipping mechanism to reduce the computational cost of Vision-Language-Action models for robotic manipulation tasks. By categorizing layers into informative and incremental types and using a prior-post skipping guidance mechanism, DySL-VLA achieves a 2.1% improvement in success length on the Calvin dataset, reduces trainable parameters by 85.7 times, and provides a 3.75x speedup compared to the RoboFlamingo baseline at the same accuracy level.</div>
<div class="mono" style="margin-top:8px">DySL-VLA通过动态跳过基于每个动作重要性的层来解决Vision-Language-Action模型在机器人操作中的高计算成本问题。它将层分为信息性和增量性两类，并使用先验后跳过指导机制来确定何时跳过层。DySL-VLA在Calvin数据集上将成功长度提高了2.1%，减少了85.7倍的可训练参数，并且在相同准确率下比RoboFlamingo基线快3.75倍。</div>
</details>
</div>
<div class="card">
<div class="title">GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion</div>
<div class="meta-line">Authors: Enda Xiang, Haoxiang Ma, Xinzhu Ma, Zicheng Liu, Di Huang</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-26T10:56:01+00:00 · Latest: 2026-02-26T10:56:01+00:00</div>
<div class="meta-line">Comments: Accepted to CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22862v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22862v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraspLDP：通过潜在扩散提高抓取策略的泛化能力</div>
<div class="mono" style="margin-top:8px">本文专注于通过模仿学习提高抓取精度和抓取策略的泛化能力。基于扩散的方法已成为机器人操作任务的主要方法。由于抓取是操作中的关键子任务，因此模仿学习得到的策略执行精确和泛化的抓取的能力值得特别关注。现有的抓取模仿学习技术往往存在抓取执行不精确、空间泛化能力有限和对象泛化能力差的问题。为了解决这些问题，我们将抓取先验知识融入到扩散策略框架中。具体而言，我们使用潜在扩散策略来引导动作片段解码，使用抓取姿态先验确保生成的运动轨迹接近可行的抓取配置。此外，我们在扩散过程中引入自监督重构目标以嵌入抓取先验：在每次逆向扩散步骤中，我们重建手腕相机图像，将抓取性从中间表示中反投影回去。仿真和真实机器人实验均表明，我们的方法显著优于基线方法，并表现出强大的动态抓取能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to improve the precision and generalization of grasping policies learned through imitation learning by incorporating grasp prior knowledge into a latent diffusion policy framework. The method uses a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring feasible grasp configurations. Additionally, a self-supervised reconstruction objective is introduced to embed the graspness prior during the diffusion process. Experimental results show that the proposed approach outperforms baseline methods and demonstrates strong dynamic grasping capabilities.</div>
<div class="mono" style="margin-top:8px">该论文旨在通过将抓取先验知识融入扩散策略框架中，提高通过模仿学习获得的抓取策略的精确度和泛化能力。方法使用潜扩散策略来指导动作片段解码，结合抓取姿态先验，确保生成的运动轨迹符合可行的抓取配置。此外，在扩散过程中引入自监督重建目标，以嵌入抓取先验。实验结果表明，提出的GraspLDP方法在模拟和真实机器人实验中均优于基线方法，并展示了强大的动态抓取能力。</div>
</details>
</div>
<div class="card">
<div class="title">Performance and Experimental Analysis of Strain-based Models for Continuum Robots</div>
<div class="meta-line">Authors: Annika Delucchi, Vincenzo Di Paola, Andreas Müller, and Matteo Zoppi</div>
<div class="meta-line">First: 2026-02-26T10:46:13+00:00 · Latest: 2026-02-26T10:46:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22854v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22854v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续机器人基于应变的模型性能及实验分析</div>
<div class="mono" style="margin-top:8px">尽管基于应变的模型在机器人学中被广泛应用，但除了均匀弯曲测试外，没有公认的比较方法来评估其性能。此外，连续机器人原型制作的日益努力突显了评估这些模型适用性以及进行全面性能评估的必要性。为解决这一差距，本研究调查了三阶应变插值方法的形状重构能力，检查其捕捉单个和组合变形效果的能力。这些结果与几何变量应变方法进行了比较和讨论。随后，通过重塑细长杆并使用相机记录结果配置，实验验证了模拟结果。使用操作臂移动杆的一端来施加杆的配置，并通过反射标记提取，无需任何其他外部传感器——即沿杆放置的应变计或力矩传感器。实验表明，模型预测与观察到的形状之间有良好的一致性，平均误差为杆长的0.58%，每种配置的平均计算时间为0.32秒，优于现有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work evaluates the performance of a third-order strain interpolation method for continuum robots by comparing it with the Geometric-Variable Strain approach. The study focuses on the shape reconstruction abilities of these models, using a slender rod reshaped by a manipulator. Experimental results show good agreement between model predictions and observed shapes, with an average error of 0.58% of the rod length and a computational time of 0.32 seconds per configuration, outperforming existing models.</div>
<div class="mono" style="margin-top:8px">这项研究评估了一种第三阶应变插值方法在连续机器人中的性能，将其与几何变量应变方法进行了比较。研究考察了该方法在不同变形下准确重构细长杆形状的能力。实验结果显示，模型预测与观察到的形状之间有良好的一致性，平均误差为细长杆长度的0.58%，每次构型的计算时间为0.32秒，优于现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">The AI Research Assistant: Promise, Peril, and a Proof of Concept</div>
<div class="meta-line">Authors: Tan Bui-Thanh</div>
<div class="meta-line">First: 2026-02-26T10:29:05+00:00 · Latest: 2026-02-26T10:29:05+00:00</div>
<div class="meta-line">Comments: 11 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22842v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22842v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI研究助手：潜力、风险与概念验证</div>
<div class="mono" style="margin-top:8px">人工智能能否真正为创造性数学研究做出贡献，还是仅仅自动化了常规计算并带来了错误的风险？我们通过详细的案例研究提供了实证证据：通过系统的人机协作发现了新的误差表示和赫mite求积规则的边界。
 与多个AI助手合作，我们超越了手动工作的成果，借助AI协助提出了并证明了多个定理。合作揭示了AI的卓越能力和关键局限。AI在代数操作、系统证明探索、文献综合和LaTeX准备方面表现出色。然而，每一步都需要严格的验证，数学直觉来制定问题，并且需要战略指导。
我们以异常透明的方式记录了完整的研究工作流程，揭示了成功的人机数学合作模式，并指出了研究人员必须预见的失败模式。我们的经验表明，在适当的怀疑和验证协议下使用时，AI工具可以有意义地加速数学发现，同时需要仔细的人类监督和深厚的专业知识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates whether AI can contribute to creative mathematical research by collaborating with humans. Through a detailed case study involving the discovery of novel error representations and bounds for Hermite quadrature rules, the research demonstrates that AI excels in algebraic manipulation, systematic proof exploration, and literature synthesis. However, human verification, mathematical intuition, and strategic direction are essential. The study provides a transparent workflow and identifies patterns in successful human-AI collaboration and failure modes to guide future research.</div>
<div class="mono" style="margin-top:8px">研究探讨了AI是否可以通过与人类合作来贡献创造性数学研究。通过发现Hermite求积规则的新误差表示和边界值的详细案例研究，研究显示AI在代数操作、系统证明探索和文献综合方面表现出色。然而，人类的验证和战略方向至关重要，强调了需要谨慎监督和深厚的专业知识。该工作提供了实证证据和人类与AI在数学合作方面的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Spatially anchored Tactile Awareness for Robust Dexterous Manipulation</div>
<div class="meta-line">Authors: Jialei Huang, Yang Ye, Yuanqing Gong, Xuezhou Zhu, Yang Gao, Kaifeng Zhang</div>
<div class="meta-line">First: 2025-10-16T12:59:34+00:00 · Latest: 2026-02-26T10:21:54+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14647v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.14647v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation requires precise geometric reasoning, yet existing visuo-tactile learning methods struggle with sub-millimeter precision tasks that are routine for traditional model-based approaches. We identify a key limitation: while tactile sensors provide rich contact information, current learning frameworks fail to effectively leverage both the perceptual richness of tactile signals and their spatial relationship with hand kinematics. We believe an ideal tactile representation should explicitly ground contact measurements in a stable reference frame while preserving detailed sensory information, enabling policies to not only detect contact occurrence but also precisely infer object geometry in the hand&#x27;s coordinate system. We introduce SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an end-to-end policy framework that explicitly anchors tactile features to the hand&#x27;s kinematic frame through forward kinematics, enabling accurate geometric reasoning without requiring object models or explicit pose estimation. Our key insight is that spatially grounded tactile representations allow policies to not only detect contact occurrence but also precisely infer object geometry in the hand&#x27;s coordinate system. We validate SaTA on challenging dexterous manipulation tasks, including bimanual USB-C mating in free space, a task demanding sub-millimeter alignment precision, as well as light bulb installation requiring precise thread engagement and rotational control, and card sliding that demands delicate force modulation and angular precision. These tasks represent significant challenges for learning-based methods due to their stringent precision requirements. Across multiple benchmarks, SaTA significantly outperforms strong visuo-tactile baselines, improving success rates by up to 30 percentage while reducing task completion times by 27 percentage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于空间锚定的触觉意识以实现稳健的灵巧操作</div>
<div class="mono" style="margin-top:8px">灵巧操作需要精确的几何推理，但现有的视触觉学习方法在处理传统基于模型方法可以轻松完成的亚毫米级精度任务时存在困难。我们发现一个关键限制：虽然触觉传感器提供了丰富的接触信息，但当前的学习框架未能有效利用触觉信号的感知丰富性和其与手部运动学的空间关系。我们认为理想的触觉表示应该明确将接触测量与稳定参考框架联系起来，同时保留详细的感官信息，使策略不仅能检测接触的发生，还能精确推断物体在手坐标系中的几何形状。我们提出了SaTA（基于空间锚定的触觉意识），这是一种端到端的策略框架，通过前向运动学将触觉特征明确锚定到手的运动学框架中，从而在无需物体模型或显式姿态估计的情况下实现精确的几何推理。我们的关键见解是，空间锚定的触觉表示使策略不仅能检测接触的发生，还能精确推断物体在手坐标系中的几何形状。我们在包括自由空间中的双臂USB-C对接、需要亚毫米级对准精度的灯泡安装以及需要精细力调节和角度精度的卡片滑动等具有挑战性的灵巧操作任务上验证了SaTA。这些任务对基于学习的方法提出了严峻的精度要求。在多个基准测试中，SaTA显著优于强大的视触觉基线，成功率达到最高30个百分点的提升，任务完成时间减少27个百分点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of achieving sub-millimeter precision in dexterous manipulation tasks, where existing visuo-tactile learning methods fall short. The authors introduce SaTA, an end-to-end policy framework that anchors tactile features to the hand&#x27;s kinematic frame, enabling accurate geometric reasoning. SaTA significantly outperforms strong visuo-tactile baselines in tasks such as bimanual USB-C mating, light bulb installation, and card sliding, improving success rates by up to 30 percentage points and reducing task completion times by 27 percentage points.</div>
<div class="mono" style="margin-top:8px">论文旨在解决现有视觉-触觉学习方法难以实现亚毫米级精度的灵巧操作任务。它提出了SaTA，一种将触觉特征锚定到手的运动坐标系中的端到端策略框架，能够实现精确的几何推理。SaTA在包括USB-C对接、灯泡安装和卡片滑动等任务中进行了测试，并显著优于强大的视觉-触觉基线，提高了成功率并减少了任务完成时间。</div>
</details>
</div>
<div class="card">
<div class="title">Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera</div>
<div class="meta-line">Authors: Seongyong Kim, Junhyeon Cho, Kang-Won Lee, Soo-Chul Lim</div>
<div class="meta-line">First: 2026-02-26T08:15:38+00:00 · Latest: 2026-02-26T08:15:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22733v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To catch a thrown object, a robot must be able to perceive the object&#x27;s motion and generate control actions in a timely manner. Rather than explicitly estimating the object&#x27;s 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object&#x27;s position and scale, allowing the policy to reason about the object&#x27;s motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Pixel2Catch：单色摄像头下基于多智能体的模拟到现实的敏捷操作转移</div>
<div class="mono" style="margin-top:8px">为了接住一个投掷物，机器人必须能够感知投掷物的运动并及时生成控制动作。不同于明确估计投掷物的三维位置，这项工作关注一种新颖的方法，即利用单个RGB图像中像素级的视觉信息来识别物体的运动。这些视觉线索捕捉了物体位置和尺度的变化，使策略能够推理物体的运动。此外，为了在由配备多指手的机器人臂组成的高自由度系统中实现稳定学习，我们设计了一种异构多智能体强化学习框架，将臂和手定义为具有不同角色的独立智能体。每个智能体通过特定的角色观察和奖励进行协同训练，学习到的策略成功地从模拟环境转移到了现实世界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of a robot catching a thrown object by focusing on recognizing object motion using pixel-level visual information from a single RGB image. It introduces a novel approach that captures changes in the object&#x27;s position and scale, enabling the robot to reason about the object&#x27;s motion without explicitly estimating its 3D position. To stabilize learning in a high-degree-of-freedom system, the authors designed a heterogeneous multi-agent reinforcement learning framework where the robot arm and multi-fingered hand are treated as independent agents with distinct roles. The policies learned in simulation were successfully transferred to the real world, demonstrating the effectiveness of this method.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过利用单个RGB摄像头的像素级视觉信息来识别物体的运动，使机器人能够接住投掷的物体。提出了一种新颖的方法，通过捕捉物体位置和尺度的变化来推断物体的运动。为了在高自由度系统中实现稳定的训练，设计了一个异构多智能体强化学习框架，将机器人手臂和多指手视为具有不同角色的独立智能体。所学策略成功地从仿真环境转移到了真实世界，验证了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline</div>
<div class="meta-line">Authors: Wenxuan Song, Jiayi Chen, Xiaoquan Sun, Huashuo Lei, Yikai Qin, Wei Zhao, Pengxiang Ding, Han Zhao, Tongxin Wang, Pengxu Hou, Zhide Zhong, Haodong Yan, Donglin Wang, Jun Ma, Haoang Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-26T06:27:37+00:00 · Latest: 2026-02-26T06:27:37+00:00</div>
<div class="meta-line">Comments: Accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs&#x27; practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考视觉-语言-行动模型的实用性：一个全面的基准和改进的基础模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型已经发展成为一种通用的机器人代理。然而，现有的VLA受到参数规模过大、预训练要求高昂以及对多样化实体适用性有限的限制。为了提高VLA的实用性，我们提出了一种全面的基准和改进的基础模型。首先，我们提出了CEBench，这是一种新的基准，涵盖了模拟和现实世界中多样化的实体，并考虑了领域随机化。我们收集了14400个模拟轨迹和1600个现实世界的专家策划轨迹，以支持在CEBench上的训练。其次，使用CEBench作为我们的测试平台，我们研究了VLA实用性方面的三个关键方面，并提出了几个关键发现。根据这些发现，我们引入了LLaVA-VLA，这是一种轻量级但强大的VLA，旨在在消费级GPU上进行实际部署。从架构上看，它结合了紧凑的VLM骨干、多视图感知、本体感受性标记和动作分块。为了消除对昂贵预训练的依赖，LLaVA-VLA采用了两阶段训练范式，包括后训练和微调。此外，LLaVA-VLA扩展了动作空间，以统一导航和操作。跨实体的实验展示了LLaVA-VLA的泛化能力和多功能性，而现实世界的移动操作实验则确立了它作为第一个端到端的移动操作VLA模型的地位。在被接受后，我们将开放所有数据集、代码和检查点，以促进可重复性和未来的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to enhance the practicality of Vision-Language-Action (VLA) models by proposing a comprehensive benchmark and an improved baseline. CEBench, a new benchmark, covers diverse embodiments in both simulation and the real world, supporting 14.4k simulated and 1.6k real-world trajectories. The study identifies key practicality aspects and introduces LLaVA-VLA, a lightweight VLA that integrates a compact vision-language model backbone with multi-view perception, proprioceptive tokenization, and action chunking. LLaVA-VLA demonstrates generalization and versatility across different embodiments and is the first end-to-end VLA for mobile manipulation, using a two-stage training paradigm and extending the action space to unify navigation and manipulation.</div>
<div class="mono" style="margin-top:8px">论文旨在通过提出一个全面的基准（CEBench）和改进的基线（LLaVA-VLA）来提升视觉-语言-动作（VLA）模型的实用性。CEBench 包含了多样化的模拟和真实世界场景，而 LLaVA-VLA 结合了紧凑的 VLM 主干网络、多视图感知和动作切片，使其能够在消费级 GPU 上进行实际部署。关键发现表明，LLaVA-VLA 能够在不同场景中泛化，并且是首个用于移动操作的端到端 VLA 模型，展示了其在实际任务中的灵活性和能力。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Imagination for Efficient Visual World Model Planning</div>
<div class="meta-line">Authors: Junha Chun, Youngjoon Jeong, Taesup Kim</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-02T07:36:14+00:00 · Latest: 2026-02-26T06:26:43+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026; Project Page: https://nikriz1.github.io/sparse_imagination/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.01392v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.01392v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nikriz1.github.io/sparse_imagination/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices. This computational burden is particularly restrictive in robotics, where resources are severely constrained. To address this limitation, we propose a Sparse Imagination for Efficient Visual World Model Planning, which enhances computational efficiency by reducing the number of tokens processed during forward prediction. Our method leverages a sparsely trained vision-based world model based on transformers with randomized grouped attention strategy, allowing the model to flexibly adjust the number of tokens processed based on the computational resource. By enabling sparse imagination during latent rollout, our approach significantly accelerates planning while maintaining high control fidelity. Experimental results demonstrate that sparse imagination preserves task performance while dramatically improving inference efficiency. This general technique for visual planning is applicable from simple test-time trajectory optimization to complex real-world tasks with the latest VLAs, enabling the deployment of world models in real-time scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏想象以提高视觉世界模型规划效率</div>
<div class="mono" style="margin-top:8px">基于世界模型的规划显著提高了在复杂环境中的决策能力，通过使代理能够模拟未来状态并做出明智的选择。这一计算负担在资源严重受限的机器人领域尤为限制性。为解决这一限制，我们提出了一种稀疏想象以提高视觉世界模型规划效率的方法，通过减少前向预测过程中处理的令牌数量来增强计算效率。该方法利用基于变压器的稀疏训练视觉世界模型，并采用随机分组注意力策略，使模型能够根据计算资源灵活调整处理的令牌数量。通过在潜在回放期间启用稀疏想象，我们的方法显著加速了规划过程，同时保持了高控制精度。实验结果表明，稀疏想象在保持任务性能的同时，大幅提高了推理效率。这一通用的视觉规划技术适用于从简单的测试时轨迹优化到复杂的现实世界任务，利用最新的VLAs，使世界模型能够在实时场景中部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the computational challenge of world model-based planning in robotics by proposing Sparse Imagination, which reduces the number of tokens processed during forward prediction. This method uses a sparsely trained vision-based world model with randomized grouped attention to flexibly adjust the number of tokens based on available resources. The approach accelerates planning while maintaining performance, as shown by experimental results that demonstrate significant improvements in inference efficiency without compromising task success.</div>
<div class="mono" style="margin-top:8px">论文提出了一种高效的视觉世界模型规划方法——稀疏想象，以提高资源受限机器人中的世界模型计算效率。通过使用随机分组注意力机制训练的稀疏视觉世界模型，该方法减少了前向预测中的处理令牌数量，并可根据可用资源灵活调整。实验表明，这种方法在保持任务性能的同时显著提高了推理效率，适用于从简单的轨迹优化到复杂的现实世界任务等多种场景。</div>
</details>
</div>
<div class="card">
<div class="title">Metamorphic Testing of Vision-Language Action-Enabled Robots</div>
<div class="meta-line">Authors: Pablo Valle, Sergio Segura, Shaukat Ali, Aitor Arrieta</div>
<div class="meta-line">First: 2026-02-26T03:32:43+00:00 · Latest: 2026-02-26T03:32:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22579v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22579v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言-动作变换测试的视觉-语言-动作机器人</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型是多模态的机器人任务控制器，给定指令和视觉输入后，它们会产生一系列低级控制动作（或运动命令），使机器人能够在物理环境中执行所要求的任务。这些系统从多个角度面临测试判据问题。一方面，必须为每个指令提示定义一个测试判据，这是一项复杂且难以泛化的做法。另一方面，当前最先进的判据通常捕捉世界的符号表示（例如，机器人和物体状态），使任务正确性评估成为可能，但无法评估其他关键方面，如VLA使能的机器人执行任务的质量。在本文中，我们探讨了变换测试（MT）是否可以缓解此上下文中的测试判据问题。为此，我们提出了两种变换关系模式和五个变换关系，以评估测试输入的变化是否影响VLA使能机器人的原始轨迹。涉及五个VLA模型、两个模拟机器人和四个机器人任务的实证研究表明，MT可以通过自动检测各种类型的失败（包括但不限于未完成的任务）来有效缓解测试判据问题。更重要的是，提出的变换关系具有泛化性，使所提出的方法适用于不同的VLA模型、机器人和任务，即使在没有测试判据的情况下也是如此。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the test oracle problem in Vision-Language-Action (VLA) models, which are multimodal robotic task controllers. To alleviate this issue, the authors propose using Metamorphic Testing (MT) to evaluate VLA models by defining two metamorphic relation patterns and five metamorphic relations. The empirical study involving five VLA models, two simulated robots, and four tasks demonstrates that MT can effectively detect various types of failures, particularly uncompleted tasks, and is generalizable across different VLA models, robots, and tasks without needing specific test oracles.</div>
<div class="mono" style="margin-top:8px">本文探讨了Vision-Language-Action (VLA)模型中的测试 oracle 问题，这些模型是多模态的机器人任务控制器。作者提出使用元测试（MT）来评估输入变化对机器人轨迹的影响，提出了两种元关系模式和五个元关系。一项涉及五个VLA模型、两个模拟机器人和四个任务的实证研究表明，MT可以有效检测各种类型的失败，包括未完成的任务，并且该方法在不同模型、机器人和任务之间具有通用性。</div>
</details>
</div>
<div class="card">
<div class="title">A Pragmatic VLA Foundation Model</div>
<div class="meta-line">Authors: Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng</div>
<div class="meta-line">First: 2026-01-26T17:08:04+00:00 · Latest: 2026-02-26T03:30:01+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/, GM-100: https://huggingface.co/datasets/robbyant/lingbot-GM-100</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18692v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18692v2">PDF</a> · <a href="https://github.com/Robbyant/lingbot-vla/">Code1</a> · <a href="https://huggingface.co/datasets/robbyant/lingbot-GM-100">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种务实的VLA基础模型</div>
<div class="mono" style="margin-top:8px">在机器人操作方面具有巨大潜力，一个强大的视觉-语言-行动（VLA）基础模型有望在任务和平台之间忠实泛化，同时确保成本效益（例如，适应所需的数据显示和GPU小时数）。为此，我们开发了LingBot-VLA，使用来自9种流行双臂机器人配置的约20,000小时的真实世界数据。通过在3种机器人平台上进行系统评估，每个平台完成100个任务，每个任务有130个训练后回放，我们的模型在性能和泛化能力方面明显优于竞争对手。我们还构建了一个高效的代码库，使用8块GPU的训练设置，每秒可处理261个样本，比现有VLA导向的代码库快1.5到2.8倍（取决于所依赖的VLM基础模型）。上述特性确保了我们的模型适合实际部署。为了推进机器人学习领域的发展，我们提供了代码、基础模型和基准数据的开放访问，重点是促进更具挑战性的任务和合理的评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents LingBot-VLA, a Vision-Language-Action foundation model developed with 20,000 hours of real-world data from nine dual-arm robot configurations. Through systematic evaluations on three robotic platforms, LingBot-VLA demonstrates superior performance and broad generalizability, outperforming competitors in 100 tasks with 130 post-training episodes per task. The model also features an efficient codebase with a throughput of 261 samples per second on an 8-GPU setup, achieving a 1.5 to 2.8 times speedup over existing VLA-oriented codebases.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种能够在任务和平台间良好泛化的VLA基础模型，同时保持成本效率。LingBot-VLA模型基于20,000小时的真实世界数据从9种双臂机器人配置中训练而成，并在三个机器人平台上展示了优越的性能，每个任务完成了100次任务和130次后训练的训练集。该模型的高效代码库在8-GPU设置下实现了每秒261个样本的吞吐量，比现有的VLA导向代码库快1.5到2.8倍。提供了代码、基础模型和基准数据的开放访问，以促进进一步的研究和评价标准。</div>
</details>
</div>
<div class="card">
<div class="title">DreamWaQ++: Obstacle-Aware Quadrupedal Locomotion With Resilient Multi-Modal Reinforcement Learning</div>
<div class="meta-line">Authors: I Made Aswin Nahrendra, Byeongho Yu, Minho Oh, Dongkyu Lee, Seunghyun Lee, Hyeonwoo Lee, Hyungtae Lim, Hyun Myung</div>
<div class="meta-line">First: 2024-09-29T13:57:09+00:00 · Latest: 2026-02-26T02:03:32+00:00</div>
<div class="meta-line">Comments: IEEE Transactions on Robotics 2026. Project site is available at https://dreamwaqpp.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.19709v2">Abs</a> · <a href="https://arxiv.org/pdf/2409.19709v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dreamwaqpp.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadrupedal robots hold promising potential for applications in navigating cluttered environments with resilience akin to their animal counterparts. However, their floating base configuration makes them vulnerable to real-world uncertainties, yielding substantial challenges in their locomotion control. Deep reinforcement learning has become one of the plausible alternatives for realizing a robust locomotion controller. However, the approaches that rely solely on proprioception sacrifice collision-free locomotion because they require front-feet contact to detect the presence of stairs to adapt the locomotion gait. Meanwhile, incorporating exteroception necessitates a precisely modeled map observed by exteroceptive sensors over a period of time. Therefore, this work proposes a novel method to fuse proprioception and exteroception featuring a resilient multi-modal reinforcement learning. The proposed method yields a controller that showcases agile locomotion performance on a quadrupedal robot over a myriad of real-world courses, including rough terrains, steep slopes, and high-rise stairs, while retaining its robustness against out-of-distribution situations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DreamWaQ++：基于鲁棒多模态强化学习的避障四足运动控制</div>
<div class="mono" style="margin-top:8px">四足机器人在导航复杂环境方面具有巨大的应用潜力，其运动方式类似于其动物原型。然而，它们的浮动基座配置使它们容易受到现实世界不确定性的影响，给其运动控制带来了巨大挑战。深度强化学习已成为实现鲁棒运动控制器的一种可行替代方案。然而，依赖于本体感觉的方法牺牲了无碰撞运动，因为它们需要前足接触来检测楼梯的存在以适应运动步态。同时，结合外体感觉需要长时间由外体感觉传感器观察到精确建模的地图。因此，本工作提出了一种新的方法，将本体感觉和外体感觉融合到鲁棒多模态强化学习中。所提出的方法在四足机器人上展示了在各种现实世界的路径上的灵活运动性能，包括崎岖地形、陡峭斜坡和高楼梯，同时保持对离分布情况的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of robust quadrupedal locomotion in cluttered environments by proposing DreamWaQ++, a resilient multi-modal reinforcement learning method that fuses proprioception and exteroception. The method enables the robot to perform agile locomotion on various terrains while maintaining robustness against real-world uncertainties. Key experimental findings demonstrate the robot&#x27;s ability to navigate rough terrains, steep slopes, and high-rise stairs without collisions, showcasing its adaptability and resilience.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为DreamWaQ++的鲁棒多模态强化学习方法，结合了本体感觉和外体感觉，以应对四足机器人在复杂环境中的稳健行走问题。该方法能够在粗糙地形、陡坡和高阶楼梯等多种地形上实现敏捷且无碰撞的行走，并且能够抵御不确定性。主要发现包括在多种实际课程中成功导航，无需精确建模的地图或前足接触来检测楼梯。</div>
</details>
</div>
<div class="card">
<div class="title">SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation</div>
<div class="meta-line">Authors: Xinyu Tan, Ningwei Bai, Harry Gardener, Zhengyang Zhong, Luoyu Zhang, Liuhaichen Yang, Zhekai Duan, Monkgogi Galeitsiwe, Zezhi Tang</div>
<div class="meta-line">First: 2026-02-26T01:16:27+00:00 · Latest: 2026-02-26T01:16:27+00:00</div>
<div class="meta-line">Comments: 7 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22514v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.
  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.
  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SignVLA：一种无手语词的视觉-语言-行动框架，用于实时手语引导的机器人操作</div>
<div class="mono" style="margin-top:8px">我们提出了，据我们所知，首个由手语驱动的视觉-语言-行动（VLA）框架，以实现直观和包容的人机交互。与依赖手语词注释作为中间监督的传统方法不同，所提出的系统采用无手语词的范式，直接将视觉手语手势映射为语义指令。这种设计减少了注释成本，并避免了手语词表示引入的信息损失，从而实现更自然和可扩展的多模态交互。
在本文中，我们专注于一个实时字母级手指拼写界面，为机器人控制提供了一个稳健且低延迟的通信通道。与大规模连续手语识别相比，字母级交互在安全关键的实体环境中提供了更高的可靠性和可解释性，以及部署可行性。所提出的流水线通过几何归一化、时间平滑和词汇精炼，将连续的手势流转换为连贯的语言命令，确保稳定和一致的交互。
此外，该框架设计用于支持未来基于变换器的无手语词手语模型的集成，从而实现可扩展的词级和句级语义理解。实验结果表明，所提出的系统在多种交互场景下能够将手语衍生的指令精确地转化为机器人动作，这些结果突显了该框架在推进无障碍、可扩展和多模态实体智能方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research introduces SignVLA, a gloss-free VLA framework for real-time sign language-guided robotic manipulation, reducing annotation costs and avoiding information loss. The system maps visual sign gestures directly to semantic instructions, ensuring natural and scalable interaction. Experiments show effective grounding of sign-derived instructions into precise robotic actions, highlighting the framework&#x27;s potential for accessible and multimodal embodied intelligence.</div>
<div class="mono" style="margin-top:8px">该论文介绍了SignVLA，一种用于实时手语指导机器人操作的无手语注释VLA框架。不同于以往依赖手语注释的方法，该系统直接将手语手势映射为语义指令，减少了注释成本并避免了信息损失。框架专注于字母级的手指拼写界面，提供可靠且低延迟的通信以实现对机器人的控制。实验结果表明，该系统能够有效将手语手势转化为精确的机器人动作，展示了其在安全关键环境中的可访问性、可扩展性和多模态交互潜力。</div>
</details>
</div>
<div class="card">
<div class="title">EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow</div>
<div class="meta-line">Authors: Daesol Cho, Youngseok Jang, Danfei Xu, Sehoon Ha</div>
<div class="meta-line">First: 2026-02-25T22:50:51+00:00 · Latest: 2026-02-25T22:50:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22461v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22461v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoAVFlow：通过第一人称视频中的3D流学习机器人政策与主动视觉</div>
<div class="mono" style="margin-top:8px">第一人称人类视频提供了可扩展的演示数据源；然而，将其部署到机器人上需要主动视点控制以保持任务关键的可见性，这往往是由于人类特定的先验知识而无法通过人类视点模仿提供。我们提出了EgoAVFlow，它通过共享的3D流表示从第一人称视频中学习操作和主动视觉，该表示支持几何可见性推理，并在无需机器人演示的情况下进行迁移。EgoAVFlow 使用扩散模型来预测机器人动作、未来3D流和相机轨迹，并在测试时通过基于预测运动和场景几何的可见性感知奖励下的奖励最大化去噪来优化视点。在主动变化视点的现实世界实验中，EgoAVFlow 一致地优于基于人类演示的先前基线，展示了有效的可见性维护和无需机器人演示的稳健操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EgoAVFlow is designed to address the challenge of using human egocentric videos for robot manipulation by incorporating active viewpoint control. It uses a shared 3D flow representation to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints to maintain task-critical visibility. Experimental results show that EgoAVFlow outperforms previous human-demo-based methods in real-world scenarios, effectively maintaining visibility and achieving robust manipulation without the need for robot demonstrations.</div>
<div class="mono" style="margin-top:8px">EgoAVFlow旨在解决使用人类第一人称视频进行机器人操作时的视角控制问题，通过共享的3D流表示来预测机器人动作、未来3D流和摄像机轨迹，并在测试时优化视角以维持任务关键的可见性。实验表明，EgoAVFlow在真实世界场景中优于之前的基于人类演示的方法，能够有效维持可见性和进行稳健的操作，无需机器人演示。</div>
</details>
</div>
<div class="card">
<div class="title">Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</div>
<div class="meta-line">Authors: Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal, Murtuza Jadliwala, Bimal Viswanath</div>
<div class="meta-line">First: 2026-02-25T18:46:30+00:00 · Latest: 2026-02-25T18:46:30+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore. To IEEE SaTML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22197v1">PDF</a> · <a href="https://github.com/mlsecviswanath/img2imgdenoiser">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#x27;s utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现成的图像到图像模型足以击败图像保护方案</div>
<div class="mono" style="margin-top:8px">生成式人工智能（GenAI）的进步导致开发了各种保护策略，以防止未经授权使用图像。这些方法依赖于在图像上添加不可感知的保护扰动，以阻止诸如风格模仿或深度伪造等滥用行为。尽管之前对这些保护的攻击需要专门的、定制的方法，但我们证明这已不再必要。我们展示了一种现成的图像到图像GenAI模型可以通过简单的文本提示重新利用为通用的“去噪器”，有效地移除各种保护扰动。在涵盖6种不同保护方案的8个案例研究中，我们的通用攻击不仅绕过了这些防御，还在保持图像对攻击者有用性的同时，优于现有的专门攻击。我们的研究结果揭示了当前图像保护领域中一个关键且普遍存在的漏洞，表明许多方案提供了虚假的安全感。我们强调迫切需要开发稳健的防御措施，并表明任何未来的保护机制都必须以现成的GenAI模型攻击为基准。代码可在以下仓库中获得：https://github.com/mlsecviswanath/img2imgdenoiser</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the vulnerability of image protection schemes by demonstrating that off-the-shelf image-to-image Generative AI models can be repurposed as generic &#x27;denoisers&#x27; using simple text prompts to remove protective perturbations. Across eight case studies involving six diverse protection schemes, the general-purpose attack outperformed existing specialized attacks while maintaining the image&#x27;s utility for adversaries. This reveals a critical vulnerability in current image protection methods, suggesting that many schemes provide a false sense of security and necessitating the development of robust defenses against off-the-shelf GenAI models.</div>
<div class="mono" style="margin-top:8px">研究通过展示可以利用现成的图像到图像生成AI模型作为通用的“去噪器”，通过简单的文本提示去除保护性干扰，来揭示图像保护方案的漏洞。在涉及六种不同保护方案的八个案例研究中，通用攻击不仅超越了现有的专门攻击，还保持了图像对攻击者的实用性。这揭示了当前图像保护方法中的关键漏洞，表明许多方案提供了虚假的安全感，并强调需要开发针对现成生成AI模型的稳健防御措施。</div>
</details>
</div>
<div class="card">
<div class="title">Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems</div>
<div class="meta-line">Authors: Georgios Kamaras, Craig Innes, Subramanian Ramamoorthy</div>
<div class="meta-line">First: 2025-10-30T16:23:46+00:00 · Latest: 2026-02-25T17:52:16+00:00</div>
<div class="meta-line">Comments: 20 pages, 18 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26656v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26656v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在指定错误领域支持的启发式适应在随机动力系统无likelihood推断中的应用</div>
<div class="mono" style="margin-top:8px">在机器人学中，无likelihood推断（LFI）可以提供适应学习代理在参数部署条件集中的领域分布。LFI假设一个任意的支持用于采样，该支持在整个初始通用先验逐步细化为更具描述性的后验过程中保持不变。然而，潜在指定错误的支持可能导致次优但错误确定的后验。为了解决这一问题，我们提出了三种启发式LFI变体：EDGE、MODE和CENTRE。每种变体都以自己的方式解释后验模式在推理步骤中的变化，并在LFI步骤中将支持的适应与后验推理结合在一起。我们首先揭示了支持指定错误的问题，并使用随机动力学基准评估我们的启发式方法。然后，我们评估启发式支持适应对动态可变形线性对象（DLO）操作任务中参数推断和策略学习的影响。对于参数化的DLO集合，推断结果提供了更精细的长度和刚度分类。当使用这些后验作为基于仿真的策略学习的领域分布时，它们会导致更稳健的对象中心代理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of potentially misspecified support in likelihood-free inference (LFI) for robotics, which can lead to suboptimal and falsely certain posteriors. To tackle this, three heuristic LFI methods—EDGE, MODE, and CENTRE—are proposed, each adapting the support based on posterior mode shifts. The methods are evaluated on stochastic dynamical benchmarks and a dynamic deformable linear object manipulation task, showing improved parameter inference and more robust agent performance.</div>
<div class="mono" style="margin-top:8px">论文针对机器人领域中潜在的采样支持不准确问题，该问题可能导致次优且虚假确定的后验分布。为此，提出了三种基于后验模式偏移的启发式LFI方法——EDGE、MODE和CENTRE，这些方法通过调整支持来改进后验推断。这些方法在随机动力学基准测试中进行了评估，并应用于动态可变形线性物体操作任务，从而提高了参数推断的精度和策略学习的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Belief Vision Language Action Models</div>
<div class="meta-line">Authors: Vaidehi Bagaria, Bijo Sebastian, Nirav Kumar Patel</div>
<div class="meta-line">First: 2026-02-24T08:02:16+00:00 · Latest: 2026-02-25T17:38:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20659v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20659v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. While semantic grounding is important, long-horizon manipulation fundamentally requires persistent, action-conditioned state representations. Current VLAs lack such representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once per task, the VLM provides high-level intent, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5 percent and 37.5 percent higher success rates on multi-stage pick-and-place and stacking tasks, respectively, compared to pi_0. It also reduces inference latency by up to five times relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show the belief module is the primary driver of performance, increasing success rates from 32.5 percent without belief to 77.5 percent with belief.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递归信念视语言行动模型</div>
<div class="mono" style="margin-top:8px">视语言行动模型必须使代理能够在部分可观测性下执行长期任务。然而，大多数现有方法仍依赖于短上下文窗口或反复查询视语言模型（VLM），这导致任务进展丢失、感知同义词下的动作重复以及高推理延迟。虽然语义定位很重要，但长期操作本质上需要持久的动作条件状态表示。当前的VLAs缺乏这样的表示，且在时间和物理推理方面表现出有限的能力，使它们不适合多阶段控制。本文引入了RB-VLA，这是一种以信念为中心的架构，通过自我监督的世界模型目标进行训练，保持一个紧凑的潜在状态编码任务相关的历史、动力学和对象交互。VLM在每次任务时查询一次，提供高层次的意图，而信念追踪任务进展，使代理在部分可观测性下具备阶段感知的、因果性的控制能力，无需存储原始观察或随时间扩展内存。信念和意图共同条件一个扩散策略，实现稳健的闭环执行。RB-VLA 在长期任务基准测试中优于先前的VLAs，分别在多阶段取放和堆叠任务中提高了52.5%和37.5%的成功率，与pi_0相比。它还将推理延迟降低了最多五倍，并消除了现有VLAs在时间步长上观察到的内存增长。消融实验表明，信念模块是性能的主要驱动因素，信念模块从无到有将成功率从32.5%提高到77.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing vision-language-action models, which are observation-driven and suffer from loss of task progress, action repetition, and high inference latency. It introduces RB-VLA, a belief-centric architecture that maintains a compact latent state for task-relevant history and dynamics. RB-VLA outperforms prior models on long-horizon tasks, achieving higher success rates and reducing inference latency. Ablation studies confirm the belief module&#x27;s critical role in performance improvement.</div>
<div class="mono" style="margin-top:8px">该研究通过引入RB-VLA，一种以信念为中心的架构，解决了现有视觉-语言-动作模型的局限性，该模型维护了一个紧凑的潜状态，用于任务相关信息的历史和动态。该模型在长时任务中表现出色，实现了更高的成功率和降低了推理延迟。消融实验表明，信念模块显著提高了性能。</div>
</details>
</div>
<div class="card">
<div class="title">QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Jingxuan Zhang, Yunta Hsieh, Zhongwei Wan, Haokun Lin, Xin Wang, Ziqi Wang, Yingtie Lei, Mi Zhang</div>
<div class="meta-line">First: 2026-02-23T19:55:54+00:00 · Latest: 2026-02-25T17:11:08+00:00</div>
<div class="meta-line">Comments: CVPR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20309v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20309v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QuantVLA：面向视觉-语言-行动模型的规模校准后训练量化</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型将感知、语言和控制统一起来，为具身智能体服务，但由于计算和内存需求迅速增加，尤其是在模型扩展到更长的时间范围和更大的骨干网络时，它们在实际部署中面临重大挑战。为了解决这些瓶颈，我们提出了QuantVLA，这是一种无需训练的后训练量化（PTQ）框架，据我们所知，这是首个针对VLA系统的PTQ方法，并且首次成功量化了扩散变压器（DiT）动作头。QuantVLA 包含三个规模校准组件：（1）一种选择性量化布局，将所有线性层（包括语言骨干和DiT）转换为整数表示，而注意力投影保持在浮点数中，以保留原始操作调度；（2）注意力温度匹配，这是一种轻量级的每头缩放机制，用于稳定注意力概率，并在推理时折叠到去量化比例中；（3）输出头平衡，这是一种每层残差接口校准，用于缓解后投影能量漂移。该框架不需要额外的训练，仅使用少量未标记的校准缓冲区，并支持低位宽权重和激活的整数内核，同时保持架构不变。在代表性的VLA模型上，QuantVLA 超过了全精度基线的任务成功率，实现了约70%的量化组件相对内存节省，并提供了1.22倍的端到端推理延迟加速，为在严格的计算、内存和功率限制下实现可扩展的低位宽具身智能提供了实际途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">QuantVLA is a training-free post-training quantization framework for vision-language-action models, addressing the compute and memory challenges of large-scale models. It introduces three scale-calibrated components: selective quantization layout, attention temperature matching, and output head balancing. QuantVLA achieves task success rates exceeding full-precision baselines, 70% relative memory savings, and a 1.22x speedup in inference latency on LIBERO models.</div>
<div class="mono" style="margin-top:8px">QuantVLA 是一种无需训练的后训练量化框架，用于解决视觉-语言-动作模型在扩展时面临的计算和内存挑战。它包含选择性量化、注意力温度匹配和输出头平衡等组件，以稳定和优化模型。QuantVLA 提高了任务成功率，将量化组件的内存使用减少了约 70%，并将端到端推理延迟加速了 1.22 倍，使其在严格的计算、内存和功率限制下更具实用性。</div>
</details>
</div>
<div class="card">
<div class="title">A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation</div>
<div class="meta-line">Authors: Georgios Kamaras, Subramanian Ramamoorthy</div>
<div class="meta-line">Venue: In IEEE Robotics and Automation Letters, Volume 10, Issue 8, August 2025, Pages 8075-8082</div>
<div class="meta-line">First: 2025-02-25T20:01:06+00:00 · Latest: 2026-02-25T17:09:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18615v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.18615v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉驱动的可变形线性物体操作中物体中心代理适应的实2仿2实分布处理方法</div>
<div class="mono" style="margin-top:8px">我们提出了一种集成（或端到端）框架，用于基于视觉感知操纵可变形线性物体（DLOs）的实2仿2实问题。使用参数化的DLO集合，我们使用无似然推断（LFI）来计算物理参数的后验分布，从而可以近似模拟每个特定DLO的行为。在训练过程中，我们使用这些后验分布进行领域随机化，在仿真中使用无模型强化学习为DLO抓取任务训练特定于物体的视知觉运动策略（即，假设只有视觉和本体感觉感知）。我们通过零样本方式在现实世界中部署仿真实训的DLO操作策略，即无需任何进一步微调来展示该方法的实用性。在此背景下，我们评估了一种流行的LFI方法在仅使用动态操作轨迹中获得的视觉和本体感觉数据对参数化DLO集合进行精细分类的能力。然后我们研究了基于仿真的策略学习和现实世界性能中结果领域分布的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper presents an integrated framework for manipulating deformable linear objects (DLOs) in the real world using visual perception. It uses likelihood-free inference to compute the posterior distributions of physical parameters for each DLO, enabling domain randomisation during simulation training. The approach trains object-specific visuomotor policies using model-free reinforcement learning and demonstrates zero-shot deployment in the real world without further fine-tuning. The study evaluates the effectiveness of this method in fine classification and its impact on sim-to-real performance.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于视觉感知操纵变形线性物体的集成框架。该框架利用似然无参数推断计算物理参数的后验分布，从而模拟DLO的行为。在使用模型自由强化学习训练视觉和本体感觉导向的策略时，该框架利用这些分布进行领域随机化。训练好的策略无需进一步微调即可在现实世界中成功部署，展示了该方法在零样本迁移中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Hongjie Fang, Shirun Tang, Mingyu Mei, Haoxiang Qin, Zihao He, Jingjing Chen, Ying Feng, Chenxi Wang, Wanxi Liu, Zaixing He, Cewu Lu, Shiquan Wang</div>
<div class="meta-line">First: 2026-02-25T16:35:24+00:00 · Latest: 2026-02-25T16:35:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22088v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://force-policy.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: https://force-policy.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>力策略：在交互框架下学习混合力-位置控制策略以应对丰富的接触操作</div>
<div class="mono" style="margin-top:8px">丰富的接触操作需要人类般的感知和力反馈整合：视觉应引导任务进展，而高频率的交互控制则需在不确定性下稳定接触。现有的基于学习的策略往往在单一网络中纠缠这些角色，权衡全局泛化与稳定局部细化之间的关系，而以控制为中心的方法通常假设已知的任务结构或仅学习控制器参数而非结构本身。在本文中，我们形式化了一个物理上合理的交互框架，即瞬时局部基，将力调节与运动执行解耦，并提出了一种从演示中恢复它的方法。基于此，我们通过提出力策略来解决这两个问题，这是一种全局-局部视觉-力策略，其中全局策略使用视觉指导自由空间动作，接触后，高频率的局部策略结合力反馈估计交互框架并执行混合力-位置控制以实现稳定的交互。在多种丰富的接触任务中的实际实验表明，与强大的基线相比，该策略在接触建立的鲁棒性、力调节的准确性以及对具有不同几何形状和物理特性的新物体的可靠泛化方面均有所提升，最终提高了接触稳定性和执行质量。项目页面：https://force-policy.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of contact-rich manipulation by proposing Force Policy, which decouples force regulation from motion execution using an interaction frame. This method combines a global policy for vision-guided free-space actions and a local policy for force feedback and hybrid force-position control upon contact. Experiments across various tasks demonstrate improved contact stability, accurate force regulation, and reliable generalization to new objects, surpassing strong baselines in both contact establishment and execution quality.</div>
<div class="mono" style="margin-top:8px">本文提出了一种混合力-位置控制策略Force Policy，通过瞬时局部基底解耦力调节与运动执行，实现全局视觉引导动作和高频力反馈控制。实验表明，Force Policy 在接触建立、力调节和对新物体的泛化方面优于强基线，提高了接触稳定性和执行质量。</div>
</details>
</div>
<div class="card">
<div class="title">FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation</div>
<div class="meta-line">Authors: Edgar Welte, Yitian Shi, Rosa Wolf, Maximillian Gilles, Rania Rayyes</div>
<div class="meta-line">First: 2026-02-25T16:06:49+00:00 · Latest: 2026-02-25T16:06:49+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22056v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowCorrect：高效交互式修正生成流策略以实现机器人操作</div>
<div class="mono" style="margin-top:8px">生成性操作策略在部署时分布偏移下可能会灾难性地失败，但许多失败是近失：机器人几乎达到了正确的姿态，并且只需一个小的纠正动作就能成功。我们提出了FlowCorrect，这是一种部署时修正框架，使用稀疏的人工干预将近失失败转化为成功，而无需重新训练整个策略。在执行过程中，人类通过轻量级的VR接口提供简短的纠正姿态干预。FlowCorrect利用这些稀疏的修正来局部适应策略，改进动作而不重新训练骨干模型，同时保持对之前学习场景的性能。我们在一个真实世界的机器人上对三个桌面任务进行了评估：拾取和放置、倒水和杯子直立。即使在低修正预算下，FlowCorrect也将难以解决的情况的成功率提高了85%，同时保持了对之前解决场景的性能。结果表明，FlowCorrect仅通过少量演示就能学习，并在真实世界机器人操作的部署时实现快速、样本高效的增量、人机交互修正生成的视知觉运动策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FlowCorrect is a deployment-time correction framework for generative manipulation policies that uses sparse human nudges to adapt the policy locally without full retraining. During execution, a human provides brief corrective pose nudges via a VR interface, which FlowCorrect uses to improve actions on near-miss failures. The framework achieves an 85% improvement in success on hard cases while maintaining performance on previously learned scenarios across three tabletop tasks. This demonstrates the effectiveness of FlowCorrect for fast, sample-efficient, and human-in-the-loop corrections in real-world robotics.</div>
<div class="mono" style="margin-top:8px">FlowCorrect 是一种部署时纠正框架，通过稀疏的人类提示来局部调整生成性操作策略，而无需重新训练。在执行过程中，人类通过轻量级的 VR 接口提供简短的纠正姿态提示，FlowCorrect 利用这些提示来改进近错失败的动作。该框架在三个桌面任务中实现了 85% 的成功率提升，同时保持了对先前学习场景的性能。这表明 FlowCorrect 在现实世界机器人中实现了快速、样本高效和人类在环的纠正。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers</div>
<div class="meta-line">Authors: Guandong Li</div>
<div class="meta-line">First: 2026-02-20T06:24:20+00:00 · Latest: 2026-02-25T15:33:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18022v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT&#x27;s multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器中的双通道注意力引导无训练图像编辑控制</div>
<div class="mono" style="margin-top:8px">基于扩散变换器（DiT）架构的扩散基础图像编辑模型需要无训练控制编辑强度。现有的注意力操作方法仅专注于键空间来调节注意力路由，而完全忽略了值空间——它控制特征聚合。本文首先揭示了DiT多模态注意力层中的键投影和值投影都表现出明显的偏差-增量结构，其中令牌嵌入紧密围绕特定层的偏差向量聚类。基于这一观察，我们提出了双通道注意力引导（DCAG），这是一种无训练框架，可以同时操作键通道（控制注意力的方向）和值通道（控制聚合的内容）。我们提供了理论分析，表明键通道通过非线性softmax函数操作，作为粗略的控制旋钮，而值通道通过线性加权求和操作，作为精细的补充。这两个维度的参数空间$(δ_k, δ_v)$能够比任何单通道方法提供更精确的编辑保真度权衡。在PIE-Bench基准（700张图像，10个编辑类别）上的大量实验表明，DCAG在所有保真度指标上都优于仅键引导，特别是在局部编辑任务如对象删除（LPIPS减少4.9%）和对象添加（LPIPS减少3.2%）方面表现最为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for training-free control over editing intensity in diffusion-based image editing models using the Diffusion Transformer (DiT) architecture. It proposes Dual-Channel Attention Guidance (DCAG), which manipulates both the Key and Value channels to control attention routing and feature aggregation, respectively. Experiments on the PIE-Bench benchmark show that DCAG outperforms Key-only guidance in all fidelity metrics, with notable improvements in localized editing tasks like object deletion and addition.</div>
<div class="mono" style="margin-top:8px">本文针对使用Diffusion Transformer (DiT)架构的基于扩散的图像编辑模型中训练-free 控制编辑强度的需求，提出了一种双通道注意力引导（DCAG）方法，该方法同时操纵Key通道和Value通道，分别控制注意力路由和特征聚合。实验结果表明，DCAG在所有保真度指标上均优于仅操纵Key通道的方法，在如对象删除和添加等局部编辑任务中表现尤为突出。</div>
</details>
</div>
<div class="card">
<div class="title">World Guidance: World Modeling in Condition Space for Action Generation</div>
<div class="meta-line">Authors: Yue Su, Sijin Chen, Haixin Shi, Mingyu Liu, Zhengshen Zhang, Ningyuan Huang, Weiheng Zhong, Zhengbang Zhu, Yuxiao Liu, Xihui Liu</div>
<div class="meta-line">First: 2026-02-25T15:27:09+00:00 · Latest: 2026-02-25T15:27:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://selen-suyue.github.io/WoGNet/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22010v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://selen-suyue.github.io/WoGNet/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界指导：条件空间中的世界建模以促进动作生成</div>
<div class="mono" style="margin-top:8px">利用未来观察建模来促进动作生成为提升视觉-语言-动作（VLA）模型的能力提供了有希望的途径。然而，现有方法难以在保持高效、可预测的未来表示和保留足够的细粒度信息以指导精确动作生成之间取得平衡。为了解决这一局限性，我们提出了WoG（世界指导）框架，该框架通过将未来观察注入动作推理管道中，将未来观察映射到紧凑的条件中。然后，VLA被训练同时预测这些压缩条件和未来动作，从而在条件空间中实现有效的世界建模以进行动作推理。我们证明，建模和预测此条件空间不仅有助于细粒度动作生成，还表现出更强的泛化能力。此外，它能够有效地从大量的人类操作视频中学习。广泛的实验在模拟和真实环境中的验证表明，我们的方法显著优于基于未来预测的现有方法。项目页面：https://selen-suyue.github.io/WoGNet/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the action generation capabilities of Vision-Language-Action models by leveraging future observation modeling. The proposed WoG framework maps future observations into compact conditions and integrates them into the action inference pipeline, allowing the model to predict both compressed conditions and future actions simultaneously. Experiments show that this approach enhances fine-grained action generation and generalization, and it performs better than existing methods based on future prediction in both simulation and real-world environments.</div>
<div class="mono" style="margin-top:8px">研究旨在通过利用未来观察建模来提升Vision-Language-Action模型的能力。提出的WoG框架将未来观察映射为紧凑的条件，使模型能够同时预测这些条件和未来动作。这种方法增强了细粒度的动作生成能力和泛化能力，并且能够有效地从人类操作视频中学习。实验表明，WoG在仿真和真实环境中的表现优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260228_0348.html">20260228_0348</a>
<a href="archive/20260227_0353.html">20260227_0353</a>
<a href="archive/20260226_0403.html">20260226_0403</a>
<a href="archive/20260225_0357.html">20260225_0357</a>
<a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0337.html">20260223_0337</a>
<a href="archive/20260222_0338.html">20260222_0338</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
