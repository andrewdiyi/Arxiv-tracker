<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-17 03:42</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260217_0342</div>
    <div class="row"><div class="card">
<div class="title">Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos</div>
<div class="meta-line">Authors: Albert J. Zhai, Kuo-Hao Zeng, Jiasen Lu, Ali Farhadi, Shenlong Wang, Wei-Chiu Ma</div>
<div class="meta-line">First: 2026-02-13T18:59:10+00:00 · Latest: 2026-02-13T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot&#x27;s ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模仿有效之处：基于人类视频的模拟筛选模块化策略学习</div>
<div class="mono" style="margin-top:8px">通过观看人类的视频来学习操作技能的能力有可能为机器人学习解锁一种新的、高度可扩展的数据来源。在这里，我们探讨了抓取操作，这类任务涉及在执行各种后抓握动作之前抓取物体。人类视频提供了学习后抓握动作的强大信号，但对于学习先决抓握行为却不太有用，尤其是对于没有人类般手的机器人。一种有前景的方法是使用模块化策略设计，利用专门的抓取生成器来生成稳定的抓取。然而，任意的稳定抓取往往与任务不兼容，阻碍了机器人执行所需的后续动作的能力。为了解决这一挑战，我们提出了感知-模拟-模仿（PSI）框架，用于使用配对的抓取轨迹筛选在模拟中处理的人类视频运动数据来训练模块化操作策略。这一模拟步骤通过添加抓取适宜性标签来扩展轨迹数据，这使得能够监督学习任务导向的抓取能力。我们通过现实世界的实验表明，我们的框架可以有效地学习精确的操作技能，而无需任何机器人数据，从而比简单地使用抓取生成器获得显著更稳健的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of learning manipulation skills from human videos by proposing a Perceive-Simulate-Imitate (PSI) framework. The method involves filtering grasp and trajectory data in simulation to generate task-compatible grasps, which are then used to train a modular policy. Experiments demonstrate that this approach enables robots to learn precise manipulation skills without relying on robot-collected data, leading to more robust performance compared to using a grasp generator alone.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过观看人类视频来使机器人学习抓取操作技能，重点关注预抓取操作任务。方法采用模块化策略设计，结合抓取生成器产生稳定抓取，并通过模拟步骤过滤和标注抓取轨迹数据，从而实现任务导向的抓取学习。实验表明，这种方法可以高效地教会机器人精确的操作技能，而无需任何机器人数据，相比直接使用抓取生成器，其性能更为稳健。</div>
</details>
</div>
<div class="card">
<div class="title">Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control</div>
<div class="meta-line">Authors: William Chen, Jagdeep Singh Bhatia, Catherine Glossop, Nikhil Mathihalli, Ria Doshi, Andy Tang, Danny Driess, Karl Pertsch, Sergey Levine</div>
<div class="meta-line">First: 2026-02-13T18:57:56+00:00 · Latest: 2026-02-13T18:57:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13193v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13193v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.
  Website: steerable-policies.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可引导的视觉-语言-行动策略：嵌入式推理和层次控制</div>
<div class="mono" style="margin-top:8px">预训练的视觉-语言模型（VLMs）可以在多种环境中进行语义和视觉推理，为机器人控制提供有价值的常识先验。然而，将这种知识有效地融入机器人行为仍然是一个开放的挑战。先前的方法通常采用分层方法，其中VLMs对要由低级策略执行的高级命令进行推理，例如视觉-语言-行动模型（VLAs）。VLMs与VLAs之间的接口通常是自然语言任务指令，这从根本上限制了VLM推理对低级行为的引导程度。因此，我们引入了可引导策略：在各种抽象层次（如子任务、动作和像素坐标）的丰富合成命令上训练的VLAs。通过提高低级可控性，可引导策略可以解锁预训练知识在VLMs中的应用，从而实现更好的任务泛化。我们通过使用学习到的高级嵌入式推理器和通过上下文学习提示VLM推理命令抽象来控制这些可引导策略，展示了这一优势。在广泛的现实世界操作实验中，这两种新颖的方法优于先前的嵌入式推理VLAs和基于VLM的分层基线，包括在具有挑战性的泛化和长时序任务上的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the controllability of robotic behaviors by grounding vision-language model reasoning in low-level actions. The method involves training vision-language-action models (VLAs) on synthetic commands at different levels of abstraction, allowing for more precise control. Key experimental findings show that these steerable policies outperform previous approaches in real-world manipulation tasks, particularly in generalization and long-horizon scenarios.</div>
<div class="mono" style="margin-top:8px">研究旨在通过利用预训练的视觉-语言模型（VLMs）提供常识先验来增强机器人的行为控制。方法是通过在不同抽象层次上训练视觉-语言-动作模型（VLAs）丰富的合成命令，以实现更好的低级可控性。关键实验发现表明，这些可引导的策略在现实世界的操作任务中表现优于先前的方法，特别是在需要泛化和长期规划的任务中。</div>
</details>
</div>
<div class="card">
<div class="title">UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph</div>
<div class="meta-line">Authors: Haichao Liu, Yuanjiang Xue, Yuheng Zhou, Haoyuan Deng, Yinan Liang, Lihua Xie, Ziwei Wang</div>
<div class="meta-line">First: 2026-02-13T16:47:26+00:00 · Latest: 2026-02-13T16:47:26+00:00</div>
<div class="meta-line">Comments: 15 pages, 12 figures, 6 tables, project page: https://henryhcliu.github.io/unimanip</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13086v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13086v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://henryhcliu.github.io/unimanip">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system&#x27;s robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniManip：基于双层代理操作图的一般用途零样本机器人操作</div>
<div class="mono" style="margin-top:8px">实现一般用途的机器人操作需要机器人能够无缝地将高层语义意图与低层物理交互结合在非结构化环境中。然而，现有的方法在零样本泛化方面存在缺陷：端到端的视觉-语言-动作（VLA）模型往往缺乏完成长期任务所需的精度，而传统的分层规划者在面对开放世界变化时则表现出语义僵化。为了解决这一问题，我们提出了UniManip，这是一种基于双层代理操作图（AOG）的框架，该框架统一了语义推理和物理接地。通过结合高层代理层进行任务编排和低层场景层进行动态状态表示，系统能够持续地将抽象规划与几何约束对齐，从而实现稳健的零样本执行。与静态流水线不同，UniManip 作为一个动态代理循环运行：它积极地从非结构化感知中实例化对象中心的场景图，通过一个安全意识的局部规划器将这些表示参数化为无碰撞轨迹，并利用结构化记忆自主诊断和恢复执行失败。广泛的实验验证了该系统在未见过的对象和任务上的稳健零样本能力，与最先进的VLA和分层基线相比，成功率分别高出22.5%和25.0%。值得注意的是，该系统能够在无需微调或重新配置的情况下，直接从固定基座设置转移到移动操作。我们的开源项目页面可以在 https://henryhcliu.github.io/unimanip 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniManip addresses the challenge of general-purpose robotic manipulation by integrating semantic reasoning and physical grounding through a Bi-level Agentic Operational Graph. It achieves robust zero-shot execution by continuously aligning abstract planning with geometric constraints, using a high-level Agentic Layer for task orchestration and a low-level Scene Layer for dynamic state representation. Experiments show a 22.5% and 25.0% higher success rate compared to state-of-the-art Vision-Language-Action and hierarchical baselines, respectively, and enables direct zero-shot transfer from fixed-base to mobile manipulation without fine-tuning.</div>
<div class="mono" style="margin-top:8px">UniManip 通过结合语义推理和物理接地的双层智能操作图来解决通用机器人操作的挑战。它通过持续将抽象规划与几何约束对齐，使用高层智能层进行任务编排和低层场景层进行动态状态表示，实现稳健的零样本执行。实验结果显示，与最先进的视觉-语言-动作和分层基线相比，成功率分别高出22.5%和25.0%，并且可以直接从固定基座转移到移动操作而无需微调或重新配置。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Robot Control: Flexible but still Fragile</div>
<div class="meta-line">Authors: Oscar Lima, Marc Vinci, Martin Günther, Marian Renz, Alexander Sung, Sebastian Stock, Johannes Brust, Lennart Niecksch, Zongyao Yi, Felix Igelbrink, Benjamin Kisliuk, Martin Atzmueller, Joachim Hertzberg</div>
<div class="meta-line">First: 2026-02-13T16:43:34+00:00 · Latest: 2026-02-13T16:43:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13081v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13081v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理型AI在机器人控制中的应用：灵活但仍然脆弱</div>
<div class="mono" style="margin-top:8px">近期工作利用生成模型的能力和常识先验进行机器人控制。在本文中，我们提出了一种代理控制系统，在该系统中，一个具备推理能力的语言模型通过在迭代规划和执行循环中选择和调用机器人技能来计划和执行任务。我们在两个物理机器人平台上部署了该系统，分别在室内移动操作（Mobipick）中的桌面抓取、放置和盒子插入，以及自主农业导航和传感（Valdemar）中。这两个场景都涉及不确定性、部分可观测性、传感器噪声和模糊的自然语言指令。该系统展示了其规划和决策过程的结构化内省，通过显式的事件检查来应对外部事件，并支持操作员干预以修改或重新定向正在进行的执行。在两个平台上，我们的概念验证实验揭示了显著的脆弱性，包括非确定性的次优行为、指令执行错误以及对提示规范的高度敏感性。同时，该架构是灵活的：将该系统转移到不同的机器人和任务领域主要需要更新系统的提示（领域模型、可利用性和行动目录），并重新绑定相同的工具接口到平台特定的技能API。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces an agentic control system that uses a reasoning-capable language model to plan and execute tasks by selecting and invoking robot skills. Deployed on two physical robots for indoor manipulation and agricultural tasks, the system demonstrates flexibility but also significant fragility, including non-deterministic behavior and sensitivity to prompt specification. Despite these issues, the architecture allows for easy transfer to different tasks and robots with minimal updates to the system prompt and skill bindings.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种使用推理能力强的语言模型来规划和执行任务的系统，通过选择和调用机器人技能。该系统部署在两个物理机器人上，用于室内操作和农业任务，展示了灵活性但也存在显著的脆弱性，包括非确定性行为和对提示规格的敏感性。尽管存在这些问题，该架构通过更新系统提示和重新绑定技能，可以轻松地转移到不同的任务和机器人。</div>
</details>
</div>
<div class="card">
<div class="title">SENSE-STEP: Learning Sim-to-Real Locomotion for a Sensory-Enabled Soft Quadruped Robot</div>
<div class="meta-line">Authors: Storm de Kam, Ebrahim Shahabi, Cosimo Della Santina</div>
<div class="meta-line">First: 2026-02-13T16:37:29+00:00 · Latest: 2026-02-13T16:37:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13078v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust closed-loop locomotion remains challenging for soft quadruped robots due to high-dimensional dynamics, actuator hysteresis, and difficult-to-model contact interactions, while conventional proprioception provides limited information about ground contact. In this paper, we present a learning-based control framework for a pneumatically actuated soft quadruped equipped with tactile suction-cup feet, and we validate the approach experimentally on physical hardware. The control policy is trained in simulation through a staged learning process that starts from a reference gait and is progressively refined under randomized environmental conditions. The resulting controller maps proprioceptive and tactile feedback to coordinated pneumatic actuation and suction-cup commands, enabling closed-loop locomotion on flat and inclined surfaces. When deployed on the real robot, the closed-loop policy outperforms an open-loop baseline, increasing forward speed by 41% on a flat surface and by 91% on a 5-degree incline. Ablation studies further demonstrate the role of tactile force estimates and inertial feedback in stabilizing locomotion, with performance improvements of up to 56% compared to configurations without sensory feedback.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SENSE-STEP: 学习基于感官的软腿足机器人从模拟到现实的运动控制</div>
<div class="mono" style="margin-top:8px">对于软腿足机器人而言，由于高维动力学、执行器滞后以及难以建模的接触交互，稳健的闭环运动控制仍然具有挑战性，而传统的本体感觉提供的关于地面接触的信息有限。在本文中，我们提出了一种基于学习的控制框架，用于装备有触觉吸盘脚的气动软腿足机器人，并通过物理硬件实验验证了该方法。控制策略通过一个分阶段的学习过程在模拟中进行训练，该过程从参考步态开始，并在随机化环境条件下逐步优化。最终生成的控制器将本体感觉和触觉反馈映射到协调的气动执行和吸盘命令，使机器人能够在平坦和倾斜的表面上实现闭环运动。当部署到实际机器人上时，闭环策略在平坦表面上的速度比开环基线提高了41%，在5度倾斜面上提高了91%。消融研究进一步表明，触觉力估计和惯性反馈在稳定运动中的作用，与没有感官反馈的配置相比，性能提高了最多56%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robust closed-loop locomotion for soft quadruped robots by developing a learning-based control framework. The approach uses tactile suction-cup feet and is trained in simulation, then validated on a physical robot. The controller improves performance, increasing forward speed by 41% on flat surfaces and 91% on a 5-degree incline compared to an open-loop baseline. Ablation studies show that tactile and inertial feedback are crucial for stabilizing locomotion, with up to 56% performance improvements.</div>
<div class="mono" style="margin-top:8px">本文通过开发基于学习的控制框架来解决软四足机器人稳健闭环运动的挑战。该方法使用触觉吸盘脚，并在模拟中通过分阶段的过程进行训练，然后在物理硬件上进行验证。实验结果表明，闭环策略显著提高了机器人的性能，使其在平坦地面上的速度提高了41%，在5度坡面上的速度提高了91%，相比开环基线。消融研究进一步强调了触觉和惯性反馈在稳定运动中的作用。</div>
</details>
</div>
<div class="card">
<div class="title">Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets</div>
<div class="meta-line">Authors: Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira</div>
<div class="meta-line">First: 2026-01-14T16:25:13+00:00 · Latest: 2026-02-13T15:34:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09605v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.09605v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this setting, MANGO outperforms all other image translation methods we tested. In certain real-world tabletop manipulation tasks, MANGO augmentation increases shifted-view success rates by over 40 percentage points compared to policies trained without augmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从固定摄像头数据集实现鲁棒视点策略的仿真实验图像翻译</div>
<div class="mono" style="margin-top:8px">基于视觉的机器人操作策略取得了近期的重大成功，但仍然对诸如摄像头视点变化等分布变化较为脆弱。机器人演示数据稀缺，且往往缺乏适当的视点变化。仿真提供了以全面覆盖不同视点的方式大规模收集机器人演示数据的方法，但带来了视觉仿真实验图像翻译的挑战。为弥合这一差距，我们提出了一种名为MANGO的无配对图像翻译方法，该方法具有新颖的分割条件InfoNCE损失、高度正则化的判别器设计以及修改后的PatchNCE损失。我们发现这些元素对于保持仿真实验图像翻译过程中的视点一致性至关重要。在训练MANGO时，我们只需要少量来自现实世界的固定摄像头数据，但我们的方法可以通过翻译模拟观察生成多种未见过的视点。在这种设置下，MANGO在我们测试的所有图像翻译方法中表现最佳。在某些现实世界的桌面操作任务中，MANGO增强提高了超过40个百分点的视点变化成功率，相较于未增强的策略。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Native Continuation for Action Chunking Flow Policies</div>
<div class="meta-line">Authors: Yufeng Liu, Hang Yu, Juntu Zhao, Bocheng Li, Di Zhang, Mingzhu Li, Wenxuan Wu, Yingdong Hu, Junyuan Xie, Junliang Guo, Dequan Wang, Yang Gao</div>
<div class="meta-line">First: 2026-02-13T14:56:06+00:00 · Latest: 2026-02-13T14:56:06+00:00</div>
<div class="meta-line">Comments: Project page: https://lyfeng001.github.io/Legato/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12978v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lyfeng001.github.io/Legato/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习原生延续以实现动作分块流程策略</div>
<div class="mono" style="margin-top:8px">动作分块使视觉语言动作（VLA）模型能够实时运行，但简单的分块执行往往在分块边界处表现出不连续性。实时分块（RTC）可以缓解这一问题，但它对外部策略而言，导致了虚假的多模态切换和不内在平滑的轨迹。我们提出了Legato，一种动作分块流程基VLA策略的训练时延续方法。具体而言，Legato从已知动作和噪声的混合体的时间表中初始化去噪，使模型接触到部分动作信息。此外，Legato重塑了学习到的流程动力学，以确保在每步指导下训练和推理中的去噪过程保持一致。Legato还在训练期间使用随机时间表条件来支持变化的推理延迟并实现可控的平滑度。实验证明，Legato生成更平滑的轨迹并在执行过程中减少了虚假的多模态切换，从而减少了犹豫并缩短了任务完成时间。广泛的现实世界实验表明，Legato在五个操作任务中始终优于RTC，分别在轨迹平滑度和任务完成时间上提高了约10%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of discontinuities in action chunking for Vision Language Action (VLA) models, proposing Legato, a training-time continuation method. Legato initializes denoising from a schedule-shaped mixture of known actions and noise, and reshapes flow dynamics to ensure consistency between training and inference. Experimental results show that Legato produces smoother trajectories and reduces spurious multimodal switching, leading to less hesitation and shorter task completion time. Legato outperforms Real-Time Chunking (RTC) across five manipulation tasks, improving trajectory smoothness and task completion time by about 10%.</div>
<div class="mono" style="margin-top:8px">论文针对Vision Language Action (VLA)模型中动作分块导致的不连续性问题，提出了Legato，一种训练时的继续方法。Legato通过混合已知动作和噪声初始化去噪，重塑流动力学以确保训练和推理之间的一致性，并在训练中使用随机调度条件以支持不同的推理延迟。实验表明，Legato生成了更平滑的轨迹，减少了不必要的多模态切换，相比Real-Time Chunking (RTC)，任务完成时间提高了约10%。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators</div>
<div class="meta-line">Authors: Bernhard Wullt, Johannes Köhler, Per Mattsson, Mikeal Norrlöf, Thomas B. Schön</div>
<div class="meta-line">First: 2025-08-29T14:45:54+00:00 · Latest: 2026-02-13T14:41:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.21677v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.21677v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial manipulators are normally operated in cluttered environments, making safe motion planning important. Furthermore, the presence of model-uncertainties make safe motion planning more difficult. Therefore, in practice the speed is limited in order to reduce the effect of disturbances. There is a need for control methods that can guarantee safe motions that can be executed fast. We address this need by suggesting a novel model predictive control (MPC) solution for manipulators, where our two main components are a robust tube MPC and a corridor planning algorithm to obtain collision-free motion. Our solution results in a convex MPC, which we can solve fast, making our method practically useful. We demonstrate the efficacy of our method in a simulated environment with a 6 DOF industrial robot operating in cluttered environments with uncertainties in model parameters. We outperform benchmark methods, both in terms of being able to work under higher levels of model uncertainties, while also yielding faster motion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有碰撞避免保证的鲁棒凸模型预测控制方法及其在机器人操作臂中的应用</div>
<div class="mono" style="margin-top:8px">工业操作臂通常在拥挤的环境中操作，因此安全运动规划非常重要。此外，模型不确定性使得安全运动规划更加困难。因此，在实践中，速度受到限制以减少干扰的影响。需要能够保证安全且快速执行的控制方法。我们通过提出一种新颖的机器人操作臂模型预测控制（MPC）解决方案来满足这一需求，其中我们两个主要组成部分是鲁棒管MPC和走廊规划算法以获得无碰撞运动。我们的解决方案产生了一个凸MPC，我们可以快速求解，使我们的方法具有实际用途。我们在具有模型参数不确定性且环境拥挤的模拟环境中使用一个6自由度工业机器人演示了我们方法的有效性。我们比基准方法在能够处理更高水平的模型不确定性的同时，还提供了更快的运动。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of safe and fast motion planning for industrial manipulators in cluttered environments, where model uncertainties can affect motion safety. It proposes a robust convex model predictive control (MPC) method combined with a corridor planning algorithm to ensure collision-free paths. The method is demonstrated to be effective in a simulated environment with a 6 DOF industrial robot, showing better performance under higher model uncertainties and faster motion compared to benchmark methods.</div>
<div class="mono" style="margin-top:8px">论文针对工业 manipulator 在充满障碍物的环境中操作时，由于模型不确定性导致的安全和快速运动规划问题。提出了一种鲁棒的凸模型预测控制 (MPC) 方法结合走廊规划算法，以确保路径无碰撞。该方法产生了一个计算效率高的 MPC 解决方案，能够在保持安全的同时实现更快的运动，特别是在较高模型不确定性下。实验在具有 6 自由度的机器人模拟环境中验证了该方法优于基准方法的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Finite-Width Neural Tangent Kernels from Feynman Diagrams</div>
<div class="meta-line">Authors: Max Guillen, Philipp Misof, Jan E. Gerken</div>
<div class="meta-line">First: 2025-08-15T15:02:40+00:00 · Latest: 2026-02-13T12:42:52+00:00</div>
<div class="meta-line">Comments: 12 pages + appendices</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.11522v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.11522v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural tangent kernels (NTKs) are a powerful tool for analyzing deep, non-linear neural networks. In the infinite-width limit, NTKs can easily be computed for most common architectures, yielding full analytic control over the training dynamics. However, at infinite width, important properties of training such as NTK evolution or feature learning are absent. Nevertheless, finite width effects can be included by computing corrections to the Gaussian statistics at infinite width. We introduce Feynman diagrams for computing finite-width corrections to NTK statistics. These dramatically simplify the necessary algebraic manipulations and enable the computation of layer-wise recursion relations for arbitrary statistics involving preactivations, NTKs and certain higher-derivative tensors (dNTK and ddNTK) required to predict the training dynamics at leading order. We demonstrate the feasibility of our framework by extending stability results for deep networks from preactivations to NTKs and proving the absence of finite-width corrections for scale-invariant nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We numerically implement the complete set of equations necessary to compute the first-order corrections for arbitrary inputs and demonstrate that the results follow the statistics of sampled neural networks for widths $n\gtrsim 20$.</div></details>
</div>
<div class="card">
<div class="title">SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies</div>
<div class="meta-line">Authors: Thies Oelerich, Gerald Ebmer, Christian Hartl-Nesic, Andreas Kugi</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-13T10:23:43+00:00 · Latest: 2026-02-13T10:23:43+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12794v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12794v1">PDF</a> · <a href="https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeFlowMPC：基于学习策略的机器人 manipulator 预测与安全轨迹规划</div>
<div class="mono" style="margin-top:8px">随着机器人逐渐融入日常生活，带来了许多重大挑战。与传统的工业应用相比，需要更多的灵活性和实时反应性。基于学习的方法可以根据演示轨迹训练强大的策略，使机器人能够泛化到类似的情况。然而，这些黑盒模型缺乏可解释性和严格的安全性保证。基于优化的方法则提供了这些保证，但缺乏所需的灵活性和泛化能力。本文提出了一种结合流匹配和在线优化的 SafeFlowMPC 方法，以结合学习和优化的优点。该方法在所有时间点都能保证安全性，并通过使用次优模型预测控制形式设计来满足实时执行的需求。SafeFlowMPC 在 KUKA 7-DoF 机器人上进行了三个真实世界的实验，包括两个抓取实验和一个动态人机物体交接实验，取得了良好的性能。有关实验的视频可在 http://www.acin.tuwien.ac.at/42d6 获取。代码可在 https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SafeFlowMPC is a method that combines flow matching and online optimization to address the challenges of real-time reactivity and safety in robot manipulation. It leverages learning-based policies for flexibility and generalization while ensuring rigorous safety guarantees through optimization. The method was tested on a KUKA 7-DoF manipulator in three real-world experiments, demonstrating strong performance in grasping and dynamic human-robot object handover tasks. The approach guarantees safety at all times and is designed for real-time execution. Video and code are available for further reference.</div>
<div class="mono" style="margin-top:8px">SafeFlowMPC 是一种结合流匹配和在线优化的方法，旨在提高机器人 manipulator 的安全性和灵活性。该方法利用基于学习的策略实现任务泛化，同时通过优化技术确保严格的安全部署。该方法在 KUKA 7-DoF 机器人上进行了三项真实世界的实验测试，展示了其在抓取和动态人机物体交接任务中的强大性能。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Coordinated Online Behavior: Trade-offs and Strategies</div>
<div class="meta-line">Authors: Lorenzo Mannocci, Stefano Cresci, Matteo Magnani, Anna Monreale, Maurizio Tesconi</div>
<div class="meta-line">First: 2025-07-16T10:25:45+00:00 · Latest: 2026-02-13T10:19:08+00:00</div>
<div class="meta-line">Comments: Postprint of the article published in the Information Sciences journal. Please, cite accordingly</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12108v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.12108v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing multimodal coordinated behavior, examining the trade-off between weakly and strongly integrated models and their ability to capture broad versus tightly aligned coordination patterns. By contrasting monomodal, flattened, and multimodal methods, we evaluate the distinct contributions of each modality and the impact of different integration strategies. Our findings show that while not all modalities provide unique insights, multimodal analysis consistently offers a more informative representation of coordinated behavior, preserving structures that monomodal and flattened approaches often lose. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态协调在线行为：权衡与策略</div>
<div class="mono" style="margin-top:8px">跨越从有益的集体行动到有害的操纵如虚假信息运动的协调在线行为已成为数字生态系统分析的关键焦点。传统方法通常依赖于单模态方法，专注于单一类型的互动，如共转发或共标签，或者独立地考虑多种模态。然而，这些方法可能会忽略多模态协调中固有的复杂动态。本研究比较了不同实现多模态协调行为的方式，探讨了弱整合和强整合模型之间的权衡以及它们捕捉广泛协调模式与紧密对齐协调模式的能力。通过对比单模态、扁平化和多模态方法，我们评估了每种模态的独特贡献以及不同整合策略的影响。我们的研究结果表明，虽然并非所有模态都提供独特的见解，但多模态分析始终提供了一种更具有信息量的协调行为表示，保留了单模态和扁平化方法通常会丢失的结构。这项工作增强了检测和分析协调在线行为的能力，为保护数字平台的完整性提供了新的视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the trade-offs and strategies in analyzing coordinated online behavior across multiple modalities, such as text and interactions. By comparing monomodal, flattened, and multimodal methods, the research finds that multimodal analysis provides a more informative representation of coordinated behavior, preserving structures that monomodal and flattened approaches often overlook. This work enhances the ability to detect and analyze coordinated online behavior, which is crucial for safeguarding digital platforms.</div>
<div class="mono" style="margin-top:8px">该研究探讨了多模态下的协调在线行为，解决了单一模态方法的局限性。通过比较弱集成和强集成模型，研究指出，多模态分析提供了更丰富的协调行为表示，保留了单一模态和扁平化方法通常丢失的结构。研究结果表明，虽然并非所有模态都提供独特的见解，但多模态分析对于检测和分析协调的在线行为至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models</div>
<div class="meta-line">Authors: Nick Heppert, Minh Quang Nguyen, Abhinav Valada</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-13T09:04:18+00:00 · Latest: 2026-02-13T09:04:18+00:00</div>
<div class="meta-line">Comments: ICRA 2026, 8 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12734v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12734v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning is a popular paradigm to teach robots new tasks, but collecting robot demonstrations through teleoperation or kinesthetic teaching is tedious and time-consuming. In contrast, directly demonstrating a task using our human embodiment is much easier and data is available in abundance, yet transfer to the robot can be non-trivial. In this work, we propose Real2Gen to train a manipulation policy from a single human demonstration. Real2Gen extracts required information from the demonstration and transfers it to a simulation environment, where a programmable expert agent can demonstrate the task arbitrarily many times, generating an unlimited amount of data to train a flow matching policy. We evaluate Real2Gen on human demonstrations from three different real-world tasks and compare it to a recent baseline. Real2Gen shows an average increase in the success rate of 26.6% and better generalization of the trained policy due to the abundance and diversity of training data. We further deploy our purely simulation-trained policy zero-shot in the real world. We make the data, code, and trained models publicly available at real2gen.cs.uni-freiburg.de.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用生成型基础模型扩展单个人类演示的模仿学习</div>
<div class="mono" style="margin-top:8px">模仿学习是一种流行的教机器人新任务的范式，但通过遥操作或示教收集机器人的演示数据是繁琐且耗时的。相比之下，直接使用我们的人体演示任务要容易得多，数据也十分丰富，但将其转移到机器人上却可能不那么简单。在本工作中，我们提出了Real2Gen，用于从单个人类演示中训练操作策略。Real2Gen 从演示中提取所需信息，并将其转移到模拟环境中，模拟环境中的可编程专家代理可以任意多次演示任务，生成无限量的数据来训练流匹配策略。我们在三个不同的真实世界任务的人类演示上评估了Real2Gen，并将其与最近的基线进行了比较。Real2Gen 的成功率平均提高了26.6%，并且由于训练数据的丰富性和多样性，训练策略的泛化能力更好。我们进一步将仅在模拟中训练的策略零样本部署到真实世界中。我们将在 real2gen.cs.uni-freiburg.de 公开数据、代码和训练模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of transferring human demonstrations to robots in imitation learning. Real2Gen extracts information from a single human demonstration and uses it to generate an unlimited amount of data in a simulation environment, which is then used to train a manipulation policy. The method shows an average 26.6% increase in success rate and better generalization compared to a recent baseline. The policy trained in simulation is also successfully deployed in the real world without additional fine-tuning. The data, code, and models are publicly available.</div>
<div class="mono" style="margin-top:8px">该研究解决了将人类演示转移到机器人中的挑战。Real2Gen 从单个人类演示中提取信息，并在模拟环境中生成无限数据，用于训练操作策略。该方法在成功率上平均提高了26.6%，并且具有更好的泛化能力。训练好的策略还成功在真实世界中零样本部署。数据、代码和模型已公开提供。</div>
</details>
</div>
<div class="card">
<div class="title">TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions</div>
<div class="meta-line">Authors: Wei Zhu, Irfan Tito Kurniawan, Ye Zhao, Mistuhiro Hayashibe</div>
<div class="meta-line">First: 2026-02-13T08:54:05+00:00 · Latest: 2026-02-13T08:54:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12724v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12724v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRANS: 地形感知强化学习在社交互动下四足机器人敏捷导航</div>
<div class="mono" style="margin-top:8px">本研究介绍了TRANS：地形感知强化学习在社交互动下的敏捷导航，这是一种用于不规则地形上四足社交导航的深度强化学习(DRL)框架。传统的四足导航通常将运动规划与运动控制分开，忽视了整体约束和地形感知。另一方面，端到端的方法更加集成，但需要高频次的传感，这通常噪声较大且计算成本高。此外，大多数现有方法假设静态环境，限制了其在人群密集环境中的应用。为了解决这些限制，我们提出了一种两阶段训练框架，包含三个DRL管道。(1) TRANS-Loco采用不对称的演员-评论家(AC)模型进行四足运动，能够在没有显式地形或接触观察的情况下穿越不平地形。(2) TRANS-Nav应用对称的AC框架进行社交导航，直接将变换后的LiDAR数据映射到以差动驱动为动力学的自我代理动作。(3) 统一管道TRANS将TRANS-Loco和TRANS-Nav集成，支持在不平且社交互动环境中的地形感知四足导航。与运动和社交导航基线的全面基准测试表明了TRANS的有效性。硬件实验进一步证实了其从仿真到现实的转移潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces TRANS, a deep reinforcement learning framework for agile navigation of quadruped robots under social interactions. It addresses the limitations of conventional methods by proposing a two-stage training framework with three DRL pipelines: TRANS-Loco for quadrupedal locomotion, TRANS-Nav for social navigation, and TRANS for integrated navigation. The framework enables traversal of uneven terrains and supports social interactions without explicit terrain observations. Experimental results show the effectiveness of TRANS against baseline methods and confirm its potential for sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">该研究提出了TRANS，一种用于不规则地形上四足机器人的社会导航的深度强化学习框架。通过提出两阶段训练框架和三个DRL管道（TRANS-Loco用于四足运动，TRANS-Nav用于社会导航，TRANS用于集成导航）来解决传统方法的局限性。实验结果表明，TRANS在运动和社交导航任务中均优于现有基线，并且硬件实验进一步验证了其从仿真到现实的转移潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios</div>
<div class="meta-line">Authors: Tian Gao, Celine Tan, Catherine Glossop, Timothy Gao, Jiankai Sun, Kyle Stachowicz, Shirley Wu, Oier Mees, Dorsa Sadigh, Sergey Levine, Chelsea Finn</div>
<div class="meta-line">First: 2026-02-09T09:54:02+00:00 · Latest: 2026-02-13T08:14:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08440v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08440v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://steervla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SteerVLA：在长尾驾驶场景中引导视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">自主驾驶中的一个基本挑战是将高层语义推理与低层反应控制相结合，以处理长尾事件并实现稳健驾驶。虽然在大规模网络数据上训练的大型视觉-语言模型（VLMs）提供了强大的常识推理能力，但它们缺乏实现安全车辆控制所需的实际经验。我们认为，有效的自主代理应该利用VLM的世界知识来引导可调节的驾驶策略，以实现低层策略控制下的稳健驾驶。为此，我们提出了SteerVLA，它利用VLM的推理能力生成细粒度的语言指令，以引导视觉-语言-行动（VLA）驾驶策略。我们方法的关键在于VLM与VLA之间丰富的语言接口，这使得高层策略能够更有效地将其推理与低层策略的控制输出联系起来。为了与车辆控制对齐提供细粒度的语言监督，我们利用VLM对现有驾驶数据进行详细语言注解的增强，我们发现这对于有效的推理和可调节性至关重要。我们在一个具有挑战性的闭环基准上评估了SteerVLA，结果显示其总体驾驶得分比最先进的方法高出4.77分，在长尾子集上高出8.04分。项目网站可访问：https://steervla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SteerVLA addresses the challenge of integrating high-level semantic reasoning with low-level reactive control in autonomous driving. It proposes a method that uses vision-language models to generate fine-grained language instructions to guide a vision-language-action driving policy. The key innovation is a rich language interface that allows high-level reasoning to be grounded in low-level control outputs. SteerVLA outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset in a closed-loop benchmark evaluation.</div>
<div class="mono" style="margin-top:8px">SteerVLA 解决了将高层次语义推理与低层次反应控制集成到自动驾驶中的挑战。它提出了一种方法，使用视觉语言模型生成细粒度的语言指令来引导视觉语言行动驾驶策略。关键创新在于一个丰富的语言接口，使高层次推理能够基于低层次控制输出进行接地。SteerVLA 在封闭环基准测试中整体驾驶得分比最先进的方法高出 4.77 分，在长尾子集上高出 8.04 分。</div>
</details>
</div>
<div class="card">
<div class="title">ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training</div>
<div class="meta-line">Authors: Rushuai Yang, Hecheng Wang, Chiming Liu, Xiaohan Yan, Yunlong Wang, Xuan Du, Shuoyu Yue, Yongcheng Liu, Chuheng Zhang, Lizhe Qi, Yi Chen, Wei Shan, Maoqing Yao</div>
<div class="meta-line">First: 2026-02-13T07:46:37+00:00 · Latest: 2026-02-13T07:46:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12691v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12691v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALOE：视觉-语言-动作模型后训练的行动级离策略评估</div>
<div class="mono" style="margin-top:8px">我们研究如何通过在线强化学习（RL）在现实世界环境中改进大型基础视觉-语言-动作（VLA）系统。这一过程的核心是价值函数，它为从经验中引导VLA学习提供学习信号。实践中，价值函数是从来自不同数据源的轨迹片段中估计出来的，包括历史策略和间歇的人工干预。从混合数据中估计当前行为质量的价值函数本质上是一个离策略评估问题。然而，先前的工作通常采用保守的在策略估计以确保稳定性，这避免了直接评估当前高容量策略，并限制了学习效果。在本文中，我们提出了ALOE，一种针对VLA后训练的行动级离策略评估框架。ALOE 使用基于片段的时间差自助法来评估单个行动序列，而不是预测最终任务结果。这种设计在稀疏奖励下提高了关键行动片段的有效归因，并支持稳定的策略改进。我们在三个实际操作任务上评估了该方法，包括智能手机包装作为高精度任务，洗衣折叠作为长期展望的可变形物体任务，以及涉及多对象感知的双臂拾取放置。在所有任务中，ALOE 提高了学习效率，而没有牺牲执行速度，表明离策略RL可以以可靠的方式重新引入现实世界的VLA后训练。有关视频和额外材料，请参阅我们的项目网站。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance large vision-language-action systems through online reinforcement learning in real-world settings by addressing the off-policy evaluation problem. ALOE, an action-level off-policy evaluation framework, is proposed to improve credit assignment for critical action chunks and support stable policy improvement. Experiments on three real-world tasks demonstrate that ALOE enhances learning efficiency without affecting execution speed, enabling reliable off-policy RL for real-world VLA post-training.</div>
<div class="mono" style="margin-top:8px">本文探讨了通过在线强化学习在现实世界中改进大型视觉-语言-动作（VLA）系统的问题。提出了ALOE，一种基于动作级别的离策略评估框架，使用基于分块的时间差分递推评估单个动作序列。这种方法增强了有效的信用分配，并在稀疏奖励下支持稳定的策略改进。该方法在三个实际任务上进行了评估，展示了改进的学习效率而不影响执行速度，从而重新引入离策略RL用于现实世界的VLA后训练。</div>
</details>
</div>
<div class="card">
<div class="title">Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution</div>
<div class="meta-line">Authors: Rui Cai, Jun Guo, Xinze He, Piaopiao Jin, Jie Li, Bingxuan Lin, Futeng Liu, Wei Liu, Fei Ma, Kun Ma, Feng Qiu, Heng Qu, Yifei Su, Qiao Sun, Dong Wang, Donghao Wang, Yunhong Wang, Rujie Wu, Diyun Xiang, Yu Yang, Hangjun Ye, Yuan Zhang, Quanyun Zhou</div>
<div class="meta-line">First: 2026-02-13T07:30:43+00:00 · Latest: 2026-02-13T07:30:43+00:00</div>
<div class="meta-line">Comments: Project page: https://xiaomi-robotics-0.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12684v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xiaomi-robotics-0.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小米机器人-0：开源的视觉-语言-行动模型及其实时执行</div>
<div class="mono" style="margin-top:8px">在本报告中，我们介绍了小米机器人-0，这是一种针对高性能和快速平滑实时执行优化的先进视觉-语言-行动（VLA）模型。我们的方法的关键在于精心设计的训练配方和部署策略。小米机器人-0首先在大规模跨体态机器人轨迹和视觉-语言数据上进行预训练，赋予其广泛且通用的行动生成能力，同时避免了对底层预训练VLM的视觉语义知识的灾难性遗忘。在后续训练中，我们提出了一些技术来训练VLA模型以异步执行，以解决实际机器人滚动过程中的推理延迟问题。在部署过程中，我们仔细对连续预测动作片段的时间步进行对齐，以确保连续且无缝的实时滚动。我们广泛地在仿真基准测试和两个需要精确和灵巧的双臂操作的挑战性实际机器人任务中评估了小米机器人-0。结果显示，我们的方法在所有仿真基准测试中均达到了最先进的性能。此外，小米机器人-0可以使用消费级GPU在实际机器人上快速且平滑地滚动，实现了两个实际机器人任务的高成功率和高吞吐量。为了促进未来的研究，代码和模型检查点已开源于https://xiaomi-robotics-0.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Xiaomi-Robotics-0 is an advanced VLA model optimized for real-time execution, pre-trained on large-scale robot trajectories and vision-language data to ensure broad action capabilities. Post-training techniques address inference latency, and deployment strategies ensure continuous real-time rollouts. The model achieves state-of-the-art performance in simulation benchmarks and high success rates on real-robot tasks using a consumer-grade GPU.</div>
<div class="mono" style="margin-top:8px">Xiaomi-Robotics-0 是一个优化用于实时执行的 VLA 模型，通过大规模机器人轨迹和视觉-语言数据的预训练来确保广泛的行动能力。后训练技术解决了推理延迟问题，部署策略确保了连续的实时滚动。该模型在仿真基准测试中达到了最先进的性能，并在真实机器人任务中使用消费级 GPU 达到了高成功率。</div>
</details>
</div>
<div class="card">
<div class="title">PMG: Parameterized Motion Generator for Human-like Locomotion Control</div>
<div class="meta-line">Authors: Chenxi Han, Yuheng Min, Zihao Huang, Ao Hong, Hang Liu, Yi Cheng, Houde Liu</div>
<div class="meta-line">First: 2026-02-13T06:38:04+00:00 · Latest: 2026-02-13T06:38:04+00:00</div>
<div class="meta-line">Comments: 2026 IEEE International Conference on Robotics &amp; Automation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12656v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12656v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMG：参数化运动生成器用于类人运动控制</div>
<div class="mono" style="margin-top:8px">近年来，基于数据的强化学习和运动跟踪的最新进展显著改善了类人运动，但仍存在关键的实践挑战。特别是，虽然低级运动跟踪和轨迹跟随控制器已经成熟，但全身参考引导方法难以适应更高层次的命令接口和多样的任务环境：它们需要大量高质量的数据集，对速度和姿态范围的鲁棒性差，并且对机器人特定的校准敏感。为了解决这些限制，我们提出了参数化运动生成器（PMG），这是一种基于人类运动结构分析的实时运动生成器，仅使用紧凑的参数化运动数据集和高维控制命令合成参考轨迹。结合模仿学习管道和基于优化的从仿真到现实的电机参数识别模块，我们在我们的类人原型ZERITH Z1上验证了整个方法，并展示了在单一集成系统中，PMG 生成自然、类人的运动，精确响应高维控制输入，包括基于VR的远程操作，并实现高效的、可验证的从仿真到现实的转移。这些结果共同确立了一条实用的、实验验证的途径，以实现自然和可部署的类人控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve humanoid locomotion by addressing the limitations of existing methods, particularly their brittleness and sensitivity to calibration. The Parameterized Motion Generator (PMG) is proposed, which uses a compact set of parameterized motion data and high-dimensional control commands to synthesize reference trajectories in real-time. The approach, validated on the ZERITH Z1 humanoid prototype, demonstrates natural, human-like locomotion and efficient, verifiable sim-to-real transfer, enabling precise responses to high-dimensional control inputs such as VR-based teleoperation.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决将整体参考引导方法适应更高层次的命令接口和多样化任务环境的挑战，来提升人形机器人的行走能力。提出了参数化运动生成器（PMG），该方法使用紧凑的参数化运动数据和高维控制命令来实时合成参考轨迹。该方法在人形原型ZERITH Z1上得到验证，展示了自然的人类行走能力、对高维控制输入的精确响应以及高效的仿真实验到现实世界的转移。</div>
</details>
</div>
<div class="card">
<div class="title">Unifying Model-Free Efficiency and Model-Based Representations via Latent Dynamics</div>
<div class="meta-line">Authors: Jashaswimalya Acharjee, Balaraman Ravindran</div>
<div class="meta-line">First: 2026-02-13T06:06:56+00:00 · Latest: 2026-02-13T06:06:56+00:00</div>
<div class="meta-line">Comments: 13 pages. Accepted at AAMAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12643v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12643v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Unified Latent Dynamics (ULD), a novel reinforcement learning algorithm that unifies the efficiency of model-free methods with the representational strengths of model-based approaches, without incurring planning overhead. By embedding state-action pairs into a latent space in which the true value function is approximately linear, our method supports a single set of hyperparameters across diverse domains -- from continuous control with low-dimensional and pixel inputs to high-dimensional Atari games. We prove that, under mild conditions, the fixed point of our embedding-based temporal-difference updates coincides with that of a corresponding linear model-based value expansion, and we derive explicit error bounds relating embedding fidelity to value approximation quality. In practice, ULD employs synchronized updates of encoder, value, and policy networks, auxiliary losses for short-horizon predictive dynamics, and reward-scale normalization to ensure stable learning under sparse rewards. Evaluated on 80 environments spanning Gym locomotion, DeepMind Control (proprioceptive and visual), and Atari, our approach matches or exceeds the performance of specialized model-free and general model-based baselines -- achieving cross-domain competence with minimal tuning and a fraction of the parameter footprint. These results indicate that value-aligned latent representations alone can deliver the adaptability and sample efficiency traditionally attributed to full model-based planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过潜在动力学统一模型自由效率与模型导向表示</div>
<div class="mono" style="margin-top:8px">我们提出了统一潜在动力学（ULD），这是一种新颖的强化学习算法，它将模型自由方法的效率与模型导向方法的表示优势统一起来，而不增加规划开销。通过将状态-动作对嵌入到一个潜在空间中，在该空间中真实的价值函数近似为线性，我们的方法支持在从低维和像素输入的连续控制到高维Atari游戏的各种领域中使用相同的超参数集。我们证明，在温和的条件下，我们基于嵌入的时间差分更新的不动点与相应线性模型导向价值扩展的不动点一致，并推导出嵌入保真度与价值近似质量之间的显式误差界。实际上，ULD采用编码器、价值和策略网络的同步更新、用于短视窗预测动力学的辅助损失以及奖励尺度归一化，以确保在稀疏奖励下稳定学习。在涵盖Gym运动、DeepMind控制（ proprioceptive和视觉）和Atari的80个环境中评估，我们的方法在性能上与专门的模型自由和通用模型导向基线相当甚至超越——实现了跨领域的熟练度，且调参最少，参数量仅为一小部分。这些结果表明，仅值对齐的潜在表示即可提供传统上归因于完整模型导向规划的适应性和样本效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to combine the efficiency of model-free methods with the representational strengths of model-based approaches in reinforcement learning. ULD achieves this by embedding state-action pairs into a latent space where the true value function is approximately linear, supporting a single set of hyperparameters across various domains. The method includes synchronized updates of encoder, value, and policy networks, auxiliary losses for short-horizon predictive dynamics, and reward-scale normalization. Experimental results show that ULD matches or exceeds the performance of specialized model-free and general model-based baselines, demonstrating cross-domain competence with minimal tuning and a reduced parameter footprint.</div>
<div class="mono" style="margin-top:8px">研究旨在通过统一潜在动态（ULD）结合模型自由方法的效率与模型导向方法的表示能力。ULD将状态-动作对嵌入到一个潜在空间中，其中真实的价值函数大约是线性的，支持在各种领域中使用一组超参数。该方法采用编码器、价值和策略网络的同步更新，以及短期预测动力学的辅助损失，实现了在多种环境中的性能与专门基线相当，且需要较少的调优和参数量。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL</div>
<div class="meta-line">Authors: Xin Liu, Yixuan Li, Yuhui Chen, Yuxing Qin, Haoran Li, Dongbin Zhao</div>
<div class="meta-line">First: 2026-02-13T05:41:55+00:00 · Latest: 2026-02-13T05:41:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12636v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12636v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于生成事件指导的双粒度对比奖励方法及其在高效体态强化学习中的应用</div>
<div class="mono" style="margin-top:8px">在强化学习（RL）中设计合适的奖励是一个重大挑战，尤其是在体态操作中。轨迹成功率奖励适用于人类评判或模型拟合，但稀疏性严重限制了RL的学习效率。虽然最近的方法通过密集奖励有效提高了RL性能，但它们依赖于高质量的人工标注数据或大量专家监督。为了解决这些问题，本文提出了一种新的框架——基于生成事件指导的双粒度对比奖励（DEG），该框架旨在在无需人工标注或大量监督的情况下寻求高效的学习密集奖励。利用大型视频生成模型的先验知识，DEG仅需少量专家视频进行领域适应，以生成每个RL时期的专用任务指导。然后，提出的双粒度奖励平衡粗粒度探索和细粒度匹配，将引导智能体在对比自监督的潜在空间中逐步逼近生成的指导视频，最终完成目标任务。在模拟和真实世界设置中的18个不同任务上的广泛实验表明，DEG不仅可以作为高效的探索刺激帮助智能体快速发现稀疏的成功奖励，还可以独立引导有效的RL和稳定的策略收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of designing suitable rewards in reinforcement learning for embodied manipulation tasks. It introduces DEG, a novel framework that uses generated episodic guidance from video generation models to create dense rewards without requiring human annotations or extensive supervision. DEG balances coarse-grained exploration and fine-grained matching to guide the agent towards the target task efficiently. Experiments on 18 diverse tasks demonstrate that DEG can effectively stimulate exploration and guide RL, leading to stable policy convergence.</div>
<div class="mono" style="margin-top:8px">该论文针对强化学习中为实体操作任务设计合适奖励的挑战。它引入了DEG框架，利用视频生成模型生成的 episodic 指导来创建密集奖励，无需人工注释。DEG 平衡粗粒度探索和细粒度匹配，以高效引导代理，从而在 18 个不同任务的模拟和真实世界设置中提高样本效率和稳定策略收敛。</div>
</details>
</div>
<div class="card">
<div class="title">Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning</div>
<div class="meta-line">Authors: Tianyi Xiang, Jiahang Cao, Sikai Guo, Guoyang Zhao, Andrew F. Luo, Jun Ma</div>
<div class="meta-line">First: 2026-02-13T05:24:58+00:00 · Latest: 2026-02-13T05:24:58+00:00</div>
<div class="meta-line">Comments: Project page: https://physics-constrained-real2sim.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12633v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12633v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://physics-constrained-real2sim.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从真实到模拟：通过物理一致的物体间推理在高度杂乱环境中的应用</div>
<div class="mono" style="margin-top:8px">从单视角观察重建物理上有效的3D场景是实现视觉感知与机器人控制之间差距的关键。然而，在需要精确接触推理的场景中，如在高度杂乱环境中进行机器人操作，几何精度本身是不够的。标准的感知管道通常忽略物理约束，导致无效状态，例如漂浮的物体或严重的相互穿插，使得下游模拟不可靠。为了解决这些限制，我们提出了一种新的物理约束下的从真实到模拟管道，该管道可以从单视角RGB-D数据中重建物理上一致的3D场景。我们方法的核心是一个可微优化管道，通过接触图显式建模空间依赖性，联合优化物体姿态和物理属性，通过可微刚体模拟进行细化。在模拟和真实世界设置中的广泛评估表明，我们重建的场景具有高度的物理精度，并忠实复制了现实世界的接触动力学，从而实现稳定可靠的接触丰富操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to reconstruct physically valid 3D scenes from single-view observations to improve robotic manipulation in cluttered environments. The method employs a physics-constrained Real-to-Sim pipeline that uses a differentiable optimization approach to model spatial dependencies via a contact graph, refining object poses and physical properties. The results show high physical fidelity and accurate contact dynamics, enabling stable manipulation in real-world settings.</div>
<div class="mono" style="margin-top:8px">研究旨在从单视角RGB-D数据重建物理上有效的3D场景，以适应复杂环境中的机器人操作。方法包括一个通过接触图建模空间依赖性的可微优化管道，通过可微刚体仿真联合优化物体姿态和物理属性。关键发现表明，重建的场景具有高物理精度，并准确地再现了真实的接触动力学，从而实现稳定可靠的接触操作。</div>
</details>
</div>
<div class="card">
<div class="title">KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN &amp; RWKV</div>
<div class="meta-line">Authors: Zhihao Chen, Yiyuan Ge, Ziyang Wang</div>
<div class="meta-line">First: 2026-02-01T09:27:56+00:00 · Latest: 2026-02-13T05:20:37+00:00</div>
<div class="meta-line">Comments: Accepted By ICRA2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01115v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01115v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link}}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAN We Flow？借助KAN与RWKV的3D流动匹配提升机器人操作</div>
<div class="mono" style="margin-top:8px">基于扩散的视觉-运动策略在建模动作分布方面表现出色，但推理效率低下，因为从噪声递归去噪到策略需要很多步骤和沉重的UNet骨干网络，这阻碍了在资源受限的机器人上的部署。流动匹配通过学习一步向量场来减轻采样负担，但之前的实现仍然继承了大型UNet风格的架构。在本文中，我们提出了KAN-We-Flow，这是一种流动匹配策略，利用视觉领域的最新进展Receptance Weighted Key Value (RWKV)和Kolmogorov-Arnold Networks (KAN)，构建了一个轻量级且高度表达的3D操作骨干网络。具体来说，我们引入了RWKV-KAN块：RWKV首先执行高效的时间/通道混合以传播任务上下文，随后的GroupKAN层应用可学习的样条基组间功能映射，对RWKV输出的动作映射进行特征级非线性校准。此外，我们引入了动作一致性正则化（ACR），这是一种轻量级的辅助损失，通过欧拉外推强制预测的动作轨迹与专家演示之间的对齐，为稳定训练和提高策略精度提供额外监督。无需使用大型UNet，我们的设计减少了86.8%的参数，保持了快速的运行时间，并在Adroit、Meta-World和DexArt基准测试中达到了最先进的成功率。我们的项目页面可以在&lt;https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/&gt; 查看</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">KAN-We-Flow is designed to improve robotic manipulation by using a lightweight flow-matching policy that combines RWKV and KAN techniques. This approach reduces the need for heavy UNet architectures, enabling faster runtime and lower parameter count. The method introduces an RWKV-KAN block and an Action Consistency Regularization to enhance the policy&#x27;s precision. Experimental results show that KAN-We-Flow achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks without relying on large UNets.</div>
<div class="mono" style="margin-top:8px">该研究通过提出KAN-We-Flow，利用RWKV和KAN构建轻量级的3D操作骨干网络，以解决基于扩散的运动视觉策略的推理效率问题。方法引入了RWKV-KAN模块和动作一致性正则化以提高策略精度。设计减少了86.8%的参数量，同时保持快速运行时间，并在Adroit、Meta-World和DexArt基准上达到了最先进的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models</div>
<div class="meta-line">Authors: Liangzhi Shi, Shuaihang Chen, Feng Gao, Yinuo Chen, Kang Chen, Tonghe Zhang, Hongzhi Zhang, Weinan Zhang, Chao Yu, Yu Wang</div>
<div class="meta-line">Venue: RSS 2026</div>
<div class="meta-line">First: 2026-02-13T05:15:50+00:00 · Latest: 2026-02-13T05:15:50+00:00</div>
<div class="meta-line">Comments: 9 pages, 12 figures. Supplementary material included. Submitted to RSS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12628v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12628v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLinf-Co：基于强化学习的仿真实际联合训练方法用于VLA模型</div>
<div class="mono" style="margin-top:8px">仿真是提供一种可扩展且低成本的方法来丰富视觉-语言-动作（VLA）训练，减少对昂贵的机器人演示的依赖。然而，大多数仿真实际联合训练方法依赖于监督微调（SFT），将仿真视为静态的演示来源，并未利用大规模闭环交互。因此，实际世界收益和泛化能力往往有限。在本文中，我们提出了一种基于\underline{\textit{RL}}的仿真实际\underline{\textit{Co}}-训练（RL-Co）框架，利用交互仿真同时保留实际世界的能力。我们的方法遵循通用的两阶段设计：首先使用混合实际和仿真演示的监督微调（SFT）预热策略，然后在仿真中使用强化学习进行微调，同时在实际数据上添加辅助监督损失以锚定策略并减轻灾难性遗忘。我们在使用两种代表性VLA架构OpenVLA和$π_{0.5}$的四个实际桌面操作任务上评估了我们的框架，观察到与仅使用实际数据微调和基于SFT的联合训练相比的一致改进，包括OpenVLA上的实际成功率提高24%和$π_{0.5}$上的提高20%。除了更高的成功率，RL联合训练还提供了更强的对未见过的任务变化的泛化能力和显著提高的实际数据效率，为利用仿真增强实际机器人部署提供了一种实用且可扩展的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current sim-real co-training methods by proposing RLinf-Co, which uses reinforcement learning to leverage interactive simulation while maintaining real-world capabilities. The method involves a two-stage process: initial warm-start with supervised fine-tuning on a mix of real and simulated data, followed by reinforcement learning fine-tuning in simulation with an auxiliary supervised loss on real data. Evaluations on four real-world tabletop manipulation tasks show consistent improvements over real-only fine-tuning and SFT-based co-training, with success rates increasing by 24% on OpenVLA and 20% on $π_{0.5}$. The approach also enhances generalization and real-world data efficiency.</div>
<div class="mono" style="margin-top:8px">本文提出了基于强化学习的RLinf-Co模拟-现实协同训练框架，旨在解决监督微调的局限性，通过结合互动模拟和现实数据。该方法分为两个阶段：初始阶段使用混合现实和模拟数据的监督微调进行预热，随后在模拟中使用强化学习进行微调，并附加一个基于现实数据的辅助监督损失。在四个现实世界任务上的评估显示，与仅使用现实数据微调和基于监督微调的协同训练相比，该方法在OpenVLA和$π_{0.5}$上的成功率分别提高了24%和20%。此外，RL协同训练增强了泛化能力和现实世界数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning</div>
<div class="meta-line">Authors: Yike Zhang, Yaonan Wang, Xinxin Sun, Kaizhen Huang, Zhiyuan Xu, Junjie Ji, Zhengping Che, Jian Tang, Jingtao Sun</div>
<div class="meta-line">First: 2026-02-13T02:28:21+00:00 · Latest: 2026-02-13T02:28:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12532v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12532v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRAFT：通过力感知课程微调适应接触丰富的操作</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在使机器人执行通用指令方面表现出强大的能力，但在接触丰富的操作任务中却面临挑战，这些任务的成功需要精确对齐、稳定的接触维持和对可变形物体的有效处理。一个基本的挑战来自于高熵视觉和语言输入与低熵但关键的力信号之间的不平衡，这通常导致模型过度依赖感知并导致控制不稳定。为了解决这个问题，我们引入了CRAFT，一种力感知课程微调框架，该框架在早期训练中通过变分信息瓶颈模块调节视觉和语言嵌入。这种课程策略鼓励模型最初优先处理力信号，然后逐步恢复对全模态信息的访问。为了实现力感知学习，我们进一步设计了一个同源的领导者-跟随者远程操作系统，该系统收集了多种接触丰富的任务中的同步视觉、语言和力数据。实验证明，CRAFT在任务成功率、对未见过的对象和新型任务变体的泛化以及在多种VLA架构上的适应性方面表现出色，从而实现了稳健和通用的接触丰富的操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of contact-rich manipulation tasks for robots by introducing CRAFT, a force-aware curriculum fine-tuning framework. CRAFT uses a variational information bottleneck module to prioritize force signals during early training, helping the model to stabilize control and handle deformable objects. Experiments show that CRAFT improves task success, generalizes well to new objects and tasks, and adapts across different VLA architectures.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决高熵视觉和语言输入与关键但低熵力信号之间的不平衡问题，提升VLA模型在接触丰富操作任务中的表现。CRAFT是一种力感知的课程微调框架，通过变分信息瓶颈模块在早期训练中优先处理力信号，然后逐渐引入多模态信息。实验表明，CRAFT能够提高任务成功率，对新对象和新任务具有良好的泛化能力，并且能够适应不同的VLA架构，从而实现更稳健和通用的操作。</div>
</details>
</div>
<div class="card">
<div class="title">What Matters in Building Vision-Language-Action Models for Generalist Robots</div>
<div class="meta-line">Authors: Xinghang Li, Peiyan Li, Long Qian, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Xinlong Wang, Di Guo, Tao Kong, Hanbo Zhang, Huaping Liu</div>
<div class="meta-line">First: 2024-12-18T17:07:20+00:00 · Latest: 2026-02-13T02:05:15+00:00</div>
<div class="meta-line">Comments: Project page: robovlms.github.io. Added limitations and future works. Fix categorization</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.14058v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.14058v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To utilize Foundation Vision Language Models (VLMs) for robotic tasks and motion planning, the community has proposed different methods for injecting action components into VLMs and building the Vision-Language-Action models (VLAs). In this work, we disclose the key factors that significantly influence the performance of VLA on robot manipulation problems and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we prefer VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>构建通用机器人视觉语言行动模型的关键因素</div>
<div class="mono" style="margin-top:8px">为了利用基础视觉语言模型（VLMs）进行机器人任务和运动规划，社区提出了不同的方法将行动组件注入VLMs并构建视觉语言行动模型（VLAs）。在本文中，我们揭示了显著影响机器人操作问题中VLAs性能的关键因素，并重点关注回答三个基本设计选择：选择哪个骨干网络，如何构建VLAs架构，以及何时添加跨体数据。获得的结果使我们坚信为什么我们更喜欢VLAs，并开发了一种新的VLAs家族RoboVLMs，它需要很少的手动设计并在三个模拟任务和真实世界实验中达到了新的最佳性能。通过包括超过8种VLM骨干网络、4种策略架构和超过600种不同设计实验的广泛实验，我们为未来VLAs的设计提供了详细的指南。除了研究之外，我们还公开了一个高度灵活的RoboVLMs框架，支持新VLM的轻松集成和各种设计选择的自由组合，以促进未来的研究。我们开源了所有细节，包括代码、模型、数据集和工具包，以及详细的训练和评估方法：robovlms.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates the key factors influencing the performance of Vision-Language-Action (VLA) models for robot manipulation tasks. Through extensive experiments with various backbones, policy architectures, and design choices, the authors identify the optimal configurations for building RoboVLMs, a new family of VLA models that achieve state-of-the-art performance. The study provides a detailed guide for future VLA design and makes the RoboVLMs framework and all related resources publicly available.</div>
<div class="mono" style="margin-top:8px">该研究探讨了影响机器人操作任务中视觉-语言-行动（VLA）模型性能的关键因素。通过使用多种骨干网络、策略架构和设计选择进行大量实验，作者确定了构建RoboVLMs（一种新的VLA模型家族）的最佳配置，该模型在三个模拟任务和真实世界实验中达到了最先进的性能。研究还提供了一套详细的未来VLA设计指南，并公开了RoboVLMs框架及相关资源。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control</div>
<div class="meta-line">Authors: Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su, Jingwen Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T07:43:24+00:00 · Latest: 2026-02-13T01:46:40+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21363v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21363v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lift-humanoid.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.Code and videos: https://lift-humanoid.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向大规模预训练与高效微调之间的人形控制鸿沟弥合</div>
<div class="mono" style="margin-top:8px">强化学习（RL）广泛应用于人形控制，其中在线策略方法如近端策略优化（PPO）通过大规模并行模拟实现稳健训练，并在某些情况下实现零样本部署到真实机器人。然而，在线策略算法的低样本效率限制了其在新环境中的安全适应。尽管离线策略RL和基于模型的RL显示出改进的样本效率，但人形上的大规模预训练与高效微调之间的差距仍然存在。在本文中，我们发现，带有大规模更新和高更新到数据（UTD）比率的离线策略Soft Actor-Critic（SAC）可靠地支持了人形运动策略的大规模预训练，实现了零样本部署到真实机器人。对于适应，我们展示了这些SAC预训练策略可以通过基于模型的方法在新环境中和离分布任务中进行微调。在新环境中的数据收集执行确定性策略，而随机探索则限制在物理信息的世界模型中。这种分离减轻了适应过程中的随机探索风险，同时保留了探索性覆盖以促进改进。总体而言，该方法结合了预训练期间大规模模拟的时钟效率与微调期间基于模型学习的样本效率。代码和视频：https://lift-humanoid.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of bridging the gap between large-scale pretraining and efficient fine-tuning for humanoid control. It uses off-policy Soft Actor-Critic (SAC) for pretraining, which is effective with large-batch updates and a high Update-To-Data ratio, enabling zero-shot deployment on real robots. For adaptation, the pre-trained policies are fine-tuned using model-based methods in new environments, with data collection in the new environment following a deterministic policy while exploration is managed through a physics-informed world model. This approach combines the efficiency of large-scale simulation with the sample efficiency of model-based learning, improving humanoid control in diverse scenarios.</div>
<div class="mono" style="margin-top:8px">本文旨在通过强化学习解决人形机器人控制中大规模预训练与高效微调之间的差距问题。作者使用带有大规模更新和高更新到数据比的离策略Soft Actor-Critic (SAC)进行预训练，实现了在真实机器人上的零样本部署。在适应过程中，他们使用基于模型的方法对预训练的策略进行微调，其中在新环境中的数据收集遵循确定性策略，而探索则限制在物理信息的世界模型中。这种方法结合了大规模模拟中的时间效率和基于模型学习中的样本效率，展示了在新环境和非分布任务中的更好性能。</div>
</details>
</div>
<div class="card">
<div class="title">Human-Like Coarse Object Representations in Vision Models</div>
<div class="meta-line">Authors: Andrey Gizdov, Andrea Procopio, Yichen Li, Daniel Harari, Tomer Ullman</div>
<div class="meta-line">First: 2026-02-12T23:59:58+00:00 · Latest: 2026-02-12T23:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12486v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12486v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans appear to represent objects for intuitive physics with coarse, volumetric bodies&#x27;&#x27; that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity&#x27;&#x27; best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉模型中的类人类粗略物体表示</div>
<div class="mono" style="margin-top:8px">人类似乎用粗糙的、体积化的身体来表示物体，以平滑凹陷，牺牲精细的视觉细节来实现高效的物理预测，但其内部结构尚不清楚。与之相反，分割模型优化的是像素级准确的掩码，这些掩码可能与这样的身体不一致。我们询问这些模型是否以及何时会获得类人类的身体。通过使用碰撞时间（TTC）行为范式，我们引入了一个比较管道和对齐度量，然后通过剪枝改变模型的训练时间、大小和有效容量。在所有操作中，与人类行为的对齐度遵循一个倒U形曲线：小/训练时间短/剪枝的模型过度分割成团块；大/完全训练的模型过度分割，边界有波动；而一个中间的理想身体粒度最好匹配人类。这表明类人类的粗糙身体是从资源限制中产生的，而不是从特定的偏见中产生的，并指出了简单的调节旋钮——早期检查点、适度的架构、轻度剪枝——以产生物理高效的表示。我们将这些结果置于资源理性的解释中，该解释平衡了识别细节与物理功能之间的关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates how vision models develop human-like representations of objects for efficient physical predictions. Using a time-to-collision behavioral paradigm, the researchers varied model training time, size, and effective capacity. They found that alignment with human behavior follows an inverse U-shaped curve, with small and briefly trained models under-segmenting into blobs, large and fully trained models over-segmenting with boundary wiggles, and intermediate models best matching human representations. This suggests that human-like coarse object representations emerge from resource constraints rather than specific biases, and highlights simple model settings that can elicit physics-efficient representations.</div>
<div class="mono" style="margin-top:8px">研究探讨了视觉模型如何发展出类似人类的对象表示以进行高效的物理预测。通过使用时间到碰撞的行为范式，研究人员在模型训练时间、大小和通过剪枝调整的有效容量上进行了变化。他们发现，与人类行为的对齐遵循一个倒U形曲线，小型和短暂训练的模型过度分割成块状，大型和完全训练的模型过度分割带有边界波动，而中间模型最接近人类的表示。这表明，类似人类的粗略对象表示是由资源约束而非特定偏见产生的，并强调了简单的模型设置以获得物理高效的表示。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Guo Ye, Zexi Zhang, Xu Zhao, Shang Wu, Haoran Lu, Shihan Lu, Han Liu</div>
<div class="meta-line">First: 2025-12-29T21:06:33+00:00 · Latest: 2026-02-12T22:35:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23864v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23864v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model&#x27;s understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习感受未来：DreamTacVLA用于接触丰富的操作</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型通过将网络规模的知识映射到机器人控制中展示了显著的泛化能力，但它们仍然无法感知物理接触。因此，它们在需要推理力、纹理和滑动的接触丰富的操作任务中表现不佳。尽管一些方法整合了低维度的触觉信号，但它们无法捕捉到此类交互中至关重要的高分辨率动力学。为解决这一局限，我们提出了DreamTacVLA框架，通过学习感受未来将VLA模型与接触物理学联系起来。我们的模型采用分层感知方案，其中高分辨率的触觉图像作为微视输入，与手腕相机的局部视图和第三人称宏观视图相结合。为了协调这些多尺度的感觉流，我们首先使用层次空间对齐（HSA）损失训练了一个统一策略，该损失将触觉标记与其手腕和第三人称视图中的空间对应物对齐。为了进一步加深模型对细粒度接触动力学的理解，我们使用预测未来触觉信号的触觉世界模型对系统进行微调。为了缓解触觉数据稀缺性和触觉传感器易磨损的性质，我们构建了一个混合的大规模数据集，该数据集来源于高保真数字孪生和真实世界实验。通过预测即将出现的触觉状态，DreamTacVLA获得了丰富的接触物理学模型，并根据实际观察和想象的后果来调整其行为。在接触丰富的操作任务中，它超越了最先进的VLA基线，成功率高达95%，突显了理解物理接触对于稳健、触觉感知的机器人代理的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance VLA models&#x27; ability to handle contact-rich manipulation tasks by integrating tactile information. DreamTacVLA uses a hierarchical perception scheme and a tactile world model to predict future tactile signals, aligning tactile tokens with visual inputs. This approach significantly improves performance on contact-rich tasks, achieving up to 95% success, demonstrating the necessity of understanding physical contact for robust robotic manipulation.</div>
<div class="mono" style="margin-top:8px">研究旨在通过整合触觉信息来增强VLA模型处理接触丰富操作任务的能力。DreamTacVLA采用分层感知方案和触觉世界模型来预测未来的触觉信号，将触觉标记与视觉输入对齐。这种方法在接触丰富任务上的表现显著提升，最高可达95%的成功率，突显了理解物理接触对于稳健的触觉机器人代理的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Multi-Robot Motion Planning for Manifold-Constrained Manipulators by Randomized Scheduling and Informed Path Generation</div>
<div class="meta-line">Authors: Weihang Guo, Zachary Kingston, Kaiyu Hang, Lydia E. Kavraki</div>
<div class="meta-line">First: 2024-11-30T05:59:42+00:00 · Latest: 2026-02-12T21:54:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.00366v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.00366v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-robot motion planning for high degree-of-freedom manipulators in shared, constrained, and narrow spaces is a complex problem and essential for many scenarios such as construction, surgery, and more. Traditional coupled methods plan directly in the composite configuration space, which scales poorly; decoupled methods, on the other hand, plan separately for each robot but lack completeness. Hybrid methods that obtain paths from individual robots together require the enumeration of many paths before they can find valid composite solutions. This paper introduces Scheduling to Avoid Collisions (StAC), a hybrid approach that more effectively composes paths from individual robots by scheduling (adding stops and coordination motion along all paths) and generates paths that are likely to be feasible by using bidirectional feedback between the scheduler and motion planner for informed sampling. StAC uses 10 to 100 times fewer paths from the low-level planner than state-of-the-art hybrid baselines on challenging problems in manipulator cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效多机器人运动规划方法：基于随机调度和启发式路径生成的流形约束操作器</div>
<div class="mono" style="margin-top:8px">在共享、受限和狭窄空间中，高自由度操作器的多机器人运动规划是一个复杂问题，对于建筑、手术等多种场景至关重要。传统耦合方法直接在复合配置空间中规划，扩展性差；而解耦方法分别为每个机器人规划路径，但缺乏完备性。混合方法通过从单个机器人中获取路径并组合，需要枚举大量路径才能找到有效的复合解。本文提出了一种混合方法Scheduling to Avoid Collisions (StAC)，该方法通过调度（添加停顿和协调运动）更有效地组合单个机器人路径，并通过调度器和运动规划器之间的双向反馈进行启发式采样生成更可能可行的路径。在操作器案例中的复杂问题上，StAC 比最先进的混合基线少使用10到100倍的低级规划器路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of efficient multi-robot motion planning for high degree-of-freedom manipulators in constrained spaces. It introduces StAC, a hybrid approach that combines scheduling and informed path generation to reduce the number of paths needed from the low-level planner compared to state-of-the-art methods. StAC uses 10 to 100 times fewer paths while still finding valid composite solutions, making it more efficient for complex scenarios like construction and surgery.</div>
<div class="mono" style="margin-top:8px">论文解决了高自由度机械臂在受限空间中的多机器人运动规划问题。提出了一种结合调度和启发式路径生成的混合方法StAC，相比现有的先进方法，StAC在复杂场景下所需的低级规划路径数量减少了10到100倍，但仍能找到有效的复合解决方案，适用于建筑和手术等场景。</div>
</details>
</div>
<div class="card">
<div class="title">Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models</div>
<div class="meta-line">Authors: Ali Subhan, Ashir Raza</div>
<div class="meta-line">Venue: CVPR 2024</div>
<div class="meta-line">First: 2026-02-12T20:40:43+00:00 · Latest: 2026-02-12T20:40:43+00:00</div>
<div class="meta-line">Comments: 16 pages, 8 figures. Reproducibility study of DragDiffusion (CVPR 2024). Submitted to TMLR Reproducibility Challenge. Code available on GitHub</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12393v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12393v1">PDF</a> · <a href="https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors&#x27; released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重现DragDiffusion：基于扩散模型的交互式点基图像编辑</div>
<div class="mono" style="margin-top:8px">DragDiffusion是一种基于扩散模型的交互式点基图像编辑方法，允许用户通过直接拖动选定的点来操作图像。该方法声称可以通过在中间时间步优化单个扩散潜在变量，结合身份保留的微调和空间正则化，实现准确的空间控制。本文使用作者发布的实现和DragBench基准，对DragDiffusion进行了重现性研究。我们重现了关于扩散时间步选择、LoRA基微调、掩码正则化强度以及UNet特征监督的主要消融研究，观察到与原始工作报告的定性和定量趋势有密切一致之处。同时，我们的实验表明，性能对少数超参数假设非常敏感，特别是优化的时间步和用于运动监督的特征级别，而其他组件则允许更广泛的运行范围。我们进一步评估了一种多时间步潜在变量优化变体，发现它并未提高空间准确性，但显著增加了计算成本。总体而言，我们的发现支持了DragDiffusion的核心主张，并明确了它们可靠重现的条件。代码可在https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to reproduce the DragDiffusion method for interactive point-based image editing, which allows users to manipulate images by dragging points. The method optimizes a single diffusion latent at an intermediate timestep, combined with identity-preserving fine-tuning and spatial regularization. Experiments confirm the original claims, showing that performance is sensitive to specific hyperparameters, particularly the optimized timestep and feature level for motion supervision. The multi-timestep latent optimization variant did not improve spatial accuracy but increased computational cost. Overall, the findings support the method&#x27;s central claims under certain conditions.</div>
<div class="mono" style="margin-top:8px">研究旨在重现基于扩散模型的交互式点编辑方法DragDiffusion。该方法通过在中间时间步优化单个扩散潜变量，并结合身份保持的微调和空间正则化来实现。实验确认了原始工作的定性和定量趋势，表明性能对特定超参数（尤其是优化的时间步和用于运动监督的特征级别）非常敏感。测试了多时间步潜变量优化变体，但未提高空间精度且增加了计算成本。研究结果在特定条件下支持了该方法的核心主张。</div>
</details>
</div>
<div class="card">
<div class="title">LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation</div>
<div class="meta-line">Authors: Yue Hu, Avery Xi, Qixin Xiao, Seth Isaacson, Henry X. Liu, Ram Vasudevan, Maani Ghaffari</div>
<div class="meta-line">First: 2026-02-12T19:22:52+00:00 · Latest: 2026-02-12T19:22:52+00:00</div>
<div class="meta-line">Comments: VLA, Navigation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12351v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper develops LongNav-R1, an end-to-end multi-turn reinforcement learning (RL) framework designed to optimize Visual-Language-Action (VLA) models for long-horizon navigation. Unlike existing single-turn paradigm, LongNav-R1 reformulates the navigation decision process as a continuous multi-turn conversation between the VLA policy and the embodied environment. This multi-turn RL framework offers two distinct advantages: i) it enables the agent to reason about the causal effects of historical interactions and sequential future outcomes; and ii) it allows the model to learn directly from online interactions, fostering diverse trajectory generation and avoiding the behavioral rigidity often imposed by human demonstrations. Furthermore, we introduce Horizon-Adaptive Policy Optimization. This mechanism explicitly accounts for varying horizon lengths during advantage estimation, facilitating accurate temporal credit assignment over extended sequences. Consequently, the agent develops diverse navigation behaviors and resists collapse during long-horizon tasks. Experiments on object navigation benchmarks validate the framework&#x27;s efficacy: With 4,000 rollout trajectories, LongNav-R1 boosts the Qwen3-VL-2B success rate from 64.3% to 73.0%. These results demonstrate superior sample efficiency and significantly outperform state-of-the-art methods. The model&#x27;s generalizability and robustness are further validated by its zero-shot performance in long-horizon real-world navigation settings. All source code will be open-sourced upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LongNav-R1：适用于长时距VLA导航的多轮RL框架</div>
<div class="mono" style="margin-top:8px">本文提出了LongNav-R1，这是一种端到端的多轮强化学习（RL）框架，旨在优化视觉-语言-行动（VLA）模型在长时距导航中的表现。与现有的单轮范式不同，LongNav-R1 将导航决策过程重新定义为VLA策略与实体环境之间连续的多轮对话。这种多轮RL框架具有两个显著优势：i) 它使智能体能够推理历史交互和顺序未来结果的因果效应；ii) 它允许模型直接从在线交互中学习，促进多样轨迹生成，避免由人类示范所施加的行为僵化。此外，我们引入了适应性时距策略优化机制。该机制在优势估计中明确考虑了时距变化，有助于在长时间序列中准确分配时间上的信用分配。因此，智能体发展出多样化的导航行为，并在长时距任务中避免崩溃。实验结果在物体导航基准测试中验证了该框架的有效性：通过4,000个回放轨迹，LongNav-R1 将Qwen3-VL-2B的成功率从64.3%提升至73.0%。这些结果展示了其优越的样本效率，并显著优于现有最先进的方法。模型的通用性和鲁棒性进一步通过其在长时距真实世界导航环境中的零样本性能得到验证。所有源代码将在发表后开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LongNav-R1 is an end-to-end multi-turn RL framework for optimizing VLA models in long-horizon navigation, which reformulates the navigation process as a multi-turn conversation. It enables the agent to reason about historical interactions and future outcomes, and learns directly from online interactions, improving sample efficiency and outperforming state-of-the-art methods. Experiments show that LongNav-R1 increases the Qwen3-VL-2B success rate from 64.3% to 73.0% with 4,000 rollout trajectories, and demonstrates generalizability and robustness in real-world settings.</div>
<div class="mono" style="margin-top:8px">LongNav-R1 是一种端到端的多轮强化学习框架，用于优化 VLA 模型在长时域导航中的表现。它将导航过程重新定义为多轮对话，使智能体能够推理历史交互和未来结果，并直接从在线交互中学习。该框架包含一种适应性策略优化机制，能够改善时间上的信用分配。实验结果显示，LongNav-R1 将成功率从 64.3% 提高到 73.0%，使用了 4,000 个展开轨迹，展示了其在样本效率和性能上的优越性，并且在实际场景中也表现出良好的泛化能力和鲁棒性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
