<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-20 03:47</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260220_0347</div>
    <div class="row"><div class="card">
<div class="title">One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation</div>
<div class="meta-line">Authors: Zhenyu Wei, Yunchao Yao, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-18T18:59:57+00:00 · Latest: 2026-02-18T18:59:57+00:00</div>
<div class="meta-line">Comments: Project Page: https://zhenyuwei2003.github.io/OHRA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16712v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhenyuwei2003.github.io/OHRA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一掌统治一切：统一灵巧操作的规范表示</div>
<div class="mono" style="margin-top:8px">当前的灵巧操作策略大多假定固定的手部设计，严重限制了它们对具有不同运动学和结构布局的新实体的泛化能力。为克服这一限制，我们引入了一个参数化的规范表示，统一了广泛的灵巧手架构。它包括一个统一的参数空间和一个规范的URDF格式，提供三个关键优势。1) 参数空间捕捉了有效学习算法训练所需的形态和运动学变化。2) 可以在我们的空间中学习一个结构化的潜在流形，其中不同实体之间的插值会产生平滑且物理上合理的形态过渡。3) 规范的URDF标准化了动作空间，同时保留了原始URDF的动力学和功能特性，使跨实体学习策略变得高效可靠。我们通过广泛的分析和实验验证了这些优势，包括抓取策略回放、VAE潜在编码和跨实体零样本转移。具体而言，我们对统一表示训练了一个VAE，以获得一个紧凑且语义丰富的潜在嵌入，并开发了一个基于规范表示的抓取策略，该策略在灵巧手之间具有泛化能力。我们通过模拟和在未见过的形态上的实际任务（例如，3指LEAP手的零样本成功率高达81.9%）展示了我们的框架如何统一了结构多样手的表示空间和动作空间，为跨手学习提供了可扩展的基础，以实现通用灵巧操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of fixed hand designs in dexterous manipulation policies by introducing a parameterized canonical representation that unifies various hand architectures. The method includes a unified parameter space and a canonical URDF format, which captures morphological and kinematic variations, enables smooth transitions between embodiments, and standardizes the action space while preserving dynamic properties. Key experimental findings show that a VAE trained on this representation achieves a 3-finger LEAP Hand zero-shot success rate of 81.9%, demonstrating the framework&#x27;s effectiveness in cross-embodiment learning for universal dexterous manipulation.</div>
<div class="mono" style="margin-top:8px">本文通过引入一个参数化的标准化表示来解决当前灵巧操作策略中固定手部设计的局限性，该表示统一了各种手部架构。该方法包括统一的参数空间和标准化的URDF格式，能够捕捉形态和运动学变化，实现不同手部之间的平滑过渡，并标准化动作空间同时保留动态特性。关键实验结果表明，基于此表示训练的VAE在3指LEAP手上的零样本成功率达到了81.9%，展示了该框架在跨手部学习中的有效性，以实现通用灵巧操作。</div>
</details>
</div>
<div class="card">
<div class="title">EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data</div>
<div class="meta-line">Authors: Ruijie Zheng, Dantong Niu, Yuqi Xie, Jing Wang, Mengda Xu, Yunfan Jiang, Fernando Castañeda, Fengyuan Hu, You Liang Tan, Letian Fu, Trevor Darrell, Furong Huang, Yuke Zhu, Danfei Xu, Linxi Fan</div>
<div class="meta-line">First: 2026-02-18T18:59:05+00:00 · Latest: 2026-02-18T18:59:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoScale：通过多样化第一人称人类数据扩展灵巧操作</div>
<div class="mono" style="margin-top:8px">人类行为是学习物理智能最具扩展性的数据来源之一，但如何有效利用它来实现灵巧操作仍不清楚。尽管先前的工作在受限环境中展示了从人类到机器人的转移，但大规模人类数据是否能支持精细的、高自由度的灵巧操作尚不清楚。我们提出了EgoScale，这是一种基于大规模第一人称人类数据的从人类到灵巧操作的转移框架。我们在一个超过20,854小时的动作标注第一人称人类视频上训练了一个视觉语言动作（VLA）模型，数据量超过先前努力的20倍，并发现人类数据规模与验证损失之间存在对数线性关系。这种验证损失与下游真实机器人性能高度相关，确立了大规模人类数据作为可预测监督源的地位。除了规模，我们引入了一个简单的两阶段转移配方：大规模人类预训练后，进行轻量级的人机对齐中期训练。这使得在最少的机器人监督下实现强大的长时灵巧操作和一次性的任务适应成为可能。我们的最终策略在使用22个自由度的灵巧机器人手中将平均成功率提高了54%，并且能够有效地转移到自由度较低的手中，表明大规模人类运动提供了可重复使用、与具体身体无关的运动先验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EgoScale is a framework for transferring human dexterity to robots using large-scale egocentric human data. The study trains a Vision Language Action model on over 20,854 hours of action-labeled human videos, showing a log-linear relationship between data scale and validation loss, which correlates with robot performance. The method involves two stages: large-scale human pretraining and lightweight human-robot alignment training, which enables strong long-horizon dexterous manipulation and one-shot task adaptation. The final policy improves the average success rate by 54% over a no-pretraining baseline using a 22-DoF robotic hand and transfers effectively to robots with fewer degrees of freedom.</div>
<div class="mono" style="margin-top:8px">EgoScale 是一个框架，利用大规模的第一人称人类数据将人类灵巧性转移到机器人上。它在一个包含超过20,854小时动作标注的人类视频上训练Vision Language Action模型，显示出数据规模与验证损失之间的对数线性关系，该关系与机器人性能相关。该方法使用两阶段转移过程：大规模的人类预训练，随后是轻量级的人机对齐，从而实现强大的灵巧操作和少量机器人监督下的单次任务适应。最终策略在使用22个自由度的灵巧机器人手时将平均成功率提高了54%，并且能够有效地转移到自由度较少的机器人手上。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</div>
<div class="meta-line">Authors: Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</div>
<div class="meta-line">First: 2026-02-18T18:55:02+00:00 · Latest: 2026-02-18T18:55:02+00:00</div>
<div class="meta-line">Comments: Project page: https://hero-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16705v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hero-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人机器人开放词汇视觉移动物体末端执行器控制学习</div>
<div class="mono" style="margin-top:8px">使用类人机器人在野外对任意物体进行视觉移动物体操作需要精确的末端执行器（EE）控制和通过视觉输入（例如RGB-D图像）对场景的广泛理解。现有方法基于现实世界的模仿学习，由于难以收集大规模训练数据集，因此表现出有限的泛化能力。本文提出了一种新的范式HERO，用于类人机器人进行物体移动物体操作，该范式结合了大型视觉模型的强大泛化能力和开放词汇理解，以及模拟训练中的强大控制性能。我们通过设计一种准确的残差感知末端执行器跟踪策略来实现这一点。该末端执行器跟踪策略结合了经典机器人学和机器学习。它使用a) 逆运动学将残差末端执行器目标转换为参考轨迹，b) 用于准确前向运动学的已学习神经前向模型，c) 目标调整，以及d) 重新规划。这些创新共同帮助我们将末端执行器跟踪误差减少3.2倍。我们使用这种准确的末端执行器跟踪器构建了一个模块化移动物体系统，其中我们使用开放词汇大型视觉模型实现强大的视觉泛化。我们的系统能够在从办公室到咖啡馆的各种真实世界环境中运行，机器人能够可靠地操作各种日常物体（例如茶杯、苹果、玩具），这些物体位于43cm至92cm高度的表面上。在模拟和现实世界中的系统模块化和端到端测试表明我们提出的系统设计的有效性。我们认为本文中的进展可以为训练类人机器人与日常物体交互开辟新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable humanoid robots to perform visual loco-manipulation with arbitrary objects in diverse environments. HERO, the proposed paradigm, integrates large vision models with classical robotics and machine learning to achieve accurate end-effector control. Key findings include a 3.2x reduction in end-effector tracking error and successful manipulation of various objects in real-world settings such as offices and coffee shops.</div>
<div class="mono" style="margin-top:8px">本文提出了HERO，一种新的框架，使类人机器人能够在多种环境中执行物体操作。该系统结合了大型视觉模型的强大泛化能力和精确的末端执行器控制，通过残差感知的跟踪策略实现。关键创新包括逆运动学、学习神经前向模型、目标调整和重新规划，这些方法将末端执行器跟踪误差减少了3.2倍。该系统成功地在办公室和咖啡馆等真实世界环境中操作各种物体，展示了其在开放词汇视觉操作中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to unfold cloth: Scaling up world models to deformable object manipulation</div>
<div class="meta-line">Authors: Jack Rome, Stephen James, Subramanian Ramamoorthy</div>
<div class="meta-line">First: 2026-02-18T18:14:41+00:00 · Latest: 2026-02-18T18:14:41+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习展开布料：扩展世界模型以应对可变形物体操作</div>
<div class="mono" style="margin-top:8px">学习操作布料既是机器人研究中的一个典型问题，也是从辅助护理到服务业等多种应用中的即时相关问题。可变形物体的复杂物理特性使得布料操作问题变得非平凡。为了创建一个能够应对各种形状、大小、折叠和皱纹模式的通用操作策略，除了通常的外观变化问题外，仔细考虑模型结构及其对泛化性能的影响变得至关重要。在本文中，我们提出了一种使用最近提出的强化学习架构DreamerV2变体的空中布料操作方法。我们的实现修改了该架构以利用表面法线输入，并修改了回放缓冲区和数据增强程序。这些修改共同代表了机器人所使用的世界模型的改进，解决了机器人操作对象的物理复杂性。我们在模拟中进行了评估，并在物理机器人设置中进行了零样本部署，展示了不同布料类型的空中展开，证明了我们提出架构的泛化优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robotic cloth manipulation, which is crucial for various applications. The authors use a modified DreamerV2 reinforcement learning architecture to handle the complex physics of cloth. By incorporating surface normals and adjusting the replay buffer and data augmentation, they enhance the world model&#x27;s ability to generalize across different cloth types. The approach is evaluated both in simulation and on a physical robot, successfully demonstrating the ability to unfold various cloths in air, highlighting the generalization benefits of their method.</div>
<div class="mono" style="margin-top:8px">该论文解决了布料操作这一关键的机器人问题，具有助人护理和服务业的应用。作者使用了DreamerV2强化学习架构的改进版本，加入了表面法线输入，并改进了回放缓冲区和数据增强方法。这些改进增强了机器人的世界模型，使其能够处理可变形物体的复杂物理特性。实验表明，改进后的架构在不同类型的布料上表现出良好的泛化能力，既在仿真中也应用于物理机器人。</div>
</details>
</div>
<div class="card">
<div class="title">Elements of Robot Morphology: Supporting Designers in Robot Form Exploration</div>
<div class="meta-line">Authors: Amy Koike, Serena Ge Guo, Xinning He, Callie Y. Kim, Dakota Sullivan, Bilge Mutlu</div>
<div class="meta-line">First: 2026-02-09T21:13:20+00:00 · Latest: 2026-02-18T17:41:00+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, Proceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09203v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09203v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人形态学要素：支持设计师进行机器人形态探索</div>
<div class="mono" style="margin-top:8px">机器人形态，即机器人的形状、结构，是人机交互（HRI）中的关键设计空间，影响着机器人的功能、表达方式以及与人的互动。尽管其重要性不言而喻，但关于设计框架如何指导系统性形态探索的研究却很少。为填补这一空白，我们提出了机器人形态学要素这一框架，该框架识别出五个基本要素：感知、关节、末端执行器、运动方式和结构。该框架源自对现有机器人的分析，支持对多样化机器人形态的结构化探索。为了实现该框架，我们开发了形态探索模块（MEB），这是一种可触控的模块，能够促进对机器人形态的手动、协作性实验。我们通过案例研究和设计研讨会对该框架和工具包进行了评估，展示了它们如何支持分析、创意生成、反思以及协作机器人设计。</div>
</details>
</div>
<div class="card">
<div class="title">FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency</div>
<div class="meta-line">Authors: Yifei Su, Ning Liu, Dong Chen, Zhen Zhao, Kun Wu, Meng Li, Zhiyuan Xu, Zhengping Che, Jian Tang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T14:12:53+00:00 · Latest: 2026-02-18T13:54:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08822v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08822v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approaches accelerate sampling in generative modeling-based visuomotor policies by adapting techniques originally developed to speed up image generation. However, a major distinction exists: image generation typically produces independent samples without temporal dependencies, while robotic manipulation requires generating action trajectories with continuity and temporal coherence. To this end, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. Concretely, we introduce a frequency consistency constraint objective that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on 40 tasks of LIBERO. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreqPolicy: 基于频率一致性的高效流基视动策略</div>
<div class="mono" style="margin-top:8px">基于生成建模的视动策略在机器人操作中得到了广泛应用，这得益于它们能够建模多模态动作分布的能力。然而，多步采样的高推理成本限制了其在实时机器人系统中的应用。现有方法通过适应原本用于加速图像生成的技术来加速基于生成建模的视动策略的采样。然而，一个主要区别在于：图像生成通常产生独立样本且没有时间依赖性，而机器人操作需要生成具有连续性和时间一致性的动作轨迹。为此，我们提出了一种名为FreqPolicy的新方法，首先在流基视动策略上施加频率一致性约束。我们的工作使动作模型能够有效地捕捉时间结构，同时支持高效、高质量的一步动作生成。具体而言，我们引入了一个频率一致性约束目标，该目标在流的不同时间步沿频率域动作特征上强制执行对齐，从而促进一步动作生成向目标分布收敛。此外，我们设计了一种自适应一致性损失来捕捉机器人操作任务中固有的结构时间变化。我们在3个仿真基准上的53个任务上评估了FreqPolicy，证明了它在现有一步动作生成器中的优越性。我们进一步将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在LIBERO的40个任务上实现了加速且未降低性能。此外，我们在真实世界机器人场景中展示了其高效性和有效性，推理频率为93.5 Hz。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FreqPolicy is a novel approach that imposes frequency consistency constraints on flow-based visuomotor policies to enable efficient and high-quality one-step action generation for robotic manipulation. It introduces a frequency consistency constraint objective and an adaptive consistency loss to promote temporal coherence. FreqPolicy outperforms existing one-step action generators on 53 tasks across three simulation benchmarks and integrates well with the vision-language-action model, maintaining performance on 40 tasks of LIBERO. It also demonstrates efficiency in real-world scenarios with an inference frequency of 93.5 Hz.</div>
<div class="mono" style="margin-top:8px">FreqPolicy 是一种通过频率一致性约束流基运动视觉策略来实现高效且高质量的一步动作生成的新方法，以适应机器人操作的需求。它引入了频率一致性约束目标和自适应一致性损失，以促进时间连贯性。FreqPolicy 在三个仿真基准上的 53 个任务中优于现有的一步动作生成器，并且与 VLA 模型集成良好，在 LIBERO 的 40 个任务上实现了加速且不降低性能。此外，它在真实世界的机器人场景中表现出高效性和有效性，推理频率为 93.5 Hz。</div>
</details>
</div>
<div class="card">
<div class="title">Reactive Motion Generation With Particle-Based Perception in Dynamic Environments</div>
<div class="meta-line">Authors: Xiyuan Zhao, Huijun Li, Lifeng Zhu, Zhikai Wei, Xianyi Zhu, Aiguo Song</div>
<div class="meta-line">First: 2026-02-18T13:48:54+00:00 · Latest: 2026-02-18T13:48:54+00:00</div>
<div class="meta-line">Comments: This paper has 20 pages, 15 figures, and 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16462v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reactive motion generation in dynamic and unstructured scenarios is typically subject to essentially static perception and system dynamics. Reliably modeling dynamic obstacles and optimizing collision-free trajectories under perceptive and control uncertainty are challenging. This article focuses on revealing tight connection between reactive planning and dynamic mapping for manipulators from a model-based perspective. To enable efficient particle-based perception with expressively dynamic property, we present a tensorized particle weight update scheme that explicitly maintains obstacle velocities and covariance meanwhile. Building upon this dynamic representation, we propose an obstacle-aware MPPI-based planning formulation that jointly propagates robot-obstacle dynamics, allowing future system motion to be predicted and evaluated under uncertainty. The model predictive method is shown to significantly improve safety and reactivity with dynamic surroundings. By applying our complete framework in simulated and noisy real-world environments, we demonstrate that explicit modeling of robot-obstacle dynamics consistently enhances performance over state-of-the-art MPPI-based perception-planning baselines avoiding multiple static and dynamic obstacles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于粒子感知的动力学环境下的反应式运动生成</div>
<div class="mono" style="margin-top:8px">在动态和非结构化的场景中，反应式运动生成通常依赖于基本静态的感知和系统动力学。准确建模动态障碍物并在感知和控制不确定性下优化无碰撞轨迹是具有挑战性的。本文从模型的角度重点揭示了反应式规划与动态制图之间的紧密联系。为了实现高效的粒子感知并具有动态特性，我们提出了一种张量化的粒子权重更新方案，该方案明确地维护了障碍物的速度和协方差。基于这种动态表示，我们提出了一种障碍物感知的MPPI基规划公式，该公式联合传播了机器人-障碍物动力学，使得未来系统的运动可以在不确定性下被预测和评估。模型预测方法被证明可以显著提高在动态环境中的安全性和反应性。通过在模拟和嘈杂的真实环境中应用我们完整的框架，我们证明了对机器人-障碍物动力学的显式建模在避免多个静态和动态障碍物方面始终优于最先进的MPPI基感知-规划基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reactive motion generation in dynamic environments by integrating dynamic perception and planning. It introduces a tensorized particle weight update scheme to maintain obstacle velocities and covariance, and proposes an obstacle-aware Model Predictive Path Integral (MPPI) planning method that jointly propagates robot-obstacle dynamics. Experimental results show that this approach significantly improves safety and reactivity compared to state-of-the-art baselines in both simulated and real-world environments with multiple static and dynamic obstacles.</div>
<div class="mono" style="margin-top:8px">本文通过结合动态感知和规划，解决了动态环境下的反应性运动生成问题。它引入了一种张量化的粒子权重更新方案，以维持障碍物的速度和协方差，并提出了一种考虑机器人-障碍物动力学的障碍物感知MPPI规划方法。结果显示，在动态障碍物存在的模拟和真实环境中，该方法在安全性和反应性方面显著优于最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation</div>
<div class="meta-line">Authors: Yixue Zhang, Kun Wu, Zhi Gao, Zhen Zhao, Pei Ren, Zhiyuan Xu, Fei Liao, Xinhua Wang, Shichao Fan, Di Wu, Qiuxuan Feng, Meng Li, Zhengping Che, Chang Liu, Jian Tang</div>
<div class="meta-line">First: 2026-02-18T13:29:43+00:00 · Latest: 2026-02-18T13:29:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16444v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16444v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robogene-boost-vla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboGene：通过多样性驱动的代理框架增强VLA预训练以实现现实世界任务生成</div>
<div class="mono" style="margin-top:8px">通用机器人操作的追求受到多样性和现实世界交互数据稀缺性的阻碍。与视觉或语言中的网页数据收集不同，机器人数据收集是一个涉及高昂物理成本的主动过程。因此，自动任务策展以最大化数据价值仍然是一个关键但尚未充分探索的挑战。现有的手动方法不可扩展且偏向于常见任务，而现成的基础模型往往会产生物理上不可行的指令。为了解决这个问题，我们引入了RoboGene，这是一种代理框架，旨在自动化生成单臂、双臂和移动机器人广泛物理可行的操作任务。RoboGene 结合了三个核心组件：多样性驱动的采样以实现广泛的任务覆盖、自我反思机制以强制执行物理约束以及人工在环改进以持续改进。我们进行了广泛的定量分析和大规模现实世界实验，收集了18000个轨迹的数据集，并引入了新的指标来评估任务的质量、可行性和多样性。结果表明，RoboGene 显著优于最先进的基础模型（例如GPT-4o、Gemini 2.5 Pro）。此外，现实世界实验表明，使用RoboGene预训练的VLA模型具有更高的成功率和更好的泛化能力，突显了高质量任务生成的重要性。我们的项目可在https://robogene-boost-vla.github.io/找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboGene is an agentic framework that automates the generation of diverse and physically plausible manipulation tasks for robotic manipulation. It integrates diversity-driven sampling, self-reflection mechanisms, and human-in-the-loop refinement. RoboGene significantly outperforms existing methods and enhances the performance of VLA models in real-world experiments, demonstrating the importance of high-quality task generation. Extensive quantitative analysis and large-scale real-world experiments were conducted to collect 18k trajectories and assess task quality, feasibility, and diversity.</div>
<div class="mono" style="margin-top:8px">RoboGene 是一个自动化框架，用于生成多样且物理上可行的机器人操作任务。它通过多样性的采样、自我反思机制和人工在环改进来确保任务的广泛覆盖和物理可行性。大量实验表明，RoboGene 超越了现有方法，并导致使用其数据集（包含18k轨迹）预训练的VLA模型在成功率和泛化能力上表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</div>
<div class="meta-line">Authors: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi</div>
<div class="meta-line">First: 2026-02-12T17:46:52+00:00 · Latest: 2026-02-18T11:55:37+00:00</div>
<div class="meta-line">Comments: VIRENA is under active development and currently in use at the University of Zurich. This preprint will be updated as new features are released. For the latest version and to inquire about demos or pilot collaborations, contact the authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12207v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12207v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA&#x27;s no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRENA：虚拟竞技场，用于研究、教育和民主创新</div>
<div class="mono" style="margin-top:8px">数字平台塑造了人们的交流、讨论和形成观点的方式。由于数据访问受限、现实世界实验的伦理限制以及现有研究工具的局限性，研究这些动态变得越来越困难。VIRENA（虚拟竞技场）是一个平台，它能够在现实社交媒体环境中进行受控实验。多个参与者可以同时在基于信息流的平台（Instagram、Facebook、Reddit）和即时通讯应用（WhatsApp、Messenger）的现实复制品中互动。由大型语言模型驱动的AI代理可以与人类一起参与，具有可配置的人设和现实行为。研究人员可以通过无需编程技能的可视化界面操控内容审核方法、预排定刺激内容，并在不同条件下运行实验。VIRENA 使得以前不切实际的研究设计成为可能：研究人类与AI的互动、在现实社会环境中实验性地比较干预措施以及观察群体讨论的展开过程。VIRENA 建立在开源技术之上，确保数据保留在机构控制之下并符合数据保护要求，目前在苏黎世大学使用中，并可供试点合作。VIRENA 的无代码界面使其跨学科和跨领域的受控社交媒体模拟变得可行。本文记录了其设计、架构和功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VIRENA is a platform designed to facilitate controlled experimentation in realistic social media environments, addressing the challenges of restricted data access and ethical constraints. It allows multiple participants to interact in replicas of platforms like Instagram, Facebook, Reddit, WhatsApp, and Messenger, with AI agents participating as configurable personas. Researchers can manipulate content moderation and run experiments through a no-code visual interface, enabling studies on human-AI interaction, moderation interventions, and group deliberation that were previously impractical. Key findings include the platform&#x27;s ability to simulate realistic social contexts and its potential for interdisciplinary research and public organization collaboration.</div>
<div class="mono" style="margin-top:8px">VIRENA 是一个平台，旨在促进在现实社交媒体环境中进行受控实验，解决数据访问受限和伦理约束的问题。它允许参与者在复制的 Instagram、Facebook、Reddit、WhatsApp 和 Messenger 等平台上互动，AI 代理以可配置的人格参与其中。研究人员可以通过无代码的可视化界面操纵内容审核并运行实验，使研究人类-AI 互动、审核干预措施和群体讨论成为可能，而这些在过去是不现实的。关键发现包括该平台能够模拟现实社会环境，并且具有跨学科研究和公共组织合作的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">AMBER: A tether-deployable gripping crawler with compliant microspines for canopy manipulation</div>
<div class="meta-line">Authors: P. A. Wigner, L. Romanello, A. Hammad, P. H. Nguyen, T. Lan, S. F. Armanini, B. B. Kocer, M. Kovac</div>
<div class="meta-line">First: 2025-12-08T16:17:56+00:00 · Latest: 2026-02-18T11:42:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07680v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.07680v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90$^\circ$ body roll and inclination, while effective climbing on branches inclined up to 67.5$^\circ$, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10$^\circ$, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. The crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing. The aerial deployment is demonstrated at a conceptual and feasibility level, while full drone-crawler integration is left as future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMBER：一种可部署的具有顺应性微钩爪的攀爬器，用于树冠操作</div>
<div class="mono" style="margin-top:8px">本文介绍了一种可空中部署的攀爬器，用于树冠内的适应性移动和操作。该系统结合了基于顺应性微钩爪的履带、双履带旋转夹持器和弹性尾巴，使其能够在不同曲率和倾斜角度的树枝上实现安全附着和稳定移动。实验表明，该攀爬器在90°身体滚转和倾斜角度下仍能可靠地抓握，有效攀爬倾斜角度达67.5°的树枝，最大速度为每秒0.55个身体长度。顺应性履带允许最大10°的偏航转向，提高在不规则表面的机动性。功率测量显示，该攀爬器的无量纲运输成本比典型悬停功率消耗低一个数量级，提供了一个坚固、低功耗的环境采样和树冠内传感平台。空中部署在概念和可行性层面进行了演示，而完整的无人机-攀爬器集成留作未来工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AMBER, a tether-deployable crawler designed for tree canopy manipulation. It features compliant microspine tracks, a dual-track rotary gripper, and an elastic tail, allowing secure attachment and stable traversal on branches of varying curvature and inclination. Key experimental results include reliable gripping up to 90° body roll and inclination, effective climbing on branches inclined up to 67.5°, and a maximum speed of 0.55 body lengths per second on horizontal branches. Power measurements indicate efficient operation with a lower cost of transport compared to typical hovering power consumption in aerial robots.</div>
<div class="mono" style="margin-top:8px">论文介绍了AMBER，一种可由缆绳部署的爬行器，用于树冠内的操作。它配备了顺应性微钩履带、双轨旋转夹持器和弹性尾巴，能够在不同曲率的树枝上实现安全附着和稳定移动。关键发现包括在90°身体滚转和倾斜角度下可靠抓握，有效攀爬倾斜角度达67.5°的树枝，最大速度为每秒0.55个身体长度。该爬行器展示了与典型飞行机器人悬停功率消耗相比更低的单位运输成本，表现出高效的运行特性。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped</div>
<div class="meta-line">Authors: Saumya Karan, Neerav Maram, Suraj Borate, Madhu Vadali</div>
<div class="meta-line">First: 2026-02-18T11:14:22+00:00 · Latest: 2026-02-18T11:14:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16371v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16371v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">SLOT (Soft Legged Omnidirectional Tetrapod), a tendon-driven soft quadruped robot with 3D-printed TPU legs, is presented to study physics-informed modeling and control of compliant legged locomotion using only four actuators. Each leg is modeled as a deformable continuum using discrete Cosserat rod theory, enabling the capture of large bending deformations, distributed elasticity, tendon actuation, and ground contact interactions. A modular whole-body modeling framework is introduced, in which compliant leg dynamics are represented through physically consistent reaction forces applied to a rigid torso, providing a scalable interface between continuum soft limbs and rigid-body locomotion dynamics. This formulation allows efficient whole-body simulation and real-time control without sacrificing physical fidelity. The proposed model is embedded into a convex model predictive control framework that optimizes ground reaction forces over a 0.495 s prediction horizon and maps them to tendon actuation through a physics-informed force-angle relationship. The resulting controller achieves asymptotic stability under diverse perturbations. The framework is experimentally validated on a physical prototype during crawling and walking gaits, achieving high accuracy with less than 5 mm RMSE in center of mass trajectories. These results demonstrate a generalizable approach for integrating continuum soft legs into model-based locomotion control, advancing scalable and reusable modeling and control methods for soft quadruped robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>腱驱动软四足动物的动态建模与MPC行走研究</div>
<div class="mono" style="margin-top:8px">SLOT（软腿全方位四足动物），一种使用3D打印TPU腿的腱驱动软四足动物机器人，用于研究仅使用四个执行器的顺应性腿足运动的物理启发式建模和控制。每条腿被建模为可变形连续体，使用离散柯西尔杆理论，能够捕捉到大弯曲变形、分布弹性、腱驱动和地面接触相互作用。引入了一种模块化的全身建模框架，在该框架中，通过在刚性躯干上施加物理一致的反作用力来表示顺应腿的动力学，提供了一种连续软肢和刚体运动动力学之间的可扩展接口。该公式允许高效的整体身体仿真和实时控制，而不牺牲物理精度。所提出的模型嵌入到凸模型预测控制框架中，该框架在0.495秒的预测窗口内优化地面反作用力，并通过物理启发的力-角关系将其映射到腱驱动。所得到的控制器在多种扰动下实现了渐近稳定性。该框架在爬行和行走步态的物理原型上进行了实验验证，实现了高精度，中心质量轨迹的RMSE小于5毫米。这些结果展示了将连续软腿整合到基于模型的运动控制中的通用方法，推进了软四足动物机器人可扩展和可重用建模与控制方法的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study presents SLOT, a tendon-driven soft quadruped robot, to investigate compliant legged locomotion using discrete Cosserat rod theory for modeling each leg as a deformable continuum. The approach integrates a modular whole-body modeling framework with a convex model predictive control (MPC) to optimize ground reaction forces and map them to tendon actuation. The controller achieves asymptotic stability under various perturbations, and experimental validation on a physical prototype shows high accuracy in center of mass trajectories with less than 5 mm RMSE during crawling and walking gaits.</div>
<div class="mono" style="margin-top:8px">该研究介绍了使用离散柯西尔杆理论建模每个可变形连续体腿部的腱驱动软四足机器人SLOT，以研究符合物理原理的建模和控制。该机器人采用模块化全身建模框架，能够高效模拟和实时控制，同时保持物理精度。提出的模型嵌入到凸模型预测控制框架中，优化地面反作用力并将其映射到腱驱动，实现了在各种扰动下的渐近稳定性。物理原型在爬行和行走姿态下的实验验证显示，中心质量轨迹的准确性低于5毫米RMSE。</div>
</details>
</div>
<div class="card">
<div class="title">Markerless 6D Pose Estimation and Position-Based Visual Servoing for Endoscopic Continuum Manipulators</div>
<div class="meta-line">Authors: Junhyun Park, Chunggil An, Myeongbo Park, Ihsan Ullah, Sihyeong Park, Minho Hwang</div>
<div class="meta-line">First: 2026-02-18T11:08:32+00:00 · Latest: 2026-02-18T11:08:32+00:00</div>
<div class="meta-line">Comments: 20 pages, 13 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16365v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16365v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuum manipulators in flexible endoscopic surgical systems offer high dexterity for minimally invasive procedures; however, accurate pose estimation and closed-loop control remain challenging due to hysteresis, compliance, and limited distal sensing. Vision-based approaches reduce hardware complexity but are often constrained by limited geometric observability and high computational overhead, restricting real-time closed-loop applicability. This paper presents a unified framework for markerless stereo 6D pose estimation and position-based visual servoing of continuum manipulators. A photo-realistic simulation pipeline enables large-scale automatic training with pixel-accurate annotations. A stereo-aware multi-feature fusion network jointly exploits segmentation masks, keypoints, heatmaps, and bounding boxes to enhance geometric observability. To enforce geometric consistency without iterative optimization, a feed-forward rendering-based refinement module predicts residual pose corrections in a single pass. A self-supervised sim-to-real adaptation strategy further improves real-world performance using unlabeled data. Extensive real-world validation achieves a mean translation error of 0.83 mm and a mean rotation error of 2.76° across 1,000 samples. Markerless closed-loop visual servoing driven by the estimated pose attains accurate trajectory tracking with a mean translation error of 2.07 mm and a mean rotation error of 7.41°, corresponding to 85% and 59% reductions compared to open-loop control, together with high repeatability in repeated point-reaching tasks. To the best of our knowledge, this work presents the first fully markerless pose-estimation-driven position-based visual servoing framework for continuum manipulators, enabling precise closed-loop control without physical markers or embedded sensing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无标记的6D姿态估计及基于位置的视觉伺服控制用于内窥镜连续型操作臂</div>
<div class="mono" style="margin-top:8px">柔性内窥镜手术系统中的连续型操作臂提供高灵活性以进行微创手术；然而，由于滞回、顺应性和末端传感有限，准确的姿态估计和闭环控制仍然具有挑战性。基于视觉的方法可以减少硬件复杂性，但通常受限于几何可观测性有限和高计算开销，限制了实时闭环应用。本文提出了一种统一框架，用于无标记立体6D姿态估计及基于位置的视觉伺服控制连续型操作臂。一个逼真的模拟管道使大规模自动训练成为可能，并具有像素级准确的注释。一种立体感知多特征融合网络联合利用分割掩码、关键点、热图和边界框来增强几何可观测性。为了在不进行迭代优化的情况下强制执行几何一致性，一个前馈渲染基础的精炼模块在单次通过中预测残差姿态修正。一种自我监督的模拟到现实的适应策略进一步使用未标记数据提高现实世界性能。广泛的现实世界验证实现了1000个样本的平均平移误差0.83毫米和平均旋转误差2.76度。基于估计姿态驱动的无标记闭环视觉伺服控制实现了准确的轨迹跟踪，平均平移误差2.07毫米，平均旋转误差7.41度，与开环控制相比，分别降低了85%和59%，并且在重复点到达任务中具有高重复性。据我们所知，这项工作首次提出了用于连续型操作臂的无标记姿态估计驱动的基于位置的视觉伺服控制框架，无需物理标记或嵌入式传感即可实现精确的闭环控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of accurate pose estimation and closed-loop control for continuum manipulators in endoscopic surgical systems. It proposes a unified framework combining markerless stereo 6D pose estimation and position-based visual servoing. The method uses a simulation pipeline for large-scale training and a stereo-aware multi-feature fusion network to improve geometric observability. A feed-forward refinement module ensures geometric consistency. Real-world validation shows mean translation and rotation errors of 0.83 mm and 2.76°, respectively, and closed-loop visual servoing achieves 85% and 59% reductions in errors compared to open-loop control. This work is the first to present a fully markerless pose-estimation-driven visual servoing framework for continuum manipulators, enhancing precision in endoscopic procedures without physical markers or embedded sensing.</div>
<div class="mono" style="margin-top:8px">本文针对内窥镜手术系统中连续 manipulator 的精确姿态估计和闭环控制难题，提出了一种无标记的立体6D姿态估计和基于位置的视觉伺服统一框架。该方法利用照片级真实的模拟管道和立体感知多特征融合网络来增强几何可观测性。前馈渲染基预测模块在单次通过中预测姿态修正，而不进行迭代优化。实验证明，平均平移误差为0.83毫米，平均旋转误差为2.76度，无标记的闭环视觉伺服控制相比开环控制分别减少了85%和59%的误差。</div>
</details>
</div>
<div class="card">
<div class="title">System Identification under Constraints and Disturbance: A Bayesian Estimation Approach</div>
<div class="meta-line">Authors: Sergi Martinez, Steve Tonneau, Carlos Mastalli</div>
<div class="meta-line">First: 2026-02-18T10:45:30+00:00 · Latest: 2026-02-18T10:45:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16358v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a Bayesian system identification (SysID) framework for jointly estimating robot&#x27;s state trajectories and physical parameters with high accuracy. It embeds physically consistent inverse dynamics, contact and loop-closure constraints, and fully featured joint friction models as hard, stage-wise equality constraints. It relies on energy-based regressors to enhance parameter observability, supports both equality and inequality priors on inertial and actuation parameters, enforces dynamically consistent disturbance projections, and augments proprioceptive measurements with energy observations to disambiguate nonlinear friction effects. To ensure scalability, we derive a parameterized equality-constrained Riccati recursion that preserves the banded structure of the problem, achieving linear complexity in the time horizon, and develop computationally efficient derivatives. Simulation studies on representative robotic systems, together with hardware experiments on a Unitree B1 equipped with a Z1 arm, demonstrate faster convergence, lower inertial and friction estimation errors, and improved contact consistency compared to forward-dynamics and decoupled identification baselines. When deployed within model predictive control frameworks, the resulting models yield measurable improvements in tracking performance during locomotion over challenging environments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a Bayesian system identification framework for robots that jointly estimates state trajectories and physical parameters with high accuracy. The method incorporates physically consistent constraints and friction models, and uses energy-based regressors to improve parameter observability. Experimental results on simulated and hardware robotic systems show faster convergence, lower estimation errors, and better contact consistency compared to baseline methods. When used in model predictive control, it improves tracking performance in challenging environments.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种贝叶斯系统识别框架，用于同时高精度估计机器人的状态轨迹和物理参数。它整合了物理一致的约束和摩擦模型，并使用能量基回归器提高参数可观察性。仿真和硬件实验表明，与传统方法相比，该方法具有更快的收敛速度、更低的估计误差和更好的接触一致性。当应用于模型预测控制时，它能提升在复杂环境中的跟踪性能。</div>
</details>
</div>
<div class="card">
<div class="title">Articulated 3D Scene Graphs for Open-World Mobile Manipulation</div>
<div class="meta-line">Authors: Martin Büchner, Adrian Röfer, Tim Engelbracht, Tim Welschehold, Zuria Bauer, Hermann Blum, Marc Pollefeys, Abhinav Valada</div>
<div class="meta-line">First: 2026-02-18T10:40:35+00:00 · Latest: 2026-02-18T10:40:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16356v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://momasg.cs.uni-freiburg.de">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>articulated 3d场景图用于开放世界移动操作</div>
<div class="mono" style="margin-top:8px">语义学使3d场景理解和基于可操作性的物体交互成为可能。然而，机器人在现实世界环境中操作时面临一个关键限制：它们无法预测物体的移动。长时程移动操作需要在语义学、几何学和运动学之间缩小差距。在本文中，我们提出了MoMa-SG，一种用于构建包含众多可交互物体的articulated场景的语义-运动3d场景图的新框架。给定包含多个物体articulation的RGB-D序列，我们进行时间分割以对象交互，并使用遮挡鲁棒点跟踪推断对象运动。然后，我们将点轨迹提升到3d，并使用一种新颖的统一旋量估计公式估计关节参数，该公式在单次优化过程中稳健地估计旋转和直线关节参数。接下来，我们将对象与估计的articulation关联，并通过在识别的开启状态下推理父-子关系来检测包含对象。我们还引入了新颖的Arti4D-语义数据集，该数据集独特地结合了层次物体语义，包括父-子关系标签和物体轴注释，跨越62个野外RGB-D序列，包含600个物体交互和三种不同的观测范式。我们对两个数据集上的MoMa-SG性能进行了广泛评估，并消融了我们方法的关键设计选择。此外，我们在四足机器人和移动操作器上的实际实验表明，我们的语义-运动场景图使在日常家庭环境中操作articulated物体变得稳健。我们提供了代码和数据：https://momasg.cs.uni-freiburg.de/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of long-horizon mobile manipulation by developing MoMa-SG, a framework that builds semantic-kinematic 3D scene graphs for articulated scenes. Given RGB-D sequences, the framework segments object interactions, infers object motion, and estimates articulation models using a unified twist estimation formulation. Key findings include robust performance on two datasets and successful real-world experiments on a quadruped and a mobile manipulator, demonstrating the ability to manipulate articulated objects in home environments.</div>
<div class="mono" style="margin-top:8px">该研究通过开发MoMa-SG框架解决了长期移动操作的挑战，该框架构建了包含语义和动力学信息的3D场景图以处理具有多个可交互物体的场景。给定RGB-D序列，该框架对物体交互进行时间分割，推断物体运动，并使用统一的旋量估计公式估计关节模型。关键发现包括该框架能够在家庭环境中稳健操作具有多个关节的物体，并引入了Arti4D-Semantic数据集，该数据集包含62个序列中的600个物体交互的层次物体语义和轴注释。在四足机器人和移动操作器上的实际实验验证了该框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Factored Latent Action World Models</div>
<div class="meta-line">Authors: Zizhao Wang, Chang Shi, Jiaheng Hu, Kevin Rohling, Roberto Martín-Martín, Amy Zhang, Peter Stone</div>
<div class="meta-line">First: 2026-02-18T07:08:14+00:00 · Latest: 2026-02-18T07:08:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16229v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16229v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning latent actions from action-free video has emerged as a powerful paradigm for scaling up controllable world model learning. Latent actions provide a natural interface for users to iteratively generate and manipulate videos. However, most existing approaches rely on monolithic inverse and forward dynamics models that learn a single latent action to control the entire scene, and therefore struggle in complex environments where multiple entities act simultaneously. This paper introduces Factored Latent Action Model (FLAM), a factored dynamics framework that decomposes the scene into independent factors, each inferring its own latent action and predicting its own next-step factor value. This factorized structure enables more accurate modeling of complex multi-entity dynamics and improves video generation quality in action-free video settings compared to monolithic models. Based on experiments on both simulation and real-world multi-entity datasets, we find that FLAM outperforms prior work in prediction accuracy and representation quality, and facilitates downstream policy learning, demonstrating the benefits of factorized latent action models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>因子潜在动作世界模型</div>
<div class="mono" style="margin-top:8px">从无动作视频中学习潜在动作已成为扩展可控世界模型学习能力的强大范式。潜在动作为用户提供了一个自然界面，可以迭代生成和操作视频。然而，大多数现有方法依赖于单一的逆向和正向动力学模型，学习控制整个场景的单一潜在动作，因此在多个实体同时作用的复杂环境中表现不佳。本文介绍了因子潜在动作模型（FLAM），这是一种分解场景为独立因素的因子动力学框架，每个因素独立推断其潜在动作并预测其下一时刻的状态值。这种因子化结构能够更准确地建模复杂的多实体动力学，并在无动作视频设置中提高视频生成质量，优于单一模型。基于对模拟和真实世界多实体数据集的实验，我们发现FLAM在预测准确性和表示质量上优于先前的工作，并促进了下游策略学习，展示了因子潜在动作模型的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the limitations of monolithic models in handling complex multi-entity environments by introducing Factored Latent Action Model (FLAM), which decomposes the scene into independent factors. FLAM learns individual latent actions for each factor, improving prediction accuracy and video generation quality compared to monolithic models. Experiments on both simulated and real-world datasets show that FLAM outperforms previous methods in terms of prediction accuracy and representation quality, and enhances downstream policy learning.</div>
<div class="mono" style="margin-top:8px">研究旨在提高在复杂环境中多个实体共存时可控制世界模型学习的规模性和准确性。提出了因子化潜动作模型（FLAM），将场景分解为独立的因子，每个因子有自己的潜动作。实验表明，FLAM在预测准确性和表示质量上优于单一模型，并在仿真和真实世界数据集中的下游策略学习中表现出优势。</div>
</details>
</div>
<div class="card">
<div class="title">BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames</div>
<div class="meta-line">Authors: Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi, Dhruv Shah, Aviral Kumar</div>
<div class="meta-line">First: 2026-02-16T18:49:56+00:00 · Latest: 2026-02-18T07:07:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15010v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15010v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bigpicturepolicies.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations. Videos are available at https://bigpicturepolicies.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BPP：通过关注关键历史帧进行长上下文机器人模仿学习</div>
<div class="mono" style="margin-top:8px">许多机器人的任务需要关注过去的观察历史。例如，在房间里寻找物品需要记住已经搜索过的地方。然而，表现最佳的机器人策略通常仅依赖于当前的观察，限制了它们在这些任务中的应用。简单地依赖过去的观察往往由于虚假相关性而失败：策略会抓住训练历史中的偶然特征，这些特征在部署到新的分布时无法泛化。我们分析了为什么策略会抓住这些虚假相关性，并发现这个问题源于训练过程中对可能历史的覆盖范围有限，这种覆盖范围随着时间范围的增加而呈指数增长。现有的正则化技术在不同任务中提供的益处不一致，因为它们没有从根本上解决这个问题。受这些发现的启发，我们提出了大图策略（BPP），该方法基于视觉-语言模型检测到的有意义的关键帧进行条件化。通过将多样化的演示投影到一组与任务相关的事件上，BPP显著减少了训练和部署之间的分布偏移，而不会牺牲表达能力。我们在四个具有挑战性的现实世界操作任务和三个模拟任务上评估了BPP，所有任务都需要历史条件化。BPP在现实世界评估中的成功率比最佳对比方法高70%。有关视频请参见https://bigpicturepolicies.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of robot imitation learning by focusing on key history frames to improve the robot&#x27;s ability to remember past observations. It proposes Big Picture Policies (BPP), which condition on a minimal set of meaningful keyframes detected by a vision-language model. This approach reduces distribution shift between training and deployment, leading to significantly higher success rates in real-world manipulation tasks compared to existing methods, achieving 70% higher success rates in evaluations. Videos are available at https://bigpicturepolicies.github.io/.</div>
<div class="mono" style="margin-top:8px">该论文旨在通过关注关键历史帧来改进机器人模仿学习，解决需要长期记忆的任务挑战。作者提出了Big Picture Policies (BPP) 方法，该方法基于视觉-语言模型检测出的有意义的关键帧进行条件判断。这种方法在训练和部署之间减少了分布差异，从而在真实世界的操作任务中取得了显著更高的成功率，平均高出70%。更多信息和视频见https://bigpicturepolicies.github.io/.</div>
</details>
</div>
<div class="card">
<div class="title">RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</div>
<div class="meta-line">Authors: Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield</div>
<div class="meta-line">Venue: CVPR 2025 Oral</div>
<div class="meta-line">First: 2024-11-25T16:21:34+00:00 · Latest: 2026-02-18T04:26:35+00:00</div>
<div class="meta-line">Comments: CVPR 2025 (Oral); Project Website: https://chanh.ee/RoboSpatial</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.16537v5">Abs</a> · <a href="https://arxiv.org/pdf/2411.16537v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face significant challenges in spatial reasoning tasks, as their training data are based on general-purpose image datasets that often lack sophisticated spatial understanding. For example, datasets frequently do not capture reference frame comprehension, yet effective spatial reasoning requires understanding whether to reason from ego-, world-, or object-centric perspectives. To address this issue, we introduce RoboSpatial, a large-scale dataset for spatial understanding in robotics. It consists of real indoor and tabletop scenes, captured as 3D scans and egocentric images, and annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial relationships, and the pairing of 2D egocentric images with 3D scans makes it both 2D- and 3D- ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robot manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboSpatial：向2D和3D视觉语言模型传授空间理解能力以应用于机器人技术</div>
<div class="mono" style="margin-top:8px">空间理解是使机器人能够感知其周围环境、对其环境进行推理并与其进行有意义互动的关键能力。在现代机器人技术中，这些能力越来越多地由视觉语言模型提供。然而，这些模型在空间推理任务中面临重大挑战，因为它们的训练数据基于一般用途的图像数据集，这些数据集往往缺乏复杂的空间理解。例如，数据集通常未能捕捉到参考框架理解，而有效的空间推理需要理解是从自我中心、世界中心还是物体中心的角度进行推理。为了解决这一问题，我们引入了RoboSpatial，这是一个用于机器人技术的空间理解大规模数据集。它包含真实的室内和台面场景，以3D扫描和自我中心图像的形式捕获，并附有与机器人相关的丰富空间信息的注释。该数据集包括100万张图像、5000个3D扫描和300万注释的空间关系，2D自我中心图像与3D扫描的配对使其既适用于2D也适用于3D。我们的实验表明，使用RoboSpatial训练的模型在下游任务如空间功能预测、空间关系预测和机器人操作上优于基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboSpatial is a large-scale dataset designed to enhance spatial understanding in robotics by providing rich spatial annotations for both 2D and 3D vision-language models. The dataset includes real indoor and tabletop scenes, 3D scans, and egocentric images, and it significantly improves models&#x27; performance on tasks like spatial affordance prediction and robot manipulation compared to baselines.</div>
<div class="mono" style="margin-top:8px">RoboSpatial 是一个大规模数据集，旨在通过为 2D 和 3D 视觉-语言模型提供丰富的空间注释来增强机器人领域的空间理解能力。该数据集包含真实的室内和桌面场景、3D 扫描和第一人称视角图像，并显著提高了模型在空间功能预测和机器人操作等任务上的表现，优于基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Personalized Agents from Human Feedback</div>
<div class="meta-line">Authors: Kaiqu Liang, Julia Kruk, Shengyi Qian, Xianjun Yang, Shengjie Bi, Yuanshun Yao, Shaoliang Nie, Mingyang Zhang, Lijuan Liu, Jaime Fernández Fisac, Shuyan Zhou, Saghar Hosseini</div>
<div class="meta-line">First: 2026-02-18T04:18:47+00:00 · Latest: 2026-02-18T04:18:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16173v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16173v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent&#x27;s ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从人类反馈中学习个性化代理</div>
<div class="mono" style="margin-top:8px">现代AI代理功能强大，但往往无法与个体用户的独特、不断变化的偏好保持一致。先前的方法通常依赖于静态数据集，要么通过交互历史训练隐式偏好模型，要么在外存中编码用户配置文件。然而，这些方法在处理新用户和随着时间变化的偏好时存在困难。我们提出了人类反馈中的个性化代理（PAHF）框架，该框架允许代理通过实时交互中的明确用户记忆进行在线持续个性化。PAHF 实现了一个三步循环：（1）在行动前寻求澄清以解决歧义，（2）将行动与从记忆中检索的偏好联系起来，（3）在行动后整合反馈以更新记忆以适应偏好的变化。为了评估这一能力，我们开发了四个阶段的协议和两个基准，分别在实体操作和在线购物中进行。这些基准量化了代理从零开始学习初始偏好并随后适应个性转变的能力。我们的理论分析和实验结果表明，结合明确的记忆与双反馈通道是至关重要的：PAHF 学习速度显著加快，并且始终优于无记忆和单通道基线，减少了初始个性化误差并使快速适应偏好变化成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of aligning AI agents with individual users&#x27; evolving preferences. It proposes PAHF, a framework that allows agents to learn online from live interactions using explicit per-user memory. PAHF consists of three steps: seeking clarification, grounding actions, and updating memory based on feedback. The framework is evaluated on two benchmarks in embodied manipulation and online shopping, showing that PAHF learns faster and adapts better to preference changes compared to baselines without memory or single feedback channels.</div>
<div class="mono" style="margin-top:8px">研究旨在解决AI代理与个体用户不断变化的偏好对齐的问题。它引入了PAHF框架，该框架允许代理从实时交互中学习，并使用用户特定的记忆。PAHF包括三个步骤：寻求澄清、将行动与偏好对接以及根据反馈更新记忆。研究通过四个阶段的协议和两个基准测试（包括实体操作和在线购物）来评估PAHF，结果显示PAHF学习速度更快，并且能够更好地适应偏好变化，优于没有记忆或单一反馈通道的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</div>
<div class="meta-line">Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</div>
<div class="meta-line">First: 2026-02-12T18:59:59+00:00 · Latest: 2026-02-18T03:42:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12281v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12281v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &quot;intention-action gap.&quot; We first characterize the test-time scaling laws for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce CoVer-VLA, a hierarchical test-time verification pipeline using the trained verifier. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses the verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer-VLA achieves 14% gains in task progress and 9% in success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展验证比扩展策略学习更能有效实现视觉-语言-行动对齐</div>
<div class="mono" style="margin-top:8px">通用机器人长期愿景依赖于它们理解和执行自然语言指令的能力。视觉-语言-行动（VLA）模型在这一目标上取得了显著进展，但它们生成的动作仍然可能与给定的指令不一致。在本文中，我们研究测试时验证作为缩小“意图-行动差距”的手段。我们首先表征了基于指令执行的测试时扩展定律，表明同时扩展重述指令的数量和生成动作的数量大大增加了测试时样本多样性，通常比独立扩展每个维度更有效地恢复正确动作。为了利用这些扩展定律，我们提出了CoVer，一种对比验证器，用于视觉-语言-行动对齐，并展示了我们的架构随着额外计算资源和数据的增加而平滑扩展。然后，我们引入了CoVer-VLA，一种分层测试时验证流水线，使用训练好的验证器。在部署时，我们的框架从视觉语言模型（VLM）预计算一组多样化的重述指令，反复为每条指令生成动作候选，然后使用验证器选择最优的高层提示和低层动作片段。与在相同数据上扩展策略预训练相比，我们的验证方法在SIMPLER基准测试中获得了22%的同分布改进和13%的异分布改进，在实际实验中进一步提高了45%。在PolaRiS基准测试中，CoVer-VLA实现了14%的任务进展和9%的成功率提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores test-time verification as a method to improve the alignment between actions and natural language instructions in Vision-Language-Action models. By jointly scaling the number of rephrased instructions and generated actions, the authors demonstrate increased test-time sample diversity. They introduce CoVer, a contrastive verifier, which scales effectively with additional resources. The CoVer-VLA pipeline, a hierarchical test-time verification system, precomputes diverse rephrased instructions and selects optimal actions, achieving 22% in-distribution and 13% out-of-distribution gains on the SIMPLER benchmark, and 14% and 9% improvements on the PolaRiS benchmark respectively.</div>
<div class="mono" style="margin-top:8px">本文探讨了测试时验证在视觉-语言-行动模型中将动作与自然语言指令对齐的有效性。通过表征指令跟随的缩放定律，作者表明联合缩放重述指令和生成的动作可以增加测试时样本多样性。他们引入了CoVer对比验证器和CoVer-VLA层次验证流水线，在SIMPLER基准测试中，与缩放策略预训练相比，该验证方法在分布内提高了22%，在分布外提高了13%，在实际实验中提高了45%。在PolaRiS基准测试中，CoVer-VLA在任务进度上提高了14%，在成功率上提高了9%。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Diffusion Policies for Robotic Manipulation of Timber Joinery under Fabrication Uncertainty</div>
<div class="meta-line">Authors: Salma Mozaffari, Daniel Ruan, William van den Bogert, Nima Fazeli, Sigrid Adriaenssens, Arash Adel</div>
<div class="meta-line">First: 2025-11-21T20:43:46+00:00 · Latest: 2026-02-17T21:12:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.17774v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.17774v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fabrication uncertainties, such as tolerance accumulation and material imperfections, pose a significant challenge to contact-rich robotic manipulation in construction by hindering precise and robust assembly. In this paper, we investigate the performance and robustness of diffusion policy learning for contact-rich assembly at the construction scale, using a tight-fitting timber mortise and tenon joint as a case study. A two-phase experimental study is conducted: first, to evaluate baseline policy performance and applicability; second, to assess policy robustness under fabrication-induced uncertainties modeled as randomized perturbations to the mortise position. The diffusion policy is trained on teleoperated demonstrations using an industrial robotic arm conditioned on end-effector pose and force/torque feedback. The best-performing policy achieved a total average success rate of 75% under perturbations up to 10 mm, including 100% success in unperturbed cases. The results demonstrate the potential of sensory-motor diffusion policies to enable high-precision contact-rich manipulation on large-scale industrial robotic arms, reducing reliance on skilled manual intervention. This work advances robotic construction under uncertainty and provides practical insights for deploying learning-based control in real-world Architectural, Engineering, and Construction applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习扩散策略以应对木材接合制造不确定性下的机器人操作</div>
<div class="mono" style="margin-top:8px">制造不确定性，如公差累积和材料缺陷，对建筑领域中接触丰富的机器人操作构成了重大挑战，因为它妨碍了精确和稳健的装配。本文探讨了扩散策略学习在建筑规模下接触丰富装配的性能和鲁棒性，以紧密配合的木材榫卯接合为例进行了研究。进行了两阶段实验研究：首先，评估基线策略的性能和适用性；其次，评估在由榫卯位置随机化扰动建模的制造不确定性下的策略鲁棒性。扩散策略在工业机器人臂上通过末端执行器姿态和力/力矩反馈的示教进行训练。表现最佳的策略在扰动最大10毫米的情况下，平均成功率达到了75%，包括100%的成功率在未扰动情况下。结果表明，感觉运动扩散策略有可能在大型工业机器人臂上实现高精度的接触丰富操作，减少对熟练手动干预的依赖。这项工作推进了在不确定性下的机器人建筑，并为在建筑、工程和施工应用中部署基于学习的控制提供了实用见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of fabrication uncertainties in robotic manipulation of timber joinery by investigating the performance and robustness of diffusion policy learning. A two-phase experimental study was conducted using a tight-fitting timber mortise and tenon joint. The diffusion policy was trained on teleoperated demonstrations and achieved a 75% average success rate under perturbations up to 10 mm, with 100% success in unperturbed cases, demonstrating the potential for high-precision contact-rich manipulation on large-scale industrial robotic arms.</div>
<div class="mono" style="margin-top:8px">本文研究了在制造不确定性下使用扩散策略进行木材接合机器人操作的方法。通过使用榫卯接合作为案例研究，进行了两阶段实验研究，评估了这些策略的性能和鲁棒性。所训练的扩散策略在最大10毫米的扰动下实现了75%的成功率，无扰动情况下100%成功，展示了在大型机器人臂上进行高精度接触丰富操作的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation</div>
<div class="meta-line">Authors: Yuxuan Kuang, Sungjae Park, Katerina Fragkiadaki, Shubham Tulsiani</div>
<div class="meta-line">First: 2026-02-17T18:59:31+00:00 · Latest: 2026-02-17T18:59:31+00:00</div>
<div class="meta-line">Comments: Project page: https://dex4d.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15828v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dex4d.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this &#x27;Anypose-to-Anypose&#x27; policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dex4D：任务无关的点轨迹策略以实现从仿真到现实的灵巧操作</div>
<div class="mono" style="margin-top:8px">学习能够完成多种日常任务的一般性策略仍然是灵巧操作中的一个开放挑战。特别是，通过现实世界的远程操作收集大规模操作数据既昂贵又难以扩展。虽然在仿真中学习提供了一种可行的替代方案，但设计多个特定任务的环境和奖励进行训练同样具有挑战性。我们提出了Dex4D框架，该框架利用仿真来学习任务无关的灵巧技能，这些技能可以灵活重组以执行各种现实世界的操作任务。具体而言，Dex4D学习了一种领域无关的3D点轨迹条件策略，该策略能够操作任何物体到任何期望的姿态。我们在数千种具有不同姿态配置的物体上进行了仿真训练，覆盖了可以在测试时组合的广泛机器人-物体交互空间。在部署时，该策略可以通过仅提示其期望的物体中心点轨迹（从生成的视频中提取）进行零样本转移，无需微调即可应用于现实世界的任务。在执行过程中，Dex4D使用在线点跟踪进行闭环感知和控制。在仿真和真实机器人上的大量实验表明，我们的方法能够实现多种灵巧操作任务的零样本部署，并且在先前基线方法上取得了持续改进。此外，我们展示了其对新型物体、场景布局、背景和轨迹的强大泛化能力，突显了所提出框架的鲁棒性和可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Dex4D is a framework designed to learn task-agnostic dexterous manipulation skills in simulation, which can be flexibly applied to various real-world tasks. It trains a &#x27;Anypose-to-Anypose&#x27; policy to manipulate any object to any desired pose across thousands of objects with diverse configurations. This policy can be directly transferred to real-world tasks without fine-tuning, using object-centric point tracks from generated videos. Experiments show that Dex4D outperforms previous methods and demonstrates strong generalization to new objects and scenarios.</div>
<div class="mono" style="margin-top:8px">Dex4D 是一个框架，旨在通过模拟学习通用的灵巧操作技能，这些技能可以在无需微调的情况下灵活地应用于各种现实世界任务。它训练了一个3D点轨迹策略，能够在数千种具有不同配置的对象上操纵任何物体到任何期望的姿态。通过从生成的视频中提取对象中心的点轨迹来部署该策略，并使用在线点跟踪进行闭环控制。实验表明，Dex4D 能够零样本部署到各种操作任务，并且在新型场景中表现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching</div>
<div class="meta-line">Authors: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza, Guanya Shi, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-17T18:59:11+00:00 · Latest: 2026-02-17T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知类人公园跑酷：通过运动匹配串联动态人类技能</div>
<div class="mono" style="margin-top:8px">尽管近年来类人机器人在多变地形上的稳定行走取得了进展，但捕捉人类高度动态运动的敏捷性和适应性仍然是一个开放的挑战。特别是，在复杂环境中进行敏捷公园跑酷不仅需要低级别的鲁棒性，还需要类似人类的运动表达性、长期技能组合以及感知驱动的决策制定。在本文中，我们提出了感知类人公园跑酷（PHP），这是一种模块化框架，使类人机器人能够自主地在具有挑战性的障碍赛道上进行长期视角导向的公园跑酷。我们的方法首先利用运动匹配，将其形式化为特征空间中的最近邻搜索，将重新定位的基本人类技能组合成长期的运动轨迹。该框架使复杂的技能链能够灵活组合和平滑过渡，同时保持动态人类运动的优雅和流畅性。接下来，我们为这些组合的运动训练基于运动跟踪的强化学习（RL）专家策略，并使用DAgger和RL的组合将其提炼为一个基于深度的、多技能学生策略。关键的是，感知与技能组合的结合使自主、上下文感知的决策成为可能：仅使用机载深度传感和离散的二维速度命令，机器人可以选择并执行跨越、攀爬、飞跃或滚落不同几何形状和高度的障碍物。我们通过在Unitree G1类人机器人上进行广泛的实地实验验证了该框架，展示了诸如攀爬高达1.25米（96%机器人高度）的障碍物等高度动态的公园跑酷技能，以及在实时障碍扰动下进行长期多障碍物穿越的闭环适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents Perceptive Humanoid Parkour (PHP), a modular framework that allows humanoid robots to autonomously perform complex parkour skills across challenging obstacle courses. The approach uses motion matching to compose human skills into long-horizon kinematic trajectories and trains reinforcement learning policies to execute these skills. Key findings include the robot&#x27;s ability to perform dynamic parkour skills like climbing obstacles up to 1.25m and traversing multiple obstacles with real-time adaptation.</div>
<div class="mono" style="margin-top:8px">本文旨在解决类人机器人在复杂环境如公园跑酷中实现人类般的敏捷性和适应性这一挑战。作者提出了感知类人公园跑酷（PHP）模块化框架，使用运动匹配将人类技能组合成长时间轨迹。然后，他们使用强化学习策略训练这些技能的执行，并仅使用机载深度传感。机器人成功执行了动态公园跑酷技能，包括攀爬高达1.25m的障碍物以及在实时障碍变化下进行多障碍物穿越和闭环适应。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Memory Injection Attacks for Multi-Turn Conversations</div>
<div class="meta-line">Authors: Christian Schlarmann, Matthias Hein</div>
<div class="meta-line">First: 2026-02-17T18:34:59+00:00 · Latest: 2026-02-17T18:34:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15927v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15927v1">PDF</a> · <a href="https://github.com/chs20/visual-memory-injection">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉记忆注入攻击在多轮对话中的应用</div>
<div class="mono" style="margin-top:8px">生成式大型视觉-语言模型（LVLMs）最近取得了显著的性能提升，其用户基础正在迅速增长。然而，特别是在长上下文多轮对话环境中，LVLMs的安全性很大程度上尚未被探索。在本文中，我们考虑了一种现实场景：攻击者上传了一个被篡改的图像到网络/社交媒体。一个善意的用户下载了该图像并将其作为输入提供给LVLM。我们设计了一种新颖的隐蔽视觉记忆注入（VMI）攻击，使得在正常提示下LVLM表现出正常行为，但一旦用户给出触发提示，LVLM会输出一个特定的目标信息来操纵用户，例如进行对抗性营销或政治说服。与之前专注于单轮攻击的工作相比，VMI即使在与用户进行长时间多轮对话后仍然有效。我们对几个最近的开放权重LVLM进行了攻击演示。因此，本文表明，在多轮对话环境中，通过篡改图像大规模操纵用户是可行的，这要求LVLMs具有更好的抗攻击鲁棒性。我们已在https://github.com/chs20/visual-memory-injection发布了源代码</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security risks of generative large vision-language models (LVLMs) in multi-turn conversation scenarios, where an attacker can manipulate images to trigger specific responses from users. The method involves a stealthy Visual Memory Injection (VMI) attack that allows the LVLM to behave normally until a triggering prompt, at which point it outputs a predetermined message. The study demonstrates the effectiveness of this attack on several recent LVLMs, highlighting the need for improved robustness against such manipulations in long-context conversations.</div>
<div class="mono" style="margin-top:8px">本文研究了生成型大型视觉语言模型（LVLM）在多轮对话中的安全风险，攻击者可以通过操纵图像在对话中注入特定信息。作者提出了一种新颖的隐形视觉记忆注入（VMI）攻击，在触发提示之前保持隐蔽，一旦触发，模型会输出预设的信息。该攻击在多轮对话后仍然有效。实验表明，通过扰动图像在多轮对话中操纵用户是可行的，强调了这些模型需要提高对抗此类攻击的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Spanning the Visual Analogy Space with a Weight Basis of LoRAs</div>
<div class="meta-line">Authors: Hila Manor, Rinon Gal, Haggai Maron, Tomer Michaeli, Gal Chechik</div>
<div class="meta-line">First: 2026-02-17T17:02:38+00:00 · Latest: 2026-02-17T17:02:38+00:00</div>
<div class="meta-line">Comments: Code and data are in https://research.nvidia.com/labs/par/lorweb</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15727v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15727v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}&#x27;$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}&#x27;$ such that $\mathbf{a} : \mathbf{a}&#x27; :: \mathbf{b} : \mathbf{b}&#x27;$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a &quot;space of LoRAs&quot;. We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of visual analogy learning, aiming to enable complex image manipulations through demonstration. It proposes LoRWeB, which uses a dynamic composition of learned transformation primitives at inference time. Key components include a learnable basis of LoRA modules and a lightweight encoder for selecting and weighing these modules. Experiments show that LoRWeB outperforms existing methods and enhances generalization to unseen transformations.</div>
<div class="mono" style="margin-top:8px">论文旨在解决通过演示实现复杂图像操作的视觉类比学习问题。提出了一种名为LoRWeB的方法，该方法在推理时动态组合学习到的变换基元，以提高泛化能力。关键组件包括一个可学习的LoRA模块基底和一个轻量级编码器，用于选择和加权这些模块。实验表明，LoRWeB在性能上优于现有方法，并显著提高了对未见过的变换的泛化能力，这表明LoRA基底分解是实现灵活视觉操作的一个有前景的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation</div>
<div class="meta-line">Authors: Yorai Shaoul, Zhe Chen, Mohamed Naveed Gul Mohamed, Federico Pecora, Maxim Likhachev, Jiaoyang Li</div>
<div class="meta-line">First: 2025-11-14T01:05:58+00:00 · Latest: 2026-02-17T15:48:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10874v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10874v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale. Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks. To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning. Within this framework, a generative model co-generates contact formations and manipulation trajectories from visual observations, while a novel motion planner conveys robots at scale. Crucially, the same planner also supports coordination at the object level, assigning manipulated objects to larger target structures and thereby unifying robot- and object-level reasoning within a single algorithmic framework. Experiments in challenging simulated environments demonstrate that our approach outperforms baselines in both motion planning and manipulation tasks, highlighting the benefits of generative co-design and integrated planning for scaling collaborative manipulation to complex multi-agent, multi-object settings. Visit gco-paper.github.io for code and demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作多机器人非抓取操作通过流匹配共生成</div>
<div class="mono" style="margin-top:8px">协调机器人团队在拥挤环境中重新定位多个物体需要同时推理机器人应在何处建立接触、接触后如何操作物体以及如何在大规模下安全高效地导航。先前的方法通常分为两种极端情况——要么学习整个任务，要么依赖特权信息和手设计的规划器——这两种方法都难以处理长时任务中的多样化物体。为应对这些挑战，我们提出了一种统一的框架，用于协作多机器人、多物体非抓取操作，该框架结合了流匹配共生成和匿名多机器人运动规划。在此框架内，生成模型从视觉观察中共同生成接触形式和操作轨迹，而一种新颖的运动规划器则在大规模下引导机器人。关键的是，同一个规划器还支持物体层面的协调，将被操作的物体分配给更大的目标结构，从而在单一算法框架内统一了机器人层面和物体层面的推理。在具有挑战性的模拟环境中进行的实验表明，我们的方法在运动规划和操作任务中均优于基线方法，突显了生成共设计和集成规划在扩展协作操作到复杂多智能体、多物体设置中的优势。请访问 gco-paper.github.io 获取代码和演示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of coordinating multiple robots to manipulate multiple objects in cluttered environments. It introduces a unified framework that combines flow-matching co-generation with anonymous multi-robot motion planning. The generative model co-generates contact formations and manipulation trajectories, while the motion planner scales the robots&#x27; movements. Experiments show that this approach outperforms baselines in both motion planning and manipulation tasks, demonstrating the benefits of integrated planning and generative co-design for complex multi-agent, multi-object settings.</div>
<div class="mono" style="margin-top:8px">研究旨在协调一群机器人在杂乱环境中操作多个物体，解决接触形成、操作和安全导航的挑战。方法结合了流匹配共生成模型和新颖的运动规划器，从视觉观察中生成接触形成和操作轨迹，并协调大规模机器人。实验表明，该方法在运动规划和操作任务中均优于基线方法，展示了生成共设计和集成规划在复杂多智能体、多物体设置中的优势。</div>
</details>
</div>
<div class="card">
<div class="title">World Action Models are Zero-shot Policies</div>
<div class="meta-line">Authors: Seonghyeon Ye, Yunhao Ge, Kaiyuan Zheng, Shenyuan Gao, Sihyun Yu, George Kurian, Suneel Indupuru, You Liang Tan, Chuning Zhu, Jiannan Xiang, Ayaan Malik, Kyungmin Lee, William Liang, Nadun Ranawaka, Jiasheng Gu, Yinzhen Xu, Guanzhi Wang, Fengyuan Hu, Avnish Narayan, Johan Bjorck, Jing Wang, Gwanghyun Kim, Dantong Niu, Ruijie Zheng, Yuqi Xie, Jimmy Wu, Qi Wang, Ryan Julian, Danfei Xu, Yilun Du, Yevgen Chebotar, Scott Reed, Jan Kautz, Yuke Zhu, Linxi &quot;Jim&quot; Fan, Joel Jang</div>
<div class="meta-line">First: 2026-02-17T15:04:02+00:00 · Latest: 2026-02-17T15:04:02+00:00</div>
<div class="meta-line">Comments: Project page: https://dreamzero0.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15922v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15922v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dreamzero0.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界动作模型是零样本策略</div>
<div class="mono" style="margin-top:8px">当前最先进的视觉-语言-动作（VLA）模型在语义泛化方面表现出色，但在将物理动作泛化到新环境中时却遇到困难。我们引入了DreamZero，这是一种基于预训练视频扩散基础模型的世界动作模型（WAM）。与VLA不同，WAM通过预测未来的世界状态和动作来学习物理动力学，使用视频作为世界演变的密集表示。通过联合建模视频和动作，DreamZero能够有效地从异构机器人数据中学习多种技能，而无需依赖重复的演示。这在真实机器人实验中将新任务和环境中的泛化性能提高了超过2倍。通过模型和系统优化，我们使一个14B的自回归视频扩散模型能够以7Hz的速度进行实时闭环控制。最后，我们展示了两种形式的跨体态迁移：仅从其他机器人或人类的视频演示中，DreamZero在未见过的任务上的性能提高了超过42%，仅需10-20分钟的数据。更令人惊讶的是，DreamZero还实现了少样本体态适应，仅需30分钟的玩数据即可转移到新的体态，同时保持零样本泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of physical motion generalization in Vision-Language-Action models by introducing DreamZero, a World Action Model that learns physical dynamics through predicting future world states. DreamZero achieves over 2x improvement in generalization to new tasks and environments compared to state-of-the-art Vision-Language-Action models in real robot experiments. It also demonstrates effective cross-embodiment transfer, showing a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data and enabling few-shot embodiment adaptation with 30 minutes of play data.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入DreamZero，一种通过预测未来世界状态来学习物理动态的World Action Model，解决Vision-Language-Action模型在物理运动泛化方面的挑战。DreamZero在真实机器人实验中相比最先进的Vision-Language-Action模型实现了超过2倍的泛化性能提升。此外，它还展示了有效的跨体态转移，仅用10-20分钟的数据就能获得超过42%的未见任务性能提升，并且仅用30分钟的玩数据就能实现少样本体态适应，同时保持零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion</div>
<div class="meta-line">Authors: Mostafa A. Atalla, Daan van Bemmel, Jack Cummings, Paul Breedveld, Michaël Wiertlewski, Aimée Sakes</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-17T14:33:17+00:00 · Latest: 2026-02-17T14:33:17+00:00</div>
<div class="meta-line">Comments: Accepted for publication in the 2026 IEEE International Conference on Robotics and Automation (ICRA) in Vienna</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between &quot;grip&quot; and &quot;slip&quot; states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>按需抓握，按需滑动：用于机器人运动的超声润滑</div>
<div class="mono" style="margin-top:8px">摩擦是陆地运动中的基本调节因素，但在机器人系统中，它几乎总是被视为由表面材料和条件决定的被动属性。在这里，我们介绍了一种超声润滑方法，用于主动控制机器人运动中的摩擦。通过在超声频率下激发共振结构，接触界面可以在“抓握”和“滑动”状态之间动态切换，从而实现运动。我们开发了两个摩擦控制模块：一种用于管状环境的圆柱设计和一种用于外部表面的平板设计，并将它们集成到模仿蠼螋和黄蜂产卵器运动的生物启发系统中。两个系统均实现了双向运动，几乎完美的运动效率超过了90%。摩擦特性实验进一步证明，在各种表面上，包括坚硬、柔软、颗粒状和生物组织界面，在干燥和潮湿条件下，以及在不同粗糙度水平的表面上，超声润滑均可显著降低摩擦，证实了超声润滑在运动任务中的广泛应用。这些发现确立了超声润滑作为一种可行的主动摩擦控制机制在机器人运动中的地位，有可能减少设计复杂性并提高机器人运动系统的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces ultrasonic lubrication as a method to actively control friction in robotic locomotion, allowing dynamic switching between &#x27;grip&#x27; and &#x27;slip&#x27; states. Two friction control modules were developed and integrated into bio-inspired systems, achieving nearly perfect locomotion efficiencies. Friction characterization experiments showed significant reduction across various surfaces under different conditions, confirming the broad applicability of ultrasonic lubrication for robotic locomotion tasks.</div>
<div class="mono" style="margin-top:8px">研究引入了超声润滑作为机器人运动中主动控制摩擦的方法，允许在‘抓握’和‘滑动’状态之间动态切换。开发了两个摩擦控制模块并集成到仿生系统中，实现了几乎完美的运动效率。摩擦特性实验显示，在不同条件下，各种表面上摩擦力显著降低，证实了超声润滑在机器人运动任务中的广泛应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions</div>
<div class="meta-line">Authors: Jieting Long, Dechuan Liu, Weidong Cai, Ian Manchester, Weiming Zhi</div>
<div class="meta-line">First: 2026-02-17T13:27:05+00:00 · Latest: 2026-02-17T13:27:05+00:00</div>
<div class="meta-line">Comments: 8 pages, 8 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15567v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15567v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot&#x27;s workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot&#x27;s control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束流模型用于适应学习到的机器人轨迹分布</div>
<div class="mono" style="margin-top:8px">机器人的运动分布通常表现出多模态性，需要灵活的生成模型来准确表示。流式流策略（SFPs）最近作为一种强大的范式，通过直接在动作空间中整合学习到的速度场来生成机器人轨迹，从而实现平滑和反应性的控制。然而，现有的公式缺乏在训练后适应轨迹以强制执行安全性和任务特定约束的机制。我们提出了一种约束感知流（CASF）框架，该框架通过在执行过程中使用依赖约束的度量来增强流式流策略，从而重塑学习到的速度场。CASF将定义在机器人工作空间或配置空间中的每个约束视为可微距离函数，并将其转换为局部度量并拉回到机器人的控制空间。远离限制区域时，该度量将退化为恒等式；在接近约束边界时，它会平滑地减弱或重新定向运动，有效地变形底层流以保持安全性。这允许轨迹在实时中进行适应，确保机器人动作遵守关节限制，避免碰撞，并保持在可行的工作空间内，同时保留流式流策略的多模态和反应性特性。我们在模拟和实际操作任务中演示了CASF，表明它生成的满足约束的轨迹保持平滑、可行且动态一致，优于标准的后处理投影基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the adaptability of robot trajectory generation by integrating safety constraints into streaming flow policies. The method involves augmenting these policies with constraint-dependent metrics that reshape the learned velocity field during execution, ensuring that trajectories remain safe and feasible. Key findings show that CASF produces smooth, feasible, and dynamically consistent trajectories in both simulated and real-world manipulation tasks, outperforming standard post-hoc projection methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过集成约束依赖度度量来增强流式流模型，实现实时适应机器人轨迹的同时确保安全和任务特定约束。方法是将工作空间或配置空间约束转换为可微距离函数。实验表明，CASF生成的轨迹既平滑又可行，且动态一致，在模拟和真实世界操作任务中均优于事后投影基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Selective Perception for Robot: Task-Aware Attention in Multimodal VLA</div>
<div class="meta-line">Authors: Young-Chae Son, Jung-Woo Lee, Yoon-Ji Choi, Dae-Kwan Ko, Soo-Chul Lim</div>
<div class="meta-line">First: 2026-02-17T12:48:59+00:00 · Latest: 2026-02-17T12:48:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15543v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15543v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人选择性感知：多模态VLA中的任务感知注意力</div>
<div class="mono" style="margin-top:8px">在机器人技术中，能够整合多视图输入的多种模态信号的视觉-语言-行动（VLA）模型已经成为了有效的方法。然而，大多数先前的工作采用静态融合方式，将所有视觉输入均匀处理，这导致不必要的计算开销，并允许与任务无关的背景信息作为噪声。受人类主动感知原理的启发，我们提出了一种动态信息融合框架，旨在最大化VLA模型的效率和鲁棒性。我们的方法引入了一种轻量级的自适应路由架构，该架构能够实时分析当前文本提示和手腕摄像头的观测结果，以预测多个摄像头视图的任务相关性。通过条件性地减弱低信息效用视图的计算，并仅向策略网络提供必要的视觉特征，我们的框架实现了与任务相关性成比例的计算效率。此外，为了高效地为路由训练获取大规模标注数据，我们建立了一个利用视觉-语言模型（VLM）的自动化标注流水线，以最小化数据收集和标注成本。在真实世界的机器人操作场景中的实验结果表明，与现有的VLA模型相比，所提出的方法在推理效率和控制性能方面均取得了显著改进，验证了在资源受限、实时机器人控制环境中动态信息融合的有效性和实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the efficiency and robustness of Vision-Language-Action (VLA) models in robotics by addressing the limitations of static fusion methods. It proposes a dynamic information fusion framework that uses a lightweight adaptive routing architecture to analyze real-time text prompts and camera observations, selectively processing only task-relevant visual information. This approach reduces unnecessary computations and improves both inference efficiency and control performance in robotic manipulation tasks, demonstrating significant advancements over existing VLA models.</div>
<div class="mono" style="margin-top:8px">论文提出了一种动态信息融合框架，用于机器人中的Vision-Language-Action (VLA)模型，该框架能够实时分析文本提示和摄像头观察结果，以选择性地处理视觉输入。这种方法减少了不必要的计算，并增强了任务相关性，从而在机器人操作任务中提高了推理效率和控制性能，优于静态融合方法。</div>
</details>
</div>
<div class="card">
<div class="title">SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies</div>
<div class="meta-line">Authors: Thies Oelerich, Gerald Ebmer, Christian Hartl-Nesic, Andreas Kugi</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-13T10:23:43+00:00 · Latest: 2026-02-17T09:36:24+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12794v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12794v2">PDF</a> · <a href="https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeFlowMPC：基于学习策略的机器人 manipulator 预测性和安全性轨迹规划</div>
<div class="mono" style="margin-top:8px">随着机器人逐渐融入日常生活，带来了许多重大挑战。与传统的工业应用相比，需要更多的灵活性和实时反应性。基于学习的方法可以根据演示的轨迹训练强大的策略，使机器人能够泛化到类似的情况。然而，这些黑盒模型缺乏可解释性和严格的安全性保证。基于优化的方法提供了这些保证，但缺乏所需的灵活性和泛化能力。本文提出了一种结合流动匹配和在线优化的 SafeFlowMPC 方法，以结合学习和优化的优点。该方法在所有时间点都能保证安全性，并通过使用次优模型预测控制形式设计来满足实时执行的需求。SafeFlowMPC 在 KUKA 7-DoF 机器人上进行了三个真实世界的实验，包括两个抓取实验和一个动态人机物体交接实验，取得了良好的性能。有关实验的视频可在 http://www.acin.tuwien.ac.at/42d6 获取。代码可在 https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SafeFlowMPC is a method that combines flow matching and online optimization to address the challenges of integrating robots into everyday life, offering both safety guarantees and real-time flexibility. It uses a suboptimal model-predictive control formulation to ensure safety and real-time execution. The method was tested on a KUKA 7-DoF manipulator in three real-world experiments, demonstrating strong performance in grasping and dynamic human-robot object handover tasks.</div>
<div class="mono" style="margin-top:8px">SafeFlowMPC 是一种结合流匹配和在线优化的方法，用于确保机器人 manipulator 的安全和灵活轨迹规划。该方法利用基于学习的策略实现泛化，并使用基于优化的方法提供安全保证。该方法在 KUKA 7-DoF 机器人上进行了三项实际实验测试，展示了其在抓取和动态人机物体交接任务中的强大性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
