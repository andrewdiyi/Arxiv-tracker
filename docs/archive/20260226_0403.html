<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-26 04:03</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260226_0403</div>
    <div class="row"><div class="card">
<div class="title">Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics</div>
<div class="meta-line">Authors: Abdulaziz Almuzairee, Henrik I. Christensen</div>
<div class="meta-line">First: 2026-02-24T18:58:11+00:00 · Latest: 2026-02-24T18:58:11+00:00</div>
<div class="meta-line">Comments: For website and code, see https://aalmuzairee.github.io/squint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21203v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aalmuzairee.github.io/squint">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>眯视：快速视觉强化学习在仿真实际机器人应用中的研究</div>
<div class="mono" style="margin-top:8px">视觉强化学习在机器人领域具有吸引力，但成本高昂——离策略方法样本高效但速度慢；在策略方法并行性好但浪费样本。近期研究表明，离策略方法在墙钟时间上比在策略方法更快进行基于状态的控制。将此扩展到视觉领域仍然具有挑战性，其中高维输入图像使训练动态复杂化，并引入了大量存储和编码开销。为了解决这些挑战，我们引入了眯视，一种视觉软演员评论家方法，该方法在墙钟时间上比先前的视觉离策略和在策略方法更快地进行训练。眯视通过并行模拟、分布评论家、分辨率眯视、层归一化、调整更新到数据的比例以及优化实现来实现这一点。我们在SO-101任务集中进行了评估，这是一个新的包含八个操纵任务的ManiSkill3套件，具有强烈的领域随机化，并展示了从模拟到实际SO-101机器人的转移。我们在单个RTX 3090 GPU上训练策略15分钟，大多数任务在不到6分钟内收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Squint is a visual Soft Actor Critic method designed to address the challenges of visual reinforcement learning in robotics. It combines parallel simulation, a distributional critic, and other techniques to achieve faster wall-clock training than previous methods. Squint demonstrates successful sim-to-real transfer on a suite of manipulation tasks in ManiSkill3, training policies in under 6 minutes on a single RTX 3090 GPU.</div>
<div class="mono" style="margin-top:8px">Squint 是一种视觉 Soft Actor Critic 方法，通过结合并行仿真、分布性评论家和其他技术来解决机器人视觉强化学习中的挑战。它实现了比之前视觉离策略和在线策略方法更快的墙钟训练速度，在单个 RTX 3090 GPU 上，大多数任务在不到 6 分钟内即可完成训练。Squint 在使用重域随机化的 SO-101 任务集上成功实现了从仿真到现实的迁移，并在实际的 SO-101 机器人上进行了演示。</div>
</details>
</div>
<div class="card">
<div class="title">HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning</div>
<div class="meta-line">Authors: Quanxin Shou, Fangqi Zhu, Shawn Chen, Puxin Yan, Zhengyang Yan, Yikun Miao, Xiaoyi Pang, Zicong Hong, Ruikai Shi, Hao Huang, Jie Zhang, Song Guo</div>
<div class="meta-line">First: 2026-02-24T18:04:31+00:00 · Latest: 2026-02-24T18:04:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21157v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, and action prediction. To this end, we propose HALO, a unified VLA model that enables embodied multimodal chain-of-thought (EM-CoT) reasoning through a sequential process of textual task reasoning, visual subgoal prediction for fine-grained guidance, and EM-CoT-augmented action prediction. We instantiate HALO with a Mixture-of-Transformers (MoT) architecture that decouples semantic reasoning, visual foresight, and action prediction into specialized experts while allowing seamless cross-expert collaboration. To enable HALO learning at scale, we introduce an automated pipeline to synthesize EM-CoT training data along with a carefully crafted training recipe. Extensive experiments demonstrate that: (1) HALO achieves superior performance in both simulated and real-world environments, surpassing baseline policy pi_0 by 34.1% on RoboTwin benchmark; (2) all proposed components of the training recipe and EM-CoT design help improve task success rate; and (3) HALO exhibits strong generalization capabilities under aggressive unseen environmental randomization with our proposed EM-CoT reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HALO：统一的视觉-语言-行动模型，用于具身多模态链式推理</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型在机器人操作方面表现出色，但在长时序或分布外场景中往往由于缺乏明确的多模态推理机制和对未来世界的预见能力而遇到困难。最近的研究在VLA模型中引入了文本链式推理或视觉子目标预测，以进行推理，但仍未能提供一个结合文本推理、视觉预见和行动预测的人类般的统一推理框架。为此，我们提出了HALO，这是一种统一的VLA模型，通过文本任务推理、细粒度的视觉子目标预测和增强的具身多模态链式推理（EM-CoT）的行动预测的顺序过程，实现具身多模态链式推理。我们使用混合的变换器（MoT）架构实例化HALO，将语义推理、视觉预见和行动预测解耦为专门的专家，同时允许跨专家无缝协作。为了使HALO能够大规模学习，我们引入了一种自动流水线来合成EM-CoT训练数据，并精心设计了训练配方。广泛的实验表明：（1）HALO在模拟和真实世界环境中均表现出色，超越基准策略pi_0 34.1%；（2）训练配方和EM-CoT设计的所有提出的组件均有助于提高任务成功率；（3）HALO在我们提出的EM-CoT推理下具有强大的泛化能力，能够应对极端的未见过的环境随机化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">HALO is a unified VLA model that enhances robotic manipulation by integrating textual task reasoning, visual subgoal prediction, and action prediction through a sequential process. It uses a Mixture-of-Transformers architecture to enable specialized experts for semantic reasoning, visual foresight, and action prediction, while allowing cross-expert collaboration. Experiments show that HALO outperforms baseline policy pi_0 by 34.1% on the RoboTwin benchmark, improves task success rates, and demonstrates strong generalization capabilities under environmental randomization.</div>
<div class="mono" style="margin-top:8px">HALO 是一个统一的 VLA 模型，通过顺序过程整合了文本任务推理、视觉子目标预测和动作预测。它使用 Mixture-of-Transformers 架构将这些过程解耦，同时允许协作。实验表明，HALO 在 RoboTwin 基准测试中比基线策略 pi_0 提高了 34.1% 的性能，提高了任务成功率，并且在环境变化下表现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">SOM-VQ: Topology-Aware Tokenization for Interactive Generative Models</div>
<div class="meta-line">Authors: Alessandro Londei, Denise Lanzieri, Matteo Benati</div>
<div class="meta-line">First: 2026-02-24T17:29:04+00:00 · Latest: 2026-02-24T17:29:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vector-quantized representations enable powerful discrete generative models but lack semantic structure in token space, limiting interpretable human control. We introduce SOM-VQ, a tokenization method that combines vector quantization with Self-Organizing Maps to learn discrete codebooks with explicit low-dimensional topology. Unlike standard VQ-VAE, SOM-VQ uses topology-aware updates that preserve neighborhood structure: nearby tokens on a learned grid correspond to semantically similar states, enabling direct geometric manipulation of the latent space. We demonstrate that SOM-VQ produces more learnable token sequences in the evaluated domains while providing an explicit navigable geometry in code space. Critically, the topological organization enables intuitive human-in-the-loop control: users can steer generation by manipulating distances in token space, achieving semantic alignment without frame-level constraints. We focus on human motion generation - a domain where kinematic structure, smooth temporal continuity, and interactive use cases (choreography, rehabilitation, HCI) make topology-aware control especially natural - demonstrating controlled divergence and convergence from reference sequences through simple grid-based sampling. SOM-VQ provides a general framework for interpretable discrete representations applicable to music, gesture, and other interactive generative domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SOM-VQ：拓扑感知标记化方法用于交互生成模型</div>
<div class="mono" style="margin-top:8px">向量量化表示能够启用强大的离散生成模型，但在标记空间中缺乏语义结构，限制了可解释的人类控制。我们引入了SOM-VQ，一种结合向量量化与自组织映射的标记化方法，以学习具有显式低维拓扑的离散码本。与标准的VQ-VAE不同，SOM-VQ 使用拓扑感知更新来保留邻域结构：在学习网格上相邻的标记对应于语义相似的状态，从而允许直接对潜在空间进行几何操作。我们证明SOM-VQ在评估的领域中生成了更可学习的标记序列，同时在编码空间中提供了明确的可导航几何结构。关键的是，拓扑组织使直观的人机交互控制成为可能：用户可以通过操纵标记空间中的距离来引导生成，从而实现语义对齐，而无需帧级约束。我们专注于人体运动生成——一个具有动力学结构、平滑的时间连续性和交互使用案例（编舞、康复、HCI）的领域，其中拓扑感知控制特别自然——通过简单的网格采样从参考序列中实现受控的发散和收敛。SOM-VQ 提供了一种适用于音乐、手势和其他交互生成领域的可解释离散表示的一般框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SOM-VQ is a tokenization method that integrates vector quantization with Self-Organizing Maps to create discrete codebooks with explicit low-dimensional topology, enhancing semantic structure in token space. This approach allows for direct geometric manipulation of the latent space, making the model more interpretable and enabling intuitive human-in-the-loop control. In human motion generation, SOM-VQ demonstrates controlled divergence and convergence from reference sequences through simple grid-based sampling, providing a general framework for interpretable discrete representations in various interactive generative domains.</div>
<div class="mono" style="margin-top:8px">SOM-VQ 是一种结合矢量量化和自组织映射的分词方法，创建具有明确低维拓扑结构的离散码本，增强生成模型的可解释性和可控性。SOM-VQ 保留了邻域结构，允许用户通过几何操作空间实现语义对齐。该方法特别适用于人体运动生成，其中它支持直观的人机交互控制，并适用于编舞、康复等交互应用场景。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-24T17:10:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向对象的零样本灵巧工具操作策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可以执行的任务集。然而，工具操作代表了一类具有挑战性的灵巧性，需要抓取细长物体、手持物体旋转以及进行有力的交互。由于收集这些行为的遥操作数据具有挑战性，因此模拟到现实的强化学习（RL）是一种有前途的替代方案。然而，先前的方法通常需要大量的工程努力来建模物体并调整每个任务的奖励函数。在本文中，我们提出了SimToolReal，朝着为工具操作推广模拟到现实的RL策略迈出了一步。我们不是专注于单一物体和任务，而是通过程序生成大量工具样物体素，并训练一个具有通用目标的单一RL策略，即操纵每个物体到随机目标姿态。这种方法使SimToolReal能够在测试时进行通用灵巧工具操作，而无需任何物体或任务特定的训练。我们证明SimToolReal在性能上比先前的重新定位和固定抓取方法高出37%，同时与在特定目标物体和任务上训练的专家RL策略的性能相当。最后，我们展示了SimToolReal在一系列日常工具上具有泛化能力，在120个现实世界操作中实现了超过24个任务、12个物体实例和6个工具类别的零样本性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SimToolReal, a policy designed for zero-shot dexterous tool manipulation in robots. It uses procedurally generated object primitives in simulation to train a single RL policy with a universal goal of manipulating objects to random poses. This approach allows the policy to generalize well without specific training for each object or task. SimToolReal outperforms previous methods by 37% and achieves strong zero-shot performance across 120 real-world rollouts involving 24 tasks, 12 object instances, and 6 tool categories.</div>
<div class="mono" style="margin-top:8px">论文提出了SimToolReal，一种用于工具操作任务的模拟到现实的强化学习方法。该方法通过在模拟中程序化生成各种工具样物体，并训练一个单一策略将这些物体移动到随机目标位置，以实现通用的灵巧工具操作。该方法在120个真实世界操作中实现了比之前重定位和固定抓取方法高出37%的性能，并且与针对特定物体和任务训练的专业策略表现相当。SimToolReal在24个任务、12个物体实例和6个工具类别上展示了强大的零样本性能。</div>
</details>
</div>
<div class="card">
<div class="title">Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing</div>
<div class="meta-line">Authors: Gayatri Indukumar, Muhammad Awais, Diana Cafiso, Matteo Lo Preti, Lucia Beccai</div>
<div class="meta-line">First: 2026-02-24T15:51:10+00:00 · Latest: 2026-02-24T15:51:10+00:00</div>
<div class="meta-line">Comments: 6 pages, 6 figures, 1 table, to be published in RoboSoft 2026 proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21028v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21028v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There is a growing need for soft robotic platforms that perform gentle, precise handling of a wide variety of objects. Existing surface-based manipulation systems, however, lack the compliance and tactile feedback needed for delicate handling. This work introduces the COmpliant Porous-Elastic Soft Sensing (COPESS) integrated with inductive sensors for adaptive object manipulation and localised sensing. The design features a tunable lattice layer that simultaneously modulates mechanical compliance and sensing performance. By adjusting lattice geometry, both stiffness and sensor response can be tailored to handle objects with varying mechanical properties. Experiments demonstrate that by easily adjusting one parameter, the lattice density, from 7 % to 20 %, it is possible to significantly alter the sensitivity and operational force range (about -23x and 9x, respectively). This approach establishes a blueprint for creating adaptive, sensorized surfaces where mechanical and sensory properties are co-optimized, enabling passive, yet programmable, delicate manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于表面的可调多孔弹性软传感操控</div>
<div class="mono" style="margin-top:8px">软机器人平台对轻柔、精确地处理各种物体的需求日益增长。然而，现有的表面操控系统缺乏必要的柔性和触觉反馈，以实现精细操作。本研究引入了COmpliant Porous-Elastic Soft Sensing (COPESS) 集成感应器，用于适应性物体操控和局部传感。设计中包含一个可调晶格层，可以同时调节机械柔性和传感性能。通过调整晶格几何结构，可以定制硬度和传感器响应，以处理具有不同机械性能的物体。实验表明，通过简单地调整晶格密度参数，从7%到20%，可以显著改变灵敏度和操作力范围（分别约-23倍和9倍）。这种方法为创建机械和传感性能协同优化的自适应传感表面奠定了蓝图，使轻柔操控成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for soft robotic platforms capable of gentle and precise handling of various objects. It introduces COPESS, a tunable lattice layer integrated with inductive sensors for adaptive manipulation and localized sensing. By adjusting the lattice density, the system can significantly change sensitivity and operational force range, demonstrating the potential for creating adaptive, sensorized surfaces that co-optimize mechanical and sensory properties for delicate manipulation.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决软机器人平台在处理各种物体时需要进行轻柔且精确操作的需求。它引入了COPESS，一种可调谐的晶格层，结合了感应传感器以实现适应性操作和局部传感。通过调整晶格密度，系统可以显著改变灵敏度和操作力范围，展示了创建机械和感知属性协同优化的适应性、传感化表面的潜力，以实现轻柔的编程操作。</div>
</details>
</div>
<div class="card">
<div class="title">Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks</div>
<div class="meta-line">Authors: Sanjay Haresh, Daniel Dijkman, Apratim Bhattacharyya, Roland Memisevic</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-24T15:30:55+00:00 · Latest: 2026-02-24T15:30:55+00:00</div>
<div class="meta-line">Comments: To appear at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21013v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many dexterous manipulation tasks are non-markovian in nature, yet little attention has been paid to this fact in the recent upsurge of the vision-language-action (VLA) paradigm. Although they are successful in bringing internet-scale semantic understanding to robotics, existing VLAs are primarily &quot;stateless&quot; and struggle with memory-dependent long horizon tasks. In this work, we explore a way to impart both spatial and temporal memory to a VLA by incorporating a language scratchpad. The scratchpad makes it possible to memorize task-specific information, such as object positions, and it allows the model to keep track of a plan and progress towards subgoals within that plan. We evaluate this approach on a split of memory-dependent tasks from the ClevrSkills environment, on MemoryBench, as well as on a challenging real-world pick-and-place task. We show that incorporating a language scratchpad significantly improves generalization on these tasks for both non-recurrent and recurrent models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我笔记：增强型VLAs在记忆依赖性操作任务中的记事本</div>
<div class="mono" style="margin-top:8px">许多灵巧的操作任务是非马尔可夫性的，但在最近的视觉-语言-动作（VLA）范式兴起中，很少有人注意到这一点。尽管它们成功地将互联网规模的语义理解带给了机器人技术，但现有的VLAs主要是“无状态”的，并且难以应对依赖记忆的长期任务。在本研究中，我们通过引入语言记事本来探索一种方法，以赋予VLAs空间和时间记忆。记事本使得能够记忆任务特定的信息，如物体位置，并允许模型跟踪计划及其子目标的进展。我们在这项研究中使用ClevrSkills环境中的记忆依赖性任务分割、MemoryBench以及一个具有挑战性的现实世界拾取和放置任务上评估了这种方法。我们展示了引入语言记事本可以显著提高这两种类型模型在这类任务上的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance</div>
<div class="meta-line">Authors: Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang, Boyan Li, Yiran Qin, Jin Liu, Can Zhao, Li Kang, Haoqin Hong, Zhenfei Yin, Philip Torr, Hao Su, Ruimao Zhang, Daolin Ma</div>
<div class="meta-line">First: 2026-01-28T04:22:47+00:00 · Latest: 2026-02-24T14:42:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20239v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.20239v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TouchGuide：通过触觉引导在推理时操控视觉运动策略</div>
<div class="mono" style="margin-top:8px">精细的和接触丰富的操作对机器人来说仍然具有挑战性，主要是因为触觉反馈的利用不足。为了解决这个问题，我们引入了TouchGuide，这是一种新颖的跨策略视觉-触觉融合范式，在低维度动作空间内融合模态。具体来说，TouchGuide 在两个阶段操作以在推理时引导预训练的扩散或流匹配视觉运动策略。首先，在早期采样过程中，策略仅使用视觉输入生成粗略的、视觉上合理的动作。其次，一个任务特定的接触物理模型（CPM）提供触觉引导，以引导和细化动作，确保其符合现实的物理接触条件。通过对比学习有限的专家演示进行训练，CPM 提供一个基于触觉的可行性评分，以引导采样过程向满足物理接触约束的细化动作方向发展。此外，为了促进TouchGuide的高质量和低成本数据训练，我们引入了TacUMI数据收集系统。TacUMI 在精度和成本之间实现了良好的权衡；通过利用刚性指尖，它获得了直接的触觉反馈，从而能够收集可靠的触觉数据。在五项具有挑战性的接触丰富任务（如系鞋带和芯片传递）上的广泛实验表明，TouchGuide 一致且显著地优于最先进的视觉-触觉策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">TouchGuide is a novel approach that integrates visuo-tactile feedback to guide robotic manipulation, especially for fine-grained and contact-rich tasks. It operates in two stages: first, a pre-trained visuomotor policy generates a visually plausible action using only visual inputs, and then a task-specific Contact Physical Model (CPM) provides tactile guidance to refine the action, ensuring it meets physical contact constraints. Experiments on five challenging tasks demonstrate that TouchGuide outperforms existing visuo-tactile policies.</div>
<div class="mono" style="margin-top:8px">TouchGuide 是一种将触觉反馈集成到视觉-运动策略中的新方法，以提高精细操作能力。该方法分为两个阶段：首先，预训练的策略使用视觉输入生成一个视觉上合理的动作，然后任务特定的接触物理模型（CPM）提供触觉指导以细化动作。CPM 通过有限专家演示的对比学习进行训练，并提供一个基于触觉的可行性评分。在五个接触丰富的任务上的实验表明，TouchGuide 在性能上优于现有的视觉-触觉策略。</div>
</details>
</div>
<div class="card">
<div class="title">PMG: Parameterized Motion Generator for Human-like Locomotion Control</div>
<div class="meta-line">Authors: Chenxi Han, Yuheng Min, Zihao Huang, Ao Hong, Hang Liu, Yi Cheng, Houde Liu</div>
<div class="meta-line">First: 2026-02-13T06:38:04+00:00 · Latest: 2026-02-24T14:34:42+00:00</div>
<div class="meta-line">Comments: Website: https://pmg-icra26.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12656v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12656v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pmg-icra26.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with high-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control. Website: https://pmg-icra26.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMG：参数化运动生成器用于类人运动控制</div>
<div class="mono" style="margin-top:8px">近年来，基于数据的强化学习和运动跟踪的进展显著改善了类人运动，但仍存在关键的实际挑战。特别是，虽然低级运动跟踪和轨迹跟随控制器已经成熟，但全身参考导向方法难以适应高级命令接口和多样的任务环境：它们需要大量高质量的数据集，对速度和姿态范围的鲁棒性较差，并且对机器人特定的校准敏感。为了解决这些限制，我们提出了参数化运动生成器（PMG），这是一种基于人类运动结构分析的实时运动生成器，仅使用紧凑的参数化运动数据集和高维控制命令合成参考轨迹。结合模仿学习管道和基于优化的从仿真到现实的电机参数识别模块，我们在我们的类人原型ZERITH Z1上验证了整个方法，并展示了在单一集成系统中，PMG生成自然、类人的运动，精确响应高维控制输入，包括基于VR的远程操作，并实现高效的、可验证的从仿真到现实的转移。这些结果共同确立了一条实用的、实验验证的通向自然和可部署的类人控制的途径。网站：https://pmg-icra26.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve humanoid locomotion by addressing the limitations of existing methods, such as the need for large datasets and sensitivity to robot-specific calibration. The Parameterized Motion Generator (PMG) is proposed, which uses a compact set of parameterized motion data and high-dimensional control commands to synthesize reference trajectories in real-time. The approach is validated on the humanoid prototype ZERITH Z1, demonstrating natural, human-like locomotion and precise response to high-dimensional control inputs, including VR-based teleoperation, and enabling efficient sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有方法需要大量数据集和对校准敏感的问题来改进人形机器人的行走。提出了参数化运动生成器（PMG），它使用紧凑的参数化运动数据和高维控制命令来实时合成参考轨迹。该方法在人形原型ZERITH Z1上得到验证，展示了自然的人类行走、对高维控制输入（包括基于VR的远程操作）的精确响应，并实现了高效的仿真实际转移。</div>
</details>
</div>
<div class="card">
<div class="title">See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis</div>
<div class="meta-line">Authors: Jaehyun Park, Minyoung Ahn, Minkyu Kim, Jonghyun Lee, Jae-Gil Lee, Dongmin Park</div>
<div class="meta-line">First: 2026-02-24T14:34:13+00:00 · Latest: 2026-02-24T14:34:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20951v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20951v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>审视并修复缺陷：通过主动数据合成使大模型和扩散模型理解视觉伪影</div>
<div class="mono" style="margin-top:8px">尽管扩散模型取得了近期进展，但AI生成的图像仍然经常包含影响真实感的视觉伪影。虽然更全面的预训练和更大的模型可能会减少伪影，但无法保证完全消除，因此伪影缓解是一个非常关键的研究领域。以往的伪影感知方法依赖于人工标注的伪影数据集，这些数据集成本高且难以扩展，突显了需要一种自动化的可靠方法来获取伪影标注的数据集。在本文中，我们提出了ArtiAgent，它可以高效地创建真实图像和伪影注入图像的配对。它包括三个代理：感知代理，通过在真实图像中识别和定位实体及其子实体；合成代理，通过在扩散变换器内的新型块级嵌入操纵引入伪影；以及策展代理，筛选合成的伪影，并为每个实例生成局部和全局解释。使用ArtiAgent，我们合成了100K张带有丰富伪影注释的图像，并展示了其在多种应用中的有效性和灵活性。代码可在链接处获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of visual artifacts in AI-generated images by proposing ArtiAgent, an automated system that creates pairs of real and artifact-injected images. ArtiAgent consists of a perception agent, a synthesis agent, and a curation agent. The perception agent recognizes entities in real images, the synthesis agent introduces artifacts using novel patch-wise embedding manipulation, and the curation agent filters and explains the synthesized artifacts. The authors synthesized 100K images with rich artifact annotations and showed the system&#x27;s effectiveness and versatility across various applications.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出ArtiAgent自动系统来解决AI生成图像中的视觉缺陷问题，该系统能够生成真实图像和注入缺陷的图像对。ArtiAgent包含三个代理：感知代理用于识别实体，合成代理用于引入缺陷，以及策展代理用于筛选和解释合成的缺陷。该系统生成了10万个带有详细缺陷注释的图像，展示了在各种应用中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Soft Surfaced Vision-Based Tactile Sensing for Bipedal Robot Applications</div>
<div class="meta-line">Authors: Jaeeun Kim, Junhee Lim, Yu She</div>
<div class="meta-line">First: 2026-02-20T22:16:49+00:00 · Latest: 2026-02-24T14:04:56+00:00</div>
<div class="meta-line">Comments: 8 pages, 11 figures, RoboSoft 2026. For the supplementary video, please visit: https://youtu.be/ceJiy9q_2Aw Section IV-D updated</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18638v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18638v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Legged locomotion benefits from embodied sensing, where perception emerges from the physical interaction between body and environment. We present a soft-surfaced, vision-based tactile foot sensor that endows a bipedal robot with a skin-like deformable layer that captures contact deformations optically, turning foot-ground interactions into rich haptic signals. From a contact image stream, our method estimates contact pose (position and orientation), visualizes shear, computes center of pressure (CoP), classifies terrain, and detects geometric features of the contact patch. We validate these capabilities on a tilting platform and in visually obscured conditions, showing that foot-borne tactile feedback improves balance control and terrain awareness beyond proprioception alone. These findings suggest that integrating tactile perception into legged robot feet improves stability, adaptability, and environmental awareness, offering a promising direction toward more compliant and intelligent locomotion systems. For the supplementary video, please visit: https://youtu.be/ceJiy9q_2Aw</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于软表面视觉触觉传感的双足机器人应用</div>
<div class="mono" style="margin-top:8px">腿足运动从身体与环境的物理交互中获得感知，这种感知源于体感传感。我们提出了一种基于视觉的软表面触觉足传感器，赋予双足机器人类似皮肤的可变形层，通过光学方式捕捉接触变形，将足-地面交互转化为丰富的触觉信号。从接触图像流中，我们的方法估计接触姿态（位置和方向）、可视化剪切力、计算压力中心（CoP）、分类地形并检测接触区域的几何特征。我们在倾斜平台上验证了这些功能，并在视觉遮挡条件下展示了足部触觉反馈如何提高平衡控制和地形意识，超越了本体感觉的局限。这些发现表明，将触觉感知整合到腿足机器人足部可以提高稳定性、适应性和环境意识，为更柔顺和智能的运动系统提供了有希望的方向。有关补充视频，请访问：https://youtu.be/ceJiy9q_2Aw</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a soft-surfaced, vision-based tactile foot sensor for bipedal robots, which captures foot-ground interactions through optical means. The method estimates contact pose, visualizes shear, computes center of pressure, classifies terrain, and detects geometric features. Experiments on a tilting platform and in visually obscured conditions show that this tactile feedback enhances balance control and terrain awareness, surpassing proprioception alone. This suggests that integrating tactile perception into legged robot feet improves stability and adaptability.</div>
<div class="mono" style="margin-top:8px">本文介绍了一种软表面、基于视觉的触觉脚传感器，通过光学方式捕捉足底与地面的交互。该方法估计接触姿态、可视化剪切力、计算中心压力、分类地形并检测接触区域的几何特征。在倾斜平台和视觉遮挡条件下进行的实验表明，这种触觉反馈可以增强平衡控制和地形感知，超越了仅依靠本体感觉。这表明将触觉感知集成到腿足机器人脚中可以提高稳定性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning</div>
<div class="meta-line">Authors: Dimitrios Dimou, José Santos-Victor, Plinio Moreno</div>
<div class="meta-line">First: 2026-02-24T13:51:09+00:00 · Latest: 2026-02-24T13:51:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20915v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20915v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we address the problem of task-oriented grasping for humanoid robots, emphasizing the need to align with human social norms and task-specific objectives. Existing methods, employ a variety of open-loop and closed-loop approaches but lack an end-to-end solution that can grasp several objects while taking into account the downstream task&#x27;s constraints. Our proposed approach employs reinforcement learning to enhance task-oriented grasping, prioritizing the post-grasp intention of the agent. We extract human grasp preferences from the ContactPose dataset, and train a hand synergy model based on the Variational Autoencoder (VAE) to imitate the participant&#x27;s grasping actions. Based on this data, we train an agent able to grasp multiple objects while taking into account distinct post-grasp intentions that are task-specific. By combining data-driven insights from human grasping behavior with learning by exploration provided by reinforcement learning, we can develop humanoid robots capable of context-aware manipulation actions, facilitating collaboration in human-centered environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于任务的灵巧机器人抓取方法研究：利用姿态协同和强化学习</div>
<div class="mono" style="margin-top:8px">本文针对类人机器人任务导向抓取的问题，强调了与人类社会规范和任务特定目标的对齐需求。现有方法采用各种开环和闭环方法，但缺乏一个端到端的解决方案，该解决方案可以在考虑下游任务约束的情况下抓取多个物体。我们提出的方法利用强化学习来增强任务导向抓取，优先考虑代理的抓取后意图。我们从ContactPose数据集中提取人类的抓取偏好，并基于变分自编码器（VAE）训练手部协同模型以模仿参与者的抓取动作。基于这些数据，我们训练了一个能够考虑任务特定抓取后意图并抓取多个物体的代理。通过结合人类抓取行为的数据驱动见解和强化学习提供的探索性学习，我们可以开发出能够在人类中心环境中进行情境感知操作的类人机器人，促进协作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to improve task-oriented grasping for humanoid robots by aligning with human social norms and task-specific objectives. The authors propose an end-to-end solution using reinforcement learning to prioritize post-grasp intentions. They extract human grasp preferences from the ContactPose dataset and train a hand synergy model based on a Variational Autoencoder (VAE) to imitate human grasping actions. The key experimental finding is that the trained agent can grasp multiple objects while considering task-specific post-grasp intentions, demonstrating context-aware manipulation capabilities.</div>
<div class="mono" style="margin-top:8px">本文旨在通过与人类社会规范和任务特定目标对齐来改进人形机器人的任务导向抓取。作者提出了一种端到端的解决方案，使用强化学习优先考虑抓取后的意图。他们从ContactPose数据集中提取人类抓取偏好，并基于变分自编码器（VAE）训练手部协同模型来模仿人类的抓取动作。关键实验发现是，训练后的代理可以考虑任务特定的抓取后意图抓取多个物体，展示了上下文感知的操纵能力。</div>
</details>
</div>
<div class="card">
<div class="title">KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion</div>
<div class="meta-line">Authors: Lei Ye, Haibo Gao, Huaiguang Yang, Peng Xu, Haoyu Wang, Tie Liu, Junqi Shan, Zongquan Deng, Liang Ding</div>
<div class="meta-line">First: 2026-02-24T12:46:34+00:00 · Latest: 2026-02-24T12:46:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20850v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20850v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Legged robots face significant challenges in navigating complex environments, as they require precise real-time decisions for foothold selection and contact planning. While existing research has explored methods to select footholds based on terrain geometry or kinematics, a critical gap remains: few existing methods efficiently validate the existence of a non-collision swing trajectory. This paper addresses this gap by introducing KCFRC, a novel approach for efficient foothold reachability analysis. We first formally define the foothold reachability problem and establish a sufficient condition for foothold reachability. Based on this condition, we develop the KCFRC algorithm, which enables robots to validate foothold reachability in real time. Our experimental results demonstrate that KCFRC achieves remarkable time efficiency, completing foothold reachability checks for a single leg across 900 potential footholds in an average of 2 ms. Furthermore, we show that KCFRC can accelerate trajectory optimization and is particularly beneficial for contact planning in confined spaces, enhancing the adaptability and robustness of legged robots in challenging environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KCFRC：基于动力学碰撞感知的足点可达性标准在腿式运动中的应用</div>
<div class="mono" style="margin-top:8px">腿式机器人在复杂环境中的导航面临重大挑战，因为它们需要在实时中做出精确的足点选择和接触规划决策。尽管现有研究已经探索了基于地形几何或动力学的足点选择方法，但仍存在一个关键缺口：很少有方法能够高效验证非碰撞摆动轨迹的存在。本文通过引入KCFRC，一种新颖的足点可达性分析方法，填补了这一缺口。我们首先正式定义了足点可达性问题，并建立了足点可达性的充分条件。基于此条件，我们开发了KCFRC算法，使机器人能够实时验证足点可达性。实验结果表明，KCFRC实现了显著的时间效率，平均在2毫秒内完成单腿900个潜在足点的可达性检查。此外，我们还展示了KCFRC可以加速轨迹优化，并特别有利于受限空间中的接触规划，从而增强腿式机器人在复杂环境中的适应性和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of legged robots navigating complex environments by introducing KCFRC, a novel approach for efficient foothold reachability analysis. KCFRC defines a formal problem and establishes a sufficient condition for foothold reachability, enabling real-time validation. Experiments show that KCFRC is highly time-efficient, completing checks in 2 ms on average, and accelerates trajectory optimization, especially in confined spaces, enhancing the adaptability and robustness of legged robots.</div>
<div class="mono" style="margin-top:8px">本文通过引入KCFRC算法，解决腿足机器人在复杂环境中的导航问题，该算法用于高效地分析脚印可达性，验证非碰撞摆动轨迹的存在性，实现实时脚印选择。实验结果显示，KCFRC平均在2毫秒内完成900个潜在脚印的检查，显著加速轨迹优化，提升腿足机器人在受限空间中的适应性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching</div>
<div class="meta-line">Authors: Lei Ye, Haibo Gao, Peng Xu, Zhelin Zhang, Junqi Shan, Ao Zhang, Wei Zhang, Ruyi Zhou, Zongquan Deng, Liang Ding</div>
<div class="meta-line">First: 2025-09-10T09:31:17+00:00 · Latest: 2026-02-24T12:33:35+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures, conference paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.08435v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.08435v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://masteryip.github.io/pegasusflow.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models offer powerful generative capabilities for robot trajectory planning, yet their practical deployment on robots is hindered by a critical bottleneck: a reliance on imitation learning from expert demonstrations. This paradigm is often impractical for specialized robots where data is scarce and creates an inefficient, theoretically suboptimal training pipeline. To overcome this, we introduce PegasusFlow, a hierarchical rolling-denoising framework that enables direct and parallel sampling of trajectory score gradients from environmental interaction, completely bypassing the need for expert data. Our core innovation is a novel sampling algorithm, Weighted Basis Function Optimization (WBFO), which leverages spline basis representations to achieve superior sample efficiency and faster convergence compared to traditional methods like MPPI. The framework is embedded within a scalable, asynchronous parallel simulation architecture that supports massively parallel rollouts for efficient data collection. Extensive experiments on trajectory optimization and robotic navigation tasks demonstrate that our approach, particularly Action-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start, significantly outperforms baselines. In a challenging barrier-crossing task, our method achieved a 100% success rate and was 18% faster than the next-best method, validating its effectiveness for complex terrain locomotion planning. https://masteryip.github.io/pegasusflow.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PegasusFlow：并行滚动去噪得分采样用于机器人扩散计划流匹配</div>
<div class="mono" style="margin-top:8px">扩散模型为机器人轨迹规划提供了强大的生成能力，但在实际部署到机器人上时，由于依赖于专家演示的模仿学习，存在一个关键瓶颈。这种范式在数据稀缺的专用机器人中往往不切实际，导致了低效且理论上不理想的训练管道。为了解决这个问题，我们提出了PegasusFlow，这是一种分层滚动去噪框架，可以直接并行从环境交互中采样轨迹得分梯度，完全绕过了专家数据的需求。我们的核心创新是一种新的采样算法，加权基函数优化（WBFO），它利用样条基函数表示来实现比传统方法（如MPPI）更高的采样效率和更快的收敛速度。该框架嵌入在可扩展的异步并行模拟架构中，支持大规模并行展开以高效收集数据。在轨迹优化和机器人导航任务的广泛实验中，我们的方法，特别是动作-价值WBFO（AVWBFO）结合强化学习预热，显著优于基线方法。在一项具有挑战性的障碍穿越任务中，我们的方法实现了100%的成功率，并且比第二好的方法快18%，验证了其在复杂地形运动规划中的有效性。https://masteryip.github.io/pegasusflow.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PegasusFlow is a hierarchical rolling-denoising framework designed to enhance robot trajectory planning by directly sampling trajectory score gradients from environmental interaction, eliminating the need for expert data. It introduces Weighted Basis Function Optimization (WBFO) for superior sample efficiency and faster convergence compared to traditional methods. Experiments show that PegasusFlow, especially the Action-Value WBFO variant, significantly outperforms baselines in trajectory optimization and robotic navigation tasks, achieving a 100% success rate in a challenging barrier-crossing task and being 18% faster than the next-best method.</div>
<div class="mono" style="margin-top:8px">PegasusFlow 是一种分层滚动去噪框架，可以直接并行从环境交互中采样轨迹得分梯度，解决依赖专家数据的瓶颈问题。它引入了新型的加权基函数优化（WBFO）算法，相比传统方法提高了样本效率和收敛速度。实验表明，PegasusFlow，尤其是结合强化学习预热的行动-价值 WBFO 变体，显著优于基线方法，在障碍穿越任务中实现了 100% 的成功率，并且比第二优方法快 18%。</div>
</details>
</div>
<div class="card">
<div class="title">SpikePingpong: Spike Vision-based Fast-Slow Pingpong Robot System</div>
<div class="meta-line">Authors: Hao Wang, Chengkai Hou, Xianglong Li, Yankai Fu, Chenxuan Li, Ning Chen, Gaole Dai, Jiaming Liu, Tiejun Huang, Shanghang Zhang</div>
<div class="meta-line">First: 2025-06-07T07:04:48+00:00 · Latest: 2026-02-24T12:03:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06690v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.06690v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning to control high-speed objects in dynamic environments represents a fundamental challenge in robotics. Table tennis serves as an ideal testbed for advancing robotic capabilities in dynamic environments. This task presents two fundamental challenges: it requires a high-precision vision system capable of accurately predicting ball trajectories under complex dynamics, and it necessitates intelligent control strategies to ensure precise ball striking to target regions. High-speed object manipulation typically demands advanced visual perception hardware capable of capturing rapid motion with exceptional temporal resolution. Drawing inspiration from Kahneman&#x27;s dual-system theory, where fast intuitive processing complements slower deliberate reasoning, there exists an opportunity to develop more robust perception architectures that can handle high-speed dynamics while maintaining accuracy. To this end, we present \textit{\textbf{SpikePingpong}}, a novel system that integrates spike-based vision with imitation learning for high-precision robotic table tennis. We develop a Fast-Slow system architecture where System 1 provides rapid ball detection and preliminary trajectory prediction with millisecond-level responses, while System 2 employs spike-oriented neural calibration for precise hittable position corrections. For strategic ball striking, we introduce Imitation-based Motion Planning And Control Technology, which learns optimal robotic arm striking policies through demonstration-based learning. Experimental results demonstrate that \textit{\textbf{SpikePingpong}} achieves a remarkable 92\% success rate for 30 cm accuracy zones and 70\% in the more challenging 20 cm precision targeting. This work demonstrates the potential of Fast-Slow architectures for advancing robotic capabilities in time-critical manipulation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpikePingpong：基于回声定位的快慢乒乓球机器人系统</div>
<div class="mono" style="margin-top:8px">在动态环境中控制高速物体是机器人技术中的基本挑战。乒乓球为提升机器人在动态环境中的能力提供了理想的试验平台。这项任务提出了两个基本挑战：需要一个高精度的视觉系统，能够在复杂动力学条件下准确预测球的轨迹，同时还需要智能控制策略以确保精确击打目标区域。高速物体操作通常需要先进的视觉感知硬件，能够以极高的时间分辨率捕捉快速运动。受到卡尼曼的双重系统理论的启发，即快速直觉处理与较慢的审慎推理相结合，存在开发能够处理高速动态同时保持准确性的更稳健感知架构的机会。为此，我们提出了\textit{\textbf{SpikePingpong}}，这是一种将回声定位视觉与模仿学习相结合的新型系统，用于实现高精度的机器人乒乓球。我们开发了一种快慢系统架构，其中系统1提供毫秒级响应的快速球检测和初步轨迹预测，而系统2采用基于回声定位的神经校准进行精确可击打位置的修正。为了实现精确击球策略，我们引入了基于演示学习的运动规划与控制技术，通过示例学习来学习最优的机器人手臂击球策略。实验结果表明，\textit{\textbf{SpikePingpong}} 在 30 厘米精度区域实现了 92% 的成功率，在更具挑战性的 20 厘米精度目标上达到了 70% 的成功率。这项工作展示了快慢架构在时间关键操作任务中提升机器人能力的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpikePingpong is a novel robotic table tennis system that integrates spike-based vision with imitation learning to handle high-speed object manipulation. It uses a Fast-Slow system architecture where System 1 provides rapid ball detection and trajectory prediction, while System 2 calibrates for precise hitting positions. The system also employs Imitation-based Motion Planning And Control Technology to learn optimal striking policies. Experiments show a 92% success rate for 30 cm accuracy and 70% for 20 cm precision targeting, highlighting its effectiveness in dynamic environments.</div>
<div class="mono" style="margin-top:8px">SpikePingpong 是一种结合了基于尖峰的视觉与模仿学习的新型机器人乒乓球系统，用于处理高速物体操作。该系统采用快速-缓慢系统架构，其中快速系统负责检测球和预测轨迹，而缓慢系统则进行精确击球位置的校准。系统还引入了基于模仿的学习运动规划与控制技术，以学习最优的机器人手臂击球策略。实验结果显示，在30厘米精度范围内成功率为92%，在更具挑战性的20厘米精度范围内成功率为70%，这表明快速-缓慢架构在时间关键型操作任务中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</div>
<div class="meta-line">Authors: Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-14T18:59:59+00:00 · Latest: 2026-02-24T11:51:26+00:00</div>
<div class="meta-line">Comments: CVPR 2026. Project page: https://jasper0314-huang.github.io/fast-thinkact/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09708v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09708v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jasper0314-huang.github.io/fast-thinkact/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fast-ThinkAct：通过可言化的潜在规划实现高效的视觉-语言-动作推理</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）任务需要在复杂的视觉场景中进行推理，并在动态环境中执行适应性动作。虽然近期关于推理VLA的研究表明显式的链式思考（CoT）可以提高泛化能力，但它们由于推理痕迹过长而面临较高的推理延迟。我们提出Fast-ThinkAct，一种高效的推理框架，通过可言化的潜在推理实现紧凑且高性能的规划。Fast-ThinkAct通过从教师中提炼，利用偏好导向的目标来对齐操作轨迹，从而转移语言和视觉规划能力，以实现体感控制。这使得增强推理策略学习能够有效地将紧凑的推理与动作执行连接起来。在多种体感操作和推理基准测试中的广泛实验表明，Fast-ThinkAct在保持有效的长时规划、少量样本适应和故障恢复的同时，相较于最先进的推理VLA，推理延迟降低了高达89.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Fast-ThinkAct is an efficient framework for Vision-Language-Action tasks that uses verbalizable latent reasoning to reduce inference latency while maintaining strong performance. It learns compact yet effective reasoning through a preference-guided objective, enabling efficient policy learning that connects reasoning to action execution. Experiments show Fast-ThinkAct reduces inference latency by up to 89.3% compared to state-of-the-art methods while preserving long-horizon planning and few-shot adaptation capabilities.</div>
<div class="mono" style="margin-top:8px">Fast-ThinkAct 是一种高效的视觉-语言-行动框架，通过偏好导向的目标对齐语言和视觉规划能力，实现紧凑而有效的潜在推理，从而将推理与行动执行有效连接，相比最先进的方法将推理延迟降低了高达89.3%，同时保持了强大的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network</div>
<div class="meta-line">Authors: Cristian Manca, Christian Scano, Giorgio Piras, Fabio Brau, Maura Pintor, Battista Biggio</div>
<div class="meta-line">First: 2026-02-03T14:50:19+00:00 · Latest: 2026-02-24T10:55:09+00:00</div>
<div class="meta-line">Comments: ITASEC-2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03596v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.03596v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers. In this work, we study the problem of detecting 5G attacks in the wild, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services. We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE-5GC：5G核心网络中检测异常的安全部署指南</div>
<div class="mono" style="margin-top:8px">基于机器学习的异常检测系统在5G核心网络中越来越被采用，用于监控复杂的高流量。然而，大多数现有方法都是在独立同分布（IID）数据可用且不存在适应性攻击者的假设下进行评估的，而在实际操作环境中这些假设很少成立。本文研究了在野外检测5G攻击的问题，重点关注实际部署环境。我们提出了5G核心网络中检测异常检测器的安全部署指南（SAGE-5GC），该指南基于领域知识并考虑潜在的敌对威胁。使用一个实际的5G核心数据集，我们首先训练了几种异常检测器，并评估它们在标准5GC控制平面网络服务（基于PFCP）的典型网络攻击下的基线性能。然后，我们将评估扩展到敌对环境，在这种环境中，攻击者试图操纵网络流量的可观察特征以逃避检测，同时保持恶意流量的预期功能。从一组可控特征开始，我们通过随机扰动分析模型的敏感性和对抗鲁棒性。最后，我们引入了一种基于遗传算法的实用优化策略，该策略仅作用于攻击者可控的特征，不需要了解底层检测模型的先验知识。我们的实验结果表明，对抗性构造的攻击可以显著降低检测性能，强调了在野外部署的5G网络中需要稳健的、安全部署的评估方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the evaluation of anomaly detection systems in 5G Core networks by proposing SAGE-5GC, a set of security-aware guidelines. The study uses a realistic 5G Core dataset to train and assess various anomaly detectors against standard cyberattacks, and then evaluates their performance under adversarial conditions where attackers manipulate network traffic features to evade detection. The research highlights that adversarial attacks can significantly reduce detection performance, emphasizing the need for robust evaluation methods in real-world deployments.</div>
<div class="mono" style="margin-top:8px">该研究提出了SAGE-5GC，一套安全导向的评估准则，以解决5G核心网络中异常检测系统的评估难题。研究使用真实的5G核心数据集训练和评估了多种异常检测器，并在标准网络攻击和对抗性条件下评估其性能。研究结果表明，对抗性攻击可以显著降低检测性能，强调了在实际部署中需要具备鲁棒性的评估方法的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">DiSPo: Diffusion-SSM based Policy Learning for Coarse-to-Fine Action Discretization</div>
<div class="meta-line">Authors: Nayoung Oh, Jaehyeong Jang, Moonkyeong Jung, Daehyung Park</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2024-09-23T05:33:35+00:00 · Latest: 2026-02-24T10:54:14+00:00</div>
<div class="meta-line">Comments: Accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.14719v4">Abs</a> · <a href="https://arxiv.org/pdf/2409.14719v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robo-dispo.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We aim to solve the problem of generating coarse-to-fine skills learning from demonstrations (LfD). To scale precision, traditional LfD approaches often rely on extensive fine-grained demonstrations with external interpolations or dynamics models with limited generalization capabilities. For memory-efficient learning and convenient granularity change, we propose a novel diffusion-state space model (SSM) based policy (DiSPo) that learns from diverse coarse skills and produces varying control scales of actions by leveraging an SSM, Mamba. Our evaluations show the adoption of Mamba and the proposed step-scaling method enable DiSPo to outperform in three coarse-to-fine benchmark tests with maximum 81% higher success rate than baselines. In addition, DiSPo improves inference efficiency by generating coarse motions in less critical regions. We finally demonstrate the scalability of actions with simulation and real-world manipulation tasks. Code and Videos are available at https://robo-dispo.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiSPo: 基于扩散-状态空间模型的粗细动作离散化策略学习</div>
<div class="mono" style="margin-top:8px">我们旨在解决从演示中生成粗细动作技能学习（LfD）的问题。为了提高精度，传统LfD方法通常依赖于大量的细粒度演示或具有有限泛化能力的动力学模型。为了实现高效学习和方便的粒度变化，我们提出了一种新颖的基于扩散-状态空间模型（SSM）的策略（DiSPo），该策略通过利用Mamba学习多样化的粗动作技能，并通过SSM产生不同规模的动作控制。我们的评估表明，采用Mamba和提出的步长缩放方法使DiSPo在三个粗细动作基准测试中表现出色，最高成功率比基线高出81%。此外，DiSPo通过在不那么关键的区域生成粗动作提高了推理效率。最后，我们在模拟和实际操作任务中展示了动作的可扩展性。代码和视频可在https://robo-dispo.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of learning coarse-to-fine skills from demonstrations (LfD) by proposing a novel diffusion-state space model (SSM) based policy (DiSPo). DiSPo leverages an SSM and Mamba to learn from diverse coarse skills and generate varying control scales of actions, outperforming baselines with up to 81% higher success rate in three benchmark tests. Additionally, DiSPo enhances inference efficiency by focusing on critical regions during motion generation. The method demonstrates scalability in both simulation and real-world manipulation tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出基于扩散状态空间模型（SSM）的策略DiSPo来解决从演示学习粗到细技能（LfD）的挑战。DiSPo利用多样化的粗技能和名为Mamba的SSM来生成不同控制尺度的动作，克服了传统LfD方法的局限性。该方法在三个基准测试中表现出色，最高成功率比基线高出81%，并通过生成关键区域外的粗动作来提高推理效率。通过模拟和实际操作任务展示了动作的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation</div>
<div class="meta-line">Authors: Zhian Su, Weijie Kong, Haonan Dong, Huixu Dong</div>
<div class="meta-line">First: 2026-02-24T09:19:50+00:00 · Latest: 2026-02-24T09:19:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20715v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20715v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have demonstrated significant potential for generalist robotic policies; however, they struggle to generalize to long-horizon complex tasks in novel real-world domains due to distribution shifts and the scarcity of high-quality demonstrations. Although reinforcement learning (RL) offers a promising avenue for policy improvement, applying it to real-world VLA fine-tuning faces challenges regarding exploration efficiency, training stability, and sample cost. To address these issues, we propose IG-RFT, a novel Interaction-Guided Reinforced Fine-Tuning system designed for flow-based VLA models. Firstly, to facilitate effective policy optimization, we introduce Interaction-Guided Advantage Weighted Regression (IG-AWR), an RL algorithm that dynamically modulates exploration intensity based on the robot&#x27;s interaction status. Furthermore, to address the limitations of sparse or task-specific rewards, we design a novel hybrid dense reward function that integrates the trajectory-level reward and the subtask-level reward. Finally, we construct a three-stage RL system comprising SFT, Offline RL, and Human-in-the-Loop RL for fine-tuning VLA models. Extensive real-world experiments on four challenging long-horizon tasks demonstrate that IG-RFT achieves an average success rate of 85.0%, significantly outperforming SFT (18.8%) and standard Offline RL baselines (40.0%). Ablation studies confirm the critical contributions of IG-AWR and hybrid reward shaping. In summary, our work establishes and validates a novel reinforced fine-tuning system for VLA models in real-world robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IG-RFT：一种用于长时距机器人操作中VLA模型的交互引导强化微调框架</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型在通用机器人策略方面显示出巨大的潜力；然而，由于分布偏移和高质量示范的稀缺性，它们难以在新颖的现实世界领域中泛化到长时距复杂任务。尽管强化学习（RL）为策略改进提供了有希望的途径，但在现实世界中对VLA进行微调时，探索效率、训练稳定性和样本成本方面存在挑战。为了解决这些问题，我们提出了一种名为IG-RFT的新颖交互引导强化微调系统，该系统专为流式VLA模型设计。首先，为了促进有效的策略优化，我们引入了交互引导优势加权回归（IG-AWR），这是一种基于机器人交互状态动态调节探索强度的RL算法。此外，为了解决稀疏或任务特定奖励的局限性，我们设计了一种新的混合密集奖励函数，该函数结合了轨迹级奖励和子任务级奖励。最后，我们构建了一个包含SFT、离线RL和人工在环RL的三阶段RL系统，用于微调VLA模型。在四个具有挑战性的长时距任务上的广泛现实世界实验表明，IG-RFT实现了85.0%的平均成功率，显著优于SFT（18.8%）和标准离线RL基线（40.0%）。消融研究证实了IG-AWR和混合奖励塑造的贡献至关重要。总之，我们的工作建立了并验证了用于现实世界机器人操作中VLA模型的新型强化微调系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces IG-RFT, a novel Interaction-Guided Reinforced Fine-Tuning system for Vision-Language-Action models in long-horizon robotic manipulation. It addresses challenges in exploration, training stability, and sample cost by proposing Interaction-Guided Advantage Weighted Regression and a hybrid dense reward function. IG-RFT achieves an average success rate of 85.0% across four tasks, outperforming standard methods. Ablation studies confirm the effectiveness of these components.</div>
<div class="mono" style="margin-top:8px">论文提出了IG-RFT，一种用于长周期机器人操作的视觉-语言-动作模型的新型强化微调系统。通过提出动态探索强度调节算法IG-AWR和混合密集奖励函数来解决探索效率和样本成本问题。IG-RFT在四个任务上的平均成功率达到了85.0%，优于标准方法。消融研究证实了IG-AWR和混合奖励塑造的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Belief Vision Language Model</div>
<div class="meta-line">Authors: Vaidehi Bagaria, Bijo Sebastian, Nirav Patel</div>
<div class="meta-line">First: 2026-02-24T08:02:16+00:00 · Latest: 2026-02-24T08:02:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20659v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing vision-language-action (VLA) models in handling long-horizon manipulation tasks under partial observability. It introduces RB-VLA, a belief-centric architecture that maintains a compact latent state to track task progress and enable phase-aware control. RB-VLA outperforms prior models on multi-stage pick-and-place and stacking tasks, with success rates 52.5% and 37.5% higher, respectively, and reduces inference latency by up to 5x compared to baselines. Ablation studies show the belief module significantly improves performance, increasing success rates from 32.5% to 77.5%.</div>
<div class="mono" style="margin-top:8px">论文针对现有视觉-语言-动作模型在处理部分可观测的长期操作任务时的局限性，提出了RB-VLA，一种以信念为中心的架构，能够维护一个紧凑的潜状态来跟踪任务进度并实现阶段感知的控制。RB-VLA在多阶段拾放和堆叠任务上优于之前的模型，具有更高的成功率和更短的推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments</div>
<div class="meta-line">Authors: Keisuke Takeshita, Takahiro Yamazaki, Tomohiro Ono, Takashi Yamamoto</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-24T07:46:31+00:00 · Latest: 2026-02-24T07:46:31+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2025. Project Page: https://toyotafrc.github.io/RobotLocalPlanner-Proj/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20645v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20645v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://toyotafrc.github.io/RobotLocalPlanner-Proj/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The objective of this study is to enable fast and safe manipulation tasks in home environments. Specifically, we aim to develop a system that can recognize its surroundings and identify target objects while in motion, enabling it to plan and execute actions accordingly. We propose a periodic sampling-based whole-body trajectory planning method, called the &quot;Robot Local Planner (RLP).&quot; This method leverages unique features of home environments to enhance computational efficiency, motion optimality, and robustness against recognition and control errors, all while ensuring safety. The RLP minimizes computation time by planning with minimal waypoints and generating safe trajectories. Furthermore, overall motion optimality is improved by periodically executing trajectory planning to select more optimal motions. This approach incorporates inverse kinematics that are robust to base position errors, further enhancing robustness. Evaluation experiments demonstrated that the RLP outperformed existing methods in terms of motion planning time, motion duration, and robustness, confirming its effectiveness in home environments. Moreover, application experiments using a tidy-up task achieved high success rates and short operation times, thereby underscoring its practical feasibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人局部规划器：一种基于周期采样的全身体态轨迹规划方法及其在家庭环境中的应用</div>
<div class="mono" style="margin-top:8px">本研究旨在使机器人能够在家庭环境中快速而安全地执行操作任务。具体而言，我们旨在开发一个能够识别周围环境并识别目标物体的系统，从而能够根据情况规划和执行相应动作。我们提出了一种基于周期采样的全身体态轨迹规划方法，称为“机器人局部规划器（RLP）”。该方法利用家庭环境的独特特征来提高计算效率、运动优化性和对识别和控制错误的鲁棒性，同时确保安全性。RLP通过使用最少的航点进行规划并生成安全轨迹来减少计算时间。此外，通过周期性执行轨迹规划来选择更优的运动，从而提高整体运动优化性。该方法还结合了对基座位置误差具有鲁棒性的逆运动学，进一步提高了鲁棒性。评估实验表明，RLP在运动规划时间、运动持续时间和鲁棒性方面优于现有方法，证实了其在家庭环境中的有效性。此外，使用整理任务的应用实验实现了高成功率和短操作时间，从而证明了其实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to enable fast and safe manipulation tasks in home environments by developing a periodic sampling-based whole-body trajectory planning method called the &#x27;Robot Local Planner (RLP). &#x27; The RLP minimizes computation time through minimal waypoints and enhances motion optimality and robustness. Evaluation experiments showed that the RLP outperformed existing methods in terms of motion planning time, motion duration, and robustness. Application experiments using a tidy-up task achieved high success rates and short operation times, confirming its practical feasibility in home environments.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过开发一种基于周期采样的整体轨迹规划方法‘机器人局部规划器（RLP）’来实现家庭环境中的快速和安全操作任务。RLP通过最少的航点规划来减少计算时间，并提高运动优化性和鲁棒性。实验结果表明，RLP在运动规划时间、运动持续时间和鲁棒性方面优于现有方法，并在整理任务中实现了高成功率和短操作时间，证明了其实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection</div>
<div class="meta-line">Authors: Zhaonian Kuang, Rui Ding, Meng Yang, Xinhu Zheng, Gang Hua</div>
<div class="meta-line">First: 2026-02-24T07:22:58+00:00 · Latest: 2026-02-24T07:22:58+00:00</div>
<div class="meta-line">Comments: IJCV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20627v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20627v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物体-场景-相机分解与重组以提高单目3D物体检测的数据效率</div>
<div class="mono" style="margin-top:8px">单目3D物体检测（M3OD）本质上是病态的，因此训练高性能的基于深度学习的M3OD模型需要大量的标注数据，这些数据具有复杂的视觉变化，来自多种场景、不同物体和相机姿态。然而，我们观察到，由于强烈的人类偏见，当拍摄图像以构建训练数据时，物体、场景和相机姿态这三个独立实体总是紧密交织在一起。具体来说，特定的3D物体总是被固定在特定场景中拍摄，因此缺乏必要的多样性。这种紧密交织导致了对均匀训练数据的不足利用和过度拟合的问题。为了解决这个问题，我们提出了一种在线物体-场景-相机分解与重组的数据处理方案，以更有效地利用训练数据。我们首先以高效的方式将训练图像完全分解为具有纹理的3D物体点模型和背景场景。然后在每个周期中，通过将3D物体插入背景场景的空闲空间，并使用纹理3D点表示法中的扰动相机姿态进行渲染，不断重组新的训练图像。这样，所有周期中的刷新训练数据可以覆盖独立物体、场景和相机姿态组合的整个光谱。该方案可以作为即插即用组件来增强M3OD模型，灵活应用于完全监督和稀疏监督设置中。在稀疏监督设置中，所有实例中离 ego-相机最近的物体被稀疏标注。然后可以灵活增加标注物体以控制标注成本。为了验证，我们的方法广泛应用于五个代表性的M3OD模型，并在KITTI和更复杂的Waymo数据集上进行了评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of insufficient data diversity in monocular 3D object detection by proposing an online object-scene-camera decomposition and recomposition method. This method decomposes training images into 3D object models and background scenes, and recomposes new images by inserting objects into the background with perturbed camera poses. This approach enhances data utilization and reduces overfitting. Experimental results show that this method improves the performance of five representative M3OD models on both KITTI and Waymo datasets, making it a flexible solution for both fully and sparsely supervised settings.</div>
<div class="mono" style="margin-top:8px">论文提出了一种在线物体-场景-相机分解重组方法，以解决单目3D物体检测中训练数据不足和过拟合的问题。该方法将训练图像分解为3D物体点模型和背景场景，并通过插入物体并使用扰动的相机姿态重新组合新的训练图像，确保了物体、场景和相机姿态组合的全面覆盖，提高了数据利用效率并减少了过拟合。该方法在五个M3OD模型上进行了验证，并在KITTI和Waymo数据集上进行了测试，展示了在完全监督和稀疏监督设置中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Scout-Rover cooperation: online terrain strength mapping and traversal risk estimation for planetary-analog explorations</div>
<div class="meta-line">Authors: Shipeng Liu, J. Diego Caporale, Yifeng Zhang, Xingjue Liao, William Hoganson, Wilson Hu, Shivangi Misra, Neha Peddinti, Rachel Holladay, Ethan Fulcher, Akshay Ram Panyam, Andrik Puentes, Jordan M. Bretzfelder, Michael Zanetti, Uland Wong, Daniel E. Koditschek, Mark Yim, Douglas Jerolmack, Cynthia Sung, Feifei Qian</div>
<div class="meta-line">First: 2026-02-21T01:41:40+00:00 · Latest: 2026-02-24T06:52:39+00:00</div>
<div class="meta-line">Comments: 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18688v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18688v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot-aided exploration of planetary surfaces is essential for understanding geologic processes, yet many scientifically valuable regions, such as Martian dunes and lunar craters, remain hazardous due to loose, deformable regolith. We present a scout-rover cooperation framework that expands safe access to such terrain using a hybrid team of legged and wheeled robots. In our approach, a high-mobility legged robot serves as a mobile scout, using proprioceptive leg-terrain interactions to estimate regolith strength during locomotion and construct spatially resolved terrain maps. These maps are integrated with rover locomotion models to estimate traversal risk and inform path planning.
  We validate the framework through analogue missions at the NASA Ames Lunar Simulant Testbed and the White Sands Dune Field. Experiments demonstrate (1) online terrain strength mapping from legged locomotion and (2) rover-specific traversal-risk estimation enabling safe navigation to scientific targets. Results show that scout-generated terrain maps reliably capture spatial variability and predict mobility failure modes, allowing risk-aware path planning that avoids hazardous regions. By combining embodied terrain sensing with heterogeneous rover cooperation, this framework enhances operational robustness and expands the reachable science workspace in deformable planetary environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Scout-Rover合作：行星模拟探索中的地形强度在线测绘与穿越风险估计</div>
<div class="mono" style="margin-top:8px">机器人辅助的行星表面探索对于理解地质过程至关重要，但许多具有科学价值的区域，如火星沙丘和月球坑洞，由于松散可变形的风化层而仍然存在危险。我们提出了一种Scout-Rover合作框架，利用混合团队的腿式和轮式机器人扩展对这些地形的安全访问。在我们的方法中，高机动性的腿式机器人作为移动侦察员，利用腿部与地形的本体感受交互来估计行进过程中的风化层强度并构建空间分辨率地形图。这些地图与机器人行进模型集成，以估计穿越风险并指导路径规划。
我们通过NASA阿姆斯月球模拟测试场和白沙沙丘场的模拟任务验证了该框架。实验展示了（1）从腿式行进中进行在线地形强度测绘，以及（2）针对特定机器人的穿越风险估计，以实现安全导航至科学目标。结果表明，侦察员生成的地形图可靠地捕捉了空间变异性并预测了移动失败模式，允许风险意识路径规划以避免危险区域。通过结合基于体感的地形感知与异构机器人合作，该框架增强了操作鲁棒性并扩展了在可变形行星环境中的可到达科学工作空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve safe access to hazardous planetary terrain by using a hybrid team of legged and wheeled robots. The legged robot, acting as a mobile scout, estimates regolith strength during locomotion and constructs spatially resolved terrain maps. These maps are integrated with rover models to estimate traversal risk and inform path planning. Experiments at the NASA Ames Lunar Simulant Testbed and the White Sands Dune Field demonstrated the ability to map terrain strength online and estimate traversal risk, enabling safe navigation to scientific targets and avoiding hazardous regions.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用腿足和轮式机器人组成的混合团队，增强对危险行星地形的安全访问。腿足机器人作为侦察员，在移动过程中估计土壤强度并构建空间地图，这些地图与轮式机器人的移动模型结合以估计穿越风险并指导路径规划。在NASA Ames Lunar Simulant Testbed和White Sands Dune Field的实验中，展示了在线绘制地形强度和估计穿越风险的能力，从而实现安全导航至科学目标并避开危险区域。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</div>
<div class="meta-line">Authors: Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</div>
<div class="meta-line">First: 2026-02-18T18:55:02+00:00 · Latest: 2026-02-24T06:15:16+00:00</div>
<div class="meta-line">Comments: Project page: https://hero-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16705v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16705v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hero-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人机器人开放词汇视觉移动物体末端执行器控制学习</div>
<div class="mono" style="margin-top:8px">使用类人机器人在野外对任意物体进行视觉移动物体操作需要精确的末端执行器（EE）控制和通过视觉输入（例如RGB-D图像）对场景的广泛理解。现有方法基于现实世界的模仿学习，由于难以收集大规模训练数据集，因此表现出有限的泛化能力。本文提出了一种新的范式HERO，用于类人机器人物体移动物体操作，结合了大型视觉模型的强大泛化能力和开放词汇理解与模拟训练中的强大控制性能。我们通过设计一种准确的残差感知末端执行器跟踪策略来实现这一点。该末端执行器跟踪策略结合了经典机器人学与机器学习。它使用a) 逆运动学将残差末端执行器目标转换为参考轨迹，b) 用于准确前运动学的已学习神经前向模型，c) 目标调整，以及d) 重新规划。这些创新共同帮助我们将末端执行器跟踪误差减少了3.2倍。我们使用这种准确的末端执行器跟踪器构建了一个模块化移动物体系统，其中我们使用开放词汇大型视觉模型实现强大的视觉泛化。我们的系统能够在从办公室到咖啡馆等多样化的现实环境中操作，机器人能够可靠地操作各种日常物体（例如茶杯、苹果、玩具），这些物体位于43cm至92cm高度的表面上。在模拟和现实世界中的系统模块化和端到端测试表明我们提出的系统设计的有效性。我们认为本文中的进展可以为训练类人机器人与日常物体交互开辟新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces HERO, a new paradigm for object manipulation by humanoid robots, combining strong generalization from large vision models with accurate end-effector control through a residual-aware tracking policy. The method includes inverse kinematics, a learned neural forward model, goal adjustment, and replanning. This approach reduces end-effector tracking error by 3.2x and enables the robot to manipulate various objects in diverse environments, demonstrating effective performance in both simulation and the real world.</div>
<div class="mono" style="margin-top:8px">本文介绍了HERO，一种新的类人机器人执行物体操作的方法，结合了大型视觉模型和通过逆运动学、学习前向模型、目标调整和重规划的残差感知末端执行器控制策略。该系统能够在办公室和咖啡馆等真实世界环境中成功操作各种物体，显示出显著提高的末端执行器跟踪准确性和鲁棒的视觉泛化能力。端到端测试在仿真和真实世界中证实了所提方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model</div>
<div class="meta-line">Authors: Haosheng Li, Weixin Mao, Zihan Lan, Hongwei Xiong, Hongan Wang, Chenyang Si, Ziwei Liu, Xiaoming Deng, Hua Chen</div>
<div class="meta-line">First: 2026-02-24T05:31:52+00:00 · Latest: 2026-02-24T05:31:52+00:00</div>
<div class="meta-line">Comments: 9 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20566v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20566v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BFA++：多视图视觉语言动作模型的分层最佳特征感知标记剪枝</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型通过利用大型视觉语言模型（VLMs）联合解释指令和视觉输入，取得了显著突破。然而，多视图输入带来的视觉标记数量大幅增加，对实时机器人操作提出了严重挑战。现有针对VLMs的加速技术，如标记剪枝，当直接应用于VLA模型时，往往会因忽视不同视图之间的关系以及未考虑机器人操作的动态和任务特定特性而导致性能下降。为解决这一问题，我们提出了一种名为BFA++的动态标记剪枝框架，专门针对VLA模型。BFA++引入了一种分层剪枝策略，由两级重要性预测器指导：视内预测器在每个图像中突出显示与任务相关的区域，以抑制空间噪声；视间预测器在不同操作阶段识别关键摄像视角，以减少跨视图冗余。这种设计能够高效地选择标记，同时保留关键的视觉线索，从而提高计算效率和操作成功率。在RoboTwin基准测试和实际机器人任务上的评估表明，BFA++在π0和RDT模型上的一致性表现优于现有方法。BFA++在π0和RDT模型上的成功率分别提高了约10%，并实现了1.8倍和1.5倍的加速。我们的结果表明，上下文敏感和任务感知的标记剪枝策略比全视觉处理更为有效，能够在实际机器人系统中实现更快的推理和更高的操作准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">BFA++ is a hierarchical token pruning framework designed to enhance multi-view vision-language-action models for robotic manipulation. It uses two-level importance predictors to prune visual tokens, improving computational efficiency and manipulation success rates. BFA++ outperforms existing methods, increasing the success rate by about 10% and achieving speedups of 1.8X and 1.5X on π0 and RDT models, respectively.</div>
<div class="mono" style="margin-top:8px">论文提出了BFA++，一种用于VLA模型的分层token剪枝框架，以提升实时机器人操作性能。它使用两级预测器高效剪枝视觉token，保留任务相关信息并减少跨视图冗余。实验表明，BFA++在π0和RDT模型上的成功率分别提高了约10%，并实现了1.8X和1.5X的加速，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</div>
<div class="meta-line">Authors: Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, Weiyu Liu, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-21T05:56:47+00:00 · Latest: 2026-02-24T04:22:29+00:00</div>
<div class="meta-line">Comments: Project website: momagen.github.io. The first four authors contribute equally. Accpeted to International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18316v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.18316v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning from large-scale, diverse human demonstrations has been shown to be effective for training robots, but collecting such data is costly and time-consuming. This challenge intensifies for multi-step bimanual mobile manipulation, where humans must teleoperate both the mobile base and two high-DoF arms. Prior X-Gen works have developed automated data generation frameworks for static (bimanual) manipulation tasks, augmenting a few human demos in simulation with novel scene configurations to synthesize large-scale datasets. However, prior works fall short for bimanual mobile manipulation tasks for two major reasons: 1) a mobile base introduces the problem of how to place the robot base to enable downstream manipulation (reachability) and 2) an active camera introduces the problem of how to position the camera to generate data for a visuomotor policy (visibility). To address these challenges, MoMaGen formulates data generation as a constrained optimization problem that satisfies hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility while navigation). This formulation generalizes across most existing automated data generation approaches and offers a principled foundation for developing future methods. We evaluate on four multi-step bimanual mobile manipulation tasks and find that MoMaGen enables the generation of much more diverse datasets than previous methods. As a result of the dataset diversity, we also show that the data generated by MoMaGen can be used to train successful imitation learning policies using a single source demo. Furthermore, the trained policy can be fine-tuned with a very small amount of real-world data (40 demos) to be succesfully deployed on real robotic hardware. More details are on our project page: momagen.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoMaGen：为双臂移动操作生成符合软硬约束的演示</div>
<div class="mono" style="margin-top:8px">从大规模多样的人类演示中进行模仿学习已被证明对训练机器人有效，但收集此类数据成本高且耗时。对于多步骤双臂移动操作，人类必须同时遥控移动基座和两个高自由度手臂，这一挑战更为严峻。先前的X-Gen工作开发了用于静态双臂操作任务的自动化数据生成框架，在仿真中通过合成新的场景配置来增强少量的人类演示。然而，这些先前的工作在处理双臂移动操作任务时存在两个主要问题：1) 移动基座引入了如何放置机器人基座以使下游操作可行（可达性）的问题；2) 活动摄像头引入了如何定位摄像头以生成用于视觉-运动策略的数据的问题（可见性）。为了解决这些挑战，MoMaGen将数据生成建模为一个满足硬约束（例如，可达性）的同时平衡软约束（例如，导航过程中的可见性）的约束优化问题。这种建模方式可以泛化到大多数现有的自动化数据生成方法，并为开发未来的方法提供了一个原则性的基础。我们在四个多步骤双臂移动操作任务上进行了评估，发现MoMaGen能够生成比以往方法更多的多样化数据集。由于数据集的多样性，我们还展示了MoMaGen生成的数据可以用于使用单一来源的演示训练成功的模仿学习策略。此外，经过少量真实世界数据（40个演示）的微调后，训练出的策略可以在真实的机器人硬件上成功部署。更多细节请参见我们的项目页面：momagen.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MoMaGen addresses the challenges of generating demonstrations for multi-step bimanual mobile manipulation by formulating data generation as a constrained optimization problem that satisfies both reachability and visibility constraints. The method enables the creation of diverse datasets for these complex tasks, which can be used to train imitation learning policies. Notably, the generated data can be fine-tuned with minimal real-world data to successfully deploy on real robotic hardware.</div>
<div class="mono" style="margin-top:8px">MoMaGen通过将数据生成问题表述为同时满足可达性和可见性约束的优化问题来解决多步骤双臂移动操作演示生成的挑战。该方法能够生成比之前方法更多的多样化数据集，这些数据集可以用于训练成功的模仿学习策略。这些策略可以通过少量的实地数据进行微调，并部署在真实的机器人硬件上。</div>
</details>
</div>
<div class="card">
<div class="title">Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination</div>
<div class="meta-line">Authors: Rakshit Trivedi, Kartik Sharma, David C Parkes</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2026-02-24T03:37:42+00:00 · Latest: 2026-02-24T03:37:42+00:00</div>
<div class="meta-line">Comments: Spotlight paper at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20517v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20517v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mimic-research.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内心言语作为行为指南：可调节模仿多种行为以实现人机协调</div>
<div class="mono" style="margin-top:8px">有效的真人-人工智能协调需要能够表现出和响应人类行为并适应变化环境的人工智能代理。模仿学习已成为构建此类代理的一种重要方法，通过训练它们模仿人类示范的行为。然而，当前的方法难以捕捉人类行为的内在多样性和非马尔可夫性质，缺乏在推理时调节行为的能力。受到人类认知过程理论的启发，其中内心言语在执行前指导行为选择，我们提出了MIMIC（模仿和控制中的内在动机建模）框架，该框架使用语言作为行为意图的内部表示。MIMIC 使用视觉-语言模型作为语言支架，训练条件变分自编码器，能够从观察中生成内心言语。基于扩散的行为克隆策略根据当前观察和生成的内心言语选择动作。MIMIC 通过使代理根据特定行为的言语进行条件化，在推理时实现精细的行为调节。跨机器人操作任务和真人-人工智能协作游戏的实验表明，MIMIC 显著提高了行为的多样性和对人类示范的忠实度，同时无需额外示范即可实现细腻的行为调节。我们开源了代码，并在以下网址提供了预训练的MIMIC代理和定性演示：https://mimic-research.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop artificial agents that can effectively imitate and adapt to human behaviors in dynamic contexts, addressing the limitations of current imitation learning methods. MIMIC, a proposed framework, uses vision-language models to generate inner speech as a guide for behavior selection, enabling the agent to steer its actions based on specific speech conditions. Experiments show that MIMIC improves behavior diversity and fidelity to human demonstrations, allowing for nuanced control without additional training data.</div>
<div class="mono" style="margin-top:8px">研究旨在开发能够展示和响应人类行为的人工智能代理，以适应不断变化的环境，实现有效的人类-人工智能协调。提出了一种名为MIMIC的框架，利用语言作为行为意图的内部表示。该框架使用条件变分自编码器和基于扩散的行为克隆策略来生成和引导行为。实验表明，MIMIC能够增强行为的多样性和对人类演示的忠实度，并且能够在无需额外训练数据的情况下实现精细的行为引导。</div>
</details>
</div>
<div class="card">
<div class="title">Grasp to Act: Dexterous Grasping for Tool Use in Dynamic Settings</div>
<div class="meta-line">Authors: Harsh Gupta, Mohammad Amin Mirzaee, Wenzhen Yuan</div>
<div class="meta-line">First: 2026-02-24T01:53:39+00:00 · Latest: 2026-02-24T01:53:39+00:00</div>
<div class="meta-line">Comments: Result videos can be found at https://grasp2act.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20466v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20466v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://grasp2act.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving robust grasping with dexterous hands remains challenging, especially when manipulation involves dynamic forces such as impacts, torques, and continuous resistance--situations common in real-world tool use. Existing methods largely optimize grasps for static geometric stability and often fail once external forces arise during manipulation. We present Grasp-to-Act, a hybrid system that combines physics-based grasp optimization with reinforcement-learning-based grasp adaptation to maintain stable grasps throughout functional manipulation tasks. Our method synthesizes robust grasp configurations informed by human demonstrations and employs an adaptive controller that residually issues joint corrections to prevent in-hand slip while tracking the object trajectory. Grasp-to-Act enables robust zero-shot sim-to-real transfer across five dynamic tool-use tasks--hammering, sawing, cutting, stirring, and scooping--consistently outperforming baselines. Across simulation and real-world hardware trials with a 16-DoF dexterous hand, our method reduces translational and rotational in-hand slip and achieves the highest task completion rates, demonstrating stable functional grasps under dynamic, contact-rich conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活抓取以行动：在动态环境中的工具使用</div>
<div class="mono" style="margin-top:8px">实现灵巧手的稳健抓取仍然是一个挑战，尤其是在涉及冲击、扭矩和持续阻力等动态力的情况下——这些情况在现实世界的工具使用中非常常见。现有方法主要优化静态几何稳定性，但在操作过程中一旦出现外部力，往往会失效。我们提出了Grasp-to-Act，这是一种结合基于物理的抓取优化和基于强化学习的抓取适应的混合系统，以在整个功能操作任务中保持稳定的抓取。我们的方法通过人类示范合成稳健的抓取配置，并采用一个自适应控制器，持续发出关节修正指令以防止手中滑动并跟踪物体轨迹。Grasp-to-Act 在五个动态工具使用任务（锤击、锯切、切割、搅拌和舀取）中实现了稳健的零样本模拟到现实世界的转移，并且在所有基准中表现最佳。在模拟和使用16-DoF灵巧手的现实世界硬件试验中，我们的方法减少了手中的平移和旋转滑动，并实现了最高的任务完成率，证明了在动态、接触丰富的条件下稳定的功能抓取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of achieving robust grasping with dexterous hands in dynamic settings, where external forces are common. It introduces Grasp-to-Act, a hybrid system combining physics-based grasp optimization with reinforcement learning for grasp adaptation. The method, informed by human demonstrations, uses an adaptive controller to prevent in-hand slip and track object trajectories, enabling successful execution of five dynamic tool-use tasks with higher task completion rates compared to baselines.</div>
<div class="mono" style="margin-top:8px">研究旨在解决在动态环境中实现灵巧手稳健抓取的挑战，其中外部力很常见。提出了一种结合基于物理的抓取优化和强化学习的抓取适应性方法——Grasp-to-Act。该方法从人类示范中合成抓取配置，并使用自适应控制器防止抓取过程中发生手内滑动。Grasp-to-Act 在五个动态工具使用任务中表现出色，模拟和真实硬件试验中均实现了高任务完成率和减少手内滑动。</div>
</details>
</div>
<div class="card">
<div class="title">DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation</div>
<div class="meta-line">Authors: Taeyeop Lee, Gyuree Kang, Bowen Wen, Youngho Kim, Seunghyeok Back, In So Kweon, David Hyunchul Shim, Kuk-Jin Yoon</div>
<div class="meta-line">First: 2025-10-07T08:18:29+00:00 · Latest: 2026-02-24T00:07:47+00:00</div>
<div class="meta-line">Comments: Project page: https://sites.google.com/view/DeLTa25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.05662v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.05662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/DeLTa25/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the prevalence of transparent object interactions in human everyday life, transparent robotic manipulation research remains limited to short-horizon tasks and basic grasping capabilities. Although some methods have partially addressed these issues, most of them have limitations in generalization to novel objects and are insufficient for precise long-horizon robot manipulation. To address this limitation, we propose DeLTa (Demonstration and Language-Guided Novel Transparent Object Manipulation), a novel framework that integrates depth estimation, 6D pose estimation, and vision-language planning for precise long-horizon manipulation of transparent objects guided by natural language task instructions. A key advantage of our method is its single-demonstration approach, which generalizes 6D trajectories to novel transparent objects without requiring category-level priors or additional training. Additionally, we present a task planner that refines the VLM-generated plan to account for the constraints of a single-arm, eye-in-hand robot for long-horizon object manipulation tasks. Through comprehensive evaluation, we demonstrate that our method significantly outperforms existing transparent object manipulation approaches, particularly in long-horizon scenarios requiring precise manipulation capabilities. Project page: https://sites.google.com/view/DeLTa25/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeLTa: 展示与语言引导的新型透明物体操作</div>
<div class="mono" style="margin-top:8px">尽管透明物体交互在人类日常生活中普遍存在，但透明机器人操作研究仍局限于短期任务和基本抓取能力。尽管一些方法部分解决了这些问题，但大多数方法在泛化到新型物体方面存在局限性，不足以进行精确的长期机器人操作。为解决这一局限，我们提出了DeLTa（展示与语言引导的新型透明物体操作），这是一种结合深度估计、6D姿态估计和视觉语言规划的新框架，用于在自然语言任务指令引导下进行精确的长期透明物体操作。我们方法的一个关键优势是其单次演示方法，该方法无需类别级别的先验知识或额外训练即可将6D轨迹泛化到新型透明物体。此外，我们还提出了一种任务规划器，用于根据单臂、手持相机机器人对长期物体操作任务的约束条件来细化VLM生成的计划。通过全面评估，我们证明了我们的方法在长期场景中显著优于现有透明物体操作方法，特别是在需要精确操作能力的情况下。项目页面：https://sites.google.com/view/DeLTa25/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DeLTa is a novel framework for precise long-horizon manipulation of transparent objects, integrating depth estimation, 6D pose estimation, and vision-language planning. It uses a single demonstration to generalize 6D trajectories to new objects without needing category-level priors. The method also includes a task planner for single-arm, eye-in-hand robots, improving performance in long-horizon tasks. Experiments show that DeLTa outperforms existing methods, especially in precise long-horizon scenarios.</div>
<div class="mono" style="margin-top:8px">DeLTa 是一种用于透明物体精确长时操作的新型框架，结合了深度估计、6D姿态估计和视觉语言规划。它使用单次演示来将6D轨迹泛化到新物体，并包含一个单臂、手持相机的机器人任务规划器。实验表明，DeLTa 在长时操作任务中表现优于现有方法，特别是在需要精确操作的场景中。</div>
</details>
</div>
<div class="card">
<div class="title">Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation</div>
<div class="meta-line">Authors: Haiyun Zhang, Stefano Dalla Gasperina, Saad N. Yousaf, Toshimitsu Tsuboi, Tetsuya Narita, Ashish D. Deshpande</div>
<div class="meta-line">First: 2025-07-31T14:29:38+00:00 · Latest: 2026-02-23T21:05:46+00:00</div>
<div class="meta-line">Comments: 8 pages, 10 figures, 1 supplementary video, submitted to RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.23592v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.23592v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hand exoskeletons are critical tools for dexterous teleoperation and immersive manipulation interfaces, but achieving accurate hand tracking remains a challenge due to user-specific anatomical variability and donning inconsistencies. These issues lead to kinematic misalignments that degrade tracking performance and limit applicability in precision tasks. We propose a subject-specific calibration framework for exoskeleton-based hand tracking that estimates virtual link parameters through residual-weighted optimization. A data-driven approach is introduced to empirically tune cost function weights using motion capture ground truth, enabling accurate and consistent calibration across users. Implemented on the Maestro hand exoskeleton with seven healthy participants, the method achieved substantial reductions in joint and fingertip tracking errors across diverse hand geometries. Qualitative visualizations using a Unity-based virtual hand further demonstrate improved motion fidelity. The proposed framework generalizes to exoskeletons with closed-loop kinematics and minimal sensing, laying the foundation for high-fidelity teleoperation and robot learning applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类外骨骼运动校准以提高手部跟踪精度以实现灵巧的远程操作</div>
<div class="mono" style="margin-top:8px">手部外骨骼是实现灵巧远程操作和沉浸式操作界面的关键工具，但由于用户特定的解剖变异性和穿戴不一致，实现精确的手部跟踪仍然是一个挑战。这些问题导致运动学错位，降低了跟踪性能并限制了在精细任务中的应用。我们提出了一种针对外骨骼的手部跟踪的个体化校准框架，通过残差加权优化估计虚拟链接参数。引入了一种数据驱动的方法，使用运动捕捉的地面真实值来实证调整成本函数权重，从而实现用户间准确且一致的校准。该方法在Maestro手部外骨骼上实施，七名健康参与者的结果显示，关节和指尖跟踪误差在不同手型中显著减少。基于Unity的虚拟手的定性可视化进一步证明了运动保真度的提高。所提出的框架适用于具有闭环运动学和最少传感的外骨骼，为高保真远程操作和机器人学习应用奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve hand tracking accuracy in dexterous teleoperation by addressing kinematic misalignments caused by user-specific anatomical differences and donning inconsistencies. The method involves a subject-specific calibration framework that uses residual-weighted optimization to estimate virtual link parameters, with a data-driven approach to tune cost function weights. The study, conducted on the Maestro hand exoskeleton with seven participants, significantly reduced joint and fingertip tracking errors, enhancing motion fidelity and applicability in precision tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决由于用户特定的解剖差异和穿戴不一致导致的动力学对齐问题，提高灵巧远程操作中的手部跟踪准确性。方法包括一个基于残差加权优化的主体特定校准框架，以估计虚拟链接参数，并使用数据驱动的方法调整成本函数权重。研究在Maestro手部外骨骼上对七名参与者进行了测试，显著减少了关节和指尖的跟踪误差，提高了运动的保真度和在精细任务中的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory</div>
<div class="meta-line">Authors: Haoyang Li, Yang You, Hao Su, Leonidas Guibas</div>
<div class="meta-line">First: 2026-02-23T20:18:35+00:00 · Latest: 2026-02-23T20:18:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20323v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20323v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从交互中学习物理原理：测试时自演化规划</div>
<div class="mono" style="margin-top:8px">可靠的物体操作需要理解跨对象和环境变化的物理属性。视觉-语言模型（VLM）规划器可以以一般性的方式推理摩擦和稳定性；然而，它们通常无法预测特定球体在特定表面滚动的方式或哪块石头能提供稳定的基础，除非有直接经验。我们提出了PhysMem，这是一种记忆框架，使VLM机器人规划器能够在测试时从交互中学习物理原理，而不更新模型参数。该系统记录经验，生成候选假设，并通过有针对性的交互验证这些假设，然后将验证的知识推广以指导未来的决策。一个关键的设计选择是在应用之前进行验证：系统测试假设与新观察结果的对比，而不是直接应用检索的经验，从而减少在物理条件变化时对先前经验的僵化依赖。我们在三个实际操作任务和四个VLM骨干网络的模拟基准上评估了PhysMem。在受控的砖块插入任务中，基于原理的抽象成功率为76%，而直接经验检索的成功率为23%，实际实验显示在30分钟部署会话中持续改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable vision-language model planners to learn physical principles from interaction at test time without updating model parameters. PhysMem, a memory framework, records experiences, generates hypotheses, and verifies them through targeted interaction. On a brick insertion task, principled abstraction achieved 76% success compared to 23% for direct experience retrieval. Real-world experiments showed consistent improvement over 30-minute deployment sessions.</div>
<div class="mono" style="margin-top:8px">研究旨在通过测试时的交互让视觉-语言模型规划器从经验中学习物理原理而不更新模型参数。PhysMem是一种记忆框架，记录经验、生成假设并通过目标交互验证它们。在砖块插入任务中，原理化抽象的成功率为76%，而直接经验检索仅为23%。实地实验在30分钟的部署会话中显示了持续改进。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260225_0357.html">20260225_0357</a>
<a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0337.html">20260223_0337</a>
<a href="archive/20260222_0338.html">20260222_0338</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
