<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-23 03:37</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260223_0337</div>
    <div class="row"><div class="card">
<div class="title">When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</div>
<div class="meta-line">Authors: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-19T18:59:20+00:00 · Latest: 2026-02-19T18:59:20+00:00</div>
<div class="meta-line">Comments: Website: https://vla-va.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vla-va.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉优先于语言：评估和缓解VLAs中的反事实失败</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动模型（VLAs）承诺将语言指令应用于机器人控制，但在实践中往往未能忠实执行语言指令。当面对缺乏强烈场景特定监督的指令时，VLAs会遭受反事实失败：它们基于数据集偏差诱导的视觉捷径行动，反复执行已学得的行为，并选择在训练期间频繁出现的对象，而不考虑语言意图。为了系统地研究这一问题，我们引入了LIBERO-CF，这是第一个用于VLAs的反事实基准，通过在视觉上合理的LIBERO布局下分配替代指令来评估语言遵循能力。我们的评估表明，反事实失败在最先进的VLAs中普遍存在但尚未得到充分探索。我们提出了反事实行动指导（CAG），这是一种简单而有效的双分支推理方案，明确地在VLAs中正则化语言条件。CAG结合了一个标准的VLA策略和一个未受语言条件的视觉-行动（VA）模块，在行动选择时进行反事实比较。这种设计减少了对视觉捷径的依赖，提高了对未观察任务的鲁棒性，并不需要额外的演示或对现有架构或预训练模型进行修改。广泛的实验表明，它可以在各种VLAs中实现即插即用集成，并带来一致的改进。例如，在LIBERO-CF中，CAG在语言遵循准确性上提高了9.7%，在未观察任务上的任务成功率提高了3.6%，使用无训练策略，配以VA模型时，进一步提高了15.5%和8.5%。在实际应用中，CAG将反事实失败减少了9.4%，平均提高了任务成功率17.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of counterfactual failures in Vision-Language-Action models (VLAs), where models act based on visual biases rather than language instructions. It introduces LIBERO-CF, a benchmark for evaluating language following capability by providing alternative instructions under visually plausible scenarios. The authors propose Counterfactual Action Guidance (CAG), a dual-branch inference scheme that improves robustness and reduces visual shortcut reliance, leading to better performance on under-observed tasks and real-world evaluations. CAG shows consistent improvements across various VLAs, with significant gains in language following accuracy and task success rates.</div>
<div class="mono" style="margin-top:8px">研究关注Vision-Language-Action模型（VLAs）中的反事实失败问题，即模型基于视觉偏见而非语言指令行动。为此，作者引入了LIBERO-CF基准，该基准在视觉上合理的场景下分配替代指令。他们提出了反事实行动指导（CAG），这是一种双分支推理方案，能够提高语言跟随准确性和任务成功率，特别是在未观察到的任务上，且无需额外训练或修改现有模型。实验表明，CAG减少了反事实失败并提高了模拟和真实世界设置中的任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Step Duration for Accurate Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds</div>
<div class="meta-line">Authors: Zhaoyang Xiang, Victor Paredes, Guillermo A. Castillo, Ayonga Hereid</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2024-03-25T19:18:25+00:00 · Latest: 2026-02-19T18:19:15+00:00</div>
<div class="meta-line">Comments: 7 pages, 7 figures. Accepted to IEEE/RSJ IROS 2025. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.17136v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.17136v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional one-step preview planning algorithms for bipedal locomotion struggle to generate viable gaits when walking across terrains with restricted footholds, such as stepping stones. To overcome such limitations, this paper introduces a novel multi-step preview foot placement planning algorithm based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of walking robots. Our proposed approach adaptively changes the step duration and the swing foot trajectory for optimal foot placement under constraints, thereby enhancing the long-term stability of the robot and significantly improving its ability to navigate environments with tight constraints on viable footholds. We demonstrate its effectiveness through various simulation scenarios with complex stepping-stone configurations and external perturbations. These tests underscore its improved performance for navigating foothold-restricted terrains, even with external disturbances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应性步长调整以实现准确的足部放置：在受限 foothold 地形上实现稳健的双足运动</div>
<div class="mono" style="margin-top:8px">传统的双足运动一步预览规划算法在跨越受限 foothold 地形（如踏石）时难以生成可行的步态。为克服这些限制，本文提出了一种基于行走机器人 Divergent Component of Motion (DCM) 的步对步离散演变的多步预览足部放置规划算法。我们提出的方法适应性地调整步长和摆动腿轨迹，以在约束条件下实现最佳足部放置，从而增强机器人的长期稳定性和显著提高其在受限 foothold 地形环境中导航的能力。通过各种包含复杂踏石配置和外部干扰的仿真场景，我们展示了其有效性。这些测试强调了其在外部干扰下导航受限 foothold 地形的改进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of bipedal locomotion on terrains with restricted footholds by proposing a multi-step preview foot placement planning algorithm. The algorithm adaptively adjusts step duration and swing foot trajectory based on the Divergent Component of Motion (DCM) to optimize foot placement. Experimental results from various simulation scenarios show that the proposed method enhances the robot&#x27;s stability and navigation capabilities in environments with tight foothold constraints, even under external disturbances.</div>
<div class="mono" style="margin-top:8px">论文提出了一种多步预览足部放置规划算法，以解决在受限 foothold 地形上的双足行走问题。该方法根据行走机器人的 Divergent Component of Motion (DCM) 调整步长和摆动腿轨迹，以增强稳定性和导航能力。通过各种包含复杂踏石配置和外部干扰的仿真测试，证明了该方法在受限 foothold 地形上的优越性能。</div>
</details>
</div>
<div class="card">
<div class="title">IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control</div>
<div class="meta-line">Authors: Qilong Cheng, Matthew Mackay, Ali Bereyhi</div>
<div class="meta-line">First: 2026-02-19T16:50:31+00:00 · Latest: 2026-02-19T16:50:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17537v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IRIS：基于学习的任务特定电影机器人手臂用于视动运动控制</div>
<div class="mono" style="margin-top:8px">机器人摄像系统能够实现超越人类能力的动态、可重复运动，但其采用受限于工业级平台的高成本和操作复杂性。我们介绍了智能机器人成像系统（IRIS），这是一种专为自主、基于学习的电影运动控制设计的6自由度 manipulator。IRIS 结合了轻量级的全3D打印硬件设计和基于动作分块与变换器（ACT）的目标条件视动模仿学习框架。该系统直接从人类示范中学习对象感知和感知平滑的摄像机轨迹，消除了显式几何编程的需要。整个平台成本低于1000美元，支持1.5公斤负载，并实现约1毫米的重复性。实际实验表明，该系统能够准确跟踪轨迹、可靠自主执行，并在多种电影运动中泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a cost-effective and easy-to-use robotic camera system for cinematic motion control. IRIS, a 6-DOF manipulator, integrates a lightweight 3D-printed hardware with a learning-based visuomotor imitation framework. The system learns camera trajectories from human demonstrations and can track trajectories accurately, execute autonomously, and generalize across various cinematic motions. The platform costs under $1,000 and supports a 1.5 kg payload with 1 mm repeatability.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种低成本且易于操作的机器人摄像系统，用于电影运动控制。IRIS 是一个6-DOF机械臂，结合了轻量级3D打印硬件和基于Action Chunking with Transformers (ACT)的学习模仿框架。该系统从人类演示中学习摄像机轨迹，并能准确跟踪轨迹、自主执行并泛化到各种电影运动中。该平台成本低于1000美元，承载能力为1.5公斤，重复精度为1毫米。</div>
</details>
</div>
<div class="card">
<div class="title">Proximal powered knee placement: a case study</div>
<div class="meta-line">Authors: Kyle R. Embry, Lorenzo Vianello, Jim Lipsey, Frank Ursetta, Michael Stephens, Zhi Wang, Ann M. Simon, Andrea J. Ikeda, Suzanne B. Finucane, Shawana Anarwala, Levi J. Hargrove</div>
<div class="meta-line">First: 2026-02-19T16:16:20+00:00 · Latest: 2026-02-19T16:16:20+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE RAS/EMBS 11th International Conference on Biomedical Robotics and Biomechatronics (BioRob 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17502v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17502v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>近端动力膝关节安置：案例研究</div>
<div class="mono" style="margin-top:8px">下肢截肢影响全球数百万人，导致行动能力下降、行走速度减慢和日常及社交活动参与度降低。动力假肢膝关节可以通过主动辅助膝关节扭矩部分恢复行动能力，改善步态对称性、坐起立转换和行走速度。然而，动力组件增加的重量可能会削弱这些益处，负面影响步态力学并增加代谢成本。因此，优化质量分布，而不是简单地减少总质量，可能提供更有效和实用的解决方案。在本探索性研究中，我们评估了在小样本组中将膝关节动力传动装置安置在上方的可行性。与下方安置相比，上方配置显示出行走速度（一名参与者提高9.2%）和步频（提高3.6%）的改善，步态对称性则表现出混合效果。运动学测量表明，两种配置下的膝关节活动范围和峰值速度相似。在斜坡和楼梯上的额外测试证实了控制策略在多种运动任务中的稳健性。初步结果显示，上方安置在功能上是可行的，精心的质量分布可以保持动力辅助的益处，同时减轻增加重量的负面影响。需要进一步的研究来确认这些趋势并指导设计和临床建议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study evaluates the feasibility of placing a powered prosthetic knee above the knee in a small cohort, finding improved walking speed and cadence compared to below-knee placement, while maintaining similar knee range of motion and peak velocity. The control strategy was robust across various tasks, suggesting that above-knee placement can preserve the benefits of powered assistance while mitigating the negative effects of added weight.</div>
<div class="mono" style="margin-top:8px">本研究评估了将假肢膝关节动力装置置于膝上方在小样本中的可行性，发现与膝下放置相比，行走速度和步频有所提高，同时膝关节活动范围和峰值速度保持相似。控制策略在各种任务中表现出色，表明膝上方放置可以保留动力辅助的好处，同时减轻额外重量的负面影响。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection</div>
<div class="meta-line">Authors: Yichen Lu, Siwei Nie, Minlong Lu, Xudong Yang, Xiaobo Zhang, Peng Zhang</div>
<div class="meta-line">First: 2026-02-19T15:54:55+00:00 · Latest: 2026-02-19T15:54:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17484v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17484v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace&#x27;s verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追踪复制像素和正则化块亲和性在复制检测中的应用</div>
<div class="mono" style="margin-top:8px">图像复制检测（ICD）旨在通过稳健的特征表示学习来识别图像对之间的篡改内容。虽然自监督学习（SSL）已经提升了ICD系统的性能，但现有的视图级对比方法由于缺乏细粒度对应关系学习，难以应对复杂的编辑。我们通过两种关键创新解决了这一限制。首先，我们提出了PixTrace——一个像素坐标追踪模块，用于在编辑变换中保持显式的空间映射。其次，我们引入了CopyNCE，这是一种几何引导的对比损失，通过从PixTrace验证的映射中提取的重叠比来正则化块亲和性。我们的方法将像素级的可追踪性与块级的相似性学习相结合，抑制了SSL训练中的监督噪声。广泛的实验不仅展示了最先进的性能（匹配器88.7% uAP / 83.9% RP90，描述符72.6% uAP / 68.4% RP90，DISC21数据集），还展示了比现有方法更好的可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve image copy detection by addressing the limitations of existing view-level contrastive methods. It introduces PixTrace, a pixel coordinate tracking module, and CopyNCE, a geometrically-guided contrastive loss, to enhance fine-grained correspondence learning. The method achieves state-of-the-art performance with 88.7% uAP and 83.9% RP90 for the matcher, and 72.6% uAP and 68.4% RP90 for the descriptor on the DISC21 dataset, while also offering better interpretability compared to previous approaches.</div>
<div class="mono" style="margin-top:8px">论文通过提出像素坐标追踪模块PixTrace和几何导向的对比损失CopyNCE，解决了图像对中检测复制像素的挑战。这些创新有助于保持显式的空间映射并正则化补丁亲和性，从而提高特征表示学习的鲁棒性。该方法在DISC21数据集上达到了最先进的性能，匹配器的uAP为88.7%，RP90为83.9%，描述符的uAP为72.6%，RP90为68.4%，同时增强了可解释性，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models</div>
<div class="meta-line">Authors: Clemence Grislain, Hamed Rahimi, Olivier Sigaud, Mohamed Chetouani</div>
<div class="meta-line">First: 2025-09-19T15:19:38+00:00 · Latest: 2026-02-19T15:45:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16072v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.16072v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://clemgris.github.io/I-FailSense/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-conditioned robotic manipulation in open-world settings requires not only accurate task execution but also the ability to detect failures for robust deployment in real-world environments. Although recent advances in vision-language models (VLMs) have significantly improved the spatial reasoning and task-planning capabilities of robots, they remain limited in their ability to recognize their own failures. In particular, a critical yet underexplored challenge lies in detecting semantic misalignment errors, where the robot executes a task that is semantically meaningful but inconsistent with the given instruction. To address this, we propose a method for building datasets targeting Semantic Misalignment Failures detection, from existing language-conditioned manipulation datasets. We also present I-FailSense, an open-source VLM framework with grounded arbitration designed specifically for failure detection. Our approach relies on post-training a base VLM, followed by training lightweight classification heads, called FS blocks, attached to different internal layers of the VLM and whose predictions are aggregated using an ensembling mechanism. Experiments show that I-FailSense outperforms state-of-the-art VLMs, both comparable in size and larger, in detecting semantic misalignment errors. Notably, despite being trained only on semantic misalignment detection, I-FailSense generalizes to broader robotic failure categories and effectively transfers to other simulation environments and real-world with zero-shot or minimal post-training. The datasets and models are publicly released on HuggingFace (Webpage: https://clemgris.github.io/I-FailSense/).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>I-FailSense：基于视觉-语言模型的通用机器人故障检测</div>
<div class="mono" style="margin-top:8px">开放世界环境中的语言条件化机器人操作不仅需要准确的任务执行，还需要检测故障的能力，以在实际环境中实现稳健部署。尽管近期视觉-语言模型（VLMs）在空间推理和任务规划方面取得了显著进步，但在识别自身故障方面仍有限制。特别是，一个关键但尚未充分探索的挑战在于检测语义对齐错误，即机器人执行的任务在语义上是有意义的，但与给定指令不一致。为解决这一问题，我们提出了一种方法，从现有的语言条件化操作数据集中构建针对语义对齐错误检测的数据集。我们还介绍了I-FailSense，一个具有基于地面仲裁的开源VLM框架，专门用于故障检测。我们的方法依赖于在基VLM上进行后训练，然后训练轻量级分类头，称为FS块，将其附加到VLM的不同内部层，并使用集成机制聚合其预测。实验表明，I-FailSense在检测语义对齐错误方面优于现有的VLM，无论是大小相当还是更大的模型。值得注意的是，尽管仅在语义对齐检测上进行训练，I-FailSense仍能泛化到更广泛的机器人故障类别，并有效转移到其他模拟环境和现实世界中，无需或只需少量后训练。数据集和模型已公开发布在HuggingFace（网址：https://clemgris.github.io/I-FailSense/）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to enhance robotic manipulation in open-world settings by developing a method for detecting semantic misalignment errors, a critical failure type. It introduces I-FailSense, an open-source vision-language model framework that post-trains a base VLM and adds lightweight classification heads to detect these errors. Experiments show that I-FailSense outperforms existing VLMs in detecting semantic misalignment errors and generalizes well to other failure categories and environments with minimal training.</div>
<div class="mono" style="margin-top:8px">论文旨在通过开发一种检测语义对齐错误的方法来增强机器人在开放环境中的操作能力。它引入了I-FailSense，这是一个开源的VLM框架，对基础VLM进行后训练，并添加了轻量级分类头来检测这些错误。实验表明，I-FailSense在检测语义对齐错误方面优于现有VLM，并且能够很好地泛化到其他故障类别和环境中，只需少量训练即可。</div>
</details>
</div>
<div class="card">
<div class="title">2Mamba2Furious: Linear in Complexity, Competitive in Accuracy</div>
<div class="meta-line">Authors: Gabriel Mongaras, Eric C. Larson</div>
<div class="meta-line">First: 2026-02-19T13:45:23+00:00 · Latest: 2026-02-19T13:45:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>2Mamba2Furious: 线性在复杂性上，竞争在准确性上</div>
<div class="mono" style="margin-top:8px">线性注意力变换器由于其效率已成为softmax注意力的强有力替代品。然而，线性注意力在表达能力上较弱，导致准确性低于softmax注意力。为了弥合softmax注意力和线性注意力之间的准确性差距，我们操控了Mamba-2，这是一种非常强大的线性注意力变体。我们首先将Mamba-2简化为其最基本和最重要的组成部分，评估哪些具体选择使其最准确。从简化后的Mamba变体（Mamba-2S）中，我们改进了A-掩码并增加了隐藏状态的阶数，从而提出了一种名为2Mamba的方法，该方法在准确性上几乎与softmax注意力相当，但在长上下文长度下却更加节省内存。我们还研究了有助于超越softmax注意力准确性的Mamba-2的元素。所有实验的代码均已提供</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the accuracy of linear attention transformers, which are more efficient than softmax attention but less expressive. By simplifying Mamba-2 to its core components and enhancing the A-mask and hidden state order, the study develops 2Mamba, which nearly matches softmax attention accuracy while being much more memory-efficient for long context lengths. Key findings include the identification of specific choices that enhance accuracy and the development of a method that is competitive in accuracy but superior in efficiency.</div>
<div class="mono" style="margin-top:8px">研究旨在提高线性注意力变压器的准确性，这类变压器比softmax注意力更节省内存但表达能力较弱。作者简化了Mamba-2这一线性注意力变体的核心组件，并改进了A-mask并增加了隐藏状态的阶数，从而提出了一种名为2Mamba的方法，该方法在长上下文长度时几乎与softmax注意力的准确性相同，同时非常节省内存。</div>
</details>
</div>
<div class="card">
<div class="title">Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises</div>
<div class="meta-line">Authors: Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbin Li, Yiming Li</div>
<div class="meta-line">First: 2025-04-30T15:21:25+00:00 · Latest: 2026-02-19T12:16:56+00:00</div>
<div class="meta-line">Comments: To appear in TIFS 2026. 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21730v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.21730v2">PDF</a> · <a href="https://github.com/NcepuQiaoTing/Cert-SSB">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample&#x27;s certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cert-SSBD: 认证样本特定平滑噪声的后门防御认证</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）容易受到后门攻击的影响，攻击者通过操纵一小部分训练数据植入隐藏的后门。受攻击的模型在干净样本上表现正常，但在后门样本上将其错误分类为攻击者指定的目标类别，对实际应用中的DNN构成了重大威胁。目前，已经提出了几种经验防御方法来缓解后门攻击，但这些方法往往被更先进的后门技术绕过。相比之下，基于随机平滑的认证防御显示出前景，通过向训练和测试样本添加随机噪声来对抗后门攻击。在本文中，我们揭示了现有的随机平滑防御隐含地假设所有样本与决策边界等距，但在实践中这可能不成立，导致认证性能不佳。为解决这一问题，我们提出了一种样本特定的认证后门防御方法，称为Cert-SSB。Cert-SSB首先使用随机梯度上升优化每个样本的噪声幅度，确保样本特定的噪声水平，然后应用于多个受污染的训练集以重新训练多个平滑模型。之后，Cert-SSB聚合多个平滑模型的预测生成最终的鲁棒预测。特别是，在这种情况下，现有的认证方法变得不适用，因为优化的噪声在不同样本之间变化。为了克服这一挑战，我们引入了一种基于存储更新的认证方法，该方法动态调整每个样本的认证区域以提高认证性能。我们在多个基准数据集上进行了广泛的实验，证明了我们提出方法的有效性。我们的代码可在https://github.com/NcepuQiaoTing/Cert-SSB/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of deep neural networks to backdoor attacks by proposing Cert-SSB, a sample-specific certified backdoor defense method. It optimizes noise magnitude for each sample using stochastic gradient ascent, retraining multiple smoothed models on poisoned training sets, and aggregating their predictions. The authors introduce a storage-update-based certification method to handle varying noise across samples, improving certification performance. Experiments on benchmark datasets show the effectiveness of this approach in defending against backdoor attacks.</div>
<div class="mono" style="margin-top:8px">论文提出了一种称为Cert-SSB的认证后门防御方法，通过使用随机梯度上升优化每个样本的噪声大小，并将其应用于多个受污染的训练集以重新训练多个平滑模型。该方法然后将这些模型的预测结果聚合以生成稳健的预测。作者引入了一种基于存储更新的认证方法，以处理样本间噪声变化的问题，从而提高认证性能。在基准数据集上的实验表明，Cert-SSB在抵御后门攻击方面非常有效。</div>
</details>
</div>
<div class="card">
<div class="title">FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment</div>
<div class="meta-line">Authors: Han Zhao, Jingbo Wang, Wenxuan Song, Shuai Chen, Yang Liu, Yan Wang, Haoang Li, Donglin Wang</div>
<div class="meta-line">First: 2026-02-19T11:00:46+00:00 · Latest: 2026-02-19T11:00:46+00:00</div>
<div class="meta-line">Comments: Project Website: https://h-zhao1997.github.io/frappe</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://h-zhao1997.github.io/frappe">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FRAPPE：通过多未来表示对齐将世界建模融入通用政策</div>
<div class="mono" style="margin-top:8px">使VLA模型能够预测环境动态，即世界建模，已被认为是提高机器人推理和泛化能力的关键。然而，当前的方法面临两个主要问题：1. 训练目标迫使模型过度强调像素级重建，这限制了语义学习和泛化；2. 在推理过程中依赖预测的未来观察通常会导致误差累积。为了解决这些挑战，我们提出了未来表示对齐通过并行渐进扩展（FRAPPE）。我们的方法采用两阶段微调策略：在中期训练阶段，模型学习预测未来观察的潜在表示；在后期训练阶段，我们并行扩展计算工作量并同时与多个不同的视觉基础模型对齐表示。通过显著提高微调效率并减少对标注动作数据的依赖，FRAPPE提供了一种可扩展且数据高效的途径，以增强通用机器人政策的世界意识。在RoboTwin基准测试和实际任务上的实验表明，FRAPPE优于现有最佳方法，并在长时序和未见过的场景中表现出强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FRAPPE addresses the limitations of current world modeling approaches by introducing a two-stage fine-tuning strategy. In the mid-training phase, the model learns to predict future latent representations, and in the post-training phase, it aligns these representations with multiple visual foundation models. This method improves fine-tuning efficiency and reduces reliance on action-annotated data, leading to better generalization in long-horizon and unseen scenarios compared to state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">FRAPPE通过引入两阶段微调策略，专注于预测未来观察的潜在表示并同时与多个视觉基础模型对齐，解决了当前视觉-语言模型（VLA）的局限性。这种方法提高了微调效率，减少了对标注动作数据的依赖，从而在机器人策略中增强了世界意识。实验表明，FRAPPE在长时序和未见过的场景中优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place</div>
<div class="meta-line">Authors: Antonio Rapuano, Yaolei Shen, Federico Califano, Chiara Gabellieri, Antonio Franchi</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-19T09:38:32+00:00 · Latest: 2026-02-19T09:38:32+00:00</div>
<div class="meta-line">Comments: Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17199v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>悬索的连续和混合动力学的非线性预测控制及其在空中抓取和放置中的应用</div>
<div class="mono" style="margin-top:8px">本文提出了一种框架，用于基于偏微分方程（PDEs）的高保真模型与适合实时控制的降阶表示相结合的空中操作可伸缩悬索的方法。PDEs使用有限差分法离散化，并采用适当的正交分解提取降阶模型（ROM），以保留主导变形模式并显著降低计算复杂性。基于此ROM，提出了一种非线性模型预测控制方案，能够稳定悬索振荡并处理混合过渡，如载荷的附着和脱离。仿真结果证实了ROM的稳定性和效率，以及控制器在各种操作条件下调节悬索动力学的有效性。附加仿真展示了ROM在受限环境中的轨迹规划应用，证明了所提方法的灵活性。总体而言，该框架使装有悬垂柔性悬索的无人驾驶航空器（UAV）能够实现实时、动力学感知的控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces a framework for controlling the dynamics of a suspended deformable cable used in aerial manipulation. It combines a high-fidelity partial differential equation model with a reduced-order model using proper orthogonal decomposition for real-time control. A nonlinear model predictive control scheme is then applied to stabilize cable oscillations and manage hybrid transitions. Simulation results demonstrate the stability, efficiency, and robustness of the reduced-order model and the controller&#x27;s effectiveness in various operating conditions, including trajectory planning in constrained environments.</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于控制悬吊柔性缆线空中操作动力学的框架。该方法结合了基于偏微分方程的高保真模型和通过主成分分解得到的简化模型，以实现实时控制。随后应用非线性预测控制方案来稳定缆线振动并处理混合过渡。仿真结果表明简化模型的稳定性和效率，以及控制器在各种条件下的有效性，包括在受限环境中进行轨迹规划。</div>
</details>
</div>
<div class="card">
<div class="title">The Bots of Persuasion: Examining How Conversational Agents&#x27; Linguistic Expressions of Personality Affect User Perceptions and Decisions</div>
<div class="meta-line">Authors: Uğur Genç, Heng Gu, Chadha Degachi, Evangelos Niforatos, Senthil Chandrasegaran, Himanshu Verma</div>
<div class="meta-line">First: 2026-02-19T09:10:41+00:00 · Latest: 2026-02-19T09:10:41+00:00</div>
<div class="meta-line">Comments: Accepted to be presented at CHI&#x27;26 in Barcelona</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA&#x27;s composite personality did not affect participants&#x27; decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>说服性的机器人：探讨对话代理的语言个性表达如何影响用户感知和决策</div>
<div class="mono" style="margin-top:8px">由大型语言模型驱动的对话代理（CAs）越来越能够通过语言表现出复杂的人格特质，但这些表现如何影响用户尚不清楚。因此，我们研究了在慈善捐赠的背景下，CAs通过语言表达的人格特质如何影响用户的决策和感知。在一项众包研究中，360名参与者与八个CAs之一互动，每个CA表现出由三个语言方面组成的人格特质：态度（乐观/悲观）、权威（权威/顺从）和推理（情感/理性）。虽然CA的整体人格特质并未影响参与者的决策，但它确实影响了他们的感知和情绪反应。特别是，与悲观CAs互动的参与者感到情绪状态较低，对活动的认同感较低，认为CA不够可信和不那么有能力，但倾向于向慈善机构捐款更多。信任感、能力和情境同理心的感知显著预测了捐款决策。我们的研究结果强调了CAs作为操纵工具的风险，它们会微妙地影响用户的感知和决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how conversational agents&#x27; linguistic expressions of personality impact user perceptions and decisions, particularly in the context of charitable giving. Participants interacted with eight conversational agents, each embodying different personality traits in terms of attitude, authority, and reasoning. Although the overall personality of the CA did not influence donation decisions, it significantly affected users&#x27; emotional states and perceptions, with pessimistic personalities leading to lower trust, competence, and affinity towards the cause, yet higher donations. The study highlights the potential for conversational agents to manipulate user perceptions and decisions subtly.</div>
<div class="mono" style="margin-top:8px">本研究探讨了对话代理通过语言表达个性如何影响用户在慈善捐赠中的感知和决策。参与者与八个具有不同个性的对话代理互动，这些个性通过态度、权威和推理三个方面体现。尽管对话代理的个性没有影响捐赠决策，但它显著影响了用户的感知和情绪反应。与悲观对话代理互动的用户感到情感上更不投入，对活动的认同感更低，认为对话代理不那么可信和有能力，但仍然倾向于捐赠更多。研究强调了对话代理通过微妙的语言线索操纵用户感知和决策的风险。</div>
</details>
</div>
<div class="card">
<div class="title">Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy</div>
<div class="meta-line">Authors: Huishi Huang, Jack Klusmann, Haozhe Wang, Shuchen Ji, Fengkang Ying, Yiyuan Zhang, John Nassour, Gordon Cheng, Daniela Rus, Jun Liu, Marcelo H Ang, Cecilia Laschi</div>
<div class="meta-line">First: 2026-02-19T06:56:47+00:00 · Latest: 2026-02-19T06:56:47+00:00</div>
<div class="meta-line">Comments: Camera-ready version for RoboSoft 2026. 8 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system&#x27;s response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理人机交互在增强现实中的抓取应用：刚柔机器人协同</div>
<div class="mono" style="margin-top:8px">混合刚柔机器人结合了刚性操作臂的精确性和柔性臂的顺应性和适应性，为不规则环境中的多功能抓取提供了有前景的方法。然而，协调混合机器人仍然具有挑战性，由于建模、感知和跨域运动学的困难。在本文中，我们提出了一种新颖的基于增强现实(AR)的物理人机交互框架，该框架使用户能够直接远程操作混合刚柔机器人执行简单的接近和抓取任务。通过AR头显，用户可以与集成到通用物理引擎中的机器人系统模拟模型进行交互，该物理引擎叠加在真实系统上，允许在实际部署之前进行模拟执行。为了确保虚拟和物理机器人之间的一致行为，我们引入了一种基于软机器人固有几何特性的实际到模拟参数识别管道，使我们能够准确地建模其静态和动态行为以及控制系统响应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of coordinating hybrid rigid-soft robots for versatile grasping in unstructured environments. It introduces an augmented reality (AR) framework for direct teleoperation of such robots, using a physics engine to simulate and superimpose the robotic system on the real one. The key finding is the development of a real-to-simulation parameter identification pipeline that ensures consistent behavior between the virtual and physical robots, enabling accurate modeling of the soft robot&#x27;s static and dynamic behavior and the control system&#x27;s response.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决协调混合刚柔机器人进行抓取任务的问题。它提出了一种基于增强现实（AR）的直接遥操作框架，允许用户在实际部署前与机器人系统的模拟模型进行交互。关键发现是开发了一种从现实到模拟的参数识别管道，确保虚拟和物理机器人的行为一致，能够准确模拟软机器人的静态和动态行为以及控制系统响应。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success</div>
<div class="meta-line">Authors: Varun Burde, Pavel Burget, Torsten Sattler</div>
<div class="meta-line">First: 2026-02-19T05:55:01+00:00 · Latest: 2026-02-19T05:55:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17101v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17101v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物体姿态估计和重建对机器人抓取成功率影响的基准测试</div>
<div class="mono" style="margin-top:8px">3D重建是许多机器人感知任务的基础层，包括6D物体姿态估计和抓取姿态生成。现代物体的3D重建方法可以从多视角图像中生成视觉和几何上令人印象深刻的网格，但标准的几何评估并不能反映重建质量如何影响下游任务，如机器人操作性能。本文通过引入一个大规模的基于物理的基准测试来填补这一空白，该基准测试根据其在抓取中的功能有效性评估6D姿态估计器和3D网格模型。我们通过在各种重建的3D网格上生成抓取并执行它们，以地面真实模型为基准，模拟使用不完美的模型生成的抓取姿态如何影响与真实物体的交互。这评估了3D重建中的姿态误差、抓取鲁棒性和几何不准确性对抓取性能的综合影响。我们的结果显示，重建伪影显著减少了抓取姿态候选的数量，但在姿态准确估计的情况下，对抓取性能的影响微乎其微。我们的结果还表明，抓取成功与姿态误差之间的关系主要由空间误差主导，即使是简单的平移误差也能揭示对称物体抓取姿态的成功率。本文为如何通过机器人进行物体操作的感知系统提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper benchmarks the effects of 3D object reconstruction on robotic grasping success by introducing a large-scale, physics-based evaluation. It analyzes how reconstruction quality impacts 6D pose estimation and grasp generation, using generated grasps on various 3D meshes and simulating interactions with the ground-truth model. The study finds that while reconstruction artifacts reduce the number of grasp candidates, accurate pose estimation mitigates their impact on grasping performance. It also highlights that spatial errors are more influential on grasp success than pose errors, even for symmetric objects.</div>
<div class="mono" style="margin-top:8px">该研究通过引入大规模的物理基准，评估物体姿态估计和重建对机器人抓取成功率的影响。研究基于抓取功能评估6D姿态估计器和3D网格模型，生成各种重建3D网格上的抓取，并在真实模型上执行这些抓取。结果表明，重建中的错误减少了可抓取姿态候选的数量，但如果姿态估计准确，则对抓取性能影响很小。研究还发现，空间误差是抓取成功的主要因素，即使是对称物体的简单平移误差也能提供关于抓取姿态成功的信息。</div>
</details>
</div>
<div class="card">
<div class="title">RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation</div>
<div class="meta-line">Authors: Yixue Zhang, Kun Wu, Zhi Gao, Zhen Zhao, Pei Ren, Zhiyuan Xu, Fei Liao, Xinhua Wang, Shichao Fan, Di Wu, Qiuxuan Feng, Meng Li, Zhengping Che, Chang Liu, Jian Tang</div>
<div class="meta-line">First: 2026-02-18T13:29:43+00:00 · Latest: 2026-02-19T04:26:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16444v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robogene-boost-vla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboGene：通过多样性驱动的代理框架增强VLA预训练以促进现实世界任务生成</div>
<div class="mono" style="margin-top:8px">通用型机器人操作追求受到现实世界多样化交互数据稀缺性的阻碍。与视觉或语言中从网络收集数据不同，机器人数据收集是一个涉及高昂物理成本的主动过程。因此，自动化任务策展以最大化数据价值仍然是一个关键但尚未充分探索的挑战。现有手动方法不可扩展且偏向于常见任务，而现成的基础模型往往会产生物理上不可行的指令。为解决这一问题，我们引入了RoboGene，这是一种设计用于自动化生成单臂、双臂和移动机器人广泛物理上可行操作任务的代理框架。RoboGene 结合了三个核心组件：多样性驱动的采样以实现广泛的任务覆盖、自我反思机制以强制执行物理约束以及人工在环改进以持续提升。我们进行了广泛的定量分析和大规模现实世界实验，收集了18000条轨迹数据，并引入了新的评估任务质量、可行性和多样性的指标。结果表明，RoboGene 显著优于最先进的基础模型（如GPT-4o、Gemini 2.5 Pro）。此外，现实世界实验表明，使用RoboGene预训练的VLA模型在成功率和泛化能力上表现更优，突显了高质量任务生成的重要性。我们的项目可在https://robogene-boost-vla.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboGene is an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks for robotic manipulation. It integrates diversity-driven sampling, self-reflection mechanisms, and human-in-the-loop refinement. RoboGene significantly outperforms existing methods and pre-trained VLA models with RoboGene achieve higher success rates and better generalization in real-world experiments, demonstrating the importance of high-quality task generation.</div>
<div class="mono" style="margin-top:8px">RoboGene 是一个自动化框架，用于生成多样且物理上可行的机器人操作任务，结合了多样性驱动采样、自我反思机制和人工在环改进。实验结果表明，RoboGene 在性能上优于最先进的基础模型，使用 RoboGene 预训练的 VLA 模型在实际任务中具有更高的成功率和更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation</div>
<div class="meta-line">Authors: Yejin Kim, Wilbert Pumacay, Omar Rayyan, Max Argus, Winson Han, Eli VanderBilt, Jordi Salvador, Abhay Deshpande, Rose Hendrix, Snehal Jauhri, Shuo Liu, Nur Muhammad Mahi Shafiullah, Maya Guru, Ainaz Eftekhar, Karen Farley, Donovan Clay, Jiafei Duan, Arjun Guru, Piper Wolters, Alvaro Herrasti, Ying-Chun Lee, Georgia Chalvatzaki, Yuchen Cui, Ali Farhadi, Dieter Fox, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-11T20:16:31+00:00 · Latest: 2026-02-19T00:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11337v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11337v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolmoSpaces：大规模开放生态系统，用于机器人导航和操作</div>
<div class="mono" style="margin-top:8px">大规模部署机器人需要应对日常情况的长尾问题。现有机器人基准中，真实环境中的场景布局、物体几何形状和任务规范的无数变化被严重低估。衡量这种程度的泛化需要物理评估无法提供的规模和多样性。我们引入了MolmoSpaces，一个完全开放的生态系统，用于支持大规模的机器人策略基准测试。MolmoSpaces 包含超过23万个多样化的室内环境，从手工制作的家庭场景到程序生成的多房间房屋，拥有13万个丰富的注释物体资产，包括4.8万个可操作物体及其4200万个稳定抓取。这些环境对模拟器是通用的，支持MuJoCo、Isaac和ManiSkill等流行选项。该生态系统支持所有类型的实体任务：静态和移动操作、导航以及需要跨整个室内环境协调感知、规划和交互的多房间长期任务。我们还设计了MolmoSpaces-Bench，一个包含8个任务的基准套件，机器人与我们的多样化场景和丰富注释物体进行交互。我们的实验表明，MolmoSpaces-Bench 在模拟到现实的关联性很强（R = 0.96，ρ = 0.98），确认了较新的更强的零样本策略在我们的基准测试中优于早期版本，并且识别了提示措辞、初始关节位置和相机遮挡的关键敏感性。通过MolmoSpaces及其开源资产和工具，我们为机器人学习研究提供了可扩展的数据生成、策略训练和基准创建的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MolmoSpaces is designed to support large-scale benchmarking of robot policies by providing a diverse set of over 230k indoor environments and 130k annotated object assets. This ecosystem, simulator-agnostic and covering a wide range of tasks, includes static and mobile manipulation, navigation, and multiroom tasks. Experiments show strong sim-to-real correlation and confirm the performance improvement of newer policies. Key sensitivities identified include prompt phrasing, initial joint positions, and camera occlusion.</div>
<div class="mono" style="margin-top:8px">MolmoSpaces 提供了超过 23 万个室内环境和 13 万个注释物体资产的多样化生态系统，支持大规模的机器人策略基准测试。该系统包括从操作和导航到多房间长期任务的多种任务。MolmoSpaces-Bench 是一个包含 8 个任务的基准套件，展示了强大的模拟到现实的关联性，并确认了新策略在基准测试中的性能提升。关键的敏感性包括提示措辞、初始关节位置和相机遮挡。</div>
</details>
</div>
<div class="card">
<div class="title">Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement</div>
<div class="meta-line">Authors: Minku Kim, Kuan-Chia Chen, Aayam Shrestha, Li Fuxin, Stefan Lee, Alan Fern</div>
<div class="meta-line">First: 2026-02-14T19:11:02+00:00 · Latest: 2026-02-18T23:55:04+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13850v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人哈诺伊：探究基于技能的整体身体控制技能组合方法</div>
<div class="mono" style="margin-top:8px">我们研究了一种基于技能的框架，用于类人方块重组，该框架通过在任务级别按顺序使用可重用技能来实现长期执行。在我们的架构中，所有技能都通过一个共享的任务无关的整体身体控制器（WBC）执行，提供了一致的闭环接口用于技能组合，而不同于使用每个技能的单独低级控制器的非共享设计。我们发现，简单地重复使用相同的预训练WBC在长期执行中会降低鲁棒性，因为新技能及其组合会诱导状态和命令分布的变化。我们通过一个简单的数据聚合程序解决了这个问题，该程序通过在域随机化下闭环技能执行的回放来增强共享-WBC的训练。为了评估该方法，我们引入了“类人哈诺伊”长周期的塔式方块重组基准，并在模拟和Digit V3类人机器人上报告了结果，展示了完全自主的长期重组，并量化了共享-WBC方法相对于非共享基线的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores a skill-based framework for humanoid box rearrangement, enabling long-term execution by sequencing reusable skills at the task level. The framework uses a shared whole-body controller (WBC) for all skills, providing a consistent closed-loop interface for skill composition. The research finds that reusing the same pretrained WBC can reduce robustness over long horizons due to shifted state and command distributions. To address this, the study introduces a data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. The approach is evaluated using the Humanoid Hanoi benchmark, showing fully autonomous rearrangement over extended horizons and demonstrating the benefits of the shared-WBC approach over non-shared baselines.</div>
<div class="mono" style="margin-top:8px">该研究探索了一种基于技能的框架，用于使类人机器人在任务级别通过序列化可重用技能来执行长周期的箱子重新排列任务。该框架使用一个共享的整体身体控制器（WBC）来执行所有技能，提供了一致的闭环接口以进行技能组合。研究发现，重新使用相同的预训练WBC会在长周期内降低鲁棒性，因为新的技能及其组合会导致状态和命令分布的变化。为解决这一问题，研究引入了一种数据聚合程序，该程序通过在域随机化下进行闭环技能执行的回放来增强共享-WBC的训练。该方法通过Humanoid Hanoi基准进行评估，展示了在长时间范围内实现完全自主的重新排列，并证明了共享-WBC方法相对于非共享基线的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming</div>
<div class="meta-line">Authors: Philip Sosnin, Jodie Knapp, Fraser Kennedy, Josh Collyer, Calvin Tsay</div>
<div class="meta-line">First: 2026-02-18T23:18:45+00:00 · Latest: 2026-02-18T23:18:45+00:00</div>
<div class="meta-line">Comments: Accepted to the 23rd International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16944v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16944v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces a verification framework that provides both sound and complete guarantees for data poisoning attacks during neural network training. We formulate adversarial data manipulation, model training, and test-time evaluation in a single mixed-integer quadratic programming (MIQCP) problem. Finding the global optimum of the proposed formulation provably yields worst-case poisoning attacks, while simultaneously bounding the effectiveness of all possible attacks on the given training pipeline. Our framework encodes both the gradient-based training dynamics and model evaluation at test time, enabling the first exact certification of training-time robustness. Experimental evaluation on small models confirms that our approach delivers a complete characterization of robustness against data poisoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用混合整数规划进行数据投毒攻击的确切认证</div>
<div class="mono" style="margin-top:8px">本研究引入了一种验证框架，为神经网络训练期间的数据投毒攻击提供了既准确又完整的保证。我们将对抗性数据操纵、模型训练和测试时评估统一在一个混合整数二次规划（MIQCP）问题中。找到所提出模型的全局最优解可以证明产生最坏情况的数据投毒攻击，同时同时限制所有可能对给定训练管道的攻击效果。我们的框架编码了基于梯度的训练动力学和测试时的模型评估，使我们能够首次对训练时的鲁棒性进行精确认证。实验评估表明，我们的方法可以完全描述对数据投毒的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work presents a verification framework using mixed-integer quadratic programming to provide exact certification of data-poisoning attacks during neural network training. It formulates adversarial data manipulation, model training, and test-time evaluation in a single optimization problem, ensuring both sound and complete guarantees. The framework enables the first exact certification of training-time robustness against data poisoning, as confirmed by experimental evaluation on small models.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种验证框架，为神经网络训练期间的数据投毒攻击提供了完整的保证。通过将问题形式化为混合整数二次规划（MIQCP）问题，该框架可以找到最坏情况的投毒攻击，并限制所有可能攻击的有效性。该方法编码了基于梯度的训练动态和测试时的模型评估，提供了首个训练时鲁棒性的精确认证。实验表明，该方法完全表征了小模型对数据投毒的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reinforcement Learning-Based Locomotion for Resource-Constrained Quadrupeds with Exteroceptive Sensing</div>
<div class="meta-line">Authors: Davide Plozza, Patricia Apostol, Paul Joseph, Simon Schläpfer, Michele Magno</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2025-05-18T20:29:23+00:00 · Latest: 2026-02-18T22:43:05+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at the IEEE International Conference on Robotics and Automation (ICRA), Atlanta 2025. The code is available at github.com/ETH-PBL/elmap-rl-controller</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12537v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12537v2">PDF</a> · <a href="http://github.com/ETH-PBL/elmap-rl-controller">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compact quadrupedal robots are proving increasingly suitable for deployment in real-world scenarios. Their smaller size fosters easy integration into human environments. Nevertheless, real-time locomotion on uneven terrains remains challenging, particularly due to the high computational demands of terrain perception. This paper presents a robust reinforcement learning-based exteroceptive locomotion controller for resource-constrained small-scale quadrupeds in challenging terrains, which exploits real-time elevation mapping, supported by a careful depth sensor selection. We concurrently train both a policy and a state estimator, which together provide an odometry source for elevation mapping, optionally fused with visual-inertial odometry (VIO). We demonstrate the importance of positioning an additional time-of-flight sensor for maintaining robustness even without VIO, thus having the potential to free up computational resources. We experimentally demonstrate that the proposed controller can flawlessly traverse steps up to 17.5 cm in height and achieve an 80% success rate on 22.5 cm steps, both with and without VIO. The proposed controller also achieves accurate forward and yaw velocity tracking of up to 1.0 m/s and 1.5 rad/s respectively. We open-source our training code at github.com/ETH-PBL/elmap-rl-controller.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于鲁棒强化学习的资源受限四足机器人外部感知运动控制</div>
<div class="mono" style="margin-top:8px">紧凑型四足机器人在实际应用场景中越来越适合部署。它们较小的尺寸促进了与人类环境的轻松集成。然而，在不平坦地形上的实时运动仍然具有挑战性，特别是由于地形感知的高计算需求。本文提出了一种适用于资源受限的小型四足机器人在挑战性地形上的鲁棒强化学习外部感知运动控制器，该控制器利用实时高程映射，并通过仔细选择深度传感器加以支持。我们同时训练了一个策略和一个状态估计器，它们一起为高程映射提供里程计源，可选地与视觉惯性里程计（VIO）融合。我们证明了额外放置一个飞行时间传感器对于保持鲁棒性的重要性，即使没有VIO，也能释放计算资源。实验表明，所提出的控制器可以完美地跨越高达17.5厘米的台阶，并在有和没有VIO的情况下，22.5厘米台阶的成功率达到80%。所提出的控制器还实现了高达1.0米/秒的前向和1.5弧度/秒的偏航速度跟踪。我们在github.com/ETH-PBL/elmap-rl-controller开源了训练代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of real-time locomotion for small quadruped robots on uneven terrains by developing a robust reinforcement learning-based exteroceptive locomotion controller. The method involves concurrent training of a policy and a state estimator, which provides odometry for elevation mapping, optionally fused with visual-inertial odometry. The controller demonstrates successful traversal of steps up to 17.5 cm in height and a 80% success rate on 22.5 cm steps, both with and without visual-inertial odometry. It also achieves accurate forward and yaw velocity tracking up to 1.0 m/s and 1.5 rad/s respectively. An additional time-of-flight sensor is shown to enhance robustness without VIO, freeing up computational resources.</div>
<div class="mono" style="margin-top:8px">本文通过开发基于强化学习的外部感知运动控制器，解决了小型四足机器人在不平地形上的实时运动问题。该方法同时训练策略和状态估计器，提供用于地形映射的里程计，可选地与视觉惯性里程计融合。控制器在有和无视觉惯性里程计的情况下，分别成功跨越了17.5 cm和22.5 cm高的台阶，成功率分别为100%和80%。它还实现了最高1.0 m/s的前向速度和1.5 rad/s的偏航速度跟踪。额外的飞行时间传感器被证明在没有视觉惯性里程计的情况下增强了鲁棒性，从而释放了计算资源。</div>
</details>
</div>
<div class="card">
<div class="title">SparTa: Sparse Graphical Task Models from a Handful of Demonstrations</div>
<div class="meta-line">Authors: Adrian Röfer, Nick Heppert, Abhinav Valada</div>
<div class="meta-line">First: 2026-02-18T21:54:35+00:00 · Latest: 2026-02-18T21:54:35+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16911v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16911v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method&#x27;s demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SparTa: 稀疏图形任务模型从少量演示中学习</div>
<div class="mono" style="margin-top:8px">高效学习长时程操作任务是机器人演示学习中的核心挑战。与最近专注于直接在动作域中学习任务的努力不同，我们关注的是推断机器人在任务中应实现什么，而不是如何实现。为此，我们使用一系列图形对象关系来表示不断变化的场景状态。我们提出了一种演示分割和聚合方法，提取一系列操作图，并估计任务各阶段的对象状态分布。与仅捕捉部分交互或短暂时间窗口的先前基于图的方法不同，我们的方法捕捉从控制开始到操作结束的完整对象交互。为了在学习多个演示时提高鲁棒性，我们还使用预训练的视觉特征进行对象匹配。在广泛的实验中，我们评估了我们方法的演示分割准确性以及从多个演示中学习找到所需最小任务模型的实用性。最后，我们在仿真和真实机器人上部署了拟合模型，证明了由此产生的任务表示支持跨环境的可靠执行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to learn long-horizon manipulation tasks efficiently from a few demonstrations by focusing on inferring the robot&#x27;s goals rather than the specific actions. The method involves representing scene states with graphical object relationships and using a demonstration segmentation and pooling approach to extract manipulation graphs and estimate object state distributions. Experiments show that the approach captures complete object interactions and improves robustness when learning from multiple demonstrations, leading to reliable task execution in both simulation and real-world settings.</div>
<div class="mono" style="margin-top:8px">研究旨在通过聚焦于推断机器人的目标而非具体动作，从少量演示中高效学习长时操作任务。方法包括用图形对象关系表示场景状态，并使用演示分割和聚合方法提取操作图并估计物体状态分布。实验表明，该方法能够捕捉完整的物体交互，并在学习多个演示时提高鲁棒性，从而在仿真和真实世界中实现可靠的执行任务。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-18T20:42:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向对象的零样本灵巧工具操作策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可以执行的任务集。然而，工具操作代表了一类具有挑战性的灵巧性，需要抓取细长物体、手持物体旋转以及进行有力的交互。由于收集这些行为的遥操作数据具有挑战性，因此模拟到现实的强化学习（RL）是一种有前景的替代方案。然而，先前的方法通常需要大量的工程努力来建模物体并调整奖励函数以适应每个任务。在本文中，我们提出了SimToolReal，朝着为工具操作生成通用的模拟到现实的RL策略迈出了一步。我们不是专注于单一物体和任务，而是通过模拟程序生成大量工具样物体素，并训练一个具有通用目标的单一RL策略，即操纵每个物体到达随机目标姿态。这种方法使SimToolReal能够在测试时进行通用灵巧工具操作，而无需任何物体或任务特定的训练。我们证明SimToolReal在120个现实世界操作中优于先前的重新瞄准和固定抓取方法37%，并且在24个任务、12个物体实例和6个工具类别上达到了与特定目标物体和任务训练的专家RL策略相当的性能。最后，我们展示了SimToolReal在各种日常工具上具有良好的零样本泛化能力，在120个现实世界操作中实现了超过24个任务、12个物体实例和6个工具类别的强大零样本性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SimToolReal, a policy designed for zero-shot dexterous tool manipulation. It addresses the challenge of sim-to-real reinforcement learning by generating a variety of tool-like object primitives in simulation and training a single policy to manipulate these objects to random goal poses. The results show that SimToolReal outperforms previous methods by 37% and achieves strong zero-shot performance across 120 real-world rollouts involving 24 tasks, 12 object instances, and 6 tool categories.</div>
<div class="mono" style="margin-top:8px">论文介绍了SimToolReal，这是一种用于零样本灵巧工具操作的策略。该方法通过在模拟中生成多种工具样物体素，并训练一个单一策略将这些物体移动到随机目标位置，来解决模拟到现实的强化学习挑战。该方法在120个涉及24个任务、12个物体实例和6种工具类别的真实世界演示中表现出色，优于先前方法37%。</div>
</details>
</div>
<div class="card">
<div class="title">One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation</div>
<div class="meta-line">Authors: Zhenyu Wei, Yunchao Yao, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-18T18:59:57+00:00 · Latest: 2026-02-18T18:59:57+00:00</div>
<div class="meta-line">Comments: Project Page: https://zhenyuwei2003.github.io/OHRA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16712v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhenyuwei2003.github.io/OHRA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一手统治一切：统一灵巧操作的规范表示</div>
<div class="mono" style="margin-top:8px">当前的灵巧操作策略大多假定固定的手部设计，严重限制了它们对具有不同运动学和结构布局的新实体的泛化能力。为克服这一限制，我们引入了一个参数化的规范表示，统一了广泛的灵巧手架构。它包括一个统一的参数空间和一个规范的URDF格式，提供三个主要优势。1）参数空间捕捉了有效学习算法训练所需的关键形态和运动学变化。2）可以在我们的空间中学习一个结构化的潜在流形，其中不同实体之间的插值会产生平滑且物理上合理的形态过渡。3）规范的URDF标准化了动作空间，同时保留了原始URDF的动力学和功能特性，使跨实体策略学习更加高效和可靠。我们通过广泛的分析和实验验证了这些优势，包括抓取策略回放、VAE潜在编码和跨实体零样本转移。具体来说，我们在统一表示上训练了一个VAE，以获得一个紧凑且语义丰富的潜在嵌入，并开发了一个基于规范表示的抓取策略，该策略在灵巧手之间具有泛化能力。我们通过模拟和在未见过的形态上的实际任务（例如，3指LEAP手的零样本成功率高达81.9%）展示了我们的框架如何统一了结构多样手的表示空间和动作空间，为跨手学习提供了可扩展的基础，以实现通用灵巧操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the limitation of fixed hand designs in dexterous manipulation policies by introducing a parameterized canonical representation. This representation unifies various hand architectures through a unified parameter space and a canonical URDF format, which enhances the generalization of manipulation policies across different hand designs. Key experimental findings include the successful training of a VAE on the unified representation to achieve a compact latent embedding, and the development of a grasping policy that generalizes across dexterous hands, with a 3-finger LEAP Hand demonstrating an 81.9% zero-shot success rate in real-world tasks.</div>
<div class="mono" style="margin-top:8px">该研究通过引入参数化的统一表示方法来解决当前灵巧操作策略中固定手部设计的局限性，该方法能够统一各种手部架构。该方法包括统一的参数空间和标准化的URDF格式，从而提高了学习效率并允许不同手部形态之间的平滑过渡。关键实验结果表明，所提出的方法在3指LEAP手上的零样本成功率达到了81.9%，并通过仿真和实际任务展示了有效的跨手部姿态策略学习能力。</div>
</details>
</div>
<div class="card">
<div class="title">EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data</div>
<div class="meta-line">Authors: Ruijie Zheng, Dantong Niu, Yuqi Xie, Jing Wang, Mengda Xu, Yunfan Jiang, Fernando Castañeda, Fengyuan Hu, You Liang Tan, Letian Fu, Trevor Darrell, Furong Huang, Yuke Zhu, Danfei Xu, Linxi Fan</div>
<div class="meta-line">First: 2026-02-18T18:59:05+00:00 · Latest: 2026-02-18T18:59:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoScale：通过多样化第一人称人类数据扩展灵巧操作</div>
<div class="mono" style="margin-top:8px">人类行为是学习物理智能最具扩展性的数据来源之一，但如何有效利用它来实现灵巧操作仍不清楚。尽管先前的工作在受限环境中展示了从人类到机器人的转移，但大规模人类数据是否能支持精细的、高自由度的灵巧操作尚不清楚。我们提出了EgoScale，这是一种基于大规模第一人称人类数据的从人类到灵巧操作的转移框架。我们在一个超过20,854小时的动作标注第一人称人类视频上训练了一个视觉语言动作（VLA）模型，数据量超过先前努力的20倍，并发现人类数据规模与验证损失之间存在对数线性关系。这种验证损失与下游真实机器人性能高度相关，确立了大规模人类数据作为可预测的监督来源。除了规模，我们引入了一个简单的两阶段转移配方：大规模人类预训练后，进行轻量级的人机对齐中期训练。这使得在最少的机器人监督下实现强大的长时灵巧操作和一次性的任务适应成为可能。我们的最终策略在使用22个自由度的灵巧机器人手中将平均成功率提高了54%，并且能够有效地转移到具有较少自由度的手上，表明大规模人类运动提供了可重复使用、与具体身体无关的运动先验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EgoScale is a framework for transferring human dexterity to robots using large-scale egocentric human data. It trains a Vision Language Action model on over 20,854 hours of human action data, showing a log-linear relationship between data scale and validation loss, which correlates with robot performance. The method involves two stages: large-scale human pretraining and lightweight robot alignment, leading to improved success rates and effective transfer to robots with different degrees of freedom.</div>
<div class="mono" style="margin-top:8px">EgoScale 是一个框架，通过大规模第一人称人类数据将人类灵巧性转移到机器人上。它在一个超过20,854小时的人类动作数据集上训练Vision Language Action模型，展示了数据规模与验证损失之间的对数线性关系，该关系与机器人性能相关。该方法包括两个阶段：大规模的人类预训练和轻量级的机器人对齐，从而提高了成功率并有效转移到具有不同自由度的手部的机器人上。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</div>
<div class="meta-line">Authors: Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</div>
<div class="meta-line">First: 2026-02-18T18:55:02+00:00 · Latest: 2026-02-18T18:55:02+00:00</div>
<div class="meta-line">Comments: Project page: https://hero-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16705v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hero-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人机器人开放词汇视觉移动物体末端执行器控制学习</div>
<div class="mono" style="margin-top:8px">使用类人机器人在野外对任意物体进行视觉移动物体操作需要精确的末端执行器（EE）控制和通过视觉输入（例如RGB-D图像）对场景的广泛理解。现有方法基于现实世界的模仿学习，由于难以收集大规模训练数据集，因此表现出有限的泛化能力。本文提出了一种新的范式HERO，用于类人机器人物体移动物体操作，结合了大型视觉模型的强大泛化能力和开放词汇理解与模拟训练中的强大控制性能。我们通过设计一种准确的残差感知末端执行器跟踪策略来实现这一点。该末端执行器跟踪策略结合了经典机器人学和机器学习。它使用a) 逆运动学将残差末端执行器目标转换为参考轨迹，b) 用于准确前运动学的已学习神经前向模型，c) 目标调整，以及d) 重新规划。这些创新共同帮助我们将末端执行器跟踪误差减少了3.2倍。我们使用这种准确的末端执行器跟踪器构建了一个模块化移动物体系统，其中使用开放词汇大型视觉模型实现强大的视觉泛化。我们的系统能够在从办公室到咖啡馆等多样化的现实环境中操作，机器人能够可靠地操作各种日常物体（例如茶杯、苹果、玩具），这些物体位于43cm至92cm高度的表面上。在模拟和现实世界中的系统模块化和端到端测试表明我们提出的系统设计的有效性。我们认为本文中的进展可以为训练类人机器人与日常物体交互开辟新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces HERO, a new paradigm for humanoid robots to perform object manipulation in diverse environments. The system combines strong generalization from large vision models with accurate end-effector control through a residual-aware tracking policy. Key innovations include inverse kinematics, a learned neural forward model, goal adjustment, and replanning, which reduce end-effector tracking error by 3.2 times. The system successfully manipulates various objects in real-world settings like offices and coffee shops, demonstrating its effectiveness in open-vocabulary visual loco-manipulation.</div>
<div class="mono" style="margin-top:8px">本文介绍了HERO，一种新的类人机器人物体操作范式，结合了大型视觉模型的强大泛化能力和通过残差感知末端执行器跟踪策略实现的精确控制。该策略将经典机器人学与机器学习相结合，使用逆运动学、学习的前向模型、目标调整和重新规划。这种方法将末端执行器跟踪误差显著降低了3.2倍。该系统利用开放词汇量的大型视觉模型进行视觉理解，成功地在各种现实环境（如办公室、咖啡馆）中操作各种物体，证明了其在仿真和现实世界中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to unfold cloth: Scaling up world models to deformable object manipulation</div>
<div class="meta-line">Authors: Jack Rome, Stephen James, Subramanian Ramamoorthy</div>
<div class="meta-line">First: 2026-02-18T18:14:41+00:00 · Latest: 2026-02-18T18:14:41+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习展开布料：将世界模型扩展到可变形物体操作</div>
<div class="mono" style="margin-top:8px">学习操作布料既是机器人研究中的一个典型问题，也是从辅助护理到服务业等多个应用领域的即时相关问题。可变形物体的复杂物理特性使得布料操作问题变得非平凡。为了创建一个能够应对各种形状、大小、折叠和皱纹模式的通用操作策略，除了通常的外观变化问题，仔细考虑模型结构及其对泛化性能的影响变得至关重要。在本文中，我们提出了一种使用最近提出的强化学习架构DreamerV2变体的方法，用于空中布料操作。我们的实现修改了该架构，使其能够利用表面法线输入，并修改了回放缓冲区和数据增强程序。这些修改共同增强了机器人使用的世界模型，解决了机器人操作的物体的物理复杂性。我们在模拟中进行了评估，并在物理机器人设置中进行了零样本部署，展示了对不同布料类型的空中展开，证明了我们提出架构的泛化优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of robotic cloth manipulation, which is crucial for various applications. It introduces an approach using DreamerV2, a reinforcement learning architecture, modified to include surface normals and enhanced replay buffer and data augmentation. The method improves the robot&#x27;s ability to handle the complex physics of deformable objects, leading to successful in-air unfolding of different cloth types in both simulation and physical deployment.</div>
<div class="mono" style="margin-top:8px">论文探讨了机器人布料操作的挑战，这对于各种应用至关重要。它提出了一种使用DreamerV2强化学习架构的方法，该架构经过修改以包含表面法线，并增强了回放缓冲区和数据增强程序。该方法提高了机器人处理可变形物体复杂物理特性的能力，在模拟和物理部署中成功地实现了不同布料类型的空中展开，展示了所提架构的泛化优势。</div>
</details>
</div>
<div class="card">
<div class="title">Elements of Robot Morphology: Supporting Designers in Robot Form Exploration</div>
<div class="meta-line">Authors: Amy Koike, Serena Ge Guo, Xinning He, Callie Y. Kim, Dakota Sullivan, Bilge Mutlu</div>
<div class="meta-line">First: 2026-02-09T21:13:20+00:00 · Latest: 2026-02-18T17:41:00+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, Proceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09203v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09203v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人形态学要素：支持设计师进行机器人形态探索</div>
<div class="mono" style="margin-top:8px">机器人形态，即机器人的形状、结构，是人机交互（HRI）中的关键设计空间，影响着机器人的功能、表达方式及其与人的互动。尽管其重要性不言而喻，但关于设计框架如何指导系统性形态探索的研究却很少。为填补这一空白，我们提出了机器人形态学要素框架，该框架识别出五个基本要素：感知、关节、末端执行器、运动和结构。该框架源自对现有机器人的分析，支持对多样化机器人形态的结构化探索。为了实现该框架，我们开发了形态探索模块（MEB），这是一种可触控的模块，能够促进对机器人形态的手动、协作性实验。我们通过案例研究和设计研讨会对该框架和工具包进行了评估，展示了它们如何支持分析、创意生成、反思以及协作机器人设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Elements of Robot Morphology, a framework that identifies five fundamental elements for robot design: perception, articulation, end effectors, locomotion, and structure. This framework supports structured exploration of diverse robot forms. The authors operationalized the framework through Morphology Exploration Blocks (MEB), tangible blocks that facilitate hands-on, collaborative experimentation. The evaluation through case studies and design workshops demonstrated that the framework and toolkit aid in analysis, ideation, reflection, and collaborative robot design.</div>
<div class="mono" style="margin-top:8px">论文提出了机器人形态学框架，识别了感知、运动、末端执行器、移动和结构五个基本要素。该框架支持对多样化机器人形态的结构化探索。作者通过实体积木Morphology Exploration Blocks (MEB) 实现了这一框架，这些积木使人们能够进行动手、协作的机器人形态实验。通过案例研究和设计研讨会的评估表明，该框架和工具包有助于分析、创意生成、反思和协作机器人设计。</div>
</details>
</div>
<div class="card">
<div class="title">FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency</div>
<div class="meta-line">Authors: Yifei Su, Ning Liu, Dong Chen, Zhen Zhao, Kun Wu, Meng Li, Zhiyuan Xu, Zhengping Che, Jian Tang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T14:12:53+00:00 · Latest: 2026-02-18T13:54:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08822v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08822v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approaches accelerate sampling in generative modeling-based visuomotor policies by adapting techniques originally developed to speed up image generation. However, a major distinction exists: image generation typically produces independent samples without temporal dependencies, while robotic manipulation requires generating action trajectories with continuity and temporal coherence. To this end, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. Concretely, we introduce a frequency consistency constraint objective that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on 40 tasks of LIBERO. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreqPolicy: 通过频率一致性高效流基视动策略</div>
<div class="mono" style="margin-top:8px">基于生成建模的视动策略在机器人操作中得到了广泛应用，这得益于它们能够建模多模态动作分布的能力。然而，多步采样的高推理成本限制了其在实时机器人系统中的应用。现有方法通过适应原本用于加速图像生成的技术来加速基于生成建模的视动策略的采样。然而，一个主要区别在于：图像生成通常产生独立样本且没有时间依赖性，而机器人操作需要生成具有连续性和时间一致性的动作轨迹。为此，我们提出了一种名为FreqPolicy的新方法，首先在流基视动策略上施加频率一致性约束。我们的工作使动作模型能够有效地捕捉时间结构，同时支持高效的一步动作生成。具体而言，我们引入了一个频率一致性约束目标，该目标在流的不同时间步长上强制频率域动作特征的对齐，从而促进一步动作生成向目标分布收敛。此外，我们设计了一种自适应一致性损失来捕捉机器人操作任务中固有的结构时间变化。我们在3个仿真基准上的53个任务上评估了FreqPolicy，证明了其在现有一步动作生成器中的优越性。我们进一步将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在LIBERO的40个任务上实现了加速且未降低性能。此外，我们在真实世界的机器人场景中展示了其高效性和有效性，推理频率为93.5 Hz。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FreqPolicy proposes a novel approach to improve the efficiency of flow-based visuomotor policies by imposing frequency consistency constraints, enabling effective temporal structure capture and high-quality one-step action generation. The method introduces a frequency consistency constraint objective and an adaptive consistency loss to promote temporal coherence in robotic manipulation tasks. Experiments on 53 simulation tasks and 40 LIBERO tasks demonstrate FreqPolicy&#x27;s superiority over existing one-step action generators and its efficiency in real-world scenarios with an inference frequency of 93.5 Hz.</div>
<div class="mono" style="margin-top:8px">FreqPolicy通过在流式模型中引入频率一致性约束来解决生成模型在视觉-运动策略中的高推理成本问题。该方法能够实现高效且高质量的一步动作生成，同时支持时间连贯性。在三个仿真基准上的53个任务中，实验表明FreqPolicy优于现有方法，并且在LIBERO模型中实现了加速且不降低性能。此外，FreqPolicy在真实世界机器人场景中的推理频率达到93.5 Hz，显示出高效性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Reactive Motion Generation With Particle-Based Perception in Dynamic Environments</div>
<div class="meta-line">Authors: Xiyuan Zhao, Huijun Li, Lifeng Zhu, Zhikai Wei, Xianyi Zhu, Aiguo Song</div>
<div class="meta-line">First: 2026-02-18T13:48:54+00:00 · Latest: 2026-02-18T13:48:54+00:00</div>
<div class="meta-line">Comments: This paper has 20 pages, 15 figures, and 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16462v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reactive motion generation in dynamic and unstructured scenarios is typically subject to essentially static perception and system dynamics. Reliably modeling dynamic obstacles and optimizing collision-free trajectories under perceptive and control uncertainty are challenging. This article focuses on revealing tight connection between reactive planning and dynamic mapping for manipulators from a model-based perspective. To enable efficient particle-based perception with expressively dynamic property, we present a tensorized particle weight update scheme that explicitly maintains obstacle velocities and covariance meanwhile. Building upon this dynamic representation, we propose an obstacle-aware MPPI-based planning formulation that jointly propagates robot-obstacle dynamics, allowing future system motion to be predicted and evaluated under uncertainty. The model predictive method is shown to significantly improve safety and reactivity with dynamic surroundings. By applying our complete framework in simulated and noisy real-world environments, we demonstrate that explicit modeling of robot-obstacle dynamics consistently enhances performance over state-of-the-art MPPI-based perception-planning baselines avoiding multiple static and dynamic obstacles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于粒子感知的动力学环境下的反应式运动生成</div>
<div class="mono" style="margin-top:8px">在动态和非结构化的场景中，反应式运动生成通常依赖于基本静态的感知和系统动力学。准确建模动态障碍物并在感知和控制不确定性下优化无碰撞轨迹是具有挑战性的。本文从模型的角度重点揭示了反应式规划与动态制图之间的紧密联系。为了实现高效的粒子感知并具有动态特性，我们提出了一种张量化的粒子权重更新方案，该方案明确地维护了障碍物的速度和协方差。基于这种动态表示，我们提出了一种障碍物感知的MPPI基规划公式，该公式联合传播了机器人-障碍物动力学，使得未来系统的运动可以在不确定性下被预测和评估。模型预测方法被证明可以显著提高在动态环境中的安全性和反应性。通过在模拟和嘈杂的真实环境中应用我们完整的框架，我们证明了对机器人-障碍物动力学的显式建模在避免多个静态和动态障碍物方面始终优于最先进的MPPI基感知-规划基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reactive motion generation in dynamic environments by integrating dynamic perception and planning. It introduces a tensorized particle weight update scheme to maintain obstacle velocities and covariance, and proposes an obstacle-aware Model Predictive Path Integral (MPPI) planning method that jointly propagates robot-obstacle dynamics. The results show significant improvements in safety and reactivity compared to state-of-the-art MPPI-based methods in both simulated and real-world environments.</div>
<div class="mono" style="margin-top:8px">本文针对动态环境下的反应性运动生成问题，通过结合动态感知和规划来解决。提出了一个张量化的粒子权重更新方案，以维持障碍物的速度和协方差，并提出了一种障碍物感知的MPPI规划方法，该方法联合传播了机器人和障碍物的动力学。结果显示，这种方法在模拟和具有动态障碍物的现实环境中，相较于最先进的方法，显著提高了安全性和反应性。</div>
</details>
</div>
<div class="card">
<div class="title">VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</div>
<div class="meta-line">Authors: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi</div>
<div class="meta-line">First: 2026-02-12T17:46:52+00:00 · Latest: 2026-02-18T11:55:37+00:00</div>
<div class="meta-line">Comments: VIRENA is under active development and currently in use at the University of Zurich. This preprint will be updated as new features are released. For the latest version and to inquire about demos or pilot collaborations, contact the authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12207v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12207v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA&#x27;s no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRENA：虚拟竞技场，用于研究、教育和民主创新</div>
<div class="mono" style="margin-top:8px">数字平台塑造了人们的交流、讨论和形成观点的方式。由于数据访问受限、对现实世界实验的伦理限制以及现有研究工具的局限性，研究这些动态变得越来越困难。VIRENA（虚拟竞技场）是一个平台，它能够在现实社交媒体环境中进行受控实验。多个参与者可以同时在基于信息流的平台（Instagram、Facebook、Reddit）和即时通讯应用（WhatsApp、Messenger）的现实复制品中互动。由大型语言模型驱动的AI代理可以与人类一起参与，具有可配置的人设和现实行为。研究人员可以通过无需编程技能的可视化界面操控内容审核方法、预排定刺激内容，并在不同条件下运行实验。VIRENA 使得以前不切实际的研究设计成为可能：研究人类与AI的互动、实验性地比较干预措施的效果以及观察群体讨论的展开。VIRENA 建立在开源技术之上，确保数据保留在机构控制之下并符合数据保护要求，目前在苏黎世大学使用中，并可供试点合作。VIRENA 的无代码界面使其跨学科和跨领域的受控社交媒体模拟变得可行。本文记录了其设计、架构和功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VIRENA is a platform designed to enable controlled experimentation in realistic social media environments, addressing challenges such as restricted data access and ethical constraints. It allows multiple participants to interact in replicas of popular platforms like Instagram, Facebook, Reddit, WhatsApp, and Messenger, with AI agents participating alongside humans. Researchers can manipulate content moderation and run experiments through a no-code visual interface. Key findings include the ability to study human-AI interaction, compare moderation interventions, and observe group deliberation in realistic contexts, making previously impractical research designs possible.</div>
<div class="mono" style="margin-top:8px">VIRENA 是一个平台，旨在实现在现实社交媒体环境中的受控实验，解决数据访问受限和伦理约束的问题。它允许参与者在复制的 Instagram、Facebook、Reddit、WhatsApp 和 Messenger 等平台上互动，同时 AI 代理可以与人类一起参与。研究人员可以通过无代码的可视化界面操纵内容审核并运行实验。主要发现包括能够研究人类与 AI 的互动、比较不同的审核干预措施以及在现实环境中观察群体讨论的过程，这些都是现有工具难以实现的。</div>
</details>
</div>
<div class="card">
<div class="title">AMBER: A tether-deployable gripping crawler with compliant microspines for canopy manipulation</div>
<div class="meta-line">Authors: P. A. Wigner, L. Romanello, A. Hammad, P. H. Nguyen, T. Lan, S. F. Armanini, B. B. Kocer, M. Kovac</div>
<div class="meta-line">First: 2025-12-08T16:17:56+00:00 · Latest: 2026-02-18T11:42:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07680v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.07680v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90$^\circ$ body roll and inclination, while effective climbing on branches inclined up to 67.5$^\circ$, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10$^\circ$, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. The crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing. The aerial deployment is demonstrated at a conceptual and feasibility level, while full drone-crawler integration is left as future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMBER：一种用于树冠操作的可伸展攀爬器，配备有顺应性微钩爪</div>
<div class="mono" style="margin-top:8px">本文介绍了一种可空中部署的攀爬器，用于树冠内的适应性移动和操作。该系统结合了顺应性微钩爪履带、双履带旋转夹持器和弹性尾巴，使其能够在不同曲率和倾斜角度的树枝上安全附着并稳定行进。实验表明，该攀爬器在90°身体滚转和倾斜角度下仍能可靠地抓握，有效攀爬倾斜角度达67.5°的树枝，最大速度为每秒0.55个身体长度。顺应性履带允许最大10°的偏航转向，提高在不规则表面的机动性。功率测量显示，该攀爬器的无量纲运输成本比典型悬停功率消耗低一个数量级，提供了一个坚固、低功耗的环境采样和树冠内传感平台。空中部署在概念和可行性层面进行了演示，而完整的无人机-攀爬器集成留作未来工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AMBER, a tether-deployable crawler designed for tree canopy manipulation. It features compliant microspine tracks, a dual-track rotary gripper, and an elastic tail, allowing secure attachment and stable traversal on branches of varying curvature and inclination. Experiments show reliable gripping up to 90° body roll and inclination, effective climbing on branches inclined up to 67.5°, and a maximum speed of 0.55 body lengths per second on horizontal branches. The crawler demonstrates efficient operation with a lower cost of transport compared to typical hovering power consumption in aerial robots.</div>
<div class="mono" style="margin-top:8px">论文介绍了AMBER，一种可从空中部署的爬行器，用于树冠操作。它配备了顺应性微钩履带、双轨旋转夹持器和弹性尾巴，能够在不同曲率和倾斜角度的树枝上实现安全附着和稳定移动。实验显示，该爬行器能够在90°身体滚转和倾斜角度下可靠抓握，有效攀爬倾斜角度达67.5°的树枝，最大速度为每秒0.55个身体长度。该爬行器展示了与典型空中机器人悬停功率消耗相比更低的单位运输成本，表现出高效的运行性能。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped</div>
<div class="meta-line">Authors: Saumya Karan, Neerav Maram, Suraj Borate, Madhu Vadali</div>
<div class="meta-line">First: 2026-02-18T11:14:22+00:00 · Latest: 2026-02-18T11:14:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16371v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16371v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">SLOT (Soft Legged Omnidirectional Tetrapod), a tendon-driven soft quadruped robot with 3D-printed TPU legs, is presented to study physics-informed modeling and control of compliant legged locomotion using only four actuators. Each leg is modeled as a deformable continuum using discrete Cosserat rod theory, enabling the capture of large bending deformations, distributed elasticity, tendon actuation, and ground contact interactions. A modular whole-body modeling framework is introduced, in which compliant leg dynamics are represented through physically consistent reaction forces applied to a rigid torso, providing a scalable interface between continuum soft limbs and rigid-body locomotion dynamics. This formulation allows efficient whole-body simulation and real-time control without sacrificing physical fidelity. The proposed model is embedded into a convex model predictive control framework that optimizes ground reaction forces over a 0.495 s prediction horizon and maps them to tendon actuation through a physics-informed force-angle relationship. The resulting controller achieves asymptotic stability under diverse perturbations. The framework is experimentally validated on a physical prototype during crawling and walking gaits, achieving high accuracy with less than 5 mm RMSE in center of mass trajectories. These results demonstrate a generalizable approach for integrating continuum soft legs into model-based locomotion control, advancing scalable and reusable modeling and control methods for soft quadruped robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>腱驱动软四足动物的动态建模与MPC行走研究</div>
<div class="mono" style="margin-top:8px">SLOT（软腿全方位四足动物），一种使用3D打印TPU腿的腱驱动软四足动物机器人，用于研究仅使用四个执行器的顺应性腿足运动的物理启发式建模和控制。每条腿被建模为可变形连续体，使用离散柯西尔杆理论，能够捕捉到大弯曲变形、分布弹性、腱驱动和地面接触相互作用。引入了一种模块化的全身建模框架，在该框架中，通过在刚性躯干上施加物理一致的反作用力来表示顺应腿的动力学，提供了一种连续软肢和刚体运动动力学之间的可扩展接口。该公式允许高效的整体身体仿真和实时控制，而不牺牲物理精度。所提出的模型嵌入到凸模型预测控制框架中，该框架在0.495秒的预测窗口内优化地面反作用力，并通过物理启发的力-角关系将其映射到腱驱动。所得到的控制器在多种扰动下实现了渐近稳定性。该框架在爬行和行走步态的物理原型上进行了实验验证，实现了高精度，中心质量轨迹的RMSE小于5毫米。这些结果展示了将连续软腿整合到基于模型的运动控制中的通用方法，推进了软四足动物机器人可扩展和可重用建模与控制方法的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study presents SLOT, a tendon-driven soft quadruped robot, to investigate the physics-informed modeling and control of compliant legged locomotion. The robot uses discrete Cosserat rod theory to model each leg as a deformable continuum, capturing large bending deformations and tendon actuation. A modular whole-body modeling framework is introduced, allowing efficient simulation and real-time control while maintaining physical fidelity. The proposed model is embedded in a convex model predictive control framework, achieving asymptotic stability under various perturbations. Experimental validation on a physical prototype during crawling and walking gaits shows high accuracy in center of mass trajectories with less than 5 mm RMSE.</div>
<div class="mono" style="margin-top:8px">研究旨在使用名为SLOT的腱驱动软四足机器人来研究可变形腿足运动的物理启发式建模和控制。机器人的腿使用离散柯西尔杆理论建模，以捕捉大变形和腱驱动。引入了一种模块化的全身建模框架，能够实现高效的仿真和实时控制，同时保持物理精度。提出的模型被集成到一个凸模型预测控制框架中，该框架优化地面反作用力并将其映射到腱驱动，实现了在各种扰动下的渐近稳定性。在爬行和行走姿态的物理原型上进行的实验验证显示，中心质量轨迹的准确性非常高，误差小于5毫米。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260222_0338.html">20260222_0338</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
