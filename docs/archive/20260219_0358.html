<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-19 03:58</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260219_0358</div>
    <div class="row"><div class="card">
<div class="title">Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation</div>
<div class="meta-line">Authors: Yuxuan Kuang, Sungjae Park, Katerina Fragkiadaki, Shubham Tulsiani</div>
<div class="meta-line">First: 2026-02-17T18:59:31+00:00 · Latest: 2026-02-17T18:59:31+00:00</div>
<div class="meta-line">Comments: Project page: https://dex4d.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15828v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dex4d.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this &#x27;Anypose-to-Anypose&#x27; policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dex4D：通用点轨迹策略框架实现模拟到现实的灵巧操作</div>
<div class="mono" style="margin-top:8px">在灵巧操作中学习能够完成多种日常任务的一般性策略仍然是一个开放的挑战。特别是，通过现实世界的远程操作收集大规模操作数据既昂贵又难以扩展。虽然在模拟中学习提供了一种可行的替代方案，但设计多个特定任务的环境和奖励进行训练同样具有挑战性。我们提出了Dex4D框架，该框架利用模拟来学习任务无关的灵巧技能，这些技能可以在测试时灵活重组以执行各种现实世界的操作任务。具体而言，Dex4D学习了一种领域无关的3D点轨迹条件策略，该策略能够操作任何物体到任何期望的姿态。我们在数千种具有不同姿态配置的物体上对这种“任意姿态到任意姿态”的策略进行了模拟训练，涵盖了可以在测试时组合的广泛机器人-物体交互空间。在部署时，该策略可以通过仅提示其期望的物体中心点轨迹（从生成的视频中提取）来零样本转移至现实世界的任务，无需微调。在执行过程中，Dex4D使用在线点跟踪进行闭环感知和控制。在模拟和真实机器人上的大量实验表明，我们的方法能够实现多种灵巧操作任务的零样本部署，并且在先前基线方法上取得了持续改进。此外，我们展示了其在新型物体、场景布局、背景和轨迹上的强大泛化能力，突显了所提出框架的鲁棒性和可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Dex4D is a framework designed to learn task-agnostic dexterous manipulation skills in simulation, which can be flexibly applied to various real-world tasks. It trains a 3D point track policy to manipulate any object to any desired pose across thousands of objects with diverse configurations. This policy can be zero-shot transferred to real-world tasks by prompting it with desired object-centric point tracks. Experiments show that Dex4D outperforms previous methods and demonstrates strong generalization to new objects and scenarios.</div>
<div class="mono" style="margin-top:8px">Dex4D 是一个框架，旨在通过模拟学习通用的灵巧操作技能，这些技能可以灵活应用于各种现实世界任务。它训练了一个3D点轨迹策略，能够在数千种具有不同配置的对象上将任何物体移动到任何期望的姿态。该策略可以通过提示其所需的物体中心点轨迹在现实世界任务中实现零样本转移。实验表明，Dex4D 在性能上优于先前的方法，并且在新对象和场景中表现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching</div>
<div class="meta-line">Authors: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza, Guanya Shi, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-17T18:59:11+00:00 · Latest: 2026-02-17T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知类人公园跑酷：通过运动匹配串联动态人类技能</div>
<div class="mono" style="margin-top:8px">尽管近年来类人机器人在多变地形上的稳定行走取得了进展，但捕捉人类高度动态运动的敏捷性和适应性仍然是一个开放的挑战。特别是，在复杂环境中进行敏捷的公园跑酷不仅需要低级别的鲁棒性，还需要类似人类的运动表达性、长期技能组合以及感知驱动的决策制定。在本文中，我们提出了感知类人公园跑酷（PHP），这是一种模块化框架，使类人机器人能够自主地在具有挑战性的障碍物课程上进行长期视角导向的公园跑酷。我们的方法首先利用运动匹配，将其形式化为特征空间中的最近邻搜索，将重新定位的原子人类技能组合成长期的运动轨迹。该框架使复杂的技能链的灵活组合和平滑过渡成为可能，同时保持动态人类运动的优雅和流畅性。接下来，我们为这些组合的运动训练基于运动跟踪的强化学习（RL）专家策略，并使用DAgger和RL的组合将其提炼为一个基于深度的、多技能学生策略。关键的是，感知与技能组合的结合使自主、上下文感知的决策成为可能：仅使用机载深度传感和离散的二维速度命令，机器人可以选择并执行跨越、攀爬、越障或滚落不同几何形状和高度的障碍物。我们通过在Unitree G1类人机器人上进行广泛的实地实验验证了该框架，展示了诸如攀爬高达1.25米（96%机器人高度）的障碍物等高度动态的公园跑酷技能，以及在实时障碍扰动下进行长期多障碍物穿越的闭环适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of achieving human-like agility in humanoid robots by presenting Perceptive Humanoid Parkour (PHP), a modular framework that enables autonomous performance of complex parkour skills. The approach uses motion matching to compose human skills into long-horizon trajectories and trains reinforcement learning policies to execute these skills. Key findings include the robot&#x27;s ability to perform dynamic parkour skills like climbing obstacles up to 1.25m and traversing multiple obstacles with real-time adaptation.</div>
<div class="mono" style="margin-top:8px">本文介绍了Perceptive Humanoid Parkour (PHP)框架，使类人机器人能够自主执行复杂的公园运动技能。该方法使用运动匹配将人类技能组合成长时间轨迹，并训练强化学习策略执行这些技能。关键发现包括机器人能够执行动态的公园运动动作，如攀爬高达1.25米的障碍物，并在实时适应障碍物变化的情况下跨越多个障碍物。</div>
</details>
</div>
<div class="card">
<div class="title">Spanning the Visual Analogy Space with a Weight Basis of LoRAs</div>
<div class="meta-line">Authors: Hila Manor, Rinon Gal, Haggai Maron, Tomer Michaeli, Gal Chechik</div>
<div class="meta-line">First: 2026-02-17T17:02:38+00:00 · Latest: 2026-02-17T17:02:38+00:00</div>
<div class="meta-line">Comments: Code and data are in https://research.nvidia.com/labs/par/lorweb</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15727v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15727v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}&#x27;$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}&#x27;$ such that $\mathbf{a} : \mathbf{a}&#x27; :: \mathbf{b} : \mathbf{b}&#x27;$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a &quot;space of LoRAs&quot;. We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of visual analogy learning, which allows for complex image manipulations through demonstration. It proposes LoRWeB, which uses a dynamic composition of learned transformation primitives at inference time, overcoming the limitations of single LoRA modules. The approach includes a learnable basis of LoRAs and a lightweight encoder for selecting and weighting these basis LoRAs. Experiments show that LoRWeB achieves state-of-the-art performance and better generalization to unseen transformations. This suggests that LoRA basis decompositions are a promising method for flexible visual manipulation.</div>
<div class="mono" style="margin-top:8px">论文针对通过演示实现复杂图像操作的视觉类比学习挑战，提出了一种名为LoRWeB的方法，该方法在推理时动态组合学习到的变换基元，克服了单一LoRA模块的局限性。该方法包括一个可学习的LoRA基底和一个轻量级编码器，用于选择和加权这些基底LoRA。实验表明，LoRWeB在性能上达到最新水平，并且在处理未见过的变换时表现出更好的泛化能力。这表明LoRA基底分解是实现灵活视觉操作的一个有前途的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation</div>
<div class="meta-line">Authors: Yorai Shaoul, Zhe Chen, Mohamed Naveed Gul Mohamed, Federico Pecora, Maxim Likhachev, Jiaoyang Li</div>
<div class="meta-line">First: 2025-11-14T01:05:58+00:00 · Latest: 2026-02-17T15:48:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10874v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10874v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale. Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks. To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning. Within this framework, a generative model co-generates contact formations and manipulation trajectories from visual observations, while a novel motion planner conveys robots at scale. Crucially, the same planner also supports coordination at the object level, assigning manipulated objects to larger target structures and thereby unifying robot- and object-level reasoning within a single algorithmic framework. Experiments in challenging simulated environments demonstrate that our approach outperforms baselines in both motion planning and manipulation tasks, highlighting the benefits of generative co-design and integrated planning for scaling collaborative manipulation to complex multi-agent, multi-object settings. Visit gco-paper.github.io for code and demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作多机器人非抓取操作通过流匹配共生成</div>
<div class="mono" style="margin-top:8px">协调一组机器人在拥挤环境中重新定位多个物体需要同时推理机器人应在何处建立接触、接触后如何操作物体以及如何在大规模下安全高效地导航。先前的方法通常分为两种极端情况——要么学习整个任务，要么依赖特权信息和手设计的规划器——这两种方法都难以处理长时任务中的多样化物体。为了解决这些挑战，我们提出了一种统一的框架，用于协作多机器人、多物体非抓取操作，该框架结合了流匹配共生成和匿名多机器人运动规划。在此框架中，生成模型从视觉观察中共同生成接触形式和操作轨迹，而一种新颖的运动规划器则在大规模下引导机器人。关键的是，同一个规划器还支持物体层面的协调，将被操作的物体分配给更大的目标结构，从而在单一算法框架内统一了机器人层面和物体层面的推理。在具有挑战性的模拟环境中进行的实验表明，我们的方法在运动规划和操作任务中均优于基线方法，突显了生成共设计和集成规划在扩展协作操作到复杂多智能体、多物体设置中的优势。请访问 gco-paper.github.io 获取代码和演示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to coordinate multiple robots for repositioning objects in cluttered environments, addressing the challenges of joint reasoning about contact formation, manipulation, and navigation. The method integrates a flow-matching co-generation model with a motion planner, enabling both object-level and robot-level coordination. Experiments show that the proposed approach outperforms existing methods in both motion planning and manipulation tasks, particularly in complex multi-agent and multi-object settings.</div>
<div class="mono" style="margin-top:8px">研究旨在协调多机器人团队在杂乱环境中操纵多个物体，解决联合推理接触形成和操作轨迹的问题。该方法结合了流匹配协同生成模型和新颖的运动规划器，实现物体级和机器人级的协调。实验表明，所提出的方法在运动规划和操作任务中均优于现有基线，展示了生成协同设计和集成规划在复杂多智能体、多物体设置中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion</div>
<div class="meta-line">Authors: Mostafa A. Atalla, Daan van Bemmel, Jack Cummings, Paul Breedveld, Michaël Wiertlewski, Aimée Sakes</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-17T14:33:17+00:00 · Latest: 2026-02-17T14:33:17+00:00</div>
<div class="meta-line">Comments: Accepted for publication in the 2026 IEEE International Conference on Robotics and Automation (ICRA) in Vienna</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between &quot;grip&quot; and &quot;slip&quot; states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>按需抓握，按需滑动：用于机器人运动的超声润滑</div>
<div class="mono" style="margin-top:8px">摩擦是陆地运动中的基本调节因子，但在机器人系统中，它几乎总是被视为由表面材料和条件决定的被动属性。在这里，我们介绍了一种超声润滑方法，用于主动控制机器人运动中的摩擦。通过在超声频率下激发共振结构，接触界面可以在“抓握”和“滑动”状态之间动态切换，从而实现运动。我们开发了两个摩擦控制模块：一种用于管状环境的圆柱设计和一种用于外部表面的平板设计，并将它们集成到模仿蠼螋和黄蜂产卵器运动的生物启发系统中。两个系统均实现了双向运动，几乎完美的运动效率超过了90%。进一步的摩擦特性实验还证明，在各种表面上，包括坚硬、柔软、颗粒状、生物组织界面，在干燥和潮湿条件下，以及在不同粗糙度水平的表面上，超声润滑可以显著降低摩擦，证实了超声润滑在运动任务中的广泛应用。这些发现确立了超声润滑作为一种可行的主动摩擦控制机制在机器人运动中的地位，有可能减少设计复杂性并提高机器人运动系统的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces ultrasonic lubrication as a method to actively control friction in robotic locomotion, allowing dynamic switching between &#x27;grip&#x27; and &#x27;slip&#x27; states. Two friction control modules were developed and integrated into bio-inspired systems, achieving nearly perfect locomotion efficiencies. Experiments showed significant friction reduction on various surfaces under different conditions, confirming the broad applicability of ultrasonic lubrication for robotic locomotion tasks.</div>
<div class="mono" style="margin-top:8px">研究引入了超声润滑作为在机器人运动中主动控制摩擦的方法，能够动态切换‘抓握’和‘滑动’状态。开发了两种摩擦控制模块并集成到仿生系统中，实现了几乎完美的运动效率。摩擦特性实验显示，在不同条件下，超声润滑在各种表面上显著降低了摩擦力，证实了其在机器人运动中的广泛应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions</div>
<div class="meta-line">Authors: Jieting Long, Dechuan Liu, Weidong Cai, Ian Manchester, Weiming Zhi</div>
<div class="meta-line">First: 2026-02-17T13:27:05+00:00 · Latest: 2026-02-17T13:27:05+00:00</div>
<div class="meta-line">Comments: 8 pages, 8 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15567v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15567v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot&#x27;s workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot&#x27;s control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束流模型以适应学习到的机器人轨迹分布</div>
<div class="mono" style="margin-top:8px">机器人的运动分布通常表现出多模态性，需要灵活的生成模型来进行准确表示。流式流策略（SFPs）最近作为一种强大的范式，通过直接在动作空间中整合学习到的速度场来生成机器人轨迹，从而实现平滑和反应性控制。然而，现有的形式缺乏在训练后适应轨迹以强制执行安全性和任务特定约束的机制。我们提出了一种约束感知流式流（CASF）框架，该框架通过执行期间与约束相关的度量来增强流式流策略，从而重新塑造学习到的速度场。CASF将约束定义在机器人的工作空间或配置空间中，作为可微距离函数，并将其转换为局部度量并拉回到机器人的控制空间。远离限制区域时，该度量简化为恒等式；接近约束边界时，它会平滑地减弱或重新定向运动，有效地变形底层流以保持安全性。这允许轨迹在实时中进行适应，确保机器人动作遵守关节限制，避免碰撞，并保持在可行的工作空间内，同时保留流式流策略的多模态性和反应性。我们在模拟和实际操作任务中展示了CASF，表明它生成的满足约束的轨迹保持平滑、可行且动态一致，优于标准的后处理投影基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the adaptability and safety of robot trajectories by addressing the limitations of existing streaming flow policies. The proposed Constraint-Aware Streaming Flow (CASF) framework integrates constraint-dependent metrics into streaming flow policies, allowing for real-time adaptation of trajectories to enforce safety and task-specific constraints. Experiments show that CASF generates smooth, feasible, and dynamically consistent trajectories that satisfy constraints, outperforming standard post-hoc projection methods.</div>
<div class="mono" style="margin-top:8px">论文提出了一种名为Constraint-Aware Streaming Flow (CASF)的方法，通过将约束依赖的度量集成到流式流策略中，以解决机器人轨迹生成的灵活性和安全性问题。CASF在执行过程中重塑学习到的流速场，确保轨迹保持在可行的工作空间内并避免碰撞，同时保留多模态性和反应性。实验结果表明，CASF生成的轨迹既平滑又可行，并且动态一致性优于标准的后处理投影基线方法，在模拟和真实世界操作任务中均表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Selective Perception for Robot: Task-Aware Attention in Multimodal VLA</div>
<div class="meta-line">Authors: Young-Chae Son, Jung-Woo Lee, Yoon-Ji Choi, Dae-Kwan Ko, Soo-Chul Lim</div>
<div class="meta-line">First: 2026-02-17T12:48:59+00:00 · Latest: 2026-02-17T12:48:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15543v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15543v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人选择性感知：多模态VLA中的任务感知注意力</div>
<div class="mono" style="margin-top:8px">在机器人学中，能够整合多视图输入的多种模态信号的视觉-语言-行动（VLA）模型已经成为了有效的方法。然而，大多数先前的工作采用静态融合方式，将所有视觉输入均匀处理，这导致了不必要的计算开销，并允许与任务无关的背景信息作为噪声。受人类主动感知原理的启发，我们提出了一种动态信息融合框架，旨在最大化VLA模型的效率和鲁棒性。我们的方法引入了一种轻量级的自适应路由架构，该架构能够实时分析当前文本提示和手腕摄像头的观测结果，以预测多个摄像头视图的任务相关性。通过条件性地减弱低信息效用视图的计算，并仅向策略网络提供必要的视觉特征，我们的框架实现了与任务相关性成比例的计算效率。此外，为了高效地为路由训练获取大规模标注数据，我们建立了一个利用视觉-语言模型（VLM）的自动化标注流水线，以最小化数据收集和标注成本。在真实世界机器人操作场景中的实验结果表明，与现有的VLA模型相比，所提出的方法在推理效率和控制性能方面均取得了显著改进，验证了在资源受限、实时机器人控制环境中动态信息融合的有效性和实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes a dynamic information fusion framework for Vision-Language-Action (VLA) models in robotics, which analyzes real-time text prompts and camera observations to selectively process visual inputs, reducing unnecessary computations and improving inference efficiency and control performance in robotic manipulation tasks. This approach uses a lightweight adaptive routing architecture to conditionally attenuate computations for less relevant camera views and selectively provide essential visual features to the policy network, demonstrating significant improvements over existing VLA models in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">论文提出了一种动态信息融合框架，用于机器人中的Vision-Language-Action (VLA)模型，该框架能够实时分析文本提示和相机观察结果，选择性地处理视觉输入，减少不必要的计算并提高推理效率和控制性能。该方法使用轻量级的自适应路由架构，根据视图的相关性条件性地减少计算，并仅向策略网络提供必要的视觉特征，实验证实在真实世界机器人操作场景中，该方法显著优于现有VLA模型。</div>
</details>
</div>
<div class="card">
<div class="title">SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies</div>
<div class="meta-line">Authors: Thies Oelerich, Gerald Ebmer, Christian Hartl-Nesic, Andreas Kugi</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-13T10:23:43+00:00 · Latest: 2026-02-17T09:36:24+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12794v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12794v2">PDF</a> · <a href="https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeFlowMPC：基于学习策略的机器人操作器预测与安全轨迹规划</div>
<div class="mono" style="margin-top:8px">随着机器人逐渐融入日常生活，带来了许多重大挑战。与传统的工业应用相比，需要更多的灵活性和实时反应性。基于学习的方法可以根据演示轨迹训练强大的策略，使机器人能够泛化到类似的情况。然而，这些黑盒模型缺乏可解释性和严格的安全性保证。基于优化的方法提供了这些保证，但缺乏所需的灵活性和泛化能力。本文提出了一种SafeFlowMPC方法，结合了流匹配和在线优化，以结合学习和优化的优点。该方法在所有时间点都能保证安全性，并通过使用次优模型预测控制形式设计来满足实时执行的需求。SafeFlowMPC在KUKA 7-DoF操作器上的三个真实世界实验中表现出色，包括两个抓取实验和一个动态人机物体交接实验。实验视频可在http://www.acin.tuwien.ac.at/42d6获取。代码可在https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SafeFlowMPC is a method that combines flow matching and online optimization to provide both safety guarantees and real-time flexibility for robot manipulators. It uses a suboptimal model-predictive control formulation to ensure safety at all times while performing tasks. The method was tested on a KUKA 7-DoF manipulator in three real-world experiments, including grasping and dynamic human-robot object handover, demonstrating strong performance and safety. The code and a video of the experiments are available online.</div>
<div class="mono" style="margin-top:8px">SafeFlowMPC 是一种结合流匹配和在线优化的方法，旨在为机器人 manipulator 提供实时灵活性和安全性保证。它使用次优模型预测控制公式来确保在执行任务时的安全性。该方法在 KUKA 7-DoF 机器人上进行了三项真实世界的实验测试，包括抓取和动态人机物体交接，展示了强大的性能和安全性。相关代码和实验视频可在网上获取。</div>
</details>
</div>
<div class="card">
<div class="title">ActionCodec: What Makes for Good Action Tokenizers</div>
<div class="meta-line">Authors: Zibin Dong, Yicheng Liu, Shiduo Zhang, Baijun Ye, Yifu Yuan, Fei Ni, Jingjing Gong, Xipeng Qiu, Hang Zhao, Yinchuan Li, Jianye Hao</div>
<div class="meta-line">First: 2026-02-17T07:07:15+00:00 · Latest: 2026-02-17T07:07:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15397v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15397v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ActionCodec：什么是好的动作分词器</div>
<div class="mono" style="margin-top:8px">利用视觉-语言模型（VLMs）的原生自回归范式的视觉-语言-动作（VLA）模型在指令遵循和训练效率方面表现出色。这一范式的核心是动作分词，但其设计主要集中在重建保真度上，未能解决其对VLA优化的直接影响。因此，“什么是好的动作分词器”这一基本问题仍然没有答案。在本文中，我们通过从VLA优化的角度建立设计原则来填补这一空白。我们基于信息论的见解，确定了一套最佳实践，包括最大化时间分词重叠、最小化词汇冗余、增强多模态互信息以及分词独立性。根据这些原则，我们引入了**ActionCodec**，这是一种高性能的动作分词器，显著提高了训练效率和VLA性能，涵盖了多种模拟和现实世界的基准测试。值得注意的是，在LIBERO上，使用ActionCodec微调的SmolVLM2-2.2B在没有任何机器人预训练的情况下达到了95.5%的成功率。通过先进的架构增强，这一数字达到了97.4%，代表了在没有机器人预训练的情况下VLA模型的新SOTA。我们认为，我们建立的设计原则以及发布的模型将为社区提供一条明确的道路，以开发更有效的动作分词器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the lack of design principles for action tokenizers in Vision-Language-Action (VLA) models, focusing on their impact on VLA optimization rather than just reconstruction fidelity. It introduces ActionCodec, an action tokenizer guided by information-theoretic insights, which enhances training efficiency and VLA performance. On the LIBERO benchmark, a SmolVLM2-2.2B model fine-tuned with ActionCodec achieves a 95.5% success rate without robotics pre-training, improving to 97.4% with architectural enhancements, setting a new SOTA for VLA models without robotics pre-training.</div>
<div class="mono" style="margin-top:8px">本文探讨了在视觉-语言-行动模型中什么是好的动作分词器，这对于指令跟随和训练效率至关重要。通过关注信息论洞察，作者提出了最大化时间分词重叠和最小化词汇冗余等设计原则。他们引入了ActionCodec，这种分词器显著提高了训练效率和VLA性能，在LIBERO上实现了95.5%的成功率，无需机器人预训练，并通过高级架构达到97.4%，成为无需机器人预训练的VLA模型的新SOTA。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Deep Reinforcement Learning against Adversarial Behavior Manipulation</div>
<div class="meta-line">Authors: Shojiro Yamabe, Kazuto Fukuchi, Jun Sakuma</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2024-06-06T08:49:51+00:00 · Latest: 2026-02-17T05:50:40+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.03862v3">Abs</a> · <a href="https://arxiv.org/pdf/2406.03862v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates behavior-targeted attacks on reinforcement learning and their countermeasures. Behavior-targeted attacks aim to manipulate the victim&#x27;s behavior as desired by the adversary through adversarial interventions in state observations. Existing behavior-targeted attacks have some limitations, such as requiring white-box access to the victim&#x27;s policy. To address this, we propose a novel attack method using imitation learning from adversarial demonstrations, which works under limited access to the victim&#x27;s policy and is environment-agnostic. In addition, our theoretical analysis proves that the policy&#x27;s sensitivity to state changes impacts defense performance, particularly in the early stages of the trajectory. Based on this insight, we propose time-discounted regularization, which enhances robustness against attacks while maintaining task performance. To the best of our knowledge, this is the first defense strategy specifically designed for behavior-targeted attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗行为操纵的鲁棒深度强化学习</div>
<div class="mono" style="margin-top:8px">本研究探讨了针对强化学习的行为导向攻击及其对策。行为导向攻击旨在通过对手在状态观察中的对抗干预来操纵受害者的期望行为。现有行为导向攻击存在一些局限性，如需要访问受害者的策略。为解决这一问题，我们提出了一种新的攻击方法，该方法利用对抗演示进行模仿学习，可以在有限访问受害者策略的情况下工作，并且环境无关。此外，我们的理论分析证明，策略对状态变化的敏感性影响防御性能，尤其是在轨迹的早期阶段。基于这一见解，我们提出了时间折扣正则化，该方法在增强对抗攻击鲁棒性的同时保持任务性能。据我们所知，这是首个专门针对行为导向攻击的防御策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores behavior-targeted attacks on reinforcement learning and introduces a novel attack method using imitation learning from adversarial demonstrations, which requires limited access to the victim&#x27;s policy and is environment-agnostic. The research also proposes time-discounted regularization to enhance robustness against attacks while preserving task performance, addressing the limitations of existing methods. Theoretical analysis shows that the policy&#x27;s sensitivity to state changes affects defense performance, especially in the early stages of the trajectory.</div>
<div class="mono" style="margin-top:8px">该研究探讨了针对强化学习的行为导向攻击，并提出了一种使用对抗演示进行模仿学习的新攻击方法。该方法仅需有限访问受害者的策略，并且环境无关。研究还提供了理论分析，表明策略对状态变化的敏感性影响防御性能，从而提出了时间折扣正则化来提高对抗攻击的鲁棒性同时保持任务性能。这是针对行为导向攻击的第一个防御策略。</div>
</details>
</div>
<div class="card">
<div class="title">OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy</div>
<div class="meta-line">Authors: Mostafa A. Atalla, Anand S. Sekar, Remi van Starkenburg, David J. Jager, Aimée Sakes, Michaël Wiertlewski, Paul Breedveld</div>
<div class="meta-line">First: 2026-02-17T02:14:30+00:00 · Latest: 2026-02-17T02:14:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15309v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15309v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule&#x27;s predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSCAR：一种受产卵器启发的自推进胶囊机器人用于结肠镜检查</div>
<div class="mono" style="margin-top:8px">自推进的胶囊机器人消除了传统结肠镜检查中的管状缠绕，减少了患者的不适。然而，在结肠这种滑腻的粘弹性环境中可靠地移动仍然是一个重大挑战。我们提出了OSCAR，一种受产卵器启发的自推进胶囊机器人，将寄生蜂的运输策略转化为结肠镜检查的推进机制。OSCAR通过一个弹簧加载的凸轮系统机械地编码了产卵器启发的运动模式，驱动十二个环形滑块在协调的、相位偏移的序列中运动。通过调整运动轮廓以最大化撤回相相对于推进相的比例，胶囊在接口处产生可控的摩擦各向异性，从而产生净向前推力。我们开发了一个包含Kelvin-Voigt表述的分析模型，以捕捉滑块与组织之间的粘弹性粘滑相互作用，将推进相和撤回相持续时间的不对称性与平均推力联系起来，并将滑块反转同步与推力稳定性联系起来。体外猪结肠的全面力特性实验揭示了平均稳态牵引力为0.85牛顿，与模型相符。此外，实验确认推力生成与速度无关，并且与相位不对称性成线性关系，与理论预测一致，突显了胶囊的可预测性能和可扩展性。在运动验证实验中，OSCAR表现出稳健的性能，平均速度为3.08毫米/秒，速度足以匹配传统结肠镜检查的回盲部插管时间。通过将相位编码的摩擦各向异性与预测模型相结合，OSCAR在低法向载荷下实现了可控的推力生成，使自推进的运动更加安全和可靠，适用于胶囊机器人结肠镜检查。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces OSCAR, a self-propelling capsule robot inspired by parasitic wasps&#x27; ovipositors, designed to address the challenges of conventional colonoscopy. It uses a spring-loaded cam system to drive twelve sliders in a phase-shifted sequence, creating a friction anisotropy that generates forward thrust. Experiments showed a mean steady-state traction force of 0.85 N, with thrust generation being speed-independent and linearly scalable with phase asymmetry. OSCAR achieved an average speed of 3.08 mm/s, matching the cecal intubation times of conventional colonoscopy, demonstrating robust performance and controllable thrust generation under low normal loads.</div>
<div class="mono" style="margin-top:8px">该论文介绍了OSCAR，一种受寄生蜂产卵器启发的自推进胶囊机器人，旨在通过解决结肠内滑腻、粘弹性环境中的移动难题来改进结肠镜检查。OSCAR使用弹簧加载的凸轮系统驱动十二个滑块以相位偏移序列运动，从而在接触面上产生可控的摩擦各向异性，产生向前推力。实验显示平均静态牵引力为0.85 N，推力生成与速度无关，并且与相位不对称性成线性关系，平均速度达到3.08 mm/s，与传统结肠镜检查时间相当。</div>
</details>
</div>
<div class="card">
<div class="title">The Information Geometry of Softmax: Probing and Steering</div>
<div class="meta-line">Authors: Kiho Park, Todd Nief, Yo Joong Choe, Victor Veitch</div>
<div class="meta-line">First: 2026-02-17T01:33:28+00:00 · Latest: 2026-02-17T01:33:28+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/KihoPark/dual-steering</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15293v1">PDF</a> · <a href="https://github.com/KihoPark/dual-steering">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper concerns the question of how AI systems encode semantic structure into the geometric structure of their representation spaces. The motivating observation of this paper is that the natural geometry of these representation spaces should reflect the way models use representations to produce behavior. We focus on the important special case of representations that define softmax distributions. In this case, we argue that the natural geometry is information geometry. Our focus is on the role of information geometry on semantic encoding and the linear representation hypothesis. As an illustrative application, we develop &quot;dual steering&quot;, a method for robustly steering representations to exhibit a particular concept using linear probes. We prove that dual steering optimally modifies the target concept while minimizing changes to off-target concepts. Empirically, we find that dual steering enhances the controllability and stability of concept manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Softmax的信息几何：探测与引导</div>
<div class="mono" style="margin-top:8px">本文探讨了AI系统如何将语义结构编码到其表示空间的几何结构中。本文的动机观察是，这些表示空间的自然几何结构应该反映模型如何使用表示来产生行为。我们专注于定义softmax分布的重要特殊案例。在这种情况下，我们认为自然的几何结构是信息几何。我们的重点是信息几何在语义编码和线性表示假设中的作用。作为说明性应用，我们开发了“双引导”方法，这是一种使用线性探测器稳健地引导表示以表现出特定概念的方法。我们证明了双引导在优化修改目标概念的同时，最小化对非目标概念的改变。实验中，我们发现双引导增强了概念操控的可控性和稳定性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates how AI systems encode semantic structure into their representation spaces, focusing on the softmax distributions. The authors propose that the natural geometry of these spaces is information geometry, which reflects how models use representations to produce behavior. They introduce a method called &#x27;dual steering&#x27; to robustly steer representations to exhibit a particular concept, proving that it optimally modifies the target concept while minimizing changes to off-target concepts. Empirically, dual steering enhances the controllability and stability of concept manipulation.</div>
<div class="mono" style="margin-top:8px">本文研究了AI系统如何将语义结构编码到其表示空间中，重点关注softmax分布中的自然几何结构是信息几何。作者引入了一种名为“双向引导”的方法，使用线性探针来稳健地引导表示以展示特定的概念。他们证明了双向引导在优化修改目标概念的同时，最小化对非目标概念的改变，并且实验证明这种方法增强了概念操作的可控性和稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">AMBER: A tether-deployable gripping crawler with compliant microspines for canopy manipulation</div>
<div class="meta-line">Authors: P. A. Wigner, L. Romanello, A. Hammad, P. H. Nguyen, T. Lan, S. F. Armanini, B. B. Kocer, M. Kovac</div>
<div class="meta-line">First: 2025-12-08T16:17:56+00:00 · Latest: 2026-02-16T20:51:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07680v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07680v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90$^\circ$ body roll and inclination, while effective climbing on branches inclined up to 67.5$^\circ$, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10$^\circ$, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. The crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing. The aerial deployment is demonstrated at a conceptual and feasibility level, while full drone-crawler integration is left as future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMBER：一种可部署的具有顺应性微钩爪的攀爬器，用于树冠操作</div>
<div class="mono" style="margin-top:8px">本文介绍了一种可空中部署的攀爬器，用于树冠内的适应性移动和操作。该系统结合了基于顺应性微钩爪的履带、双履带旋转夹持器和弹性尾巴，使其能够在不同曲率和倾斜角度的树枝上实现安全附着和稳定移动。实验表明，该攀爬器在90°身体滚转和倾斜角度下仍能可靠地抓握，有效攀爬倾斜角度达67.5°的树枝，最大速度为每秒0.55个身体长度。顺应性履带允许最大10°的偏航转向，提高在不规则表面的机动性。功率测量显示，该攀爬器的无量纲运输成本比典型悬停功率消耗低一个数量级，提供了一个坚固、低功耗的环境采样和树冠内传感平台。空中部署在概念和可行性层面进行了演示，而完整的无人机-攀爬器集成留作未来工作。</div>
</details>
</div>
<div class="card">
<div class="title">Clone-Robust Weights in Metric Spaces: Handling Redundancy Bias for Benchmark Aggregation</div>
<div class="meta-line">Authors: Damien Berriaud, Roger Wattenhofer</div>
<div class="meta-line">First: 2025-02-05T19:50:51+00:00 · Latest: 2026-02-16T20:44:16+00:00</div>
<div class="meta-line">Comments: Accepted at AAMAS&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.03576v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.03576v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We are given a set of elements in a metric space. The distribution of the elements is arbitrary, possibly adversarial. Can we weigh the elements in a way that is resistant to such (adversarial) manipulations? This problem arises in various contexts. For instance, the elements could represent data points, requiring robust domain adaptation. Alternatively, they might represent tasks to be aggregated into a benchmark; or questions about personal political opinions in voting advice applications. This article introduces a theoretical framework for dealing with such problems. We propose clone-proof weighting functions as a solution concept. These functions distribute importance across elements of a set such that similar objects (``clones&#x27;&#x27;) share (some of) their weights, thus avoiding a potential bias introduced by their multiplicity. Our framework extends the maximum uncertainty principle to accommodate general metric spaces and includes a set of axioms -- symmetry, continuity, and clone-proofness -- that guide the construction of weighting functions. Finally, we address the existence of weighting functions satisfying our axioms in the significant case of Euclidean spaces and propose a general method for their construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>度量空间中的克隆稳健权重：处理冗余偏差的基准聚合</div>
<div class="mono" style="margin-top:8px">我们给定一个度量空间中的元素集合。这些元素的分布是任意的，可能是敌对的。我们能否以一种对这种（敌对）操纵具有抵抗力的方式对这些元素进行加权？这个问题在各种情况下都会出现。例如，这些元素可以代表数据点，需要稳健的领域适应。或者它们可以代表要聚合为基准的任务；或者投票建议应用程序中关于个人政治观点的问题。本文介绍了一个处理此类问题的理论框架。我们提出克隆抗性加权函数作为解决方案的概念。这些函数在集合中的元素之间分配重要性，使得相似的对象（“克隆”）共享（部分）其权重，从而避免由其多重性引入的潜在偏差。我们的框架将最大不确定性原则扩展到一般度量空间，并包括对称性、连续性和克隆抗性等公理，这些公理指导加权函数的构建。最后，我们解决了在欧几里得空间中满足我们公理的加权函数的存在性问题，并提出了一种通用的构建方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the problem of weighing elements in a metric space to resist adversarial manipulations, which is relevant for robust domain adaptation, benchmark aggregation, and voting advice applications. It introduces clone-proof weighting functions that distribute importance such that similar objects share weights, thereby mitigating redundancy bias. The framework extends the maximum uncertainty principle to general metric spaces and includes axioms for symmetry, continuity, and clone-proofness. The study proves the existence of such weighting functions in Euclidean spaces and provides a construction method.</div>
<div class="mono" style="margin-top:8px">论文解决了在度量空间中为元素分配权重以抵抗恶意操纵的问题，这适用于稳健的领域适应、基准聚合和投票建议应用。它引入了克隆抗性权重函数，这些函数将重要性分配给相似的元素，以避免由于它们的重复性带来的偏差。该框架将最大不确定性原则扩展到一般的度量空间，并包括构建此类函数的一系列公理。研究证明了这些函数在欧几里得空间中的存在性，并提出了一种通用的方法来构建它们。</div>
</details>
</div>
<div class="card">
<div class="title">Active Matter as a framework for living systems-inspired Robophysics</div>
<div class="meta-line">Authors: Giulia Janzen, Gaia Maselli, Juan F. Jimenez, Lia Garcia-Perez, D A Matoz Fernandez, Chantal Valeriani</div>
<div class="meta-line">First: 2025-11-18T16:16:27+00:00 · Latest: 2026-02-16T20:07:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14624v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.14624v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robophysics investigates the physical principles that govern living-like robots operating in complex, realworld environments. Despite remarkable technological advances, robots continue to face fundamental efficiency limitations. At the level of individual units, locomotion remains a challenge, while at the collective level, robot swarms struggle to achieve shared purpose, coordination, communication, and cost efficiency. This perspective article examines the key challenges faced by bio-inspired robotic collectives and highlights recent research efforts that incorporate principles from active-matter physics and biology into the modeling and design of robot swarms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>活性物质作为受生物系统启发的机器人物理学框架</div>
<div class="mono" style="margin-top:8px">机器人物理学研究指导类生命机器人在复杂现实环境中的物理原理。尽管取得了显著的技术进步，但机器人仍然面临根本的效率限制。在个体层面，运动仍然是一个挑战，而在集体层面，机器人群难以实现共享目标、协调、通信和成本效率。本文探讨了生物启发的机器人群体所面临的关键挑战，并强调了将活性物质物理学和生物学原理纳入机器人群的建模和设计中的近期研究努力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the application of active matter principles to improve the efficiency and coordination of bio-inspired robotic collectives. It addresses the challenges of individual robot locomotion and collective swarm coordination, communication, and cost efficiency. The study highlights recent research that integrates active-matter physics and biological principles to enhance robotic swarm performance.</div>
<div class="mono" style="margin-top:8px">本文探讨了将活性物质原理应用于提高仿生机器人集体的效率和协调性的方法。它解决了单个机器人运动和集体机器人群协调、通信和成本效率方面的挑战。研究强调了将活性物质物理学和生物学原理整合到机器人群建模和设计中的最新努力，以提升其性能。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning</div>
<div class="meta-line">Authors: Mohammad Hadi Foroughi, Seyed Hamed Rastegar, Mohammad Sabokrou, Ahmad Khonsari</div>
<div class="meta-line">First: 2026-02-16T19:59:53+00:00 · Latest: 2026-02-16T19:59:53+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication in IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15161v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15161v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用层特定漏洞在联邦学习中植入后门攻击</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）允许在边缘设备上分布式模型训练，同时保持数据本地性。这种分散的方法已成为解决敏感用户数据上协作学习长期隐私问题的有前途的解决方案。然而，FL的分散性质暴露了新的安全漏洞，尤其是威胁模型完整性的后门攻击。为研究这一关键问题，本文提出了层平滑攻击（LSA），这是一种新颖的后门攻击，利用神经网络中的层特定漏洞。首先，通过层子代换分析方法系统地识别出对后门成功贡献最大的后门关键（BC）层。随后，LSA战略性地操纵这些BC层，以注入持久性后门而不被最先进的防御机制检测到。在多种模型架构和数据集上的广泛实验表明，LSA在保持主要任务高模型准确率的同时，实现了高达97%的后门成功率，一致地绕过了现代FL防御。这些发现揭示了当前FL安全框架中的基本漏洞，表明未来的防御必须包含层感知的检测和缓解策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the security vulnerability of federated learning (FL) through a novel Layer Smoothing Attack (LSA) that exploits layer-specific vulnerabilities in neural networks. The study identifies backdoor-critical layers and strategically manipulates them to inject persistent backdoors without being detected by existing defenses. The experiments show that LSA achieves a high backdoor success rate of up to 97% while maintaining model accuracy, effectively bypassing modern FL defenses.</div>
<div class="mono" style="margin-top:8px">本文研究了一种名为Layer Smoothing Attack (LSA)的新颖后门攻击，该攻击利用联邦学习（FL）中的层特定漏洞植入持久后门。LSA通过系统性的Layer Substitution Analysis识别关键后门层（BC层），并操纵这些层以绕过最先进的防御机制。实验表明，LSA的后门成功率高达97%，同时保持模型在主要任务上的准确性，揭示了当前FL安全框架中的根本性漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames</div>
<div class="meta-line">Authors: Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi, Dhruv Shah, Aviral Kumar</div>
<div class="meta-line">First: 2026-02-16T18:49:56+00:00 · Latest: 2026-02-16T18:49:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15010v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BPP：通过关注关键历史帧进行长上下文机器人模仿学习</div>
<div class="mono" style="margin-top:8px">许多机器人的任务需要关注过去的观察历史。例如，在房间里寻找物品需要记住已经搜索过的地方。然而，表现最好的机器人策略通常仅依赖于当前的观察，限制了它们在这些任务中的应用。简单地依赖过去的观察往往由于虚假相关性而失败：策略会抓住训练历史中的偶然特征，这些特征在部署到新的分布时无法泛化。我们分析了为什么策略会抓住这些虚假相关性，并发现这个问题源于训练过程中对可能历史的覆盖有限，随着时间范围的增加而呈指数增长。现有的正则化技术在不同任务中提供的益处不一致，因为它们没有从根本上解决这个问题。受这些发现的启发，我们提出了大图策略（BPP），该方法通过视觉-语言模型检测到的有意义的关键帧进行条件化。通过将多样化的演示投影到一组与任务相关的事件上，BPP显著减少了训练和部署之间的分布偏移，而不会牺牲表达能力。我们在四个具有挑战性的现实世界操作任务和三个模拟任务上评估了BPP，所有任务都需要历史条件化。BPP在现实世界评估中的成功率比最佳对比方法高70%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of robot imitation learning that requires considering past observations. It proposes Big Picture Policies (BPP), which condition on key historical frames identified by a vision-language model, to mitigate the issue of spurious correlations. BPP significantly improves success rates in real-world manipulation tasks, achieving 70% higher success compared to the best existing methods.</div>
<div class="mono" style="margin-top:8px">本文旨在通过关注关键历史帧来解决机器人的模仿学习问题，以提高机器人对过去观察的记忆能力，这对于在房间中寻找物品等任务至关重要。作者提出了一种大图策略（BPP），该策略基于视觉-语言模型检测到的有意义的关键帧进行条件判断。这种方法在训练和部署之间减少了分布差异，从而在真实世界的操作任务中取得了显著更高的成功率，比现有方法高出70%。BPP在真实世界评估中的表现证明了其在处理长历史任务方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI</div>
<div class="meta-line">Authors: En Yu, Haoran Lv, Jianjian Sun, Kangheng Lin, Ruitao Zhang, Yukang Shi, Yuyang Chen, Ze Chen, Ziheng Zhang, Fan Jia, Kaixin Liu, Meng Zhang, Ruitao Hao, Saike Huang, Songhan Xie, Yu Liu, Zhao Wu, Bin Xie, Pengwei Zhang, Qi Yang, Xianchi Deng, Yunfei Wei, Enwen Zhang, Hongyang Peng, Jie Zhao, Kai Liu, Wei Sun, Yajun Wei, Yi Yang, Yunqiao Zhang, Ziwei Yan, Haitao Yang, Hao Liu, Haoqiang Fan, Haowei Zhang, Junwen Huang, Yang Chen, Yunchao Ma, Yunhuan Yang, Zhengyuan Du, Ziming Liu, Jiahui Niu, Yucheng Zhao, Daxin Jiang, Wenbin Tang, Xiangyu Zhang, Zheng Ge, Erjin Zhou, Tiancai Wang</div>
<div class="meta-line">First: 2026-02-16T17:59:16+00:00 · Latest: 2026-02-16T17:59:16+00:00</div>
<div class="meta-line">Comments: Authors are listed in alphabetical order. Code is available at https://github.com/Dexmal/dexbotic</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14974v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14974v1">PDF</a> · <a href="https://github.com/Dexmal/dexbotic">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DM0：面向物理AI的具身原生视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">超越传统将互联网预训练模型适应物理任务的范式，我们提出了DM0，一种面向物理AI的具身原生视觉-语言-行动（VLA）框架。与将物理接地视为微调后的附带考虑不同，DM0从一开始就通过从异构数据源中学习来统一具身操作和导航。我们的方法遵循一个全面的三阶段管道：预训练、中期训练和后期训练。首先，我们使用多种语料库对视觉-语言模型（VLM）进行大规模统一预训练——无缝地整合了网络文本、自动驾驶场景和具身交互日志，以共同获取语义知识和物理先验。随后，我们基于VLM构建了一个流匹配行动专家。为了协调高层次推理与低层次控制，DM0采用了一种混合训练策略：对于具身数据，行动专家的梯度不反向传播到VLM以保持泛化表示，而VLM仍然可以在非具身数据上进行训练。此外，我们引入了具身空间支架策略来构建空间链式思考（CoT）推理，有效地约束了行动解空间。在RoboChallenge基准测试上的实验表明，DM0在Table30的专家和通用设置中均实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DM0 is an Embodied-Native Vision-Language-Action framework for Physical AI, which unifies embodied manipulation and navigation through pretraining on diverse data sources. It employs a three-stage pipeline: pretraining, mid-training, and post-training. DM0 uses a hybrid training strategy to preserve generalized representations for embodied data while allowing the Vision-Language Model to be trained on non-embodied data. The model introduces an Embodied Spatial Scaffolding strategy to constrain the action solution space. Experiments show that DM0 outperforms existing methods on the RoboChallenge benchmark in both Specialist and Generalist settings on Table30.</div>
<div class="mono" style="margin-top:8px">DM0 是一个用于物理人工智能的嵌入式视觉-语言-行动框架，通过在多种数据源上进行预训练来统一嵌入式操作和导航。它采用三阶段管道：预训练、中期训练和后期训练。DM0 使用混合训练策略来保留嵌入式数据的泛化表示，同时允许视觉-语言模型在非嵌入式数据上进行训练。该模型引入了嵌入式空间支架策略来约束行动解空间。实验表明，DM0 在 RoboChallenge 基准测试中的 Specialist 和 Generalist 设置下的 Table30 上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Language Movement Primitives: Grounding Language Models in Robot Motion</div>
<div class="meta-line">Authors: Yinlong Dai, Benjamin A. Christie, Daniel J. Evans, Dylan P. Losey, Simon Stepputtis</div>
<div class="meta-line">First: 2026-02-02T21:41:08+00:00 · Latest: 2026-02-16T16:41:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02839v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02839v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling robots to perform novel manipulation tasks from natural language instructions remains a fundamental challenge in robotics, despite significant progress in generalized problem solving with foundational models. Large vision and language models (VLMs) are capable of processing high-dimensional input data for visual scene and language understanding, as well as decomposing tasks into a sequence of logical steps; however, they struggle to ground those steps in embodied robot motion. On the other hand, robotics foundation models output action commands, but require in-domain fine-tuning or experience before they are able to perform novel tasks successfully. At its core, there still remains the fundamental challenge of connecting abstract task reasoning with low-level motion control. To address this disconnect, we propose Language Movement Primitives (LMPs), a framework that grounds VLM reasoning in Dynamic Movement Primitive (DMP) parameterization. Our key insight is that DMPs provide a small number of interpretable parameters, and VLMs can set these parameters to specify diverse, continuous, and stable trajectories. Put another way: VLMs can reason over free-form natural language task descriptions, and semantically ground their desired motions into DMPs -- bridging the gap between high-level task reasoning and low-level position and velocity control. Building on this combination of VLMs and DMPs, we formulate our LMP pipeline for zero-shot robot manipulation that effectively completes tabletop manipulation problems by generating a sequence of DMP motions. Across 20 real-world manipulation tasks, we show that LMP achieves 80% task success as compared to 31% for the best-performing baseline. See videos at our website: https://collab.me.vt.edu/lmp</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言运动基元：将语言模型与机器人运动相结合</div>
<div class="mono" style="margin-top:8px">尽管基础模型在通用问题解决方面取得了显著进展，使机器人能够从自然语言指令中执行新的操作任务仍然是机器人技术中的一个基本挑战。大型视觉和语言模型（VLMs）能够处理高维输入数据以进行视觉场景和语言理解，并将任务分解为一系列逻辑步骤；然而，它们难以将这些步骤与实体化的机器人运动联系起来。另一方面，机器人基础模型输出动作命令，但在具备领域内微调或经验之前，它们无法成功执行新的任务。归根结底，仍然存在将抽象的任务推理与低级运动控制连接起来的基本挑战。为了解决这一断层，我们提出了语言运动基元（LMPs）框架，该框架将VLM推理与动态运动基元（DMP）参数化相结合。我们的核心见解是，DMPs提供了少量可解释的参数，而VLMs可以设置这些参数以指定多样、连续和稳定的轨迹。换句话说：VLMs可以对自由形式的自然语言任务描述进行推理，并将它们所期望的运动语义地映射到DMPs——弥合了高层任务推理与低级位置和速度控制之间的差距。基于这种VLMs和DMPs的结合，我们提出了LMP流水线，用于零样本机器人操作，通过生成一系列DMP运动来有效完成桌面操作问题。在20项真实世界的操作任务中，我们展示了LMP实现了80%的任务成功率，而最佳基线仅为31%。请参见我们网站上的视频：https://collab.me.vt.edu/lmp</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of enabling robots to perform novel manipulation tasks from natural language instructions. It proposes Language Movement Primitives (LMPs), which ground large vision and language models (VLMs) in Dynamic Movement Primitive (DMP) parameterization. The key finding is that LMPs achieve 80% task success across 20 real-world manipulation tasks, significantly outperforming the best baseline which achieved 31%. This approach bridges the gap between high-level task reasoning and low-level motion control by allowing VLMs to specify DMP parameters for diverse and stable trajectories.</div>
<div class="mono" style="margin-top:8px">研究旨在通过弥合高级任务推理与低级运动控制之间的差距，使机器人能够根据自然语言指令执行新的操作任务。方法是使用语言运动基元（LMPs），将大型视觉和语言模型（VLMs）嵌入到动态运动基元（DMP）参数化中。关键实验结果表明，LMP在20项真实世界的操作任务中实现了80%的任务成功率，远高于最佳基线的31%成功率。LMP框架能够生成一系列DMP运动来完成桌面操作问题。请参见https://collab.me.vt.edu/lmp 查看演示视频。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning</div>
<div class="meta-line">Authors: Sunghwan Kim, Woojeh Chung, Zhirui Dai, Dwait Bhatt, Arth Shukla, Hao Su, Yulun Tian, Nikolay Atanasov</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-10-04T17:40:53+00:00 · Latest: 2026-02-16T16:14:48+00:00</div>
<div class="meta-line">Comments: ICRA 2026, project page: https://existentialrobotics.org/sbp_page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03885v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03885v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we demonstrate that mobile manipulation policies utilizing a 3D latent map achieve stronger spatial and temporal reasoning than policies relying solely on images. We introduce Seeing the Bigger Picture (SBP), an end-to-end policy learning approach that operates directly on a 3D map of latent features. In SBP, the map extends perception beyond the robot&#x27;s current field of view and aggregates observations over long horizons. Our mapping approach incrementally fuses multiview observations into a grid of scene-specific latent features. A pre-trained, scene-agnostic decoder reconstructs target embeddings from these features and enables online optimization of the map features during task execution. A policy, trainable with behavior cloning or reinforcement learning, treats the latent map as a state variable and uses global context from the map obtained via a 3D feature aggregator. We evaluate SBP on scene-level mobile manipulation and sequential tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons globally over the scene, (ii) leverages the map as long-horizon memory, and (iii) outperforms image-based policies in both in-distribution and novel scenes, e.g., improving the success rate by 15% for the sequential manipulation task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从全局视角出发：用于移动操作策略学习的3D潜在映射</div>
<div class="mono" style="margin-top:8px">在本文中，我们展示了利用3D潜在映射的移动操作策略在空间和时间推理方面比仅依赖图像的策略更强。我们引入了全局视角（SBP），这是一种端到端的策略学习方法，直接在3D潜在特征图上操作。在SBP中，地图扩展了感知范围，超越了机器人的当前视野，并在长时间范围内聚合观察结果。我们的映射方法将多视图观察结果逐步融合到场景特定的潜在特征网格中。一个预先训练的、场景无关的解码器从这些特征中重建目标嵌入，并在任务执行期间使地图特征在线优化成为可能。一个策略，通过行为克隆或强化学习训练，将潜在映射视为状态变量，并使用通过3D特征聚合器获得的地图中的全局上下文。我们在场景级移动操作和序列式桌面操作任务上评估了SBP。我们的实验表明，SBP (i) 在整个场景上进行全局推理，(ii) 利用地图作为长时记忆，并且 (iii) 在分布内和新场景中都优于基于图像的策略，例如，在序列操作任务中将成功率提高了15%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces SBP, an end-to-end policy learning approach for mobile manipulation that uses a 3D latent map to enhance spatial and temporal reasoning. The method incrementally fuses multiview observations into a grid of latent features, with a pre-trained decoder reconstructing target embeddings. The policy treats the latent map as a state variable and uses global context from the map for task execution. Experiments show that SBP outperforms image-based policies, reasoning globally over the scene and leveraging the map as long-horizon memory, with a 15% improvement in success rate for sequential manipulation tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用3D潜在地图来增强移动操作策略，以实现更好的空间和时间推理。方法Seeing the Bigger Picture (SBP)涉及一种端到端的策略学习方法，该方法直接操作3D潜在特征图，将感知范围扩展到机器人当前视野之外，并在长时间内聚合观察结果。关键发现表明，SBP 提高了全局场景推理能力，使用地图作为长期记忆，并在场景内和新场景中均优于基于图像的策略，例如在顺序操作任务中的成功率提高了15%。</div>
</details>
</div>
<div class="card">
<div class="title">Affordance Transfer Across Object Instances via Semantically Anchored Functional Map</div>
<div class="meta-line">Authors: Xiaoxiang Dong, Weiming Zhi</div>
<div class="meta-line">First: 2026-02-16T16:04:47+00:00 · Latest: 2026-02-16T16:04:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14874v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14874v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语义锚定的功能映射在对象实例间转移功能</div>
<div class="mono" style="margin-top:8px">传统的学习从演示（LfD）通常需要繁琐的物理演示收集，这既耗时又难以扩展。最近的进展表明，机器人可以通过提取交互提示来从人类视频中学习，而无需直接参与机器人操作。然而，一个基本的挑战仍然存在：如何在不同功能相似但几何结构显著不同的对象实例之间泛化演示的交互。在本文中，我们提出了语义锚定的功能映射（SemFM），这是一种从单个视觉演示中在对象间转移功能的框架。从图像中重建的粗略网格开始，我们的方法识别出对象间的语义对应的功能区域，选择互斥的语义锚点，并使用功能映射在表面上传播这些约束，以获得密集且语义一致的对应关系。这使得演示的交互区域能够在几何结构多样的对象间以轻量且可解释的方式转移。在合成对象类别和实际机器人操作任务上的实验表明，我们的方法能够以适度的计算成本实现准确的功能转移，使其非常适合实际的机器人感知到行动管道。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of transferring affordances across different object instances by proposing Semantic Anchored Functional Maps (SemFM). Starting from a coarse mesh, the method identifies semantically corresponding functional regions, selects mutually exclusive semantic anchors, and propagates these constraints to obtain a dense, semantically consistent correspondence. Experiments demonstrate that this approach enables accurate affordance transfer with low computational cost, suitable for practical robotic manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了一种语义锚定功能映射(SemFM)方法，以解决不同物体实例间转移功能的问题。从粗略的网格开始，该方法识别出语义对应的功能区域，选择锚点并传播这些约束，以实现密集且语义一致的对应关系。实验结果表明，该方法能够以较低的计算成本实现准确的功能转移，适用于实际的机器人感知到行动管道。</div>
</details>
</div>
<div class="card">
<div class="title">EmbeWebAgent: Embedding Web Agents into Any Customized UI</div>
<div class="meta-line">Authors: Chenyang Ma, Clyde Fare, Matthew Wilson, Dave Braines</div>
<div class="meta-line">First: 2026-02-16T15:59:56+00:00 · Latest: 2026-02-16T15:59:56+00:00</div>
<div class="meta-line">Comments: Technical Report; Live Demo: https://youtu.be/Cy06Ljee1JQ</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14865v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14865v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EmbeWebAgent: 将网络代理嵌入到任何自定义UI中</div>
<div class="mono" style="margin-top:8px">大多数网络代理在人类界面级别运行，观察屏幕截图或原始DOM树，而没有应用程序级别的访问权限，这限制了其稳健性和动作表达能力。然而，在企业环境中，可以显式控制前端和后端。我们提出了EmbeWebAgent框架，该框架使用轻量级前端钩子（经过筛选的ARIA和基于URL的观察，以及通过WebSocket暴露的每页函数注册表）直接嵌入现有UI中，并使用可重用的后端工作流进行推理和执行操作。EmbeWebAgent对底层技术栈（例如React或Angular）无依赖性，支持从GUI原语到更高层次复合体的混合粒度动作，并通过MCP工具协调导航、操作和领域特定分析。我们的演示显示了最小的重制努力和基于实时UI的稳健多步骤行为。在线演示：https://youtu.be/Cy06Ljee1JQ</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EmbeWebAgent is designed to enhance the robustness and expressiveness of web agents by embedding them directly into existing user interfaces. It uses lightweight frontend hooks and a WebSocket-based backend workflow to control both frontend and backend elements. The framework supports various actions from GUI primitives to complex composites and can navigate, manipulate, and analyze data in enterprise settings. The demo demonstrates minimal retrofitting and robust multi-step behaviors in a live UI setting.</div>
<div class="mono" style="margin-top:8px">EmbeWebAgent通过直接嵌入现有UI来解决网页代理的局限性，提供更强的鲁棒性和动作表达能力。它使用轻量级的前端钩子和基于WebSocket的后端工作流来控制前端和后端。该框架支持多种操作，并可以协调导航和领域特定的分析。演示展示了在实时UI环境中实现最小的重制努力和稳健的多步骤行为。</div>
</details>
</div>
<div class="card">
<div class="title">An Agentic Operationalization of DISARM for FIMI Investigation on Social Media</div>
<div class="meta-line">Authors: Kevin Tseng, Juan Carlos Toledano, Bart De Clerck, Yuliia Dukach, Phil Tinn</div>
<div class="meta-line">First: 2026-01-21T15:50:13+00:00 · Latest: 2026-02-16T14:35:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15109v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15109v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The interoperability of data and intelligence across allied partners and their respective end-user groups is considered a foundational enabler of the collective defense capability -- both conventional and hybrid -- of NATO countries. Foreign Information Manipulation and Interference (FIMI) and related hybrid activities are conducted across various societal dimensions and infospheres, posing an ever greater challenge to threat characterization, sustained situational awareness, and response coordination. Recent advances in AI have further reduced the cost of AI-augmented trolling and interference activities, such as through the generation and amplification of manipulative content. Despite the introduction of the DISARM framework as a standardized metadata and analytical framework for FIMI, operationalizing it at the scale of social media remains a challenge. We propose a framework-agnostic, agent-based operationalization of DISARM to investigate FIMI on social media. We develop an agent coordination pipeline in which specialized agentic AI components collaboratively (1) detect candidate manipulative behaviors and (2) map these behaviors onto standard DISARM taxonomies in a transparent manner. We evaluate the approach on two real-world datasets annotated by domain practitioners. Our results show that the approach is effective in scaling the predominantly manual and heavily interpretive work of FIMI analysis -- including uncovering more than 30 previously undetected Russian bot accounts during manual analysis -- and provides a direct contribution to enhancing situational awareness and data interoperability in the context of operating in media- and information-rich settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DISARM的代理化操作及其在社交媒体FIMI调查中的应用</div>
<div class="mono" style="margin-top:8px">跨盟友伙伴及其各自的最终用户群体的数据和情报互操作性被认为是北约国家集体防御能力——无论是传统还是混合——的基础性使能器。外国信息操纵和干扰（FIMI）及相关混合活动在各种社会维度和信息空间中进行，对威胁特征分析、持续态势感知和响应协调构成了越来越大的挑战。最近的人工智能进展进一步降低了增强型人工智能辅助骚扰和干扰活动的成本，例如通过生成和放大操纵性内容。尽管已经引入了DISARM框架作为标准化的元数据和分析框架来应对FIMI，但在社交媒体上实现这一框架仍然存在挑战。我们提出了一种框架无关的基于代理的DISARM操作化方法，以调查社交媒体上的FIMI。我们开发了一个代理协调管道，其中专门的代理AI组件协作地（1）检测候选操纵行为，并（2）以透明的方式将这些行为映射到标准的DISARM分类学中。我们通过两个由领域专家注释的真实数据集评估了该方法。我们的结果表明，该方法在扩展FIMI分析的大部分手动和高度解释性工作方面是有效的——包括在手动分析中发现超过30个之前未被检测到的俄罗斯机器人账户——并且直接为在媒体和信息丰富环境中操作时增强态势感知和数据互操作性做出了贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the interoperability of data and intelligence for NATO&#x27;s collective defense against FIMI activities. It proposes an agent-based operationalization of the DISARM framework to automate the detection and mapping of manipulative behaviors on social media. The approach uses specialized AI components to collaboratively identify and categorize manipulative content, demonstrating effectiveness in scaling manual analysis and uncovering previously undetected Russian bot accounts.</div>
<div class="mono" style="margin-top:8px">研究旨在通过增强数据和情报的互操作性来提升北约在对抗FIMI活动中的集体防御能力。它提出了一种基于代理的DISARM框架操作化方法，以自动化检测和分类社交媒体上的操纵行为。该方法使用专门的AI组件协作识别和分类操纵内容，展示了在扩大手动分析规模方面的有效性，并发现了之前未被发现的俄罗斯机器人账号。</div>
</details>
</div>
<div class="card">
<div class="title">Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments</div>
<div class="meta-line">Authors: Ruofeng Wei, Kai Chen, Yui Lun Ng, Yiyao Ma, Justin Di-Lang Ho, Hon Sing Tong, Xiaomei Wang, Jing Dai, Ka-Wai Kwok, Qi Dou</div>
<div class="meta-line">First: 2026-02-16T11:46:14+00:00 · Latest: 2026-02-16T11:46:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14666v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实时单目2D和3D内腔场景感知以控制柔性内窥镜器械</div>
<div class="mono" style="margin-top:8px">内腔手术为早期胃肠道和泌尿道癌症提供了微创手术选项，但受限于手术工具和陡峭的学习曲线。机器人系统，尤其是连续体机器人，提供了灵活的器械，能够实现精确的组织切除，可能改善手术结果。本文介绍了连续体机器人系统在内腔手术中的视觉感知平台。我们的目标是利用基于单目内窥镜图像的感知算法来识别柔性器械的位置和方向，并测量它们与组织的距离。我们引入了基于学习的2D和3D感知算法，并开发了一个物理上逼真的模拟器，该模拟器模拟了柔性器械的动力学。该模拟器生成了逼真的内腔场景，使柔性机器人的控制和大量数据收集成为可能。使用连续体机器人原型，我们进行了模块级和系统级评估。结果表明，我们的算法提高了柔性器械的控制能力，轨迹跟随任务的操纵时间减少了70%以上，并增强了对手术场景的理解，从而实现了稳健的内腔手术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve endoluminal surgery by developing a visual perception platform for continuum robotic systems. The method involves using monocular image-based algorithms to identify the position and orientation of flexible instruments and measure their distances from tissues. Key findings include a reduction in manipulation time by over 70% for trajectory-following tasks and enhanced understanding of surgical scenarios, leading to more robust endoluminal surgeries.</div>
<div class="mono" style="margin-top:8px">研究旨在通过开发连续机器人系统的视觉感知平台来改善内镜手术。方法是使用单目内窥镜图像算法来识别柔性器械的位置和方向，并测量它们与组织的距离。关键发现包括轨迹跟随任务的操纵时间减少了超过70%，并且增强了对手术场景的理解，从而实现了更稳健的内镜手术。</div>
</details>
</div>
<div class="card">
<div class="title">BoundPlanner: A convex-set-based approach to bounded manipulator trajectory planning</div>
<div class="meta-line">Authors: Thies Oelerich, Christian Hartl-Nesic, Florian Beck, Andreas Kugi</div>
<div class="meta-line">First: 2025-02-18T21:16:11+00:00 · Latest: 2026-02-16T10:41:24+00:00</div>
<div class="meta-line">Comments: Published at RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.13286v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.13286v2">PDF</a> · <a href="http://github.com/TU-Wien-ACIN-CDS/BoundPlanner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online trajectory planning enables robot manipulators to react quickly to changing environments or tasks. Many robot trajectory planners exist for known environments but are often too slow for online computations. Current methods in online trajectory planning do not find suitable trajectories in challenging scenarios that respect the limits of the robot and account for collisions. This work proposes a trajectory planning framework consisting of the novel Cartesian path planner based on convex sets, called BoundPlanner, and the online trajectory planner BoundMPC. BoundPlanner explores and maps the collision-free space using convex sets to compute a reference path with bounds. BoundMPC is extended in this work to handle convex sets for path deviations, which allows the robot to optimally follow the path within the bounds while accounting for the robot&#x27;s kinematics. Collisions of the robot&#x27;s kinematic chain are considered by a novel convex-set-based collision avoidance formulation independent on the number of obstacles. Simulations and experiments with a 7-DoF manipulator show the performance of the proposed planner compared to state-of-the-art methods. The source code is available at github.com/TU-Wien-ACIN-CDS/BoundPlanner and videos of the experiments can be found at www.acin.tuwien.ac.at/42d4.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BoundPlanner：基于凸集的方法进行有界机械臂轨迹规划</div>
<div class="mono" style="margin-top:8px">在线轨迹规划使机器人机械臂能够快速应对变化的环境或任务。已存在许多针对已知环境的机器人轨迹规划器，但通常速度过慢，无法进行在线计算。当前的在线轨迹规划方法在具有挑战性的场景中无法找到符合机器人限制且考虑碰撞的合适轨迹。本研究提出了一种轨迹规划框架，包括基于凸集的新颖笛卡尔路径规划器BoundPlanner和在线轨迹规划器BoundMPC。BoundPlanner使用凸集探索和映射碰撞自由空间，计算具有界限的参考路径。在此工作中扩展了BoundMPC以处理路径偏差的凸集，这使机器人能够在界限内最优地跟随路径，同时考虑机器人的运动学。通过一种新颖的基于凸集的碰撞避免公式考虑了机器人运动链的碰撞，该公式独立于障碍物数量。7-自由度机械臂的仿真和实验显示了所提规划器与现有最佳方法的性能。源代码可在github.com/TU-Wien-ACIN-CDS/BoundPlanner获取，实验视频可在www.acin.tuwien.ac.at/42d4找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">BoundPlanner is a novel trajectory planning framework that uses convex sets to explore and map the collision-free space, enabling quick and safe online trajectory planning for robot manipulators. It consists of BoundPlanner for computing reference paths with bounds and BoundMPC for optimal path following while considering the robot&#x27;s kinematics and collision avoidance. Experiments with a 7-DoF manipulator demonstrate the planner&#x27;s superior performance compared to existing methods in handling challenging scenarios and respecting the robot&#x27;s limits.</div>
<div class="mono" style="margin-top:8px">BoundPlanner 是一种使用凸集探索和映射碰撞自由空间的轨迹规划框架，使机器人 manipulator 能够快速进行在线计算。它包括 BoundPlanner，用于计算带有界限的参考路径，以及 BoundMPC，用于在考虑机器人运动学和碰撞避免的情况下最优地跟随路径。7-DoF 机器人的实验展示了该规划器与现有先进方法相比的性能，显示出更高的效率和碰撞避免能力。</div>
</details>
</div>
<div class="card">
<div class="title">BoundMPC: Cartesian path following with error bounds based on model predictive control in the joint space</div>
<div class="meta-line">Authors: Thies Oelerich, Florian Beck, Christian Hartl-Nesic, Andreas Kugi</div>
<div class="meta-line">First: 2024-01-10T10:30:22+00:00 · Latest: 2026-02-16T10:25:37+00:00</div>
<div class="meta-line">Comments: 17 pages, 20 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.05057v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.05057v2">PDF</a> · <a href="https://github.com/thieso/boundmpc">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces the BoundMPC strategy, an innovative online model-predictive path-following approach for robot manipulators. This joint-space trajectory planner allows the following of Cartesian reference paths in the end-effector&#x27;s position and orientation, including via-points, within the desired asymmetric bounds of the orthogonal path error. These bounds encode the obstacle-free space and additional task-specific constraints in Cartesian space. Contrary to traditional path-following concepts, BoundMPC purposefully deviates from the Cartesian reference path in position and orientation to account for the robot&#x27;s kinematics, leading to more successful task executions for Cartesian reference paths. Furthermore the simple reference path formulation is computationally efficient and allows for replanning during the robot&#x27;s motion. This feature makes it possible to use this planner for dynamically changing environments and varying goals. The flexibility and performance of BoundMPC are experimentally demonstrated by five scenarios on a 7-DoF Kuka LBR iiwa 14 R820 robot. The first scenario shows the transfer of a larger object from a start to a goal pose through a confined space where the object must be tilted. The second scenario deals with grasping an object from a table where the grasping point changes during the robot&#x27;s motion, and collisions with other obstacles in the scene must be avoided. The adaptability of BoundMPC is showcased in scenarios such as the opening of a drawer, the transfer of an open container, and the wiping of a table, where it effectively handles task-specific constraints. The last scenario highlights the possibility of accounting for collisions with the entire robot&#x27;s kinematic chain. The code is readily available at https://github.com/thieso/boundmpc, inspiring you to explore its potential and adapt it to your specific robotic tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BoundMPC：基于关节空间模型预测控制的笛卡尔路径跟踪及其误差边界</div>
<div class="mono" style="margin-top:8px">本文介绍了BoundMPC策略，这是一种创新的在线模型预测路径跟踪方法，适用于机器人操作器。该关节空间轨迹规划器允许末端执行器位置和姿态的笛卡尔参考路径跟踪，包括途径点，同时在期望的正交路径误差的非对称边界内进行。这些边界编码了笛卡尔空间中的无障碍空间和额外的任务特定约束。与传统的路径跟踪概念不同，BoundMPC故意在位置和姿态上偏离笛卡尔参考路径，以考虑机器人的运动学，从而更成功地执行笛卡尔参考路径任务。此外，简单的参考路径公式计算效率高，并允许机器人运动期间重新规划。这一特性使其能够用于动态变化的环境和变化的目标。BoundMPC的灵活性和性能通过在7自由度Kuka LBR iiwa 14 R820机器人上进行的五个场景实验得到了实验验证。第一个场景展示了通过狭窄空间将较大物体从起始位置转移到目标位置的过程，其中物体必须倾斜。第二个场景涉及从桌子上抓取物体，其中抓取点在机器人运动过程中发生变化，必须避免与其他障碍物的碰撞。BoundMPC的适应性在诸如抽屉的打开、开放容器的转移和桌子的擦拭等场景中得到了展示，其中它有效地处理了任务特定的约束。最后一个场景展示了考虑整个机器人运动链碰撞的可能性。代码可以在https://github.com/thieso/boundmpc获取，鼓励您探索其潜力并将其适应到您的特定机器人任务。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Selection as Power: Bounding Decision Authority in Autonomous Agents</div>
<div class="meta-line">Authors: Jose Manuel de la Chica Rodriguez, Juan Manuel Vera Díaz</div>
<div class="meta-line">First: 2026-02-16T10:10:47+00:00 · Latest: 2026-02-16T10:10:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14606v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14606v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous agentic systems are increasingly deployed in regulated, high-stakes domains where decisions may be irreversible and institutionally constrained. Existing safety approaches emphasize alignment, interpretability, or action-level filtering. We argue that these mechanisms are necessary but insufficient because they do not directly govern selection power: the authority to determine which options are generated, surfaced, and framed for decision. We propose a governance architecture that separates cognition, selection, and action into distinct domains and models autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained, while selection and action autonomy are bounded through mechanically enforced primitives operating outside the agent&#x27;s optimization space. The architecture integrates external candidate generation (CEFL), a governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers. We evaluate the system across multiple regulated financial scenarios under adversarial stress targeting variance manipulation, threshold gaming, framing skew, ordering effects, and entropy probing. Metrics quantify selection concentration, narrative diversity, governance activation cost, and failure visibility. Results show that mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines. This work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向权力的选择：约束自主代理的决策权</div>
<div class="mono" style="margin-top:8px">自主代理系统在监管严格、高风险的领域中越来越被部署，这些领域的决策可能是不可逆的且受到机构限制。现有安全方法强调对齐、可解释性或行为级过滤。我们认为，这些机制是必要的，但不充分，因为它们没有直接管理选择权力：决定哪些选项被生成、呈现和框架化以供决策的权力。我们提出了一种治理架构，将认知、选择和行动分离到不同的领域，并将自主性建模为主权向量。认知自主性保持不受约束，而选择和行动自主性则通过机械执行的原语在代理优化空间之外进行限制。该架构整合了外部候选生成（CEFL）、受治理的约简器、提交揭示熵隔离、理由验证和故障时电路断路器。我们在多个受监管的金融场景下评估了该系统，这些场景在对抗性压力下针对方差操纵、阈值游戏、框架偏差、排序效应和熵探测。度量标准量化了选择集中度、叙述多样性、治理激活成本和失败可见性。结果表明，机械选择治理是可实现、可审计的，并防止了确定性结果的捕获，同时保留了推理能力。尽管概率集中仍然存在，但该架构在相对于传统标量管道的自主选择权方面是可测量地受到限制的。这项工作将治理重新定义为有界因果权力，而不是内部意图对齐，为在不可接受的沉默失败领域部署自主代理提供了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for governance in autonomous agents deployed in high-stakes domains, where decisions can be irreversible. It proposes a governance architecture that separates cognition, selection, and action, and models autonomy as a vector of sovereignty. The architecture includes mechanisms like CEFL, a governed reducer, and fail-loud circuit breakers to bound selection power. Experiments across financial scenarios show that this architecture is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity.</div>
<div class="mono" style="margin-top:8px">论文针对高风险领域中部署的自主代理系统提出了治理需求。它提出了一种治理架构，将认知、选择和行动分离，重点在于限制选择权力。该系统使用CEFL、受控减少器和断言电路断路器等机制来确保选择权力受到约束。在金融场景中的实验表明，该架构是可实现、可审计的，并且能够有效防止确定性结果的捕获，同时保持推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models</div>
<div class="meta-line">Authors: Liangzhi Shi, Shuaihang Chen, Feng Gao, Yinuo Chen, Kang Chen, Tonghe Zhang, Hongzhi Zang, Weinan Zhang, Chao Yu, Yu Wang</div>
<div class="meta-line">First: 2026-02-13T05:15:50+00:00 · Latest: 2026-02-16T05:44:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12628v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12628v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越模仿：基于强化学习的仿真实际协同训练方法研究</div>
<div class="mono" style="margin-top:8px">仿真实验提供了一种可扩展且低成本的方法来丰富视觉-语言-动作（VLA）训练，减少了对昂贵的真实机器人演示的依赖。然而，大多数仿真实际协同训练方法依赖于监督微调（SFT），将仿真视为静态的演示来源，并未充分利用大规模闭环交互。因此，实际世界中的收益和泛化能力往往受到限制。在本文中，我们提出了一种基于\underline{\textit{RL}}的仿真实际\underline{\textit{Co}}-训练（RL-Co）框架，该框架利用交互仿真同时保留实际世界的能力。我们的方法遵循通用的两阶段设计：首先使用混合的真实和仿真演示进行监督微调（SFT）预热策略，然后在仿真中使用强化学习进行微调，同时在实际数据上添加辅助的监督损失以锚定策略并减轻灾难性遗忘。我们在四种实际桌面上的操纵任务上使用两种代表性VLA架构OpenVLA和$π_{0.5}$评估了我们的框架，并观察到与仅使用实际数据微调和基于SFT的协同训练相比的一致改进，包括OpenVLA的成功率提高了24%，$π_{0.5}$提高了20%。除了更高的成功率，强化学习协同训练还提供了更强的对未见过的任务变化的泛化能力和显著提高的实际数据效率，为利用仿真增强实际机器人部署提供了一种实用且可扩展的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current sim-real co-training methods by proposing an RL-based framework that combines interactive simulation with real-world capabilities. The method uses a two-stage design, starting with supervised fine-tuning on a mix of real and simulated data, followed by reinforcement learning in simulation with an auxiliary supervised loss on real data to prevent forgetting. Evaluations on four real-world tabletop manipulation tasks show consistent improvements over real-only fine-tuning and SFT-based co-training, with success rates increasing by 24% on OpenVLA and 20% on $π_{0.5}$. The framework also enhances generalization and real-world data efficiency.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于RL的协同训练框架，以解决当前基于模拟的方法在视觉-语言-动作模型中的局限性。该方法结合了在混合真实和模拟数据上进行的监督微调与在模拟中进行的强化学习，同时引入了辅助监督损失以防止灾难性遗忘。该方法在OpenVLA和$π_{0.5}$上分别实现了24%和20%的成功率提升，并增强了对未见过的任务变体的泛化能力，同时提高了真实世界数据的效率。</div>
</details>
</div>
<div class="card">
<div class="title">A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation</div>
<div class="meta-line">Authors: Steven Oh, Tomoya Takahashi, Cristian C. Beltran-Hernandez, Yuki Kuroda, Masashi Hamaya</div>
<div class="meta-line">First: 2026-02-16T03:45:04+00:00 · Latest: 2026-02-16T03:45:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14434v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14434v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://project-page-manager.github.io/CLAW/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有各向异性可选刚度的柔软手腕用于接触丰富操作中的鲁棒机器人学习</div>
<div class="mono" style="margin-top:8px">在未结构化环境中进行接触丰富操作任务对机器人学习提出了显著的鲁棒性挑战，其中意外碰撞可能导致损坏并妨碍策略获取。现有软末端执行器面临根本性限制：它们要么提供有限的变形范围，要么缺乏方向刚度控制，要么需要复杂的执行系统，这会损害其实用性。本研究引入了CLAW（Compliant Leaf-spring Anisotropic soft Wrist），这是一种通过使用两个正交的叶片弹簧和具有锁定机制的旋转关节的简单而有效的设计来解决这些限制的新型柔软手腕机制。CLAW 提供了6自由度的大范围变形（40毫米横向，20毫米垂直），具有可调的各向异性刚度，可以在三种不同的模式下调节，同时保持轻巧的结构（330克）和低成本（550美元）。使用模仿学习的实验评估表明，CLAW 在基准的销插入任务中的成功率达到了76%，优于Fin Ray夹爪（43%）和刚性夹爪替代品（36%）。CLAW 成功处理了各种接触丰富的场景，包括具有严格公差的精密装配和精细物体操作，展示了其在接触丰富领域实现鲁棒机器人学习的潜力。项目页面：https://project-page-manager.github.io/CLAW/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the robustness challenges in robot learning for contact-rich manipulation tasks by introducing CLAW, a novel soft wrist mechanism. CLAW combines two orthogonal leaf springs and rotary joints with a locking mechanism to provide large 6-degree-of-freedom deformation, anisotropic stiffness tunable across three modes, and lightweight construction. Experimental results show that CLAW achieves a 76% success rate in peg-insertion tasks, outperforming both the Fin Ray gripper and rigid gripper alternatives. It successfully handles diverse contact-rich scenarios, indicating its potential for robust robot learning in such environments.</div>
<div class="mono" style="margin-top:8px">本研究通过引入CLAW，一种新型软腕机制，解决了机器人在接触丰富环境中学习的鲁棒性挑战。CLAW结合了两个正交的叶片弹簧和旋转关节以及锁定机制，提供了6自由度的大范围变形、可调的各向异性刚度以及轻巧的结构。实验评估显示，CLAW在 peg 插入任务中的成功率达到了76%，优于Fin Ray夹爪和刚性夹爪的替代方案。CLAW能够成功处理各种接触丰富的场景，表明其在接触丰富环境中实现鲁棒机器人学习的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">EigenSafe: A Spectral Framework for Learning-Based Probabilistic Safety Assessment</div>
<div class="meta-line">Authors: Inkyu Jang, Jonghae Park, Sihyun Cho, Chams E. Mballo, Claire J. Tomlin, H. Jin Kim</div>
<div class="meta-line">First: 2025-09-22T13:12:13+00:00 · Latest: 2026-02-16T02:48:48+00:00</div>
<div class="meta-line">Comments: Inkyu Jang and Jonghae Park contributed equally to this work. Project Webpage: https://eigen-safe.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17750v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17750v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eigen-safe.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present EigenSafe, an operator-theoretic framework for safety assessment of learning-enabled stochastic systems. In many robotic applications, the dynamics are inherently stochastic due to factors such as sensing noise and environmental disturbances, and it is challenging for conventional methods such as Hamilton-Jacobi reachability and control barrier functions to provide a well-calibrated safety critic that is tied to the actual safety probability. We derive a linear operator that governs the dynamic programming principle for safety probability, and find that its dominant eigenpair provides critical safety information for both individual state-action pairs and the overall closed-loop system. The proposed framework learns this dominant eigenpair, which can be used to either inform or constrain policy updates. We demonstrate that the learned eigenpair effectively facilitates safe reinforcement learning. Further, we validate its applicability in enhancing the safety of learned policies from imitation learning through robot manipulation experiments using a UR3 robotic arm in a food preparation task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenSafe：一种基于谱的用于学习驱动的随机系统安全评估框架</div>
<div class="mono" style="margin-top:8px">我们提出了EigenSafe，一种操作理论框架，用于评估学习驱动的随机系统安全性。在许多机器人应用中，由于感测噪声和环境干扰等因素，动态过程本质上是随机的，传统的安全评估方法如哈密尔顿-雅可比可达性和控制屏障函数难以提供与实际安全概率紧密相关的校准良好的安全评估。我们推导出一个线性算子，它支配了安全概率的动态规划原理，并发现其主导特征对提供单个状态-动作对和整个闭环系统的关键安全信息至关重要。所提出的框架学习这个主导特征对，可以用于指导或约束策略更新。我们证明了学习到的主导特征对有效地促进了安全强化学习。此外，我们通过使用UR3机器人手臂在食品准备任务中的机器人操作实验验证了其在增强学习策略安全性方面的适用性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
