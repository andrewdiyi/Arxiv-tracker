<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-16 03:38</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260216_0338</div>
    <div class="row"><div class="card">
<div class="title">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</div>
<div class="meta-line">Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</div>
<div class="meta-line">First: 2026-02-12T18:59:59+00:00 · Latest: 2026-02-12T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12281v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &quot;intention-action gap.&#x27;&#x27; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &quot;boot-time compute&quot; and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展验证比扩展策略学习更能有效实现视觉-语言-行动对齐</div>
<div class="mono" style="margin-top:8px">通用机器人长期愿景依赖于它们理解和执行自然语言指令的能力。视觉-语言-行动（VLA）模型在这一目标上取得了显著进展，但它们生成的动作仍然可能与给定的指令不一致。在本文中，我们研究测试时验证作为缩小“意图-行动差距”的手段。我们首先表征了基于指令的执行的测试时扩展定律，证明了同时扩展重述指令的数量和生成动作的数量大大增加了测试时样本多样性，通常比独立扩展每个维度更有效地恢复正确的动作。为了利用这些扩展定律，我们提出了CoVer，一种对比验证器，用于视觉-语言-行动对齐，并展示了我们的架构随着额外计算资源和数据的增加而平滑扩展。然后，我们介绍了“启动时计算”和一个分层验证推理管道，用于VLAs。在部署时，我们的框架从视觉语言模型（VLM）预计算一组多样化的重述指令，反复为每条指令生成动作候选，然后使用验证器选择最优的高层提示和低层动作片段。与在相同数据上扩展策略预训练相比，我们的验证方法在SIMPLER基准测试中获得了22%的同分布改进和13%的异分布改进，在实际实验中进一步提高了45%。在PolaRiS基准测试中，CoVer实现了14%的任务进展和9%的成功率改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores test-time verification as a method to improve alignment between actions and natural language instructions for vision-language-action models. It demonstrates that jointly scaling rephrased instructions and generated actions increases test-time sample diversity, often more efficiently than scaling each dimension independently. The proposed CoVer architecture scales gracefully with additional resources, and the framework precomputes diverse rephrased instructions, generating action candidates and using a verifier to select the optimal prompt and actions. Compared to scaling policy pre-training, the verification approach shows 22% in-distribution and 13% out-of-distribution gains on the SIMPLER benchmark, with further improvements in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</div>
<div class="mono" style="margin-top:8px">本文研究了测试时验证作为提高视觉-语言-动作模型中动作与自然语言指令之间对齐的方法。研究表明，同时扩大重述指令的数量和生成动作的数量可以增加测试时样本多样性，从而更有效地恢复正确的动作。提出的CoVer架构在额外资源下平滑扩展，并且该框架预先计算多样化的重述指令，使用验证器选择最优的动作。与扩大策略预训练相比，在SIMPLER基准上，验证方法在分布内性能上提高了22%，在分布外性能上提高了13%，在实际实验中进一步提高了性能。在PolaRiS基准上，CoVer实现了14%更高的任务进度和9%更高的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Is Online Linear Optimization Sufficient for Strategic Robustness?</div>
<div class="meta-line">Authors: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng</div>
<div class="meta-line">First: 2026-02-12T18:41:55+00:00 · Latest: 2026-02-12T18:41:55+00:00</div>
<div class="meta-line">Comments: 26 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12253v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12253v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller&#x27;s manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.
  In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\sqrt{T \log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\sqrt{T (\log K+\log(T/δ)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the strategic robustness of bidding algorithms in repeated Bayesian first-price auctions. It explores whether simple online linear optimization (OLO) algorithms can achieve both optimal regret and strategic robustness. The main finding is that sublinear linearized regret is sufficient for strategic robustness. The authors provide black-box reductions that convert OLO algorithms into no-regret, strategically robust bidding algorithms, achieving improved regret bounds in both known and unknown value distribution settings.</div>
<div class="mono" style="margin-top:8px">本文研究了在重复的贝叶斯第一价格拍卖中竞价算法的战略稳健性。探讨了是否可以通过简单的在线线性优化（OLO）算法同时实现最优遗憾和战略稳健性。主要发现是，次线性线性化遗憾足以实现战略稳健性。作者提供了将OLO算法转换为无遗憾、战略稳健竞价算法的黑盒转换，实现了在已知和未知价值分布情况下改进的遗憾界。</div>
</details>
</div>
<div class="card">
<div class="title">VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</div>
<div class="meta-line">Authors: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi</div>
<div class="meta-line">First: 2026-02-12T17:46:52+00:00 · Latest: 2026-02-12T17:46:52+00:00</div>
<div class="meta-line">Comments: VIRENA is under active development and currently in use at the University of Zurich, supported by the DIZH Innovation Program: 2nd Founder-Call. This preprint will be updated as new features are released. For the latest version and to inquire about demos or pilot collaborations, contact the authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12207v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA&#x27;s no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRENA：虚拟竞技场，用于研究、教育和民主创新</div>
<div class="mono" style="margin-top:8px">数字平台塑造了人们的交流、讨论和形成观点的方式。由于数据访问受限、现实世界实验的伦理限制以及现有研究工具的局限性，研究这些动态变得越来越困难。VIRENA（虚拟竞技场）是一个平台，它能够在现实社交媒体环境中进行受控实验。多个参与者同时在基于信息流的平台（Instagram、Facebook、Reddit）和即时通讯应用（WhatsApp、Messenger）的现实复制品中互动。由大型语言模型驱动的AI代理与人类一起参与，具有可配置的人设和现实行为。研究人员可以通过无需编程技能的可视化界面操控内容审核方法、预排定刺激内容，并在不同条件下运行实验。VIRENA 使得以前不切实际的研究设计成为可能：在现实社会环境中研究人类与AI的互动、实验性地比较干预措施的效果以及观察群体讨论的展开过程。VIRENA 建立在开源技术之上，确保数据保留在机构控制之下并符合数据保护要求，目前在苏黎世大学使用，并可供试点合作。VIRENA 的无代码界面使其跨学科和跨领域的受控社交媒体模拟变得可行。本文档记录了其设计、架构和功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VIRENA is a platform designed to enable controlled experimentation in realistic social media environments, addressing the challenges of restricted data access and ethical constraints. It allows multiple participants to interact in replicas of social media platforms like Instagram, Facebook, and WhatsApp, with AI agents participating alongside humans. Researchers can manipulate content moderation and run experiments through a no-code visual interface. Key findings include the ability to study human-AI interactions, compare moderation interventions, and observe group deliberation in realistic settings, making previously impractical research designs feasible.</div>
<div class="mono" style="margin-top:8px">VIRENA 是一个平台，旨在通过现实社交媒体环境中的受控实验来解决数据访问受限和伦理约束的问题。它允许参与者在 Instagram、Facebook、Reddit、WhatsApp 和 Messenger 等平台的复制品中互动，同时由大型语言模型驱动的 AI 代理与人类一起参与。研究人员可以通过无代码的可视化界面操纵内容审核并运行实验。关键发现包括能够研究人类与 AI 的互动、实验性地比较审核干预措施以及在现实环境中观察群体讨论，从而使以前不切实际的研究设计成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">Sub--Riemannian boundary value problems for Optimal Geometric Locomotion</div>
<div class="meta-line">Authors: Oliver Gross, Florine Hartwig, Martin Rumpf, Peter Schröder</div>
<div class="meta-line">First: 2026-02-12T17:32:20+00:00 · Latest: 2026-02-12T17:32:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12199v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a geometric model for optimal shape-change-induced motions of slender locomotors, e.g., snakes slithering on sand. In these scenarios, the motion of a body in world coordinates is completely determined by the sequence of shapes it assumes. Specifically, we formulate Lagrangian least-dissipation principles as boundary value problems whose solutions are given by sub-Riemannian geodesics. Notably, our geometric model accounts not only for the energy dissipated by the body&#x27;s displacement through the environment, but also for the energy dissipated by the animal&#x27;s metabolism or a robot&#x27;s actuators to induce shape changes such as bending and stretching, thus capturing overall locomotion efficiency. Our continuous model, together with a consistent time and space discretization, enables numerical computation of sub-Riemannian geodesics for three different types of boundary conditions, i.e., fixing initial and target body, restricting to cyclic motion, or solely prescribing body displacement and orientation. The resulting optimal deformation gaits qualitatively match observed motion trajectories of organisms such as snakes and spermatozoa, as well as known optimality results for low-dimensional systems such as Purcell&#x27;s swimmers. Moreover, being geometrically less rigid than previous frameworks, our model enables new insights into locomotion mechanisms of, e.g., generalized Purcell&#x27;s swimmers. The code is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>子-黎曼边界值问题在最优几何行进中的应用</div>
<div class="mono" style="margin-top:8px">我们提出了一种几何模型，用于最优的形状变化引起的细长行进器的运动，例如蛇在沙地上滑行。在这种情况下，身体在世界坐标中的运动完全由其假设的形状序列决定。具体来说，我们将拉格朗日最小耗散原理表述为边界值问题，其解由子-黎曼测地线给出。值得注意的是，我们的几何模型不仅考虑了身体通过环境移动时消耗的能量，还考虑了动物的新陈代谢或机器人执行器诱导形状变化（如弯曲和拉伸）时消耗的能量，从而捕捉到整体行进效率。我们的连续模型，结合一致的时间和空间离散化，能够计算三种不同类型的边界条件下的子-黎曼测地线，即固定初始和目标身体、限制为循环运动，或仅规定身体位移和方向。由此产生的最优变形步态定性上与蛇和精子等生物观察到的运动轨迹以及低维系统（如普尔塞游动者）已知的最优性结果相符。此外，由于几何上比以前的框架更不僵硬，我们的模型为，例如广义普尔塞游动者的行进机制提供了新的见解。该代码已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a geometric model for optimal shape-change-induced motions of slender locomotors, such as snakes slithering on sand. The model formulates Lagrangian least-dissipation principles as boundary value problems solved by sub-Riemannian geodesics, accounting for both displacement and shape change energy. The model successfully computes optimal deformation gaits that match observed motion trajectories and provides new insights into locomotion mechanisms, surpassing previous frameworks in flexibility. The code is publicly available for further research.</div>
<div class="mono" style="margin-top:8px">本文提出了一种几何模型，用于描述蛇等细长生物在沙地上滑行等形状变化引起的运动。该模型将拉格朗日最小耗散原理表述为由子黎曼测地线解决的边界值问题，同时考虑了位移和形状变化的能量。该模型成功计算了与观察到的运动轨迹相匹配的最优变形步态，并提供了关于运动机制的新见解，比之前的框架更具灵活性。代码已公开，可供进一步研究使用。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-12T16:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图像对话的内省视觉思考</div>
<div class="mono" style="margin-top:8px">当前的大规模视觉-语言模型（LVLMs）通常依赖基于单次视觉编码的文本推理，这往往会导致精细视觉信息的丢失。最近提出的“通过图像思考”试图通过外部工具或代码操作图像来缓解这一限制；然而，由此产生的视觉状态往往缺乏语言语义的充分支撑，影响了跨模态对齐的有效性——特别是在需要在远距离区域或多个图像之间推理视觉语义或几何关系时。为了解决这些挑战，我们提出了一种新的“基于图像的对话”框架，将视觉操作重新构想为语言引导的特征调制。在表达性语言提示的指导下，模型动态地对多个图像区域进行联合重新编码，从而增强了语言推理与视觉状态更新之间的耦合。我们通过ViLaVT实现这一范式，这是一种新型的LVLM，配备了专门设计用于此类交互式视觉推理的动态视觉编码器，并通过结合监督微调和强化学习的两阶段课程训练，以促进有效的推理行为。在八个基准测试中的广泛实验表明，ViLaVT在多个图像和视频基的复杂空间推理任务中取得了显著且一致的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of current large vision-language models (LVLMs) that rely on text-only reasoning and often lose fine-grained visual information. It proposes &#x27;chatting with images&#x27; as a new framework that reframes visual manipulation as language-guided feature modulation. ViLaVT, a novel LVLM equipped with a dynamic vision encoder, is trained using a two-stage curriculum combining supervised fine-tuning and reinforcement learning. Experiments show that ViLaVT improves performance, especially in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">论文针对当前大型视觉-语言模型（LVLM）依赖于纯文本推理的局限性，提出了一种新的框架‘与图像对话’来增强视觉操作。该框架通过语言提示指导对多个图像区域的动态特征调制，提高跨模态对齐。模型ViLaVT采用结合监督微调和强化学习的两阶段课程进行训练。实验表明，ViLaVT在复杂多图像和基于视频的空间推理任务中表现出色，优于现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Multi Graph Search for High-Dimensional Robot Motion Planning</div>
<div class="meta-line">Authors: Itamar Mishani, Maxim Likhachev</div>
<div class="meta-line">First: 2026-02-12T15:50:15+00:00 · Latest: 2026-02-12T15:50:15+00:00</div>
<div class="meta-line">Comments: Submitted for Publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12096v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12096v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://multi-graph-search.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often come at the cost of generating unpredictable, inconsistent motions or requiring excessive computational resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based motion planning algorithm that generalizes classical unidirectional and bidirectional search to a multi-graph setting. MGS maintains and incrementally expands multiple implicit graphs over the state space, focusing exploration on high-potential regions while allowing initially disconnected subgraphs to be merged through feasible transitions as the search progresses. We prove that MGS is complete and bounded-suboptimal, and empirically demonstrate its effectiveness on a range of manipulation and mobile manipulation tasks. Demonstrations, benchmarks and code are available at https://multi-graph-search.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高维机器人运动规划的多图搜索</div>
<div class="mono" style="margin-top:8px">高维机器人系统（如机械臂和移动机械臂）的高效运动规划对于实时操作和可靠部署至关重要。尽管在规划算法方面的进展提高了对高维状态空间的扩展性，但这些改进往往伴随着生成不可预测、不一致的运动或需要大量计算资源和内存的问题。在本文中，我们引入了多图搜索（MGS），这是一种基于搜索的运动规划算法，将经典的单向和双向搜索推广到多图设置。MGS 维护并逐步扩展状态空间中的多个隐式图，专注于探索高潜力区域，并允许初始不相连的子图通过可行的过渡在搜索过程中合并。我们证明了MGS是完备的，并且在一系列操作和移动操作任务上展示了其有效性。演示、基准测试和代码可在 https://multi-graph-search.github.io/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the efficiency of motion planning for high-dimensional robotic systems to support real-time operation. The Multi-Graph Search (MGS) method is introduced, which generalizes classical unidirectional and bidirectional search to a multi-graph setting. MGS maintains multiple implicit graphs and focuses exploration on high-potential regions, allowing initially disconnected subgraphs to merge through feasible transitions. Experiments show MGS is effective in various manipulation and mobile manipulation tasks, being complete and bounded-suboptimal.</div>
<div class="mono" style="margin-top:8px">本文针对高维机器人系统的高效运动规划问题，提出了一种新的基于搜索的算法Multi-Graph Search (MGS)。MGS将经典的单向和双向搜索推广到多图设置，维护并扩展多个隐式图，以聚焦于高潜力区域的探索。该算法被证明是完备的且具有有界次优性，并通过实验证明其在各种操作和移动操作任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations</div>
<div class="meta-line">Authors: Ruiqian Nai, Boyuan Zheng, Junming Zhao, Haodong Zhu, Sicong Dai, Zunhao Chen, Yihang Hu, Yingdong Hu, Tong Zhang, Chuan Wen, Yang Gao</div>
<div class="meta-line">First: 2026-02-06T12:10:47+00:00 · Latest: 2026-02-12T15:32:00+00:00</div>
<div class="meta-line">Comments: Website: https://humanoid-manipulation-interface.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06643v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06643v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://humanoid-manipulation-interface.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人操作界面：基于无机器人演示的类人全身操作</div>
<div class="mono" style="margin-top:8px">当前的类人全身操作方法主要依赖于遥操作或视觉模拟到现实的强化学习，受到硬件物流和复杂奖励工程的限制。因此，展示的自主技能受到限制，通常仅限于受控环境。在本文中，我们提出了类人操作界面（HuMI），这是一种便携且高效的框架，用于在各种环境中学习多种全身操作任务。HuMI 通过使用便携式硬件捕获丰富的全身运动来实现无机器人数据收集。这些数据驱动了一个分层学习管道，将人类动作转化为灵巧且可行的类人技能。在五个全身任务（包括跪下、蹲下、投掷、行走和双臂操作）的广泛实验中，HuMI 的数据收集效率提高了3倍，并在未见过的环境中达到了70%的成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current approaches for humanoid whole-body manipulation, such as teleoperation and visual sim-to-real reinforcement learning, which are constrained by hardware and complex reward engineering. The authors introduce the Humanoid Manipulation Interface (HuMI), a portable framework that captures rich whole-body motion data without the need for a robot. This data is used to train a hierarchical learning pipeline that converts human motions into dexterous humanoid skills. Experiments across five tasks show that HuMI increases data collection efficiency by 3x and achieves a 70% success rate in new environments.</div>
<div class="mono" style="margin-top:8px">研究解决了当前人形机器人操作方法的限制，如遥操作和视觉模拟到现实的强化学习，这些方法受限于硬件和复杂的奖励工程。提出了人形操作界面（HuMI），这是一种便携式框架，无需机器人即可收集丰富的全身运动数据。这些数据用于层次化的学习管道，将人类动作转化为灵巧的人形技能。在五个任务中的实验显示，数据收集效率提高了3倍，并且在新环境中成功率达到70%。</div>
</details>
</div>
<div class="card">
<div class="title">VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model</div>
<div class="meta-line">Authors: Yanjiang Guo, Tony Lee, Lucy Xiaoyang Shi, Jianyu Chen, Percy Liang, Chelsea Finn</div>
<div class="meta-line">First: 2026-02-12T15:21:47+00:00 · Latest: 2026-02-12T15:21:47+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/vla-w">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLAW：视觉-语言-动作策略与世界模型的迭代联合改进</div>
<div class="mono" style="margin-top:8px">本文的目标是通过迭代在线交互来提高视觉-语言-动作（VLA）模型的性能和可靠性。由于在现实世界中收集策略回放数据成本高昂，我们研究是否可以使用一个学习到的模拟器——特别是动作条件下的视频生成模型——来生成额外的回放数据。不幸的是，现有的世界模型缺乏用于策略改进所需的物理精度：它们主要在缺乏多种不同物理交互（特别是失败案例）覆盖的数据集上进行训练，并且难以准确模拟接触丰富的物体操作中的细微但关键的物理细节。我们提出了一种简单的迭代改进算法，使用现实世界的回放数据来提高世界模型的精度，然后可以使用该模型生成补充的合成数据以改进VLA模型。在我们对真实机器人进行的实验中，我们使用这种方法来提高最先进的VLA模型在多个下游任务上的性能。与基线策略相比，我们实现了39.2%的绝对成功率提升，使用生成的合成回放数据训练则实现了11.6%的提升。有关视频，请访问此匿名网站：https://sites.google.com/view/vla-w</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to enhance the performance and reliability of vision-language-action models through iterative online interaction. To address the high cost of collecting real-world policy rollouts, the authors propose using a learned simulator, specifically an action-conditioned video generation model, to generate additional data. The proposed method iteratively improves the world model&#x27;s fidelity with real-world rollouts, which in turn generates synthetic data to further enhance the VLA model. Experiments on a real robot show a 39.2% absolute success rate improvement over the base policy and a 11.6% improvement from training with synthetic rollouts.</div>
<div class="mono" style="margin-top:8px">本文旨在通过迭代在线交互来提升视觉-语言-动作（VLA）模型的性能和可靠性。为了解决收集真实世界策略回放数据的高成本问题，作者提出使用一个学习到的模拟器，即动作条件下的视频生成模型，来生成额外的回放数据。然而，现有的世界模型缺乏必要的物理精度。作者提出了一种迭代改进算法，使用真实世界的回放数据来提高世界模型的精度，然后可以生成合成数据以改进VLA模型。在真实机器人上的实验表明，与基线策略相比，绝对成功率提高了39.2%，并且从使用生成的合成回放数据训练中获得了11.6%的改进。</div>
</details>
</div>
<div class="card">
<div class="title">HoloBrain-0 Technical Report</div>
<div class="meta-line">Authors: Xuewu Lin, Tianwei Lin, Yun Du, Hongyu Xie, Yiwei Jin, Jiawei Li, Shijie Wu, Qingze Wang, Mengdi Li, Mengao Zhao, Ziang Li, Chaodong Huang, Hongzhe Bi, Lichao Huang, Zhizhong Su</div>
<div class="meta-line">First: 2026-02-12T15:21:04+00:00 · Latest: 2026-02-12T15:21:04+00:00</div>
<div class="meta-line">Comments: 32 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train&quot; paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HoloBrain-0 技术报告</div>
<div class="mono" style="margin-top:8px">在本工作中，我们介绍了HoloBrain-0，这是一个综合性的视觉-语言-行动（VLA）框架，它弥合了基础模型研究与可靠现实世界机器人部署之间的差距。我们系统的核心是一个新颖的VLA架构，它明确地整合了机器人实体先验，包括多视图相机参数和运动学描述（URDF），以增强三维空间推理并支持多种实体。我们通过可扩展的“先预训练再后训练”范式验证了这一设计，实现了在RoboTwin 2.0、LIBERO和GenieSim等模拟基准上的最新成果，同时在具有挑战性的长期现实世界操作任务上也取得了出色的结果。值得注意的是，我们高效的0.2亿参数变体与更大规模的基础模型相比具有竞争力，能够实现低延迟的设备端部署。为了进一步加速研究和实际应用，我们完全开源了整个HoloBrain生态系统，其中包括：(1) 强大的预训练VLA基础；(2) 多个模拟套件和现实世界任务的后训练检查点；以及(3) RoboOrchard，一个全栈VLA基础设施，用于数据收集、模型训练和部署。结合标准化的数据收集协议，这一发布为社区提供了一条完整的、可重复的通往高性能机器人操作的道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">HoloBrain-0 is a comprehensive Vision-Language-Action framework that integrates robot embodiment priors to enhance 3D spatial reasoning. It uses a pre-train then post-train paradigm and achieves state-of-the-art results on simulation benchmarks and strong results on real-world manipulation tasks. The efficient variant with 0.2B parameters enables low-latency on-device deployment. The system is fully open-sourced, including pre-trained foundations, post-trained checkpoints, and RoboOrchard infrastructure for data curation and deployment.</div>
<div class="mono" style="margin-top:8px">HoloBrain-0 是一个综合性的视觉-语言-行动框架，整合了机器人本体先验知识以增强三维空间推理。它采用预训练然后后训练的范式，并在模拟基准测试和实际操作任务中取得了领先的结果。高效的0.2B参数版本支持低延迟的设备端部署。该框架完全开源，包括预训练模型和一个完整的堆栈基础设施，用于数据收集、模型训练和部署。</div>
</details>
</div>
<div class="card">
<div class="title">When would Vision-Proprioception Policies Fail in Robotic Manipulation?</div>
<div class="meta-line">Authors: Jingxian Lu, Wenke Xia, Yuxuan Wu, Zhiwu Lu, Di Hu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-12T15:00:48+00:00 · Latest: 2026-02-12T15:00:48+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12032v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12032v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot&#x27;s motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception&#x27;s gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人操作中视觉-本体感觉策略何时会失效？</div>
<div class="mono" style="margin-top:8px">本体感觉信息对于精确伺服控制至关重要，因为它提供了实时的机器人状态。它与视觉的协作被高度期望能增强复杂任务中操作策略的表现。然而，最近的研究报告了视觉-本体感觉策略泛化的不一致观察。在本工作中，我们通过进行时间控制实验来研究这一问题。我们发现，在机器人运动转换的任务子阶段，需要目标定位时，视觉模态在视觉-本体感觉策略中的作用有限。进一步的分析表明，策略自然倾向于简洁的本体感觉信号，这些信号在训练中能更快地减少损失，从而主导优化并抑制视觉模态在运动转换阶段的学习。为了解决这一问题，我们提出了带有阶段指导的梯度调整（GAP）算法，该算法能够适应性地调节本体感觉的优化，从而在视觉-本体感觉策略中实现动态协作。具体来说，我们利用本体感觉来捕捉机器人状态，并估计轨迹中每个时间步属于运动转换阶段的概率。在策略学习过程中，我们应用细粒度调整，根据估计的概率减少本体感觉梯度的幅度，从而实现稳健且泛化的视觉-本体感觉策略。综合实验表明，GAP在模拟和真实环境、单臂和双臂设置中均适用，并且兼容传统的和视觉-语言-动作模型。我们相信这项工作可以为机器人操作中视觉-本体感觉策略的发展提供有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the limitations of vision-proprioception policies in robotic manipulation by conducting temporally controlled experiments. It finds that during motion transitions requiring target localization, the vision modality plays a limited role, and the policy tends to rely more on proprioceptive signals. To address this, the authors propose the Gradient Adjustment with Phase-guidance (GAP) algorithm, which dynamically adjusts the optimization of proprioception to enhance the collaboration between vision and proprioception, leading to more robust and generalizable policies. Experiments show that GAP improves performance in both simulated and real-world environments for one-arm and dual-arm setups, and is compatible with various models.</div>
<div class="mono" style="margin-top:8px">研究探讨了为何在机器人操作任务中，视觉- proprioception策略在机器人运动转换阶段可能会失效。研究人员通过时间控制实验发现，在这些阶段，视觉模态的作用有限，因为策略倾向于依赖更快学习的 proprioception信号。为解决这一问题，他们提出了 Gradient Adjustment with Phase-guidance (GAP) 算法，该算法动态调整 proprioception的优化，以增强视觉模态的学习。综合实验表明，GAP 在模拟和真实环境中的单臂和双臂设置中都能提高视觉-proprioception策略的鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Accelerating Robotic Reinforcement Learning with Agent Guidance</div>
<div class="meta-line">Authors: Haojun Chen, Zili Zou, Chengdong Ma, Yaoxiang Pu, Haotong Zhang, Yuanpei Chen, Yaodong Yang</div>
<div class="meta-line">First: 2026-02-12T14:09:32+00:00 · Latest: 2026-02-12T14:09:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11978v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://agps-rl.github.io/agps">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corrections, yet this approach faces a scalability barrier. Reliance on human supervisors imposes a 1:1 supervision ratio that limits fleet expansion, suffers from operator fatigue over extended sessions, and introduces high variance due to inconsistent human proficiency. We present Agent-guided Policy Search (AGPS), a framework that automates the training pipeline by replacing human supervisors with a multimodal agent. Our key insight is that the agent can be viewed as a semantic world model, injecting intrinsic value priors to structure physical exploration. By using executable tools, the agent provides precise guidance via corrective waypoints and spatial constraints for exploration pruning. We validate our approach on two tasks, ranging from precision insertion to deformable object manipulation. Results demonstrate that AGPS outperforms HIL methods in sample efficiency. This automates the supervision pipeline, unlocking the path to labor-free and scalable robot learning. Project website: https://agps-rl.github.io/agps.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用代理指导加速机器人强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）为自主机器人通过试错掌握通用操作技能提供了强大的范式。然而，其实际应用受到严重样本效率低下的限制。最近的人机协作（HIL）方法通过使用人类修正来加速训练，但这种方法面临可扩展性障碍。依赖人类监督者导致1:1的监督比例，限制了机群扩展，长时间操作导致操作员疲劳，并由于人类熟练程度不一致而引入高方差。我们提出了代理引导策略搜索（AGPS）框架，通过用多模态代理取代人类监督者来自动化训练流程。我们的核心见解是，代理可以被视为语义世界模型，注入内在价值先验以结构化物理探索。通过使用可执行工具，代理通过纠正航点和空间约束提供精确指导，以进行探索剪枝。我们在从精确插入到可变形物体操作的两个任务上验证了我们的方法。结果表明，AGPS在样本效率上优于HIL方法。这自动化了监督流程，开启了无劳动力和可扩展的机器人学习之路。项目网站：https://agps-rl.github.io/agps/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the sample inefficiency in Reinforcement Learning (RL) for robotic manipulation tasks, proposing Agent-guided Policy Search (AGPS) to automate the training process. AGPS uses a multimodal agent to provide precise guidance through corrective waypoints and spatial constraints, replacing the need for human supervisors. Experiments show that AGPS outperforms Human-in-the-Loop (HIL) methods in sample efficiency, enabling more scalable and labor-free robot learning.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出Agent-guided Policy Search (AGPS)解决机器人操作中强化学习的样本低效问题。AGPS 使用多模态代理取代人类监督者，通过纠正航点和空间约束提供精确指导。实验表明，AGPS 在精确插入和可变形物体操作任务上的样本效率优于 Human-in-the-Loop (HIL) 方法，实现了无劳动力和可扩展的机器人学习监督管道。</div>
</details>
</div>
<div class="card">
<div class="title">Temporally Unified Adversarial Perturbations for Time Series Forecasting</div>
<div class="meta-line">Authors: Ruixian Su, Yukun Bao, Xinze Zhang</div>
<div class="meta-line">First: 2026-02-12T13:37:45+00:00 · Latest: 2026-02-12T13:37:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11940v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While deep learning models have achieved remarkable success in time series forecasting, their vulnerability to adversarial examples remains a critical security concern. However, existing attack methods in the forecasting field typically ignore the temporal consistency inherent in time series data, leading to divergent and contradictory perturbation values for the same timestamp across overlapping samples. This temporally inconsistent perturbations problem renders adversarial attacks impractical for real-world data manipulation. To address this, we introduce Temporally Unified Adversarial Perturbations (TUAPs), which enforce a temporal unification constraint to ensure identical perturbations for each timestamp across all overlapping samples. Moreover, we propose a novel Timestamp-wise Gradient Accumulation Method (TGAM) that provides a modular and efficient approach to effectively generate TUAPs by aggregating local gradient information from overlapping samples. By integrating TGAM with momentum-based attack algorithms, we ensure strict temporal consistency while fully utilizing series-level gradient information to explore the adversarial perturbation space. Comprehensive experiments on three benchmark datasets and four representative state-of-the-art models demonstrate that our proposed method significantly outperforms baselines in both white-box and black-box transfer attack scenarios under TUAP constraints. Moreover, our method also exhibits superior transfer attack performance even without TUAP constraints, demonstrating its effectiveness and superiority in generating adversarial perturbations for time series forecasting models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间统一的对抗扰动用于时间序列预测</div>
<div class="mono" style="margin-top:8px">尽管深度学习模型在时间序列预测方面取得了显著成功，但它们对对抗样本的脆弱性仍然是一个关键的安全问题。然而，时间序列预测领域的现有攻击方法通常忽略了时间序列数据中固有的时间一致性，导致在重叠样本中同一时间戳的扰动值出现分歧和矛盾。这种时间不一致的扰动问题使得对抗攻击在实际数据操纵中变得不切实际。为了解决这一问题，我们引入了时间统一的对抗扰动（TUAPs），它施加了一个时间统一的约束，以确保所有重叠样本中每个时间戳的扰动相同。此外，我们提出了一种新的时间戳梯度累积方法（TGAM），它提供了一种模块化和高效的方法，通过从重叠样本中聚合局部梯度信息来有效生成TUAPs。通过将TGAM与基于动量的攻击算法结合，我们确保了严格的时间一致性，同时充分利用了系列级梯度信息来探索对抗扰动空间。在三个基准数据集和四个代表性的最新模型上的全面实验表明，在TUAP约束条件下，我们的方法在白盒和黑盒转移攻击场景中均显著优于基线方法。此外，即使在没有TUAP约束的情况下，我们的方法也表现出优越的转移攻击性能，这证明了其在生成时间序列预测模型的对抗扰动方面的有效性和优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of deep learning models in time series forecasting to adversarial attacks, which often ignore temporal consistency. It introduces Temporally Unified Adversarial Perturbations (TUAPs) to enforce identical perturbations for each timestamp across overlapping samples. The Timestamp-wise Gradient Accumulation Method (TGAM) is proposed to generate TUAPs efficiently. Experiments show that the method outperforms baselines in both white-box and black-box scenarios, and even without TUAP constraints, it demonstrates superior transfer attack performance.</div>
<div class="mono" style="margin-top:8px">研究针对时间序列预测中深度学习模型对对抗样本的脆弱性，这些问题通常忽视了时间一致性。研究引入了时间统一的对抗扰动（TUAP）和时间戳梯度累积方法（TGAM），以确保在重叠样本中每个时间戳的扰动一致。实验表明，在TUAP约束下以及不使用约束的情况下，所提出的方法在白盒和黑盒攻击场景中均优于基线方法，证明了其在生成时间序列预测模型的对抗扰动方面的有效性和优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control</div>
<div class="meta-line">Authors: Yu Deng, Yufeng Jin, Xiaogang Jia, Jiahong Xue, Gerhard Neumann, Georgia Chalvatzaki</div>
<div class="meta-line">First: 2026-02-12T13:30:24+00:00 · Latest: 2026-02-12T13:30:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11934v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11934v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a &quot;blind spot&quot; for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robot-DIFT：提炼扩散特征以实现几何一致的视觉运动控制</div>
<div class="mono" style="margin-top:8px">我们假设通用机器人操作的关键瓶颈不仅在于数据规模或策略容量，还在于当前视觉骨干与闭环控制的物理要求之间的结构性不匹配。尽管最先进的视觉编码器（包括在VLAs中使用的那些）优化了语义不变性以稳定分类，但操作通常需要几何敏感性，即能够将毫米级的姿态变化映射到可预测的特征变化。它们的判别性目标在精细控制方面形成了“盲区”，而生成的扩散模型在其潜在流形中内嵌了几何依赖性，鼓励保持密集的多尺度空间结构。然而，直接将随机扩散特征用于控制受到随机不稳定、推理延迟和微调期间表示漂移的阻碍。为了解决这一问题，我们提出了Robot-DIFT框架，通过流形蒸馏将几何信息的来源与推理过程分离。通过将冻结的扩散教师提炼成确定性的空间语义特征金字塔网络（S2-FPN），我们保留了生成模型丰富的几何先验，同时确保了时间稳定性、实时执行和对漂移的鲁棒性。在大规模DROID数据集上预训练后，Robot-DIFT在几何一致性和控制性能方面优于最先进的判别性基线，支持了模型学习如何“看”决定了它如何“做”的观点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the structural mismatch between current visual backbones and the geometric requirements of closed-loop control in robot manipulation. The method involves using a framework called Robot-DIFT, which decouples geometric information from inference through Manifold Distillation, converting a stochastic diffusion model into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN). The key experimental findings show that Robot-DIFT outperforms existing discriminative baselines in terms of geometric consistency and control performance, suggesting that the learning process of visual perception influences the ability to perform precise actions.</div>
<div class="mono" style="margin-top:8px">研究旨在解决当前视觉骨干与闭环控制中几何需求之间的结构不匹配问题。提出的Robot-DIFT框架通过Manifold Distillation将几何信息与推理过程分离，将生成模型的随机特征转换为确定性的Spatial-Semantic Feature Pyramid Network (S2-FPN)。这种方法提高了几何一致性和控制性能，在DROID数据集上优于现有判别性基线。</div>
</details>
</div>
<div class="card">
<div class="title">General Humanoid Whole-Body Control via Pretraining and Fast Adaptation</div>
<div class="meta-line">Authors: Zepeng Wang, Jiangxing Wang, Shiqing Yao, Yu Zhang, Ziluo Ding, Ming Yang, Yuxuan Wang, Haobin Jiang, Chao Ma, Xiaochuan Shi, Zongqing Lu</div>
<div class="meta-line">First: 2026-02-12T13:26:22+00:00 · Latest: 2026-02-12T13:26:22+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11929v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11929v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning a general whole-body controller for humanoid robots remains challenging due to the diversity of motion distributions, the difficulty of fast adaptation, and the need for robust balance in high-dynamic scenarios. Existing approaches often require task-specific training or suffer from performance degradation when adapting to new motions. In this paper, we present FAST, a general humanoid whole-body control framework that enables Fast Adaptation and Stable Motion Tracking. FAST introduces Parseval-Guided Residual Policy Adaptation, which learns a lightweight delta action policy under orthogonality and KL constraints, enabling efficient adaptation to out-of-distribution motions while mitigating catastrophic forgetting. To further improve physical robustness, we propose Center-of-Mass-Aware Control, which incorporates CoM-related observations and objectives to enhance balance when tracking challenging reference motions. Extensive experiments in simulation and real-world deployment demonstrate that FAST consistently outperforms state-of-the-art baselines in robustness, adaptation efficiency, and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用类人全身控制通过预训练和快速适应</div>
<div class="mono" style="margin-top:8px">学习通用的全身控制器对于类人机器人来说仍然具有挑战性，因为运动分布的多样性、快速适应的难度以及在高动态场景中保持平衡的需要。现有方法通常需要任务特定的训练，或者在适应新运动时表现下降。在本文中，我们提出了FAST，一种通用类人全身控制框架，能够实现快速适应和稳定运动跟踪。FAST引入了Parseval引导残差策略适应，该方法在正交性和KL约束下学习一个轻量级的增量动作策略，从而能够高效地适应分布外运动，同时减轻灾难性遗忘。为了进一步提高物理鲁棒性，我们提出了质心感知控制，该方法结合了质心相关的观测和目标，以增强在跟踪具有挑战性的参考运动时的平衡。广泛的仿真实验和实际部署表明，FAST在鲁棒性、适应效率和泛化方面始终优于最先进的基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a general whole-body control framework for humanoid robots that can adapt quickly and maintain robust balance. FAST, the proposed framework, uses Parseval-Guided Residual Policy Adaptation to learn a lightweight delta action policy under orthogonality and KL constraints, allowing efficient adaptation to new motions while preventing catastrophic forgetting. Additionally, Center-of-Mass-Aware Control is introduced to enhance balance during motion tracking. Experiments show that FAST outperforms existing methods in robustness, adaptation efficiency, and generalization in both simulation and real-world settings.</div>
<div class="mono" style="margin-top:8px">研究旨在解决为类人机器人学习通用全身控制器的挑战，包括动作多样性、快速适应和动态平衡。FAST框架被提出，其中包括Parseval-Guided Residual Policy Adaptation以实现对新动作的高效适应，以及Center-of-Mass-Aware Control以增强物理鲁棒性。实验表明，FAST在鲁棒性、适应效率和泛化能力方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Manipulate Anything: Revealing Data Scaling Laws in Bounding-Box Guided Policies</div>
<div class="meta-line">Authors: Yihao Wu, Jinming Ma, Junbo Tan, Yanzhao Yu, Shoujie Li, Mingliang Zhou, Diyun Xiang, Xueqian Wang</div>
<div class="meta-line">First: 2026-02-12T12:34:56+00:00 · Latest: 2026-02-12T12:34:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11885v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based policies show limited generalization in semantic manipulation, posing a key obstacle to the deployment of real-world robots. This limitation arises because relying solely on text instructions is inadequate to direct the policy&#x27;s attention toward the target object in complex and dynamic environments. To solve this problem, we propose leveraging bounding-box instruction to directly specify target object, and further investigate whether data scaling laws exist in semantic manipulation tasks. Specifically, we design a handheld segmentation device with an automated annotation pipeline, Label-UMI, which enables the efficient collection of demonstration data with semantic labels. We further propose a semantic-motion-decoupled framework that integrates object detection and bounding-box guided diffusion policy to improve generalization and adaptability in semantic manipulation. Throughout extensive real-world experiments on large-scale datasets, we validate the effectiveness of the approach, and reveal a power-law relationship between generalization performance and the number of bounding-box objects. Finally, we summarize an effective data collection strategy for semantic manipulation, which can achieve 85\% success rates across four tasks on both seen and unseen objects. All datasets and code will be released to the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习操控一切：揭示基于边界框指导策略的数据缩放定律</div>
<div class="mono" style="margin-top:8px">基于扩散的策略在语义操控方面表现出有限的泛化能力，成为在现实世界中部署机器人的一大障碍。这一局限性源于仅依赖文本指令不足以引导策略关注复杂和动态环境中的目标物体。为了解决这一问题，我们提出利用边界框指令直接指定目标物体，并进一步探讨语义操控任务中是否存在数据缩放定律。具体而言，我们设计了一种手持分割设备和自动注释流水线Label-UMI，以高效地收集带有语义标签的演示数据。我们还提出了一种语义-运动解耦框架，将物体检测与基于边界框的扩散策略相结合，以提高语义操控中的泛化能力和适应性。通过在大规模数据集上的广泛实际实验，我们验证了该方法的有效性，并揭示了泛化性能与边界框物体数量之间的幂律关系。最后，我们总结了一种有效的数据收集策略，该策略可以在四种任务中实现85%的成功率，涵盖已见和未见物体。所有数据集和代码将向社区开放。</div>
</details>
</div>
<div class="card">
<div class="title">JEPA-VLA: Video Predictive Embedding is Needed for VLA Models</div>
<div class="meta-line">Authors: Shangchen Miao, Ningya Feng, Jialong Wu, Ye Lin, Xu He, Dong Li, Mingsheng Long</div>
<div class="meta-line">First: 2026-02-12T11:20:43+00:00 · Latest: 2026-02-12T11:20:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11832v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JEPA-VLA: 视频预测嵌入对于VLA模型是必需的</div>
<div class="mono" style="margin-top:8px">基于预训练视觉-语言模型（VLMs）的近期视觉-语言-行动（VLA）模型在机器人操作方面取得了显著进步。然而，当前的VLA模型仍然存在样本效率低和泛化能力有限的问题。本文认为这些限制与被忽视的组件——预训练视觉表示——密切相关，后者在环境理解和策略先验知识方面提供的信息不足。通过深入分析，我们发现VLA中常用的视觉表示，无论是通过语言-图像对比学习还是基于图像的自我监督学习预训练，仍然无法充分捕捉关键的任务相关信息和诱导有效的策略先验知识，即在成功执行任务时环境如何演变的预见性知识。相比之下，我们发现基于视频预训练的预测嵌入，特别是V-JEPA 2，能够灵活地忽略不可预测的环境因素并编码任务相关的时间动态，从而有效弥补现有视觉表示在VLA中的关键不足。基于这些观察，我们提出了JEPA-VLA，这是一种简单而有效的方法，能够适应性地将预测嵌入整合到现有的VLA中。我们的实验表明，JEPA-VLA在包括LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务在内的多种基准测试中取得了显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of vision-language-action (VLA) models in terms of sample efficiency and generalization by proposing JEPA-VLA, which integrates predictive embeddings pretrained on videos. The authors argue that current VLA models lack sufficient knowledge for environment understanding and policy prior due to inadequate visual representations. Experiments show that JEPA-VLA significantly improves performance across various benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.</div>
<div class="mono" style="margin-top:8px">本文通过提出JEPA-VLA，将基于视频预训练的预测嵌入整合到现有VLA模型中，来解决VLA模型在样本效率和泛化能力方面的局限性。作者认为，由于当前VLA模型的视觉表示不足，缺乏足够的环境理解和策略先验知识。实验表明，JEPA-VLA在包括LIBERO、LIBERO-plus、RoboTwin2.0和真实机器人任务在内的多种基准测试中显著提高了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Translating Flow to Policy via Hindsight Online Imitation</div>
<div class="meta-line">Authors: Yitian Zheng, Zhangchen Ye, Weijun Dong, Shengjie Wang, Yuyang Liu, Chongjie Zhang, Chuan Wen, Yang Gao</div>
<div class="meta-line">First: 2025-12-22T11:06:06+00:00 · Latest: 2026-02-12T10:30:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19269v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19269v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过前瞻在线模仿将流程转化为策略</div>
<div class="mono" style="margin-top:8px">近年来，层次化机器人系统利用高级规划器提出任务计划，并利用低级策略生成机器人动作。这种设计允许在不涉及动作或甚至非机器人数据源（例如视频）上训练规划器，从而提供可转移的高级指导。然而，将这些高级计划转化为可执行的动作仍然具有挑战性，尤其是在高质量机器人数据稀缺的情况下。为此，我们提出通过在线交互改进低级策略。具体而言，我们的方法收集在线轨迹，回顾性地从实现结果中注释相应的高级目标，并将这些前瞻重新标记的经验整合以更新一个目标条件模仿策略。我们的方法，前瞻流程条件在线模仿（HinFlow），使用二维点流作为高级规划器来实现这一理念。在模拟和物理世界中的多种操作任务中，我们的方法在基线策略上的性能提高了超过2倍，显著优于现有方法。此外，我们的框架能够从跨体态视频数据训练的规划器中获取策略，展示了其在可扩展和可转移机器人学习方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of translating high-level task plans into executable robot actions by proposing HinFlow, which uses hindsight online imitation. It collects rollouts, annotates high-level goals from outcomes, and updates a goal-conditioned imitation policy. The method improves performance by more than 2 times over the base policy and outperforms existing methods across various manipulation tasks in both simulation and the physical world. Additionally, it demonstrates the potential for acquiring policies from planners trained on cross-embodiment video data, enhancing scalability and transferability in robot learning.</div>
<div class="mono" style="margin-top:8px">论文通过提出HinFlow方法，解决将高层任务计划转化为可执行的机器人动作的挑战。该方法收集回放，从结果中反向标注高层目标，并更新目标条件模仿策略。该方法在各种模拟和物理世界的操作任务中，性能提高了超过2倍，显著优于现有方法。此外，该框架还展示了从跨体态视频数据训练的规划器中获取策略的潜力，增强了机器人学习的可扩展性和转移性。</div>
</details>
</div>
<div class="card">
<div class="title">TopoFair: Linking Topological Bias to Fairness in Link Prediction Benchmarks</div>
<div class="meta-line">Authors: Lilian Marey, Mathilde Perez, Tiphaine Viard, Charlotte Laclau</div>
<div class="meta-line">First: 2026-02-12T10:29:44+00:00 · Latest: 2026-02-12T10:29:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11802v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph link prediction (LP) plays a critical role in socially impactful applications, such as job recommendation and friendship formation. Ensuring fairness in this task is thus essential. While many fairness-aware methods manipulate graph structures to mitigate prediction disparities, the topological biases inherent to social graph structures remain poorly understood and are often reduced to homophily alone. This undermines the generalization potential of fairness interventions and limits their applicability across diverse network topologies. In this work, we propose a novel benchmarking framework for fair LP, centered on the structural biases of the underlying graphs. We begin by reviewing and formalizing a broad taxonomy of topological bias measures relevant to fairness in graphs. In parallel, we introduce a flexible graph generation method that simultaneously ensures fidelity to real-world graph patterns and enables controlled variation across a wide spectrum of structural biases. We apply this framework to evaluate both classical and fairness-aware LP models across multiple use cases. Our results provide a fine-grained empirical analysis of the interactions between predictive fairness and structural biases. This new perspective reveals the sensitivity of fairness interventions to beyond-homophily biases and underscores the need for structurally grounded fairness evaluations in graph learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TopoFair：链接链接预测基准中的拓扑偏差与公平性</div>
<div class="mono" style="margin-top:8px">图链接预测(LP)在社会影响的应用中起着关键作用，例如工作推荐和友谊形成。因此，在这个任务中确保公平性是至关重要的。尽管许多公平意识的方法通过修改图结构来减轻预测差异，但社会图结构固有的拓扑偏差仍然理解不足，并且通常仅归因于同质性。这削弱了公平干预措施的泛化潜力，并限制了它们在不同网络拓扑结构中的应用。在本文中，我们提出了一种新的公平LP基准框架，以图下层结构的拓扑偏差为中心。我们首先回顾并形式化了一种广泛的拓扑偏差度量分类，这些度量与图中的公平性相关。同时，我们引入了一种灵活的图生成方法，该方法同时确保对真实世界图模式的忠实性，并允许在广泛的结构偏差范围内进行可控变化。我们应用此框架评估了多种经典和公平意识的LP模型。我们的结果提供了预测公平性和结构偏差之间相互作用的精细实证分析。这种新的视角揭示了公平干预措施对超越同质性的偏差的敏感性，并强调了在图学习中进行结构上合理的公平性评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the importance of fairness in graph link prediction, which is crucial for applications like job recommendation and friendship formation. The authors propose a new benchmarking framework, TopoFair, to evaluate fairness in link prediction by focusing on topological biases in social graph structures. They introduce a flexible graph generation method to control various structural biases and apply it to both classical and fairness-aware link prediction models. The results highlight the sensitivity of fairness interventions to biases beyond homophily and emphasize the need for structurally grounded fairness evaluations in graph learning.</div>
<div class="mono" style="margin-top:8px">该研究关注图链接预测中的公平性问题，特别是在如职业推荐和友谊形成等社会影响较大的应用中。作者提出了一种新的基准框架TopoFair，通过关注社交图中的结构偏见来评估公平性。他们引入了一种灵活的图生成方法，以控制各种结构偏见，并将其应用于经典和公平意识的链接预测模型。研究结果表明，公平性干预措施对超越同质性的偏见非常敏感，并强调了在图学习中进行结构导向的公平性评估的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling World Model for Hierarchical Manipulation Policies</div>
<div class="meta-line">Authors: Qian Long, Yueze Wang, Jiaxi Song, Junbo Zhang, Peiyan Li, Wenxuan Wang, Yuqi Wang, Haoyang Li, Shaoxuan Xie, Guocai Yao, Hanbo Zhang, Xinlong Wang, Zhongyuan Wang, Xuguang Lan, Huaping Liu, Xinghang Li</div>
<div class="meta-line">First: 2026-02-11T16:12:33+00:00 · Latest: 2026-02-12T10:16:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10983v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10983v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vista-wm.github.io/}{https://vista-wm.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \href{https://vista-wm.github.io/}{https://vista-wm.github.io}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展世界模型以实现分层操作策略</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在通用机器人操作方面很有前景，但在分布外（OOD）设置中仍然脆弱，尤其是在有限的真实机器人数据情况下。为了解决泛化瓶颈，我们引入了一种分层的视觉-语言-动作框架\our{}，该框架利用大规模预训练世界模型的泛化能力，以实现稳健且可泛化的VISual Subgoal TAsk分解（VISTA）。我们的分层框架\our{}由世界模型作为高层规划者和VLA作为低层执行者组成。高层世界模型首先将操作任务分解为带有目标图像的子任务序列，低层策略遵循文本和视觉指导生成动作序列。与原始的纯文本目标规范相比，这些合成的目标图像为低层策略提供了视觉和物理上的细节，使其能够在未见过的对象和新场景中进行泛化。我们在大量分布外场景中验证了视觉目标合成和我们的分层VLA策略，并且在世界模型生成的指导下的相同结构的VLA在新场景中的性能可以从14%提升到69%。结果表明，我们的方法在分布外场景中明显优于之前的基线方法。项目页面：\href{https://vista-wm.github.io/}{https://vista-wm.github.io}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the generalization of Vision-Language-Action (VLA) models for robot manipulation tasks, especially in out-of-distribution settings. The method involves a hierarchical framework that uses a pre-trained world model as a high-level planner to decompose manipulation tasks into subtasks with goal images, which are then executed by a VLA model. The key finding is that using synthesized goal images improves the performance of the VLA model, achieving a boost from 14% to 69% in novel scenarios compared to raw textual goals, particularly in out-of-distribution settings.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决Vision-Language-Action (VLA)模型在分布外设置中的脆弱性，增强其在机器人操作中的鲁棒性。方法是采用一个分层框架，其中预训练的世界模型作为高层规划者将操作任务分解为带有目标图像的子任务，然后由VLA模型作为低层执行者执行这些任务。关键发现表明，使用合成的目标图像可以提高对未见过的对象和新场景的泛化能力，在分布外场景中的性能从14%提升到69%，优于之前的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation</div>
<div class="meta-line">Authors: Yutong Liang, Shiyi Xu, Yulong Zhang, Bowen Zhan, He Zhang, Libin Liu</div>
<div class="meta-line">First: 2026-01-09T15:16:31+00:00 · Latest: 2026-02-12T08:29:48+00:00</div>
<div class="meta-line">Comments: 12 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05844v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05844v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pku-mocca.github.io/Dextercap-Page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Capturing fine-grained hand-object interactions is challenging due to severe self-occlusion from closely spaced fingers and the subtlety of in-hand manipulation motions. Existing optical motion capture systems rely on expensive camera setups and extensive manual post-processing, while low-cost vision-based methods often suffer from reduced accuracy and reliability under occlusion. To address these challenges, we present DexterCap, a low-cost optical capture system for dexterous in-hand manipulation. DexterCap uses dense, character-coded marker patches to achieve robust tracking under severe self-occlusion, together with an automated reconstruction pipeline that requires minimal manual effort. With DexterCap, we introduce DexterHand, a dataset of fine-grained hand-object interactions covering diverse manipulation behaviors and objects, from simple primitives to complex articulated objects such as a Rubik&#x27;s Cube. We release the dataset and code to support future research on dexterous hand-object interaction. Project website: https://pku-mocca.github.io/Dextercap-Page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DexterCap：一种低成本且自动化的灵巧手物交互捕捉系统</div>
<div class="mono" style="margin-top:8px">由于手指紧密排列导致的严重自遮挡以及在手内操作中的细微动作，精细的手物交互捕捉极具挑战性。现有的光学运动捕捉系统依赖昂贵的摄像设备和大量的手动后期处理，而低成本的基于视觉的方法在遮挡下往往准确性降低且可靠性差。为应对这些挑战，我们提出了DexterCap，一种低成本的光学捕捉系统，用于灵巧的手内操作。DexterCap 使用密集的字符编码标记斑块，在严重自遮挡下实现稳健的跟踪，并结合了一个自动重建流水线，只需少量的手动努力。借助DexterCap，我们引入了DexterHand 数据集，涵盖了从简单几何体到复杂关节物体（如魔方）的各种精细的手物交互行为。我们发布了该数据集和代码，以支持未来灵巧手物交互研究。项目网站：https://pku-mocca.github.io/Dextercap-Page/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DexterCap is a low-cost optical capture system designed to track fine-grained hand-object interactions, especially during dexterous in-hand manipulation, which is challenging due to self-occlusion. It uses dense, character-coded marker patches for robust tracking and an automated reconstruction pipeline for minimal manual effort. The system enables the creation of DexterHand, a comprehensive dataset of diverse hand-object interactions, including simple and complex objects. This dataset and the code are publicly released to facilitate further research in dexterous hand-object manipulation.</div>
<div class="mono" style="margin-top:8px">DexterCap 是一个经济实惠且自动化的系统，用于捕捉详细的手部与物体交互，解决了自遮挡和动作细微的问题。它使用密集的字符编码标记斑块进行鲁棒跟踪，并配有自动重建流水线。该系统引入了 DexterHand 数据集，涵盖了各种操作行为和物体的精细手部与物体交互，已公开发布以支持未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI</div>
<div class="meta-line">Authors: Hongzhi Zang, Shu&#x27;ang Yu, Hao Lin, Tianxing Zhou, Zefang Huang, Zhen Guo, Xin Xu, Jiakai Zhou, Yuze Sheng, Shizhe Zhang, Feng Gao, Wenhao Tang, Yufeng Yue, Quanlu Zhang, Xinlei Chen, Chao Yu, Yu Wang</div>
<div class="meta-line">First: 2026-02-08T06:23:43+00:00 · Latest: 2026-02-12T08:08:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07837v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.07837v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLinf-USER：现实世界中体态人工智能在线策略学习的统一和可扩展系统</div>
<div class="mono" style="margin-top:8px">直接在物理世界中进行在线策略学习是体态智能的一个有前途但具有挑战性的方向。与模拟不同，现实世界系统无法随意加速、廉价重置或大量复制，这使得大规模数据收集、异构部署和长时程有效训练变得困难。这些挑战表明，现实世界中的策略学习不仅是算法问题，从根本上说是一个系统问题。我们提出了USER，一个统一和可扩展的现实世界在线策略学习系统。USER通过统一的硬件抽象层将物理机器人视为与GPU同等重要的硬件资源，从而实现异构机器人自动发现、管理和调度。为了解决云边通信问题，USER引入了基于隧道的自适应通信平面、分布式数据通道以进行流量本地化，以及流式多处理器感知的权重同步，以调节GPU端的开销。在此基础设施之上，USER组织学习为一个完全异步框架，具有持久且缓存意识的缓冲区，能够高效地进行长时程实验，并具备强大的故障恢复能力和对历史数据的重用。此外，USER提供了可扩展的奖励、算法和策略抽象，支持在统一管道中进行CNN/MLP、生成策略和大型视觉-语言-动作（VLA）模型的在线模仿或强化学习。在模拟和现实世界中的结果表明，USER能够实现多机器人协调、异构操作器、边缘-云协作与大型模型以及长时间异步训练，为现实世界中的在线策略学习提供了一个统一和可扩展的系统基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges of online policy learning in the physical world, presenting USER, a unified and extensible system. USER tackles issues like scalable data collection and heterogeneous deployment by providing a unified hardware abstraction layer and an adaptive communication plane. Key findings include multi-robot coordination, support for large models, and long-running asynchronous training, demonstrating USER&#x27;s effectiveness in real-world and simulated environments.</div>
<div class="mono" style="margin-top:8px">论文针对物理世界中的在线策略学习挑战，提出了USER系统，该系统统一并扩展了硬件资源以管理异构机器人。USER包括一个统一的硬件抽象层，用于自动发现和调度机器人，以及一个适应性的通信平面来处理云边交互，并且具有一个完全异步的框架，用于高效地进行长期实验。关键发现表明，USER支持多机器人协调、异构执行器、边缘-云协作与大型模型以及长期异步训练，为物理世界的在线策略学习提供了一个统一和扩展的系统基础。</div>
</details>
</div>
<div class="card">
<div class="title">Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes</div>
<div class="meta-line">Authors: Jeongho Noh, Tai Hyoung Rhee, Eunho Lee, Jeongyun Kim, Sunwoo Lee, Ayoung Kim</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-12T07:25:52+00:00 · Latest: 2026-02-12T07:25:52+00:00</div>
<div class="meta-line">Comments: Accepted to ICRA 2026. 9 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11660v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11660v1">PDF</a> · <a href="https://github.com/jeonghonoh/clutt3r-seg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable 3D instance segmentation is fundamental to language-grounded robotic manipulation. Its critical application lies in cluttered environments, where occlusions, limited viewpoints, and noisy masks degrade perception. To address these challenges, we present Clutt3R-Seg, a zero-shot pipeline for robust 3D instance segmentation for language-grounded grasping in cluttered scenes. Our key idea is to introduce a hierarchical instance tree of semantic cues. Unlike prior approaches that attempt to refine noisy masks, our method leverages them as informative cues: through cross-view grouping and conditional substitution, the tree suppresses over- and under-segmentation, yielding view-consistent masks and robust 3D instances. Each instance is enriched with open-vocabulary semantic embeddings, enabling accurate target selection from natural language instructions. To handle scene changes during multi-stage tasks, we further introduce a consistency-aware update that preserves instance correspondences from only a single post-interaction image, allowing efficient adaptation without rescanning. Clutt3R-Seg is evaluated on both synthetic and real-world datasets, and validated on a real robot. Across all settings, it consistently outperforms state-of-the-art baselines in cluttered and sparse-view scenarios. Even on the most challenging heavy-clutter sequences, Clutt3R-Seg achieves an AP@25 of 61.66, over 2.2x higher than baselines, and with only four input views it surpasses MaskClustering with eight views by more than 2x. The code is available at: https://github.com/jeonghonoh/clutt3r-seg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Clutt3R-Seg：杂乱场景中基于语言的抓取稀疏视图3D实例分割</div>
<div class="mono" style="margin-top:8px">可靠的3D实例分割是基于语言的机器人操作的基础。其关键应用在于杂乱环境，其中遮挡、有限视角和噪声掩膜会降低感知效果。为了解决这些挑战，我们提出了Clutt3R-Seg，这是一种用于杂乱场景中基于语言的抓取的鲁棒3D实例分割零样本管道。我们的核心思想是引入层次化的语义实例树。与先前尝试细化噪声掩膜的方法不同，我们的方法将它们用作信息性线索：通过跨视图分组和条件替换，树抑制了过度分割和不足分割，产生了视图一致的掩膜和鲁棒的3D实例。每个实例都丰富了开放词汇语义嵌入，使从自然语言指令中准确选择目标成为可能。为了处理多阶段任务中的场景变化，我们进一步引入了一种一致性感知更新，仅通过单张后交互图像保留实例对应关系，从而实现高效的适应而无需重新扫描。Clutt3R-Seg在合成和真实世界数据集上进行了评估，并在真实机器人上进行了验证。在所有设置中，它在杂乱和稀疏视图场景中始终优于最先进的基线。即使在最具挑战性的重杂乱序列中，Clutt3R-Seg的AP@25也达到了61.66，比基线高出2.2倍，而且仅使用四个输入视图，它就超过了使用八视图的MaskClustering超过2倍。代码可在：https://github.com/jeonghonoh/clutt3r-seg/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Clutt3R-Seg is a zero-shot pipeline for 3D instance segmentation in cluttered scenes, addressing challenges like occlusions and limited viewpoints. It introduces a hierarchical instance tree of semantic cues to suppress over- and under-segmentation, yielding view-consistent masks and robust 3D instances. The method enriches each instance with open-vocabulary semantic embeddings and uses a consistency-aware update to adapt to scene changes efficiently. Clutt3R-Seg outperforms state-of-the-art baselines in cluttered and sparse-view scenarios, achieving an AP@25 of 61.66 and surpassing MaskClustering with fewer input views.</div>
<div class="mono" style="margin-top:8px">Clutt3R-Seg 是一种针对杂乱场景的零样本 3D 实例分割管道，解决遮挡和有限视角的问题。它引入了一种基于语义线索的分层实例树来抑制过度和不足分割，生成视图一致的掩码和稳健的 3D 实例。该方法为每个实例添加开放词汇量的语义嵌入，并使用一致性感知更新来保持实例对应关系，从而在无需重新扫描的情况下实现高效适应。Clutt3R-Seg 在杂乱和稀疏视角场景中优于最先进的基线方法，AP@25 达到 61.66，并且使用更少的输入视图超过了 MaskClustering。</div>
</details>
</div>
<div class="card">
<div class="title">DiSPo: Diffusion-SSM based Policy Learning for Coarse-to-Fine Action Discretization</div>
<div class="meta-line">Authors: Nayoung Oh, Jaehyeong Jang, Moonkyeong Jung, Daehyung Park</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2024-09-23T05:33:35+00:00 · Latest: 2026-02-12T07:21:21+00:00</div>
<div class="meta-line">Comments: Accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.14719v3">Abs</a> · <a href="https://arxiv.org/pdf/2409.14719v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robo-dispo.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We aim to solve the problem of generating coarse-to-fine skills learning from demonstrations (LfD). To scale precision, traditional LfD approaches often rely on extensive fine-grained demonstrations with external interpolations or dynamics models with limited generalization capabilities. For memory-efficient learning and convenient granularity change, we propose a novel diffusion-state space model (SSM) based policy (DiSPo) that learns from diverse coarse skills and produces varying control scales of actions by leveraging an SSM, Mamba. Our evaluations show the adoption of Mamba and the proposed step-scaling method enable DiSPo to outperform in three coarse-to-fine benchmark tests with maximum 81% higher success rate than baselines. In addition, DiSPo improves inference efficiency by generating coarse motions in less critical regions. We finally demonstrate the scalability of actions with simulation and real-world manipulation tasks. Code and Videos are available at https://robo-dispo.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiSPo: 基于扩散-状态空间模型的粗细动作离散化策略学习</div>
<div class="mono" style="margin-top:8px">我们旨在解决从演示中生成粗细动作技能学习（LfD）的问题。为了提高精度，传统的LfD方法通常依赖于大量的细粒度演示或具有有限泛化能力的动力学模型。为了实现高效学习和方便的粒度变化，我们提出了一种新颖的基于扩散-状态空间模型（SSM）的策略（DiSPo），该策略通过利用Mamba从多样化的粗动作技能中学习，并通过SSM产生不同规模的动作控制。我们的评估表明，采用Mamba和提出的步长缩放方法使DiSPo在三个粗细动作基准测试中表现出色，最高成功率比基线高出81%。此外，DiSPo通过在不那么关键的区域生成粗动作提高了推理效率。最后，我们在模拟和实际操作任务中展示了动作的可扩展性。代码和视频可在https://robo-dispo.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of learning coarse-to-fine skills from demonstrations in robotics. It proposes DiSPo, a policy learning method using a diffusion-state space model (SSM) and Mamba for efficient learning and flexible control scale adjustment. DiSPo outperforms baseline methods in three benchmark tests, achieving up to 81% higher success rates and improving inference efficiency by generating coarse motions in less critical regions.</div>
<div class="mono" style="margin-top:8px">论文旨在通过提出基于扩散状态空间模型（SSM）的DiSPo策略，解决从演示中学习粗细动作技能（LfD）的挑战。DiSPo利用多样化的粗动作技能和名为Mamba的SSM，生成不同控制尺度的动作，避免了需要大量细粒度的演示。该方法在三个基准测试中表现出色，最高成功率比基线高出81%，并通过生成关键区域外的粗动作来提高推理效率。通过仿真和实际操作任务展示了动作的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning</div>
<div class="meta-line">Authors: Yufeng Tian, Shuiqi Cheng, Tianming Wei, Tianxing Zhou, Yuanhang Zhang, Zixian Liu, Qianwei Han, Zhecheng Yuan, Huazhe Xu</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-12T06:56:29+00:00 · Latest: 2026-02-12T06:56:29+00:00</div>
<div class="meta-line">Comments: Published to ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11643v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11643v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://skyrainwind.github.io/ViTaS/index.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches mostly focus on the alignment of visual and tactile features and the integration mechanism tends to be direct concatenation. Consequently, they struggle to effectively cope with occluded scenarios due to neglecting the inherent complementary nature of both modalities and the alignment may not be exploited enough, limiting the potential of their real-world deployment. In this paper, we present ViTaS, a simple yet effective framework that incorporates both visual and tactile information to guide the behavior of an agent. We introduce Soft Fusion Contrastive Learning, an advanced version of conventional contrastive learning method and a CVAE module to utilize the alignment and complementarity within visuo-tactile representations. We demonstrate the effectiveness of our method in 12 simulated and 3 real-world environments, and our experiments show that ViTaS significantly outperforms existing baselines. Project page: https://skyrainwind.github.io/ViTaS/index.html.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViTaS: 视觉触觉软融合对比学习在视动学习中的应用</div>
<div class="mono" style="margin-top:8px">触觉信息在人类操作任务中起着重要作用，并且最近在机器人操作中引起了越来越多的关注。然而，现有的方法主要集中在视觉和触觉特征的对齐上，集成机制往往是直接连接。因此，它们在处理由于忽视了这两种模态的内在互补性而导致的遮挡场景时表现不佳，对齐可能没有充分利用，限制了它们在实际部署中的潜力。在本文中，我们提出了ViTaS，这是一种简单而有效的框架，结合了视觉和触觉信息来引导代理的行为。我们引入了软融合对比学习，这是一种改进的常规对比学习方法，并引入了CVAE模块来利用视触觉表示中的对齐和互补性。我们在12个模拟和3个真实环境中的实验表明，ViTaS 显著优于现有基线。项目页面：https://skyrainwind.github.io/ViTaS/index.html。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ViTaS, a framework that combines visual and tactile information to enhance robotic manipulation. It uses Soft Fusion Contrastive Learning and a CVAE module to better align and utilize the complementary nature of visual and tactile data. Experiments in 12 simulated and 3 real-world environments show that ViTaS outperforms existing methods in handling occluded scenarios and real-world deployment challenges.</div>
<div class="mono" style="margin-top:8px">论文提出了ViTaS框架，结合视觉和触觉信息以提升机器人的操作能力。该方法使用Soft Fusion Contrastive Learning和CVAE模块更好地对齐和利用视觉与触觉数据的互补性。实验在12个模拟和3个真实环境中的结果显示，ViTaS在处理遮挡场景和实际部署挑战方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Do MLLMs Really Understand Space? A Mathematical Reasoning Evaluation</div>
<div class="meta-line">Authors: Shuo Lu, Jianjie Cheng, Yinuo Xu, Yongcan Yu, Lijun Sheng, Peijie Wang, Siru Jiang, Yongguan Hu, Run Ling, Yihua Shao, Ao Ma, Wei Feng, Lingxiao He, Meng Wang, Qianlong Xie, Xingxing Wang, Ran He, Jian Liang</div>
<div class="meta-line">First: 2026-02-12T06:37:55+00:00 · Latest: 2026-02-12T06:37:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11635v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11635v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have achieved strong performance on perception-oriented tasks, yet their ability to perform mathematical spatial reasoning, defined as the capacity to parse and manipulate two- and three-dimensional relations, remains unclear. Humans easily solve textbook-style spatial reasoning problems with over 95\% accuracy, but we find that most leading MLLMs fail to reach even 60\% on the same tasks. This striking gap highlights spatial reasoning as a fundamental weakness of current models. To investigate this gap, we present MathSpatial, a unified framework for evaluating and improving spatial reasoning in MLLMs. MathSpatial includes three complementary components: (i) MathSpatial-Bench, a benchmark of 2K problems across three categories and eleven subtypes, designed to isolate reasoning difficulty from perceptual noise; (ii) MathSpatial-Corpus, a training dataset of 8K additional problems with verified solutions; and (iii) MathSpatial-SRT, which models reasoning as structured traces composed of three atomic operations--Correlate, Constrain, and Infer. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial achieves competitive accuracy while reducing tokens by 25\%. MathSpatial provides the first large-scale resource that disentangles perception from reasoning, enabling precise measurement and comprehensive understanding of mathematical spatial reasoning in MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MLLMs 真的了解空间吗？一种数学推理评估</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）在感知导向任务上取得了出色的表现，但它们执行数学空间推理的能力，即解析和操作二维和三维关系的能力，仍然不清楚。人类以超过95%的准确率轻松解决教科书式的空间推理问题，但我们发现，大多数领先的MLLMs在相同任务上甚至无法达到60%的准确率。这一显著差距突显了当前模型在空间推理方面的根本弱点。为了研究这一差距，我们提出了MathSpatial，这是一种统一框架，用于评估和提高MLLMs的空间推理能力。MathSpatial 包括三个互补的组件：(i) MathSpatial-Bench，一个包含2000个问题的基准，涵盖三个类别和十一种子类型，旨在将推理难度与感知噪声分离；(ii) MathSpatial-Corpus，一个包含8000个额外问题的训练数据集，附有验证过的解决方案；(iii) MathSpatial-SRT，它将推理建模为由三种原子操作——关联、约束和推理——组成的结构化痕迹。实验表明，对Qwen2.5-VL-7B进行微调，在MathSpatial上实现了竞争力的同时减少了25%的标记。MathSpatial 提供了第一个大规模资源，将感知与推理分离，使数学空间推理在MLLMs中的精确测量和全面理解成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to evaluate the ability of multimodal large language models (MLLMs) in performing mathematical spatial reasoning, which is crucial for parsing and manipulating two- and three-dimensional relations. The study introduces MathSpatial, a unified framework comprising MathSpatial-Bench, MathSpatial-Corpus, and MathSpatial-SRT, to assess and enhance spatial reasoning in MLLMs. Experiments show that fine-tuning Qwen2.5-VL-7B on MathSpatial improves accuracy while reducing token usage by 25%. This work highlights the significant gap between human and MLLM performance in spatial reasoning and provides a comprehensive resource for measuring and understanding this capability in MLLMs.</div>
<div class="mono" style="margin-top:8px">该研究评估了多模态大型语言模型（MLLMs）在数学空间推理方面的能力，这对于理解二维和三维关系至关重要。尽管在感知导向任务上表现出色，但大多数领先MLLMs在解决空间推理问题时准确率较低。为解决这一问题，研究人员开发了MathSpatial框架，包括基准（MathSpatial-Bench）、训练数据集（MathSpatial-Corpus）和推理模型（MathSpatial-SRT）。对Qwen2.5-VL-7B进行MathSpatial微调后，准确率有所提高，同时减少了25%的令牌使用量。这项工作提供了一个新的资源，用于测量和理解MLLMs在数学空间推理方面的表现，突显了它们在这一领域的当前局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Vi-TacMan: Articulated Object Manipulation via Vision and Touch</div>
<div class="meta-line">Authors: Leiyao Cui, Zihang Zhao, Sirui Xie, Wenhuan Zhang, Zhi Han, Yixin Zhu</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-10-07T18:02:50+00:00 · Latest: 2026-02-12T06:24:23+00:00</div>
<div class="meta-line">Comments: ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.06339v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.06339v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous manipulation of articulated objects remains a fundamental challenge for robots in human environments. Vision-based methods can infer hidden kinematics but can yield imprecise estimates on unfamiliar objects. Tactile approaches achieve robust control through contact feedback but require accurate initialization. This suggests a natural synergy: vision for global guidance, touch for local precision. Yet no framework systematically exploits this complementarity for generalized articulated manipulation. Here we present Vi-TacMan, which uses vision to propose grasps and coarse directions that seed a tactile controller for precise execution. By incorporating surface normals as geometric priors and modeling directions via von Mises-Fisher distributions, our approach achieves significant gains over baselines (all p&lt;0.0001). Critically, manipulation succeeds without explicit kinematic models -- the tactile controller refines coarse visual estimates through real-time contact regulation. Tests on more than 50,000 simulated and diverse real-world objects confirm robust cross-category generalization. This work establishes that coarse visual cues suffice for reliable manipulation when coupled with tactile feedback, offering a scalable paradigm for autonomous systems in unstructured environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vi-TacMan：通过视觉和触觉进行 articulated 物体操作</div>
<div class="mono" style="margin-top:8px">在人类环境中，机器人自主操作 articulated 物体仍然是一个基本挑战。基于视觉的方法可以推断出隐藏的运动学，但在不熟悉的物体上可能会产生不精确的估计。基于触觉的方法通过接触反馈实现稳健控制，但需要准确的初始化。这表明自然的协同作用：视觉提供全局指导，触觉提供局部精度。然而，没有框架系统地利用这种互补性进行通用的 articulated 操作。在这里，我们提出了 Vi-TacMan，它使用视觉来提出抓取和粗略方向，作为触觉控制器的种子，以实现精确执行。通过将表面法线作为几何先验并使用 von Mises-Fisher 分布建模方向，我们的方法在基线之上取得了显著的改进（所有 p&lt;0.0001）。关键的是，操作无需显式的运动学模型——触觉控制器通过实时接触调节来细化粗略的视觉估计。在超过 50,000 个模拟和多样化的实际物体上进行的测试证实了跨类别稳健的泛化能力。这项工作表明，当与触觉反馈结合时，粗略的视觉提示足以实现可靠的操纵，为自主系统在非结构化环境中的应用提供了可扩展的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of autonomously manipulating articulated objects using a hybrid vision and touch approach. Vi-TacMan uses vision to propose grasps and directions, which are then refined by a tactile controller. This method achieves significant improvements over baseline techniques and demonstrates robust performance across various objects without requiring explicit kinematic models, showcasing the potential of combining visual and tactile sensing for reliable manipulation in unstructured environments.</div>
<div class="mono" style="margin-top:8px">Vi-TacMan 通过结合视觉和触觉来解决自主操纵 articulated 物体的挑战。它利用视觉提出抓取和方向，然后由触觉控制器进行细化。该方法在没有显式运动学模型的情况下显示出显著改进，并且能够在各种物体上实现稳健的操作，展示了跨类别的一般化能力。</div>
</details>
</div>
<div class="card">
<div class="title">RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation</div>
<div class="meta-line">Authors: Xiangyu Chen, Chuhao Zhou, Yuxi Liu, Jianfei Yang</div>
<div class="meta-line">First: 2025-10-16T23:15:34+00:00 · Latest: 2026-02-12T03:56:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.15189v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.15189v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Precise robot manipulation is critical for fine-grained applications such as chemical and biological experiments, where even small errors (e.g., reagent spillage) can invalidate an entire task. Existing approaches often rely on pre-collected expert demonstrations and train policies via imitation learning (IL) or offline reinforcement learning (RL). However, obtaining high-quality demonstrations for precision tasks is difficult and time-consuming, while offline RL commonly suffers from distribution shifts and low data efficiency. We introduce a Role-Model Reinforcement Learning (RM-RL) framework that unifies online and offline training in real-world environments. The key idea is a role-model strategy that automatically generates labels for online training data using approximately optimal actions, eliminating the need for human demonstrations. RM-RL reformulates policy learning as supervised training, reducing instability from distribution mismatch and improving efficiency. A hybrid training scheme further leverages online role-model data for offline reuse, enhancing data efficiency through repeated sampling. Extensive experiments show that RM-RL converges faster and more stably than existing RL methods, yielding significant gains in real-world manipulation: 53% improvement in translation accuracy and 20% in rotation accuracy. Finally, we demonstrate the successful execution of a challenging task, precisely placing a cell plate onto a shelf, highlighting the framework&#x27;s effectiveness where prior methods fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RM-RL: 模型示范强化学习在精确机器人操作中的应用</div>
<div class="mono" style="margin-top:8px">精确的机器人操作对于化学和生物实验等精细应用至关重要，即使是微小的错误（例如试剂溢出）也可能使整个任务无效。现有方法通常依赖于预先收集的专家示范，并通过模仿学习（IL）或离线强化学习（RL）训练策略。然而，获取精确任务的高质量示范既困难又耗时，而离线RL通常会遭受分布偏移和低数据效率的问题。我们提出了一种角色模型强化学习（RM-RL）框架，该框架在真实环境中的在线和离线训练进行了统一。关键思想是一种角色模型策略，该策略使用近似最优动作自动为在线训练数据生成标签，从而消除了对人类示范的需求。RM-RL将策略学习重新表述为监督训练，减少了分布不匹配带来的不稳定性并提高了效率。混合训练方案进一步利用在线角色模型数据进行离线重用，通过重复采样提高数据效率。大量实验表明，RM-RL比现有RL方法更快、更稳定地收敛，实现在真实世界操作中的显著改进：翻译精度提高53%，旋转精度提高20%。最后，我们展示了精确放置细胞板到架子上的复杂任务的成功执行，突显了该框架在先前方法失败时的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenges of precise robot manipulation in fine-grained applications by introducing RM-RL, a framework that combines online and offline training. It uses a role-model strategy to generate labels for online training data, reducing the need for human demonstrations and improving data efficiency. Experiments show that RM-RL outperforms existing methods, achieving 53% improvement in translation accuracy and 20% in rotation accuracy.</div>
<div class="mono" style="margin-top:8px">研究旨在提高精确的机器人操作，适用于化学和生物实验等应用，其中小错误可能会导致整个任务无效。该研究引入了RM-RL框架，结合了在线和离线训练，在真实环境中使用角色模型策略生成在线训练数据的标签。这种方法减少了不稳定性并提高了数据效率，使得收敛更快且更稳定，相比现有方法。实验结果显示，在平移精度上提高了53%，在旋转精度上提高了20%，并且该框架成功执行了将细胞板精确放置到架子上的挑战性任务，展示了其在先前方法失败情况下的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Thought Purity: A Defense Framework For Chain-of-Thought Attack</div>
<div class="meta-line">Authors: Zihao Xue, Zhen Bi, Long Ma, Zhenlin Hu, Yan Wang, Xueshu Chen, Zhenfang Liu, Kang Zhao, Jie Xiao, Jungang Lou</div>
<div class="meta-line">First: 2025-07-16T15:09:13+00:00 · Latest: 2026-02-12T03:14:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12314v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.12314v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) leverage Chain-of-Thought (CoT) reasoning to solve complex tasks, but this explicit reasoning process introduces a critical vulnerability: adversarial manipulation of the thought chain itself, known as Chain-of-Thought Attacks (CoTA). Such attacks subtly corrupt the reasoning path to produce erroneous outputs, challenging conventional defenses that often sacrifice model utility for safety. To address this, we propose Thought Purity(TP), a defense framework that shifts from passive refusal to active reasoning recovery. TP integrates a safety-aware data pipeline with reinforcement learning, employing a dual-reward mechanism to teach models to dynamically identify and isolate malicious logic while preserving correct reasoning. Experiments on multiple model families demonstrate that TP significantly reduces the attack success rate of CoTA while maintaining or enhancing the model&#x27;s performance on benign tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维纯净：链式思维攻击的防御框架</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）利用链式思维（CoT）推理来解决复杂任务，但这种显式的推理过程引入了一个关键的漏洞：对手对思维链本身的操纵，称为链式思维攻击（CoTA）。此类攻击通过微妙地篡改推理路径来产生错误输出，挑战了传统的防御措施，这些措施往往为了安全而牺牲模型的实用性。为了解决这一问题，我们提出了思维纯净（TP），这是一种防御框架，从被动拒绝转向积极的推理恢复。TP 结合了一个安全意识数据管道和强化学习，采用双重奖励机制来教导模型动态识别和隔离恶意逻辑，同时保持正确的推理。在多个模型家族上的实验表明，TP 显著降低了 CoTA 的攻击成功率，同时保持或提升了模型在良性任务上的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of Large Reasoning Models (LRMs) to Chain-of-Thought Attacks (CoTA), where adversarial manipulation of the reasoning process leads to erroneous outputs. To counter this, the authors propose Thought Purity (TP), a defense framework that uses a safety-aware data pipeline and reinforcement learning to dynamically identify and isolate malicious logic, thereby preserving correct reasoning. Experiments show that TP effectively reduces CoTA success rates while maintaining or improving model performance on benign tasks.</div>
<div class="mono" style="margin-top:8px">论文针对大型推理模型（LRMs）在链式思维攻击（CoTA）下的脆弱性，即攻击者通过操纵推理过程来产生错误输出的问题。提出了一种名为Thought Purity (TP)的防御框架，该框架结合了安全感知的数据管道和强化学习，帮助模型检测和隔离恶意逻辑，从而恢复正确的推理。实验表明，TP可以降低CoTA的成功率，同时不损害模型在良性任务上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Persuasive Interactions between Generative Social Agents and Humans: The Knowledge-based Persuasion Model (KPM)</div>
<div class="meta-line">Authors: Stephan Vonschallen, Friederike Eyssel, Theresa Schmiedel</div>
<div class="meta-line">First: 2026-02-12T01:53:28+00:00 · Latest: 2026-02-12T01:53:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11483v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11483v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative social agents (GSAs) use artificial intelligence to autonomously communicate with human users in a natural and adaptive manner. Currently, there is a lack of theorizing regarding interactions with GSAs, and likewise, few guidelines exist for studying how they influence user attitudes and behaviors. Consequently, we propose the Knowledge-based Persuasion Model (KPM) as a novel theoretical framework. According to the KPM, a GSA&#x27;s self, user, and context-related knowledge drives its persuasive behavior, which in turn shapes the attitudes and behaviors of a responding human user. By synthesizing existing research, the model offers a structured approach to studying interactions with GSAs, supporting the development of agents that motivate rather than manipulate humans. Accordingly, the KPM encourages the integration of responsible GSAs that adhere to social norms and ethical standards with the goal of increasing user wellbeing. Implications of the KPM for research and application domains such as healthcare and education are discussed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解生成性社会代理与人类之间有说服力的互动：基于知识的说服模型（KPM）</div>
<div class="mono" style="margin-top:8px">生成性社会代理（GSAs）利用人工智能自主以自然和适应性的方式与人类用户交流。目前，关于与GSAs互动的理论研究不足，也没有多少研究指导如何研究它们对用户态度和行为的影响。因此，我们提出了基于知识的说服模型（KPM）作为新的理论框架。根据KPM，GSAs的自我、用户和情境相关知识驱动其说服行为，进而影响回应的人类用户的态度和行为。通过综合现有研究，该模型提供了一种结构化的研究方法，支持开发能够激励而非操控人类的代理。因此，KPM鼓励集成遵守社会规范和伦理标准的责任型GSAs，以提高用户福祉。讨论了KPM在研究和应用领域如医疗保健和教育中的意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research proposes the Knowledge-based Persuasion Model (KPM) to understand persuasive interactions between generative social agents (GSAs) and humans. The KPM suggests that a GSA&#x27;s self, user, and context-related knowledge influence its persuasive behavior, which in turn affects the user&#x27;s attitudes and behaviors. Key findings include the structured approach to studying GSA interactions and the encouragement of responsible GSAs that promote user wellbeing.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出知识基础说服模型（KPM）来理解生成型社交代理（GSAs）与人类之间的说服性互动。模型认为，GSAs的自我、用户和情境相关知识会影响其说服行为，进而影响人类用户的态度和行为。主要发现包括KPM为研究GSAs互动提供的结构化方法，以及其在开发遵守社会规范和伦理标准的GSAs方面的作用，以提高用户福祉。</div>
</details>
</div>
<div class="card">
<div class="title">EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos</div>
<div class="meta-line">Authors: Tao Zhang, Song Xia, Ye Wang, Qin Jin</div>
<div class="meta-line">Venue: icra 2026</div>
<div class="meta-line">First: 2026-02-12T00:41:01+00:00 · Latest: 2026-02-12T00:41:01+00:00</div>
<div class="meta-line">Comments: icra 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11464v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11464v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zt375356.github.io/EasyMimic-Project/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EasyMimic：低成本家庭机器人从人类视频模仿学习框架</div>
<div class="mono" style="margin-top:8px">机器人模仿学习常常受到大规模现实数据收集成本高的限制。对于设计用于家庭使用的低成本机器人来说，这一挑战尤为显著，因为它们必须既用户友好又经济实惠。为了解决这一问题，我们提出了EasyMimic框架，这是一种低成本且可复制的解决方案，使机器人能够快速从标准RGB摄像头拍摄的人类视频演示中学习操作策略。该方法首先从视频中提取3D手部轨迹。然后，动作对齐模块将这些轨迹映射到低成本机器人上的夹爪控制空间。为了弥合人类与机器人之间的领域差距，我们引入了一种简单且用户友好的手部视觉增强策略。然后，我们使用一种协同训练方法，对处理后的人类数据和少量机器人数据进行微调，从而实现快速适应新任务。在低成本LeRobot平台上进行的实验表明，EasyMimic在各种操作任务中均表现出色。它显著减少了对昂贵机器人数据收集的依赖，为将智能机器人引入家庭提供了一条实用途径。项目网站：https://zt375356.github.io/EasyMimic-Project/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the high cost of collecting real-world data for robot imitation learning, especially for low-cost robots. The EasyMimic framework extracts 3D hand trajectories from human video demonstrations and maps them to the robot&#x27;s gripper control space. It uses a co-training method with both human and robot data to bridge the domain gap. Experiments show that EasyMimic performs well on various manipulation tasks on the LeRobot platform, significantly reducing the need for expensive data collection.</div>
<div class="mono" style="margin-top:8px">研究旨在解决机器人模仿学习中大规模真实数据收集成本高的问题，特别是针对家庭使用的低成本机器人。EasyMimic框架从人类视频演示中提取3D手部轨迹，并使用动作对齐模块将其映射到机器人的夹爪控制空间。它引入了手部视觉增强策略和协同训练方法以适应新任务。实验表明，EasyMimic在各种操作任务中表现出色，显著减少了对昂贵机器人数据收集的依赖。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
