<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-10 14:02</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260210_1402</div>
    <div class="row"><div class="card">
<div class="title">TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation</div>
<div class="meta-line">Authors: Qinwen Xu, Jiaming Liu, Rui Zhou, Shaojun Shi, Nuowei Han, Zhuoyang Liu, Chenyang Gu, Shuo Gu, Yang Yue, Gao Huang, Wenzhao Zheng, Sirui Han, Peng Jia, Shanghang Zhang</div>
<div class="meta-line">First: 2026-02-09T18:59:52+00:00 · Latest: 2026-02-09T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09023v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TwinRL-VLA：数字孪生驱动的强化学习在现实世界机器人操作中的应用</div>
<div class="mono" style="margin-top:8px">尽管具有强大的泛化能力，视觉-语言-动作（VLA）模型仍然受到专家演示成本高昂和现实世界交互不足的限制。虽然在线强化学习（RL）在提高基础模型的一般性方面显示出潜力，但在现实世界环境中将RL应用于VLA操作仍然受到探索效率低和探索空间受限的阻碍。通过系统性的现实世界实验，我们观察到在线RL的有效探索空间与其监督微调（SFT）的数据分布密切相关。受此观察的启发，我们提出了TwinRL，这是一种数字孪生与现实世界协作的RL框架，旨在扩展和引导VLA模型的探索。首先，我们从智能手机拍摄的场景中高效重建了一个高保真度的数字孪生，使现实和模拟环境之间能够进行现实的双向转移。在SFT预热阶段，我们引入了使用数字孪生扩展探索空间的策略，以扩大数据轨迹分布的支持。在此增强的初始化基础上，我们提出了一个从模拟到现实的引导探索策略，以进一步加速在线RL。具体而言，TwinRL在部署前在数字孪生中高效并行地执行在线RL，有效地弥合了离线和在线训练阶段之间的差距。随后，我们利用高效的数字孪生采样来识别故障多但信息丰富的配置，并使用这些配置来指导现实机器人上的目标导向的人机交互式滚动。在我们的实验中，TwinRL在由现实世界演示覆盖的分布内区域和分布外区域均达到了100%的成功率，比之前的现实世界RL方法至少快30%，并且在四个任务上平均只需约20分钟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the generalization and exploration efficiency of Vision-Language-Action (VLA) models in real-world robotic manipulation. TwinRL, a digital twin-real-world collaborative reinforcement learning framework, is proposed. It reconstructs a high-fidelity digital twin from smartphone-captured scenes to facilitate bidirectional data transfer. TwinRL expands the exploration space during supervised fine-tuning and uses sim-to-real guided exploration to accelerate online RL. The method achieves 100% success in both in-distribution and out-of-distribution regions, with at least a 30% speedup over previous methods, requiring only about 20 minutes on average across four tasks.</div>
<div class="mono" style="margin-top:8px">TwinRL-VLA 是一种数字孪生-现实世界协作的强化学习框架，旨在提高 Vision-Language-Action 模型在真实世界机器人操作中的探索效率和泛化能力。它利用从智能手机拍摄的场景中重建的高度逼真数字孪生，在监督微调阶段扩展探索空间，并指导在线强化学习。TwinRL 在分布内和分布外区域均实现了 100% 的成功率，并且比之前的方法至少快 30%，平均只需约 20 分钟即可完成四个任务。</div>
</details>
</div>
<div class="card">
<div class="title">$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies</div>
<div class="meta-line">Authors: Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai, Hongyang Li, Huijie Wang, Jin Chen, Kaiyang Wu, Li Chen, Lirui Zhao, Modi Shi, Ping Luo, Qingwen Bu, Shijia Peng, Tianyu Li, Yibo Yuan</div>
<div class="meta-line">First: 2026-02-09T18:59:45+00:00 · Latest: 2026-02-09T18:59:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09021v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09021v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$χ_{0}$: 资源感知鲁棒操作通过驯服分布不一致性</div>
<div class="mono" style="margin-top:8px">高可靠性的长期机器人操作传统上依赖大规模数据和计算能力来理解复杂的现实世界动力学。然而，我们发现现实世界鲁棒性的主要瓶颈不仅在于资源规模，还在于人类演示分布、策略学到的归纳偏见和测试时执行分布之间的分布偏移——这是一种系统性不一致性，导致多阶段任务中的累积错误。为了缓解这些不一致性，我们提出了$χ_{0}$，一种资源高效的框架，具有专门设计的有效模块，以实现机器人操作的生产级鲁棒性。我们的方法基于三个技术支柱：(i) 模型算术，一种权重空间合并策略，能够高效地吸收不同演示的多样化分布，从物体外观到状态变化；(ii) 阶段优势，一种阶段感知的优势估计器，提供稳定、密集的进步信号，克服了之前非阶段方法的数值不稳定性；(iii) 训练部署对齐，通过时空增强、启发式DAgger修正和时间片段平滑来弥合分布差距。$χ_{0}$ 使两台双臂机器人能够协作执行长期的服装操作，涵盖从平整、折叠到挂不同衣物的任务。我们的方法展示了高可靠性的自主性；我们能够从任意初始状态连续运行系统24小时不间断。实验验证了$χ_{0}$ 在成功率上比最先进的$π_{0.5}$ 高出近250%，仅使用20小时数据和8个A100 GPU。代码、数据和模型将被发布以促进社区的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of achieving high-reliability long-horizon robotic manipulation by focusing on distributional inconsistencies among human demonstrations, learned policies, and test-time execution. It introduces $χ_{0}$, a resource-efficient framework that includes Model Arithmetic, Stage Advantage, and Train-Deploy Alignment to mitigate these inconsistencies. Experiments show that $χ_{0}$ significantly outperforms the state-of-the-art method $π_{0.5}$ by 250% in success rate, using only 20 hours of data and 8 A100 GPUs.</div>
<div class="mono" style="margin-top:8px">论文旨在通过缓解分布不一致问题来实现高可靠性的长期机器人操作。提出了$χ_{0}$框架，包含模型算术、阶段优势和训练部署对齐三个技术支柱。这些组件有助于处理多样分布并提供稳定的进度信号，成功完成多阶段任务。实验表明，$χ_{0}$在成功率达到近250%的提升上超越了最先进的方法$π_{0.5}$，且使用了更少的数据和计算资源。</div>
</details>
</div>
<div class="card">
<div class="title">Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models</div>
<div class="meta-line">Authors: Zichen Jeff Cui, Omar Rayyan, Haritheja Etukuru, Bowen Tan, Zavier Andrianarivo, Zicheng Teng, Yihang Zhou, Krish Mehta, Nicholas Wojno, Kevin Yuanbo Wu, Manan H Anjaria, Ziyuan Wu, Manrong Mao, Guangxun Zhang, Binit Shah, Yejin Kim, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah</div>
<div class="meta-line">First: 2026-02-09T18:58:50+00:00 · Latest: 2026-02-09T18:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09017v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cap-policy.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/</div></details>
</div>
<div class="card">
<div class="title">Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction</div>
<div class="meta-line">Authors: Hongyi Chen, Tony Dong, Tiancheng Wu, Liquan Wang, Yash Jangir, Yaru Niu, Yufei Ye, Homanga Bharadhwaj, Zackory Erickson, Jeffrey Ichnowski</div>
<div class="meta-line">First: 2026-02-09T18:56:02+00:00 · Latest: 2026-02-09T18:56:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09013v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从RGB人体视频中通过4D手-物轨迹重建学习灵巧操作策略</div>
<div class="mono" style="margin-top:8px">多指机器人手部操作和抓取由于高维动作空间和大规模训练数据获取的困难而具有挑战性。现有方法主要依赖穿戴设备或专用传感设备的人类远程操作来捕捉手-物交互，这限制了其可扩展性。在本工作中，我们提出了一种无需设备的框架VIDEOMANIP，可以直接从RGB人体视频中学习灵巧操作。利用计算机视觉的最新进展，VIDEOMANIP通过估计人体手部姿态、物体网格并重新定位重建的人体动作来从单目视频中重建显式的4D机器人-物体轨迹，从而进行操作学习。为了使重建的机器人数据适合灵巧操作训练，我们引入了基于交互的手-物接触优化和以交互为中心的抓取建模，以及一种从单个视频生成多样化训练轨迹的演示合成策略，从而在无需额外机器人演示的情况下实现泛化策略学习。在仿真中，使用Inspire手学习的抓取模型在20种不同物体上实现了70.25%的成功率。在现实世界中，从RGB视频中训练的操作策略在使用LEAP手执行七个任务时实现了平均62.86%的成功率，优于基于重新定位的方法15.87%。项目视频可在videomanip.github.io获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of multi-finger robotic hand manipulation by proposing VIDEOMANIP, a framework that learns dexterous manipulation policies directly from RGB human videos. It reconstructs 4D hand-object trajectories using computer vision techniques and retargets these motions to robotic hands. The method includes hand-object contact optimization and a demonstration synthesis strategy to generate diverse training data. Experiments show a 70.25% success rate in simulation and an average 62.86% success rate in real-world tasks, outperforming existing retargeting-based methods.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种名为VIDEOMANIP的无设备框架，直接从RGB人体视频中学习灵巧的操纵策略。该框架利用计算机视觉技术重建4D手-物体轨迹，并将这些轨迹重新映射到机器人手上。该框架包括手-物体接触优化和演示合成策略，以生成多样化的训练数据。实验结果显示，在模拟中的成功率达到了70.25%，在现实世界任务中的平均成功率达到了62.86%，优于现有的基于重新映射的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure</div>
<div class="meta-line">Authors: Zirui Li, Xuefeng Bai, Kehai Chen, Yizhi Li, Jian Yang, Chenghua Lin, Min Zhang</div>
<div class="meta-line">First: 2026-02-09T15:25:12+00:00 · Latest: 2026-02-09T15:25:12+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在隐性推理动态：因果结构的实证研究</div>
<div class="mono" style="margin-top:8px">潜在或连续的隐性推理方法用一系列内部潜在步骤取代了明确的文本推理，但这些中间计算难以通过基于相关性的探针进行评估。在本文中，我们将潜在隐性推理视为在表示空间中的可操控因果过程，通过将潜在步骤建模为结构因果模型（SCM）中的变量，并通过逐步的$\mathrm{do}$-干预分析其影响。我们研究了两种代表性范式（即Coconut和CODI）在数学和一般推理任务上的表现，以探讨三个关键问题：（1）哪些步骤对于正确性是因果必要的，以及何时答案可以早期决定；（2）影响如何在步骤之间传播，这种结构与明确的CoT相比有何不同；（3）中间轨迹是否保留了竞争的答案模式，以及输出级承诺与表示级承诺在步骤之间有何不同。我们发现，潜在步骤的预算行为更像是分阶段的功能性而非均匀的额外深度，并且我们发现早期输出偏差与晚期表示承诺之间存在持续的差距。这些结果促使我们进行条件模式和稳定性分析——以及相应的训练/解码目标——作为更可靠的工具来解释和改进潜在推理系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates latent chain-of-thought methods by modeling them as a causal process in representation space using structural causal models (SCM). The authors study Coconut and CODI on mathematical and general reasoning tasks to explore the necessity of latent steps, the propagation of influence, and the retention of answer modes. Key findings include that latent-step budgets exhibit staged functionality rather than homogeneous depth, and there is a persistent gap between early output bias and late representational commitment.</div>
<div class="mono" style="margin-top:8px">本文通过将潜在步骤建模为结构因果模型（SCM）中的变量，并使用逐步do干预来探索潜在链式思维中的因果结构。研究在数学和一般推理任务上考察了两个范式，以了解哪些步骤对于正确性是必要的，影响如何在步骤之间传播，以及中间轨迹是否保留竞争答案模式。关键发现包括潜在步骤预算表现出阶段功能性和非局部路由，早期输出偏差与晚期表征承诺之间存在持续差距，这表明需要条件模式和稳定性意识分析来解释和改进这些系统。</div>
</details>
</div>
<div class="card">
<div class="title">Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch</div>
<div class="meta-line">Authors: Cuijie Xu, Shurui Zheng, Zihao Su, Yuanfan Xu, Tinghao Yi, Xudong Zhang, Jian Wang, Yu Wang, Jinchen Yu</div>
<div class="meta-line">First: 2026-02-09T15:18:12+00:00 · Latest: 2026-02-09T15:18:12+00:00</div>
<div class="meta-line">Comments: 14 pages, 9 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08776v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xucj98.github.io/mind-the-gap-page/}{project">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot&#x27;s executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to &quot;Intent Cloning&quot; (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator&#x27;s strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a &quot;virtual equilibrium point&quot;, effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意差距：通过意图-执行差异学习视觉运动策略中的隐式阻抗</div>
<div class="mono" style="margin-top:8px">远程操作本质上依赖于人类操作者作为闭环控制器主动补偿硬件缺陷，包括延迟、机械摩擦和缺乏明确的力反馈。标准行为克隆（BC）通过模仿机器人执行的轨迹，根本上忽略了这种补偿机制。在本研究中，我们提出了一种双状态条件框架，将学习目标转向“意图克隆”（主命令）。我们认为，意图-执行差异，即主命令与从命令之间的差异，不是噪声，而是物理上编码隐式交互力的关键信号，并且算法上揭示了操作者克服系统动力学的策略。通过预测主意图，我们的策略学会生成一个“虚拟平衡点”，从而实现隐式阻抗控制。此外，通过明确地基于这种差异的历史进行条件处理，模型执行隐式系统识别，将跟踪误差视为外部力以关闭控制回路。为了弥合由于推理延迟造成的时序差距，我们将策略进一步形式化为轨迹修复者，以确保连续控制。我们在一个无传感器、低成本的双臂设置上验证了我们的方法。在涉及接触丰富操作和动态跟踪的任务中，实证结果显示出明显的差距：标准执行克隆由于无法克服接触刚度和跟踪滞后而失败，而我们的差异感知方法实现了稳健的成功。这为低成本硬件提供了一个简约的行为克隆框架，使其能够在不依赖显式力感知的情况下实现力感知和动态补偿。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of standard Behavior Cloning in teleoperation by proposing a Dual-State Conditioning framework that focuses on Intent Cloning. The method leverages the Intent-Execution Mismatch to learn implicit impedance control, effectively closing the control loop without explicit force feedback. Experimental results show that the proposed approach outperforms standard execution-cloning in tasks requiring dynamic tracking and contact-rich manipulation, demonstrating robust performance even in the presence of contact stiffness and tracking lag.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种Dual-State Conditioning框架，专注于&#x27;Intent Cloning&#x27;以学习隐式阻抗控制。方法利用意图-执行差异揭示操作者克服系统动力学的策略。实验结果表明，该方法通过预测主命令并基于差异的历史进行条件化，成功完成了接触丰富的操作和动态跟踪任务，而标准的执行克隆方法由于接触刚度和跟踪滞后而失败。</div>
</details>
</div>
<div class="card">
<div class="title">SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</div>
<div class="meta-line">Authors: Hanzhen Wang, Jiaming Xu, Yushun Xiang, Jiayi Pan, Yongkang Zhou, Yong-Lu Li, Guohao Dai</div>
<div class="meta-line">First: 2025-09-06T06:22:19+00:00 · Latest: 2026-02-09T13:23:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05614v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.05614v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pruning is a typical acceleration technique for compute-bound models by removing computation on unimportant values. Recently, it has been applied to accelerate Vision-Language-Action (VLA) model inference. However, existing acceleration methods focus on local information from the current action step and ignore the global context, leading to &gt;20% success rate drop and limited speedup in some scenarios. In this paper, we point out spatial-temporal consistency in VLA tasks: input images in consecutive steps exhibit high similarity, and propose the key insight that token selection should combine local information with global context of the model. Based on this, we propose SpecPrune-VLA, a training-free, two-level pruning method with heuristic control. (1) Action-level static pruning. We leverage global history and local attention to statically reduce visual tokens per action. (2) Layer-level dynamic pruning. We prune tokens adaptively per layer based on layer-wise importance. (3) Lightweight action-aware controller: We classify actions as coarse- or fine-grained by the speed of the end effector and adjust pruning aggressiveness accordingly. Extensive experiments show that SpecPrune-VLA achieves up to 1.57$\times$ speedup in LIBERO simulation and 1.70$\times$ on real-world tasks, with negligible success rate degradation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpecPrune-VLA：通过动作感知自推测性剪枝加速视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">剪枝是一种通过移除不重要计算来加速计算密集型模型的典型技术。最近，它被应用于加速视觉-语言-动作（VLA）模型推理。然而，现有的加速方法集中在当前动作步骤的局部信息上，而忽略了全局上下文，导致在某些场景中成功率下降超过20%，并且加速效果有限。在本文中，我们指出了VLA任务中的时空一致性：连续步骤中的输入图像表现出高度相似性，并提出关键见解，即token选择应结合局部信息和模型的全局上下文。基于此，我们提出了SpecPrune-VLA，这是一种无需训练的两级剪枝方法，具有启发式控制。 (1) 动作级静态剪枝。我们利用全局历史和局部注意力静态减少每个动作的视觉token数量。 (2) 层级动态剪枝。我们基于每层的重要性逐层动态剪枝token。 (3) 轻量级动作感知控制器：我们通过末端执行器的速度将动作分类为粗粒度或细粒度，并相应调整剪枝的激进程度。广泛的实验表明，SpecPrune-VLA 在LIBERO仿真中实现了高达1.57倍的加速，在实际任务中实现了高达1.70倍的加速，且成功率下降可以忽略不计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to accelerate Vision-Language-Action (VLA) models by addressing the limitations of existing pruning methods that focus on local information and ignore global context. SpecPrune-VLA, a two-level pruning method, is proposed to combine local and global information. It includes action-level static pruning that uses global history and local attention, layer-level dynamic pruning that adapts per layer based on importance, and a lightweight action-aware controller that adjusts pruning aggressiveness based on action type. Experiments show SpecPrune-VLA achieves up to 1.70$\times$ speedup with minimal impact on success rate.</div>
<div class="mono" style="margin-top:8px">论文通过引入全局上下文来解决现有VLA模型剪枝方法的局限性。SpecPrune-VLA 是一种两级剪枝方法，结合了基于全局历史和局部注意力的动作级静态剪枝，以及基于层重要性的动态层级剪枝。此外，一个动作感知控制器根据末端执行器的速度调整剪枝的激进程度。该方法在仿真中可实现最高1.57倍的加速，在实际任务中可实现最高1.70倍的加速，同时几乎不影响成功率。</div>
</details>
</div>
<div class="card">
<div class="title">SafeLink: Safety-Critical Control Under Dynamic and Irregular Unsafe Regions</div>
<div class="meta-line">Authors: Songqiao Hu, Zidong Wang, Zeyi Liu, Zhen Shen, Xiao He</div>
<div class="meta-line">First: 2025-03-19T14:16:37+00:00 · Latest: 2026-02-09T13:17:56+00:00</div>
<div class="meta-line">Comments: 12 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16551v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.16551v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Control barrier functions (CBFs) provide a theoretical foundation for safety-critical control in robotic systems. However, most existing methods rely on explicit analytical expressions of unsafe state regions, which are often impractical for irregular and dynamic unsafe regions. This paper introduces SafeLink, a novel CBF construction method based on cost-sensitive incremental random vector functional-link (RVFL) neural networks. By designing a valid cost function, SafeLink assigns different sensitivities to safe and unsafe state points, thereby eliminating false negatives in classification of unsafe state points. Under the constructed CBF, theoretical guarantees are established regarding system safety and the Lipschitz continuity of the control inputs. Furthermore, incremental update theorems are provided, enabling precise real-time adaptation to changes in unsafe regions. An analytical expression for the gradient of SafeLink is also derived to facilitate control input computation. The proposed method is validated on the endpoint position control task of a nonlinear two-link manipulator. Experimental results demonstrate that the method effectively learns the unsafe regions and rapidly adapts as these regions change, achieving computational speeds significantly faster than baseline methods while ensuring the system safely reaches its target position.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeLink：在动态和不规则的不安全区域下的安全控制</div>
<div class="mono" style="margin-top:8px">控制屏障函数（CBFs）为机器人系统中的安全关键控制提供了理论基础。然而，大多数现有方法依赖于不安全状态区域的显式解析表达式，这对于不规则和动态的不安全区域往往是不切实际的。本文介绍了SafeLink，一种基于成本敏感增量随机向量函数链接（RVFL）神经网络的新型CBF构建方法。通过设计有效的成本函数，SafeLink 对安全和不安全状态点赋予不同的敏感度，从而在分类不安全状态点时消除假阴性。在构建的CBF下，建立了关于系统安全性和控制输入Lipschitz连续性的理论保证。此外，提供了增量更新定理，使系统能够精确实时地适应不安全区域的变化。还推导了SafeLink的梯度解析表达式，以方便控制输入的计算。所提出的方法在非线性双连杆末端位置控制任务上进行了验证。实验结果表明，该方法能够有效地学习不安全区域，并且在这些区域变化时能够迅速适应，同时计算速度显著快于基线方法，确保系统安全地达到目标位置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SafeLink is a novel method for constructing control barrier functions (CBFs) to ensure safety in robotic systems, especially when dealing with irregular and dynamic unsafe regions. It uses cost-sensitive incremental random vector functional-link neural networks to assign different sensitivities to safe and unsafe state points, improving the classification of unsafe states. The method provides theoretical guarantees for system safety and control input continuity, and allows for real-time adaptation to changes in unsafe regions. Experiments on a nonlinear two-link manipulator show that SafeLink effectively learns and adapts to unsafe regions, achieving faster computational speeds than baseline methods while maintaining safety.</div>
<div class="mono" style="margin-top:8px">SafeLink 是一种基于成本敏感增量随机向量函数链接神经网络的新颖控制屏障函数（CBF）构建方法，用于解决机器人系统中不规则和动态不安全区域带来的挑战。通过为安全和不安全状态点分配不同的敏感性，SafeLink 确保了不安全状态分类的准确性，并提供了系统安全性和控制输入连续性的理论保证。实验结果表明，SafeLink 能够有效地学习和适应变化的不安全区域，计算速度比基线方法快得多，同时保持系统的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs</div>
<div class="meta-line">Authors: Yukun Jiang, Hai Huang, Mingjie Li, Yage Zhang, Michael Backes, Yang Zhang</div>
<div class="meta-line">First: 2026-02-09T13:12:54+00:00 · Latest: 2026-02-09T13:12:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08621v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08621v1">PDF</a> · <a href="https://github.com/TrustAIRLab/UnsafeMoE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer&#x27;s router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏模型，稀疏安全：混合专家LLM中的不安全路径</div>
<div class="mono" style="margin-top:8px">通过在Transformer层中引入路由器以选择性激活专家，混合专家（MoE）架构显著降低了大型语言模型（LLMs）的计算成本，同时保持了竞争力，尤其是在大规模参数模型中。然而，先前的工作主要集中在实用性和效率上，而对这种稀疏架构相关的安全风险则研究不足。在本研究中，我们展示了MoE LLM的安全性与其架构一样稀疏，通过发现不安全路径：一旦激活，这些路由配置会将安全输出转化为有害输出。具体来说，我们首先引入了路由器安全性重要性评分（RoSais）来量化每一层路由器的安全关键性。仅操纵高RoSais路由器即可将默认路由转变为不安全路径。例如，在JailbreakBench上，遮蔽DeepSeek-V2-Lite中的5个路由器将攻击成功率（ASR）提高了4倍以上至0.79，突显了路由器操纵可能在MoE LLM中自然发生的一种固有风险。我们进一步提出了一种细粒度的令牌-层级随机优化框架（F-SOUR）来发现更具体的不安全路径，该框架明确考虑了输入令牌的顺序性和动态性。在四个代表性MoE LLM家族中，F-SOUR在JailbreakBench和AdvBench上的平均ASR分别为0.90和0.98。最后，我们概述了防御视角，包括安全意识路由禁用和路由器训练，作为保护MoE LLM的有前途的方向。我们希望我们的工作能为未来MoE LLM的红队测试和保护提供信息。我们的代码可在https://github.com/TrustAIRLab/UnsafeMoE/获取。</div>
</details>
</div>
<div class="card">
<div class="title">Mimic Intent, Not Just Trajectories</div>
<div class="meta-line">Authors: Renming Huang, Chendong Zeng, Wenjing Tang, Jingtian Cai, Cewu Lu, Panpan Cai</div>
<div class="meta-line">First: 2026-02-09T12:44:35+00:00 · Latest: 2026-02-09T12:44:35+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08602v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories&#x27;&#x27; (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模仿意图，而不仅仅是轨迹</div>
<div class="mono" style="margin-top:8px">尽管生成建模和预训练在灵巧操作方面取得了令人印象深刻的成果，模仿学习（IL）在视觉-语言-动作（VLA）模型等最先进的方法中仍然难以适应环境变化和技能转移。我们认为这源于模仿原始轨迹而未能理解其背后的意图。为了解决这一问题，我们提出在端到端的IL中明确分离行为意图和执行细节：《“模仿意图，而不仅仅是轨迹”（MINT）》。我们通过多尺度频域标记化实现这一目标，这强制执行了动作片段表示的频谱分解。我们学习具有多尺度粗细结构的动作标记，并强制最粗的标记捕捉低频全局结构，而更细的标记编码高频细节。这产生了一个抽象的“意图标记”，有助于规划和转移，以及多尺度“执行标记”，使其能够精确适应环境动力学。在此层次结构的基础上，我们的策略通过逐级自回归生成轨迹，执行逐步的“意图到执行推理”，从而提高学习效率和泛化能力。关键的是，这种分离使技能能够通过简单地将演示中的意图标记注入自回归生成过程实现“一针见血”的转移。在几个操作基准测试和真实机器人上的实验表明，我们的方法具有最先进的成功率、优越的推理效率、对干扰的鲁棒泛化能力以及有效的“一针见血”转移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current imitation learning methods by proposing a new approach called MINT, which focuses on disentangling behavior intent from execution details. The method uses multi-scale frequency-space tokenization to decompose action representations and generate abstract intent and execution tokens. This enables efficient learning and robust generalization, and allows for one-shot skill transfer by injecting the intent token. Experiments show improved success rates, efficiency, and robustness compared to existing methods.</div>
<div class="mono" style="margin-top:8px">论文针对模仿学习模型在适应环境变化和技能转移方面仅关注轨迹的局限性，提出了一种名为MINT的方法，通过多尺度频域分词技术将行为意图与执行细节分离。这种方法生成了抽象的意图令牌和详细的执行令牌，促进了高效学习和鲁棒泛化。实验结果显示，MINT实现了最先进的成功率、优越的推理效率和有效的单次转移技能。</div>
</details>
</div>
<div class="card">
<div class="title">A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation</div>
<div class="meta-line">Authors: Kenghou Hoi, Yuze Wu, Annan Ding, Junjie Wang, Anke Zhao, Chengqian Zhang, Fei Gao</div>
<div class="meta-line">First: 2026-02-09T12:40:34+00:00 · Latest: 2026-02-09T12:40:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08599v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08599v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种精确的实时力感知抓取系统以实现稳健的空中操作</div>
<div class="mono" style="margin-top:8px">空中操作需要力感知能力以实现安全有效的抓取和物理交互。以往的工作往往依赖于沉重且昂贵的力传感器，这些传感器不适合典型的四旋翼平台，或者在抓取过程中不提供力反馈，从而增加了对脆弱物体造成损害的风险。为了解决这些限制，我们提出了一种新颖的力感知抓取框架，结合了六个低成本的敏感皮肤式触觉传感器。我们引入了一种基于磁性的触觉传感模块，提供高精度的三维力测量。我们通过参考霍尔传感器消除地磁干扰，并简化了与以往工作相比的校准过程。所提出的框架使精确的力感知抓取控制成为可能，允许安全操作脆弱物体并实时测量抓取物品的重量。该系统通过全面的实地实验得到验证，包括气球抓取、动态负载变化测试和消融研究，展示了其在各种空中操作场景中的有效性。我们的方法实现了完全机载操作，无需外部运动捕捉系统，显著增强了力敏感空中操作的实用性。补充视频可在以下链接查看：https://www.youtube.com/watch?v=mbcZkrJEf1I.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for force-aware capabilities in aerial manipulation to ensure safe and effective grasping. It proposes a novel framework using six low-cost tactile sensors and a magnetic-based tactile sensing module for precise three-dimensional force measurements. The system eliminates geomagnetic interference and simplifies calibration, enabling real-time force-aware grasping control and weight measurement. Experimental validation through various scenarios, including balloon grasping and dynamic load tests, confirms its effectiveness and practicality in aerial manipulation tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种精确的实时力感知抓取系统，以解决以往方法中使用昂贵的力传感器或缺乏力反馈的问题。提出的框架使用六个低成本的触觉传感器和一个基于磁性的触觉传感模块进行高精度的力测量，通过抓取气球和动态负载测试等实验验证其有效性，能够安全地处理脆弱物体并实时测量其重量。该系统完全在机载运行，无需外部运动捕捉系统，显著增强了其在空中操作中的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs</div>
<div class="meta-line">Authors: Ahmed Salem, Andrew Paverd, Sahar Abdelnabi</div>
<div class="meta-line">First: 2026-02-09T12:01:32+00:00 · Latest: 2026-02-09T12:01:32+00:00</div>
<div class="meta-line">Comments: Accepted at IEEE SaTML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08563v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08563v1">PDF</a> · <a href="https://github.com/microsoft/implicitMemory">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无状态却不忘怀：LLM中的隐性记忆作为隐藏通道</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）通常被视为无状态的：一旦交互结束，除非显式存储并重新提供，否则不会假设任何信息会持续存在。我们通过引入隐性记忆挑战这一假设——模型能够在看似独立的交互之间携带状态，通过在其自身输出中编码信息并在重新引入这些输出作为输入时恢复这些信息。这种机制不需要任何显式记忆模块，却能在推理请求之间创建持久的信息通道。作为具体演示，我们引入了一种新的时间炸弹类时间后门。与传统的在单一触发输入下激活的后门不同，时间炸弹仅在一系列交互满足通过隐性记忆积累的隐藏条件后激活。我们展示了这种行为可以通过简单的提示或微调今天就可诱导。除了这一案例研究，我们还分析了隐性记忆更广泛的影响，包括隐蔽的跨代理通信、基准污染、目标操纵和训练数据污染。最后，我们讨论了检测挑战，并提出了压力测试和评估的方向，旨在预见和控制未来的发展。为了促进未来研究，我们发布了代码和数据：https://github.com/microsoft/implicitMemory。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied.</div>
<div class="mono" style="margin-top:8px">论文通过引入隐性记忆挑战了大型语言模型（LLMs）的状态无存性，使得模型能够在不进行显式存储的情况下跨交互保留信息。作者通过一种新的时间炸弹类临时后门来展示这一点，这种后门在一系列交互后满足隐藏条件时才会激活。研究揭示了更广泛的隐性记忆影响，如隐蔽的跨代理通信和基准测试污染，并讨论了检测挑战和评估方法以应对这些问题。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Sampling to Guide Universal Manipulation RL</div>
<div class="meta-line">Authors: Marc Toussaint, Cornelius V. Braun, Eckart Cobo-Briesewitz, Sayantan Auddy, Armand Jordana, Justin Carpentier</div>
<div class="meta-line">First: 2026-02-09T11:54:45+00:00 · Latest: 2026-02-09T11:54:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08557v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08557v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>受限采样以引导通用操作RL</div>
<div class="mono" style="margin-top:8px">我们考虑如何利用基于模型的求解器来引导训练一个通用策略，以从任何可行的起始状态控制到任何可行的目标状态，在接触丰富的操作环境中。尽管强化学习（RL）在这样的环境中展示了其优势，但在稀疏奖励设置中，它可能难以充分探索和发现复杂的操作策略。我们的方法基于这样一个想法：在这样的操作过程中存在一个低维流形，包含所有可行且可能访问的状态，并使用来自该流形的采样器来引导RL。我们提出了采样引导的RL，该方法使用基于模型的约束求解器高效地采样满足可微碰撞、接触和力约束的可行配置，并利用这些配置来引导RL以生成通用（目标条件）操作策略。我们研究了直接使用这些数据来偏置状态访问，以及使用黑盒优化开放环轨迹之间的随机配置来施加状态偏置，并可选地添加行为克隆损失。在最小化的双球操作设置中，采样引导的RL发现了复杂的操作策略，并在达到任何静态稳定状态方面取得了高成功率。在更具挑战性的熊猫臂设置中，我们的方法在接近零的基线下实现了显著的成功率，并展示了广泛的复杂全身接触操作策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to leverage model-based solvers to guide the training of a universal policy for manipulation tasks in contact-rich environments. The method, Sample-Guided RL, uses a sampler to efficiently generate feasible configurations that satisfy various constraints and guides the RL process. In experiments, the approach successfully discovers complex manipulation strategies and achieves high success rates in a double sphere setting and a significant success rate in a more challenging panda arm setting.</div>
<div class="mono" style="margin-top:8px">研究旨在利用模型导向的求解器来引导通用策略的训练，以应对接触丰富的操作任务。方法Sample-Guided RL使用采样器高效生成满足多种约束的可行配置，并引导RL探索和发现复杂的操作策略。实验结果显示，该方法在双球操作设置中成功发现了复杂的策略，并实现了高成功率；在更具挑战性的熊猫臂设置中，该方法展示了广泛的复杂全身接触操作策略，并显著提高了成功率。</div>
</details>
</div>
<div class="card">
<div class="title">UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation</div>
<div class="meta-line">Authors: Haoming Ye, Yunxiao Xiao, Cewu Lu, Panpan Cai</div>
<div class="meta-line">First: 2026-02-09T11:35:21+00:00 · Latest: 2026-02-09T11:35:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08537v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniPlan：移动操作中的统一PDDL规划框架结合视觉-语言任务规划</div>
<div class="mono" style="margin-top:8px">将VLM推理与符号规划相结合已被证明是现实世界机器人任务规划的一个有前途的方法。现有工作如UniDomain有效从真实世界的演示中学习符号操作领域，并用规划领域定义语言(PDDL)描述这些领域，并成功应用于实际任务。然而，这些领域仅限于桌面操作。我们提出了UniPlan，这是一种用于大型室内环境中的长期移动操作任务规划的视觉-语言系统，它将场景拓扑、视觉和机器人能力统一为一个整体的PDDL表示。UniPlan程序化地将来自UniDomain学习到的桌面领域扩展到支持导航、门穿越和双臂协调。它基于包含场景图像锚定的导航地标视觉-拓扑地图运行。给定语言指令，UniPlan从地图中检索任务相关节点，并使用VLM将锚定的图像接地为任务相关对象及其PDDL状态；接着，它将这些节点重新连接到一个压缩的、密集连接的拓扑地图，该地图也用PDDL表示，连接性和成本来自原始地图；最后，使用现成的PDDL求解器生成移动操作计划。在包含真实世界图像的大规模地图上进行的人类指定任务评估中，UniPlan在成功率、计划质量和计算效率方面显著优于VLM和LLM+PDDL规划。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniPlan is a vision-language task planning system for mobile manipulation in large indoor environments. It integrates scene topology, visuals, and robot capabilities into a unified PDDL representation, extending tabletop manipulation domains to support navigation, door traversal, and bimanual coordination. UniPlan outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency when evaluated on human-raised tasks in a large-scale map with real-world imagery.</div>
<div class="mono" style="margin-top:8px">UniPlan 是一种用于大型室内环境长时间移动操作的任务规划系统，它将场景拓扑、视觉信息和机器人能力统一到一个 PDDL 表示中，扩展了桌面操作域以支持导航、门穿越和双臂协调。给定语言指令，UniPlan 会检索相关节点，使用 VLM 进行语义化，然后使用 PDDL 解算器生成操作计划。实验表明，UniPlan 在成功率、计划质量和计算效率方面优于基于 VLM 和 LLM+PDDL 的规划方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots</div>
<div class="meta-line">Authors: Mingqi Yuan, Tao Yu, Wenqi Ge, Xiuyong Yao, Huijiang Wang, Jiayu Chen, Bo Li, Wei Zhang, Wenjun Zeng, Hua Chen, Xin Jin</div>
<div class="meta-line">Venue: IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2025</div>
<div class="meta-line">First: 2025-06-25T14:35:33+00:00 · Latest: 2026-02-09T10:08:57+00:00</div>
<div class="meta-line">Comments: 19 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20487v5">Abs</a> · <a href="https://arxiv.org/pdf/2506.20487v5">PDF</a> · <a href="https://github.com/yuanmingqi/awesome-bfm-papers">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots are drawing significant attention as versatile platforms for complex motor control, human-robot interaction, and general-purpose physical intelligence. However, achieving efficient whole-body control (WBC) in humanoids remains a fundamental challenge due to sophisticated dynamics, underactuation, and diverse task requirements. While learning-based controllers have shown promise for complex tasks, their reliance on labor-intensive and costly retraining for new scenarios limits real-world applicability. To address these limitations, behavior(al) foundation models (BFMs) have emerged as a new paradigm that leverages large-scale pre-training to learn reusable primitive skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a wide range of downstream tasks. In this paper, we present a comprehensive overview of BFMs for humanoid WBC, tracing their development across diverse pre-training pipelines. Furthermore, we discuss real-world applications, current limitations, urgent challenges, and future opportunities, positioning BFMs as a key approach toward scalable and general-purpose humanoid intelligence. Finally, we provide a curated and regularly updated collection of BFM papers and projects to facilitate more subsequent research, which is available at https://github.com/yuanmingqi/awesome-bfm-papers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为基础模型综述：类人机器人全身控制系统的下一代控制体系</div>
<div class="mono" style="margin-top:8px">类人机器人因其在复杂运动控制、人机交互和通用物理智能方面的多功能平台而受到广泛关注。然而，由于复杂的动力学、欠驱动和多样的任务需求，实现类人机器人全身控制（WBC）仍然是一个基本挑战。尽管基于学习的控制器在复杂任务中显示出潜力，但它们依赖于劳动密集型和昂贵的新场景重新训练限制了其实用性。为了解决这些限制，行为基础模型（BFMs）作为一种新范式出现，通过大规模预训练学习可重用的原始技能和广泛的先验行为，从而实现对广泛下游任务的零样本或快速适应。在本文中，我们对BFMs在类人WBC中的全面概述进行了介绍，追溯了它们在不同预训练管道中的发展。此外，我们讨论了实际应用、当前限制、紧迫挑战和未来机会，将BFMs定位为实现可扩展和通用类人智能的关键方法。最后，我们提供了一个经过精心挑选并定期更新的BFMs论文和项目集合，以促进后续研究，该集合可在https://github.com/yuanmingqi/awesome-bfm-papers获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the development of behavior foundation models (BFMs) for efficient whole-body control in humanoid robots. Motivated by the need for versatile platforms for complex motor control and human-robot interaction, BFMs leverage large-scale pre-training to learn reusable skills and behavioral priors, enabling rapid adaptation to various tasks. Key findings include the ability of BFMs to support zero-shot learning and reduce the need for labor-intensive retraining, making them a promising approach for scalable humanoid intelligence.</div>
<div class="mono" style="margin-top:8px">本文探讨了行为基础模型（BFMs）在人形机器人全身控制中的发展，解决了由于复杂动力学和欠驱动带来的高效控制挑战。BFMs通过大规模预训练学习可重用的技能和行为先验，实现零样本或快速适应多种任务。研究涵盖了多样化的预训练管道，并讨论了实际应用、局限性和未来机会，将BFMs定位为人形智能可扩展的关键方法。</div>
</details>
</div>
<div class="card">
<div class="title">Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</div>
<div class="meta-line">Authors: Yuyang Li, Yinghan Chen, Zihang Zhao, Puhao Li, Tengyu Liu, Siyuan Huang, Yixin Zhu</div>
<div class="meta-line">First: 2025-12-10T17:35:13+00:00 · Latest: 2026-02-09T10:08:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09851v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09851v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of tactile policy(66.3%) and vision-only policy (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同时触觉-视觉感知学习多模态机器人操作</div>
<div class="mono" style="margin-top:8px">机器人操作需要丰富的多模态感知和有效的学习框架来处理复杂的现实世界任务。透明皮肤（STS）传感器结合了触觉和视觉感知，提供了有前景的传感能力，而现代模仿学习则提供了强大的策略获取工具。然而，现有的STS设计缺乏同时的多模态感知，并且触觉跟踪不可靠。此外，将这些丰富的多模态信号整合到基于学习的操作管道中仍然是一个开放的挑战。我们介绍了TacThru，这是一种STS传感器，能够同时进行视觉感知并提取稳健的触觉信号，以及TacThru-UMI，这是一种利用这些多模态信号进行操作的模仿学习框架。我们的传感器具有完全透明的弹性体、持续照明、新型关键线标记和高效的跟踪功能，而我们的学习系统则通过基于Transformer的扩散策略将这些信号整合在一起。在五个具有挑战性的现实世界任务上的实验表明，TacThru-UMI 的平均成功率达到了85.5%，显著优于触觉策略（66.3%）和仅视觉策略（55.4%）的基线。该系统在关键场景中表现出色，包括薄而柔软物体的接触检测和需要多模态协调的精确操作。这项工作表明，将同时的多模态感知与现代学习框架相结合，可以实现更精确和适应性强的机器人操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance robotic manipulation by integrating simultaneous tactile and visual perception using a novel See-through-skin (STS) sensor called TacThru, which provides reliable tactile and visual data. The TacThru-UMI framework uses imitation learning with a Transformer-based Diffusion Policy to leverage these multimodal signals. Experiments on five real-world tasks show that TacThru-UMI achieves an 85.5% success rate, significantly outperforming tactile and vision-only policies. The system excels in detecting contact with thin and soft objects and in precision manipulation requiring multimodal coordination.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用新型See-through-skin (STS)传感器TacThru，同时集成触觉和视觉感知，提升机器人的操作能力，该传感器能够提供可靠的触觉和视觉数据。TacThru-UMI框架利用模仿学习和基于Transformer的扩散策略来利用这些多模态信号。实验结果显示，TacThru-UMI在五个真实世界任务中的成功率达到了85.5%，显著优于触觉和纯视觉策略。该系统在检测薄软物体接触和需要多模态协调的精细操作方面表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios</div>
<div class="meta-line">Authors: Tian Gao, Celine Tan, Catherine Glossop, Timothy Gao, Jiankai Sun, Kyle Stachowicz, Shirley Wu, Oier Mees, Dorsa Sadigh, Sergey Levine, Chelsea Finn</div>
<div class="meta-line">First: 2026-02-09T09:54:02+00:00 · Latest: 2026-02-09T09:54:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08440v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08440v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://steervla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SteerVLA：在长尾驾驶场景中引导视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">自主驾驶中的一个基本挑战是将高层语义推理与低层反应控制相结合，以实现稳健的驾驶。虽然在大规模网络数据上训练的视觉-语言模型（VLMs）提供了强大的常识推理能力，但它们缺乏进行安全车辆控制所需的接地经验。我们认为，有效的自主代理应该利用VLM的世界知识来引导可调的驾驶策略，以实现低层策略在驾驶场景中的稳健控制。为此，我们提出了SteerVLA，它利用VLM的推理能力生成细粒度的语言指令，以引导视觉-语言-行动（VLA）驾驶策略。我们方法的关键在于VLM和VLA之间丰富的语言接口，这使得高层策略能够更有效地将其推理与低层策略的控制输出联系起来。为了提供与车辆控制对齐的细粒度语言监督，我们利用VLM对现有的驾驶数据进行详细语言注解的增强，我们发现这对于有效的推理和可引导性至关重要。我们在一个具有挑战性的闭环基准上评估了SteerVLA，其总体驾驶得分比最先进的方法高出4.77分，在长尾子集上高出8.04分。项目网站可访问：https://steervla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of integrating high-level semantic reasoning with low-level reactive control in autonomous driving. It proposes SteerVLA, which uses a vision-language model to generate detailed language instructions to guide a vision-language-action driving policy. The method significantly improves performance, achieving a 4.77-point increase in overall driving score and an 8.04-point increase on long-tail scenarios compared to state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">SteerVLA 解决了在自主驾驶中将高层次语义推理与低层次反应控制相结合的挑战。它提出了一种方法，利用视觉语言模型生成详细的语言指令来引导视觉语言行动驾驶策略。关键创新在于高层次模型和低层次策略之间的丰富语言接口，这增强了推理在控制输出中的定位。SteerVLA 在封闭环基准测试中的整体驾驶得分上比最先进的方法高出 4.77 分，在长尾子集上高出 8.04 分。</div>
</details>
</div>
<div class="card">
<div class="title">Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence</div>
<div class="meta-line">Authors: Jinxian Zhou, Ruihai Wu, Yiwei Liu, Yiwen Hou, Xunzhe Zhou, Checheng Yu, Licheng Zhong, Lin Shao</div>
<div class="meta-line">First: 2026-02-09T09:30:23+00:00 · Latest: 2026-02-09T09:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08425v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08425v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://biadapt-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bi-Adapt: 少量样本的双臂适应方法以实现新型3D物体类别语义对应下的双臂操作</div>
<div class="mono" style="margin-top:8px">双臂操作对于机器人执行复杂任务至关重要但极具挑战性，需要两臂之间的协调合作。然而，现有的双臂操作方法往往依赖于昂贵的数据收集和训练，难以高效地将新类别未见过的物体进行泛化。本文提出了一种名为Bi-Adapt的新框架，通过语义对应实现跨类别操作映射。Bi-Adapt利用视觉基础模型的强大能力进行微调，仅使用有限数据在新类别上表现出显著的零样本泛化能力。在仿真和真实环境中的广泛实验验证了该方法的有效性和高效率，即使在有限数据下也能在不同基准任务中实现较高的成功率。项目网站: https://biadapt-project.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the efficiency of bimanual manipulation for robots by addressing the challenge of generalizing to unseen objects in novel categories. Bi-Adapt uses semantic correspondence and vision foundation models for cross-category affordance mapping, requiring limited data for fine-tuning. The framework demonstrates notable zero-shot generalization and high success rates in both simulation and real-world environments with minimal data, showcasing its efficiency and effectiveness.</div>
<div class="mono" style="margin-top:8px">研究动机是使机器人能够高效执行复杂的双臂操作任务，并能泛化到未见过的新类别中的新对象。Bi-Adapt这一新颖框架利用语义对应和视觉基础模型进行跨类别操作映射。该方法在仿真和真实环境中均实现了显著的零样本泛化和高成功率，使用少量数据即可完成不同基准任务。</div>
</details>
</div>
<div class="card">
<div class="title">BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models</div>
<div class="meta-line">Authors: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li</div>
<div class="meta-line">First: 2026-02-09T08:47:14+00:00 · Latest: 2026-02-09T08:47:14+00:00</div>
<div class="meta-line">Comments: 38 pages, 9 figures. Project page:https://bimanibench.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08392v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bimanibench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiManiBench：一种评估多模态大型语言模型双臂协调性的分层基准</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）在增强具身人工智能方面取得了显著进展，使用它们来评估机器人的智能已经成为一个关键趋势。然而，现有的框架主要局限于单臂操作，未能捕捉到像举起重锅这样的双臂任务所需的时空协调。为了解决这个问题，我们引入了BiManiBench，这是一种分层基准，评估MLLMs在三个层级上的表现：基本的空间推理、高级动作规划和低级末端执行器控制。我们的框架隔离了独特的双臂挑战，如手臂可达性和运动学约束，从而区分感知幻觉与规划失败。对超过30个最先进的模型的分析表明，尽管在高级推理方面表现出色，MLLMs在双臂空间定位和控制方面仍然存在困难，经常导致相互干扰和顺序错误。这些发现表明当前的范式缺乏对相互运动学约束的深刻理解，突显了未来研究需要关注双臂碰撞避免和细粒度的时间序列的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">BiManiBench is a hierarchical benchmark designed to evaluate the bimanual coordination capabilities of Multimodal Large Language Models (MLLMs). It assesses models across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Analysis of over 30 state-of-the-art models indicates that while MLLMs excel in high-level reasoning, they struggle with dual-arm spatial grounding and control, often leading to mutual interference and sequencing errors. This suggests a lack of understanding of mutual kinematic constraints and highlights the need for future research focusing on inter-arm collision-avoidance and fine-grained temporal sequencing.</div>
<div class="mono" style="margin-top:8px">BiManiBench 是一个层次化的基准，用于评估多模态大型语言模型（MLLMs）的双臂协调能力。它在三个层级上评估模型：基本的空间推理、高级的动作规划和低级的末端执行器控制。研究发现，尽管高级推理能力强，MLLMs 在双臂空间定位和控制方面经常出现相互干扰和顺序错误，表明缺乏对双臂碰撞避免和精细时间序列的理解。</div>
</details>
</div>
<div class="card">
<div class="title">OpenGVL -- Benchmarking Visual Temporal Progress for Data Curation</div>
<div class="meta-line">Authors: Paweł Budzianowski, Emilia Wiśnios, Michał Tyrolski, Gracjan Góral, Igor Kulakov, Viktor Petrenko, Krzysztof Walas</div>
<div class="meta-line">Venue: CoRL 2025</div>
<div class="meta-line">First: 2025-09-22T02:52:55+00:00 · Latest: 2026-02-09T08:29:03+00:00</div>
<div class="meta-line">Comments: Workshop on Making Sense of Data in Robotics: Composition, Curation, and Interpretability at Scale at CoRL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17321v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.17321v4">PDF</a> · <a href="http://github.com/budzianowski/opengvl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \href{github.com/budzianowski/opengvl}{OpenGVL}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenGVL -- 评估视觉时间进程在数据整理中的基准测试</div>
<div class="mono" style="margin-top:8px">数据稀缺仍然是机器人技术进步的最大限制因素之一。然而，野外可用的机器人数据量正在呈指数级增长，为大规模数据利用创造了新的机会。可靠的阶段性任务完成预测可以帮助自动标注和整理这些数据。最近提出了生成性价值学习（GVL）方法，利用视觉语言模型（VLMs）中的知识从视觉观察中预测任务进度。在此基础上，我们提出了OpenGVL，这是一个全面的基准测试，用于估计涉及机器人和人类主体的多种挑战性操作任务的进度。我们评估了公开可用的开源基础模型的能力，结果显示开源模型家族显著落后于闭源模型，仅在时间进度预测任务上达到闭源模型性能的大约70%。此外，我们展示了OpenGVL如何作为自动数据整理和过滤的实用工具，使大规模机器人数据集的质量评估变得高效。我们将在github.com/budzianowski/opengvl发布基准测试和完整的代码库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to address the challenge of data scarcity in robotics by leveraging visual-temporal progress prediction to automatically annotate and curate large-scale robotics data. It introduces OpenGVL, a benchmark for estimating task progress across various manipulation tasks. The study evaluates open-source and closed-source foundation models, finding that open-source models perform about 70% as well as closed-source models in temporal progress prediction tasks. OpenGVL is also shown to be useful for automated data curation and filtering, enhancing the quality assessment of robotics datasets.</div>
<div class="mono" style="margin-top:8px">研究旨在通过利用视觉时间进度预测来解决机器人领域的数据稀缺问题，自动标注和整理大规模的机器人数据。OpenGVL 基于生成价值学习（GVL）方法构建了一个基准，评估开源和闭源模型在各种操作任务中的任务进度估计能力。研究发现，开源模型在时间进度预测任务中的表现仅为闭源模型的约70%。OpenGVL 还可以作为自动化数据整理和大规模机器人数据集质量评估的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Backtracking Feedback</div>
<div class="meta-line">Authors: Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin, Dingcheng Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-02-09T08:23:19+00:00 · Latest: 2026-02-09T08:23:19+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08377v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08377v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model&#x27;s live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient &quot;backtrack by x tokens&quot; signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带有回溯反馈的强化学习</div>
<div class="mono" style="margin-top:8px">针对大型语言模型（LLMs）在对抗攻击和分布内错误中的关键安全需求，我们提出了带有回溯反馈的强化学习（RLBF）框架。该框架在先前方法如BSAFE的基础上，主要通过强化学习（RL）阶段使模型能够动态纠正自身的生成错误。通过在模型实时输出上使用批评家反馈的强化学习过程，LLMs 被训练以识别并从实际出现的安全违规中恢复，通过发出“回溯 x 个词”的高效信号，然后继续自回归生成。这一RL过程对于抵御复杂的对抗策略至关重要，包括中间填充、贪婪坐标梯度（GCG）攻击和解码参数操纵。为了进一步支持回溯能力的获取，我们还提出了一种增强的监督微调（SFT）数据生成策略（BSAFE+）。该方法通过将违规注入原本安全的连贯文本中，改进了先前的数据生成技术，为回溯机制提供了更有效的初始训练。全面的经验评估表明，RLBF在各种基准和模型规模上显著降低了攻击成功率，实现了更优的安全效果，同时保留了基础模型的核心功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement Learning with Backtracking Feedback (RLBF) is introduced to enhance the safety of Large Language Models (LLMs) against adversarial attacks and in-distribution errors. The method uses RL to train models to dynamically correct their own generation errors by emitting a &#x27;backtrack by x tokens&#x27; signal and continuing generation autoregressively. This process is effective against various sophisticated adversarial strategies. Additionally, an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+) is proposed to improve initial training for the backtracking mechanism. Empirical evaluations show that RLBF significantly reduces attack success rates across different benchmarks and model sizes, maintaining model utility.</div>
<div class="mono" style="margin-top:8px">Reinforcement Learning with Backtracking Feedback (RLBF) 通过训练大型语言模型（LLM）动态纠正其生成错误，来解决其鲁棒安全性问题。RLBF 使用强化学习阶段，让模型在检测到安全违规时发出“回退 x 个令牌”的信号，然后继续自回归生成。这种方法通过改进的监督微调数据生成策略（BSAFE+）提供更有效的初始训练，优于之前的 BSAFE 技术。实证评估表明，RLBF 在各种基准测试和模型规模下显著降低了攻击成功率，同时保持了模型的基本功能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Human-Like Badminton Skills for Humanoid Robots</div>
<div class="meta-line">Authors: Yeke Chen, Shihao Dong, Xiaoyu Ji, Jingkai Sun, Zeren Luo, Liu Zhao, Jiahui Zhang, Wanyue Li, Ji Ma, Bowen Xu, Yimin Han, Yudong Zhao, Peng Lu</div>
<div class="meta-line">First: 2026-02-09T08:09:52+00:00 · Latest: 2026-02-09T08:09:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a &quot;mimic&quot; to a capable &quot;striker.&quot; Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>赋予类人机器人类似人类的羽毛球技能</div>
<div class="mono" style="margin-top:8px">在高需求的体育项目如羽毛球中实现多样且类似人类的表现仍然是类人机器人技术的一大挑战。与标准的移动或静态操作不同，这项任务需要无缝整合全身爆发性协调和精确、时间敏感的拦截。尽管最近的进步已经实现了逼真的运动模仿，但在不牺牲风格自然性的前提下，将运动模仿与功能性的、物理感知的击打行为结合起来仍然是一个非平凡的问题。为了解决这个问题，我们提出了模仿到互动的渐进强化学习框架，旨在将机器人从“模仿者”进化为“击打者”。我们的方法从人类数据中建立了一个稳健的运动先验，将其提炼为紧凑的基于模型的状态表示，并通过对抗先验稳定动力学。至关重要的是，为了克服专家演示稀疏的问题，我们引入了一种流形扩展策略，将离散的击打点泛化为密集的互动体积。我们通过模拟中掌握各种技能，包括吊球和杀球，验证了我们的框架。此外，我们展示了首次在类人机器人中实现零样本模拟到现实的类人羽毛球技能转移，成功在物理世界中复制了人类运动员的动能优雅和功能性精确。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable humanoid robots to perform human-like badminton skills, focusing on the integration of explosive whole-body coordination and precise timing. The proposed Imitation-to-Interaction framework uses progressive reinforcement learning to evolve a robot from a mimic to a capable striker. Key findings include the successful mastery of various skills in simulation and the first zero-shot transfer of anthropomorphic badminton skills to a real humanoid robot, replicating human athletes&#x27; kinetic elegance and functional precision.</div>
<div class="mono" style="margin-top:8px">本文解决了为类人机器人实现类似人类的羽毛球表现的挑战。提出了一种渐进式的强化学习框架——模仿到互动，该框架使机器人从模仿者进化为熟练的击球者。方法从人类数据中建立了一个稳健的运动先验，将其提炼为紧凑的状态表示，并通过对抗先验稳定动力学。关键发现包括在模拟中成功掌握了多种羽毛球技巧，并实现了第一个零样本从模拟到现实的类人羽毛球技能转移，复制了人类运动员的动能优雅和功能精度。</div>
</details>
</div>
<div class="card">
<div class="title">Nimbus: A Unified Embodied Synthetic Data Generation Framework</div>
<div class="meta-line">Authors: Zeyu He, Yuchang Zhang, Yuanzhen Zhou, Miao Tao, Hengjie Li, Hui Wang, Yang Tian, Jia Zeng, Tai Wang, Wenzhe Cai, Yilun Chen, Ning Gao, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-01-29T09:27:31+00:00 · Latest: 2026-02-09T06:57:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21449v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21449v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Nimbus：统一的具身合成数据生成框架</div>
<div class="mono" style="margin-top:8px">扩大数据的体积和多样性对于具身智能的泛化至关重要。虽然合成数据生成提供了比昂贵的物理数据获取更具扩展性的替代方案，但现有的管道仍然碎片化且任务特定。这种隔离导致了重大的工程效率低下和系统不稳定，无法支持基础模型训练所需的持续、高吞吐量的数据生成。为了解决这些挑战，我们提出了Nimbus，这是一种统一的合成数据生成框架，旨在整合异构导航和操作管道。Nimbus 引入了一种模块化的四层架构，该架构通过异步阶段分离轨迹规划、渲染和存储。通过实现动态管道调度、全局负载均衡、分布式容错和后端特定的渲染优化，该系统最大限度地利用了CPU、GPU和I/O资源。我们的评估表明，与未优化的基线相比，Nimbus 在端到端吞吐量上实现了2-3倍的改进，并确保在大规模分布式环境中具有稳健的长期运行能力。该框架是InternData套件的生产核心，能够实现无缝跨域数据合成。</div>
</details>
</div>
<div class="card">
<div class="title">ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects</div>
<div class="meta-line">Authors: Josh Pinskier, Sarah Baldwin, Stephen Rodan, David Howard</div>
<div class="meta-line">First: 2026-02-09T05:35:08+00:00 · Latest: 2026-02-09T05:35:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08285v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate change, invasive species and human activities are currently damaging the world&#x27;s coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReefFlex：一种用于有机和脆弱物体软体机器人抓取的生成设计框架</div>
<div class="mono" style="margin-top:8px">气候变化、入侵物种和人类活动正在以前所未有的速度破坏世界的珊瑚礁，威胁其丰富的生物多样性和渔业，并减少沿海防护。解决这一巨大挑战需要可扩展的珊瑚再生技术，能够培育气候适应性强的物种并加速自然再生过程；这些行动因缺乏安全和坚固的工具来处理脆弱的珊瑚而受阻。我们研究了ReefFlex，一种生成软手指设计方法，探索了多样化的软手指空间，以产生一组能够安全抓取在杂乱环境中脆弱且几何形状各异的珊瑚的候选方案。我们的关键见解是将异质抓取编码为一组简化且可处理的运动基元，创建了一个简化且可处理的多目标优化问题。为了评估该方法，我们设计了一种软机器人用于珊瑚礁恢复，该机器人在岸上水族养殖场生长和操作珊瑚，以供未来珊瑚礁移植使用。我们证明ReefFlex提高了抓取成功率和抓取质量（抗干扰性、定位精度），并在珊瑚操作过程中减少了不良事件的发生。ReefFlex提供了一种设计用于复杂操作的软末端执行器的一般方法，并为以前无法实现的领域如珊瑚处理恢复的自动化铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ReefFlex is a generative design framework for soft robotic fingers that can safely grasp fragile and geometrically diverse coral in cluttered environments. It encodes heterogeneous grasping into a set of motion primitives, simplifying the optimization problem. The method was evaluated through a soft robot designed for reef rehabilitation, which demonstrated improved grasp success, disturbance resistance, and positioning accuracy compared to reference designs. This approach offers a generalizable method for designing soft end-effectors for complex handling tasks, particularly in coral restoration.</div>
<div class="mono" style="margin-top:8px">ReefFlex 是一种生成设计框架，用于设计能够安全抓取脆弱且几何形状多样的珊瑚的软手指。它将异质抓取编码为一组运动基元，简化了优化问题。该方法通过一个用于珊瑚修复的软机器人进行了评估，该机器人显示了更高的抓取成功率和质量，并且减少了操作过程中遇到的不良事件，相比参考设计具有优势。这种方法为珊瑚处理自动化铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer</div>
<div class="meta-line">Authors: Ke Zhang, Lixin Xu, Chengyi Song, Junzhe Xu, Xiaoyi Lin, Zeyu Jiang, Renjing Xu</div>
<div class="meta-line">First: 2026-02-09T05:16:48+00:00 · Latest: 2026-02-09T05:16:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08278v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08278v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://davidlxu.github.io/DexFormer-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation remains one of the most challenging problems in robotics, requiring coherent control of high-DoF hands and arms under complex, contact-rich dynamics. A major barrier is embodiment variability: different dexterous hands exhibit distinct kinematics and dynamics, forcing prior methods to train separate policies or rely on shared action spaces with per-embodiment decoder heads. We present DexFormer, an end-to-end, dynamics-aware cross-embodiment policy built on a modified transformer backbone that conditions on historical observations. By using temporal context to infer morphology and dynamics on the fly, DexFormer adapts to diverse hand configurations and produces embodiment-appropriate control actions. Trained over a variety of procedurally generated dexterous-hand assets, DexFormer acquires a generalizable manipulation prior and exhibits strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand. Our results show that a single policy can generalize across heterogeneous hand embodiments, establishing a scalable foundation for cross-embodiment dexterous manipulation. Project website: https://davidlxu.github.io/DexFormer-web/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DexFormer：通过历史条件化的变压器实现跨体态灵巧操作</div>
<div class="mono" style="margin-top:8px">灵巧操作仍然是机器人技术中最具挑战性的问题之一，需要在复杂的、接触丰富的动力学条件下对高自由度的手和臂进行协调控制。主要障碍是体态差异：不同的灵巧手具有不同的运动学和动力学，迫使先前的方法训练单独的策略或依赖于共享的动作空间，并为每个体态使用解码器头部。我们提出了DexFormer，这是一种基于修改后的变压器骨干的端到端、动力学感知的跨体态策略，它基于历史观察进行条件化。通过使用时间上下文实时推断形态和动力学，DexFormer能够适应各种手部配置并产生适合体态的控制动作。DexFormer在多种程序生成的灵巧手资产上训练，获得了可泛化的操作先验，并在Leap Hand、Allegro Hand和Rapid Hand上表现出强大的零样本迁移能力。我们的结果表明，单一策略可以跨异构手部体态泛化，为跨体态灵巧操作奠定了可扩展的基础。项目网站：https://davidlxu.github.io/DexFormer-web/</div>
</details>
</div>
<div class="card">
<div class="title">Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes</div>
<div class="meta-line">Authors: Seunghoon Jeong, Eunho Lee, Jeongyun Kim, Ayoung Kim</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-09T04:50:36+00:00 · Latest: 2026-02-09T04:50:36+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures, 4 tables, accepted to ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08266v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08266v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向对象的具有信息性下一最佳视角的对象感知3D 高斯点积在拥挤场景中的应用</div>
<div class="mono" style="margin-top:8px">在不可避免的遮挡和不完整的观察条件下，选择信息性视角对于构建可靠的表示至关重要。在此背景下，3D 高斯点积（3DGS）具有独特优势，因为它可以明确指导后续视角的选择，并利用新观察结果进行表示的细化。然而，现有方法仅依赖几何线索，忽视了与操作相关的语义，并倾向于优先利用而非探索。为解决这些局限性，我们提出了一种实例感知的下一最佳视角（NBV）策略，通过利用对象特征优先考虑未探索区域。具体而言，我们的对象感知3DGS将实例级信息提炼为一热对象向量，用于计算置信加权信息增益，从而指导与错误和不确定高斯相关的区域的识别。此外，我们的方法可以轻松适应对象中心的NBV，从而将视角选择集中在目标对象上，提高重建对物体放置的鲁棒性。实验表明，与基线相比，我们的NBV策略在合成数据集上将深度误差降低了77.14%，在真实世界的GraspNet数据集上降低了34.10%。此外，与针对整个场景进行NBV相比，针对特定对象进行NBV可额外降低该对象25.60%的深度误差。我们还通过实际的机器人操作任务进一步验证了该方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of selecting informative viewpoints in cluttered scenes for building reliable 3D representations using 3D Gaussian Splatting. It introduces an object-aware Next Best View (NBV) policy that leverages object features to prioritize underexplored regions, reducing depth errors by up to 77.14% on synthetic data and 34.10% on real-world data compared to existing methods. Additionally, focusing on specific objects further reduces depth error by 25.60%.</div>
<div class="mono" style="margin-top:8px">本文针对在杂乱场景中选择信息性视点以构建可靠3D表示的问题，使用3D高斯散点图方法。提出了一种基于对象的Next Best View (NBV)策略，利用对象特征优先选择未探索区域，合成数据集上深度误差最多减少77.14%，真实世界GraspNet数据集上减少34.10%。此外，专注于特定对象进一步减少25.60%的深度误差，在真实世界机器人操作任务中得到验证。</div>
</details>
</div>
<div class="card">
<div class="title">Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control</div>
<div class="meta-line">Authors: Yuanzhu Zhan, Yufei Jiang, Muqing Cao, Junyi Geng</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-09T04:10:39+00:00 · Latest: 2026-02-09T04:10:39+00:00</div>
<div class="meta-line">Comments: 9 pages, 7 figures. Accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08251v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>接触感知在轨混合控制的空中操作</div>
<div class="mono" style="margin-top:8px">空中操作（AM）有望将无人驾驶航空器（UAV）从被动检查任务扩展到包括抓取、组装和现场维护在内的接触丰富任务。大多数先前的AM演示依赖于外部运动捕捉（MoCap）并强调粗略交互的位置控制，限制了其部署能力。我们提出了一种完全在轨感知-控制管道，无需MoCap即可实现准确的运动跟踪和调节接触力矩。主要组成部分包括（1）一种增强的视觉惯性里程计（VIO）估计器，带有仅在交互期间激活的接触一致性因子，这会收紧接触框架周围的不确定性并减少漂移，以及（2）基于图像的视觉伺服（IBVS）以减轻感知-控制耦合，结合一种混合力-运动控制器，该控制器调节接触力矩和横向运动以实现稳定接触。实验表明，我们的方法仅使用机载传感即可闭合感知到力矩的环路，接触时的速度估计改进了66.01%，可靠地接近目标并稳定地保持力，指向可部署的野外空中操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of performing aerial manipulation tasks by developing an onboard perception-control pipeline that enables UAVs to perform contact-rich tasks such as grasping and assembly without relying on external motion capture. The key components include an augmented visual-inertial odometry estimator and a hybrid force-motion controller. Experiments demonstrate that the approach improves velocity estimation by 66.01% at contact, allows reliable target approach, and achieves stable force holding, making it suitable for deployable, real-world aerial manipulation.</div>
<div class="mono" style="margin-top:8px">该论文通过开发一种机载感知控制管道来解决执行空中操作任务的挑战，使无人机能够执行接触丰富的任务如抓取和组装，无需依赖外部运动捕捉。关键组件包括具有接触一致性因子的增强视觉惯性里程计估计器和混合力-运动控制器。实验表明，该方法可以准确跟踪运动并调节接触力矩，接触时的速度估计改善了66.01%，实现了可靠的靶向接近和稳定的力保持，为实际环境中的空中操作提供了可部署的途径。</div>
</details>
</div>
<div class="card">
<div class="title">STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction</div>
<div class="meta-line">Authors: Jinhao Li, Yuxuan Cong, Yingqiao Wang, Hao Xia, Shan Huang, Yijia Zhang, Ningyi Xu, Guohao Dai</div>
<div class="meta-line">First: 2026-02-09T03:50:40+00:00 · Latest: 2026-02-09T03:50:40+00:00</div>
<div class="meta-line">Comments: 13 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08245v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STEP：时空一致性预测的预热视觉运动策略</div>
<div class="mono" style="margin-top:8px">扩散策略最近因其能够建模动作序列的分布并捕捉多模态性而在机器人操作中的视觉运动控制中崭露头角。然而，迭代去噪会导致显著的推理延迟，限制了实时闭环系统中的控制频率。现有的加速方法要么减少采样步骤，要么通过直接预测绕过扩散，要么重用过去的动作，但往往难以同时保持动作质量并实现一致的低延迟。在本文中，我们提出了一种轻量级的时空一致性预测机制STEP，以构建既与目标动作在分布上接近又在时间上一致的高质量预热动作，而不牺牲原始扩散策略的生成能力。然后，我们提出了一种基于时间动作变化的加速度感知扰动注入机制，以适应性地调节执行激励，防止执行停滞，特别是在实际任务中。我们还提供了一种理论分析，表明提出的预测诱导局部收缩映射，确保在扩散细化过程中动作误差的收敛。我们在九个模拟基准和两个实际任务上进行了广泛的评估。值得注意的是，与BRIDGER和DDIM相比，STEP在RoboMimic基准和实际任务中分别实现了平均21.6%和27.5%更高的成功率。这些结果表明，STEP在推理延迟和成功率的帕累托前沿上始终优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the high inference latency of diffusion policies in real-time robotic manipulation by proposing STEP, which uses spatiotemporal consistency prediction to generate high-quality warm-start actions. The method also includes a velocity-aware perturbation injection mechanism to prevent execution stall. Experimental results show that STEP achieves a 21.6% and 27.5% higher success rate than BRIDGER and DDIM, respectively, on simulated and real-world tasks, while maintaining low latency.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出STEP，利用时空一致性预测生成高质量的预热动作，以解决实时机器人操作中扩散策略的高推理延迟问题。该方法还包括一种基于时间动作变化的自适应扰动注入机制，以防止执行停滞。实验结果显示，STEP在模拟和真实世界任务上的成功率分别比BRIDGER和DDIM高出平均21.6%和27.5%，同时保持低延迟。</div>
</details>
</div>
<div class="card">
<div class="title">MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</div>
<div class="meta-line">Authors: Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath</div>
<div class="meta-line">First: 2025-12-18T18:59:03+00:00 · Latest: 2026-02-09T03:24:03+00:00</div>
<div class="meta-line">Comments: 25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16909v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16909v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hybridrobotics.github.io/MomaGraph/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MomaGraph：具备状态感知的统一场景图模型，用于体态任务规划</div>
<div class="mono" style="margin-top:8px">家庭中的移动机械臂必须同时导航和操作。这需要一种紧凑且语义丰富的场景表示，能够捕捉物体的位置、功能以及哪些部分可以操作。场景图是自然的选择，但先前的工作往往将空间关系和功能关系分开处理，将场景视为静态快照，不考虑物体状态或时间更新，也忽略了当前任务最相关的信息。为了解决这些限制，我们引入了MomaGraph，这是一种将空间功能关系和部分级交互元素整合在一起的统一场景表示。然而，要推进这种表示需要合适的数据和严格的评估，这些方面目前仍然不足。因此，我们贡献了MomaGraph-Scenes，这是首个包含丰富注释、任务驱动的家庭环境场景图的大规模数据集，以及MomaGraph-Bench，这是一个涵盖从高层规划到细粒度场景理解的六个推理能力的系统评估套件。在此基础上，我们进一步开发了MomaGraph-R1，这是一种7B参数的视觉语言模型，通过强化学习在MomaGraph-Scenes上进行训练。MomaGraph-R1预测任务导向的场景图，并在Graph-then-Plan框架下作为零样本任务规划器。广泛的实验表明，我们的模型在开源模型中达到了最先进的结果，准确率达到71.6%（比最佳基线高11.4%），并且在公共基准测试中具有良好的泛化能力，并且能够有效地转移到真实机器人实验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MomaGraph addresses the limitations of prior scene graph representations by integrating spatial-functional relationships and part-level interactive elements. It introduces MomaGraph-Scenes, a large-scale dataset of richly annotated, task-driven scene graphs for household environments, and MomaGraph-Bench, an evaluation suite for embodied agents. MomaGraph-R1, a 7B vision-language model, predicts task-oriented scene graphs and serves as a zero-shot task planner, achieving 71.6% accuracy on the benchmark, surpassing previous models by 11.4%.</div>
<div class="mono" style="margin-top:8px">MomaGraph通过整合空间功能关系和部分级交互元素来解决先前场景图表示的局限性。它引入了MomaGraph-Scenes，一个包含丰富注释和任务驱动的场景图的大规模数据集，以及MomaGraph-Bench，一个用于体感代理的评估套件。MomaGraph-R1，一个7B的视觉语言模型，预测任务导向的场景图，并作为零样本任务规划器，其在基准测试中的准确率为71.6%，超越最佳基线11.4%。</div>
</details>
</div>
<div class="card">
<div class="title">AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act</div>
<div class="meta-line">Authors: Pengyuan Guo, Zhonghao Mai, Zhengtong Xu, Kaidi Zhang, Heng Zhang, Zichen Miao, Arash Ajoudani, Zachary Kingston, Qiang Qiu, Yu She</div>
<div class="meta-line">First: 2026-02-02T05:30:14+00:00 · Latest: 2026-02-09T03:21:20+00:00</div>
<div class="meta-line">Comments: Added appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01662v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups&#x27; setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgenticLab：一个能够看见、思考和行动的现实世界机器人代理平台</div>
<div class="mono" style="margin-top:8px">近期大型视觉-语言模型（VLM）的进步展示了通用的开放词汇感知和推理能力，但在不规则、真实环境中的长期闭环执行能力尚不清楚。基于VLM的抓取操作管道在不同研究组的设置之间难以比较，许多评估依赖于模拟、特权状态或特别设计的设置。我们提出了AgenticLab，这是一个模型无关的机器人代理平台和开放世界抓取操作基准。AgenticLab 提供了一个闭环代理管道，用于感知、任务分解、在线验证和重新规划。使用AgenticLab，我们在不规则环境中对最先进的基于VLM的代理进行基准测试。我们的基准测试揭示了离线视觉-语言测试（例如，VQA和静态图像理解）无法捕捉到的几种失败模式，包括多步语义一致性的失效、遮挡和场景变化下的物体语义以及不足以进行可靠操作的空间推理。我们将发布完整的硬件和软件堆栈，以支持可重复的评估并加速通用机器人代理的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to evaluate the real-world manipulation capability of large vision-language models (VLMs) in unstructured environments. AgenticLab, a model-agnostic platform, provides a closed-loop pipeline for perception, task decomposition, and replanning. Experiments on real-robot tasks reveal several failure modes not captured by offline tests, such as multi-step grounding consistency issues, object grounding under occlusion, and insufficient spatial reasoning for reliable manipulation.</div>
<div class="mono" style="margin-top:8px">研究旨在评估大型视觉-语言模型（VLMs）在非结构化环境中的实际操作能力。AgenticLab 是一个模型无关的平台，提供了一个闭环的感知、任务分解和重新规划的管道。实验表明，这些失败模式（如多步语义一致性的问题、遮挡下的物体定位和空间推理不足）在离线测试中并未被捕捉到。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
