<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-22 03:38</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260222_0338</div>
    <div class="row"><div class="card">
<div class="title">When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</div>
<div class="meta-line">Authors: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-19T18:59:20+00:00 · Latest: 2026-02-19T18:59:20+00:00</div>
<div class="meta-line">Comments: Website: https://vla-va.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vla-va.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉优先于语言：评估和缓解VLAs中的反事实失败</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动模型（VLAs）承诺将语言指令应用于机器人控制，但在实践中往往未能忠实执行语言指令。当面对缺乏强烈场景特定监督的指令时，VLAs会遭受反事实失败：它们基于由数据集偏差诱导的视觉捷径行动，反复执行已学得的行为，并选择在训练期间频繁出现的对象，而不管语言意图如何。为了系统地研究这一问题，我们引入了LIBERO-CF，这是第一个用于VLAs的反事实基准，通过在视觉上合理的LIBERO布局下分配替代指令来评估语言遵循能力。我们的评估表明，反事实失败在最先进的VLAs中普遍存在但尚未得到充分探索。我们提出了反事实行动指导（CAG），这是一种简单而有效的双分支推理方案，明确地在VLAs中正则化语言条件。CAG将标准的VLA策略与一个未受语言条件的视觉-行动（VA）模块结合，使反事实比较在行动选择期间成为可能。这种设计减少了对视觉捷径的依赖，提高了对未观察任务的鲁棒性，并且无需额外演示或修改现有架构或预训练模型。广泛的实验表明，它可以在各种VLAs中实现即插即用集成并带来一致的改进。例如，在LIBERO-CF中，CAG在语言遵循准确性上提高了9.7%，在未观察任务上的任务成功率提高了3.6%，使用无训练策略，配以VA模型时，进一步提高了15.5%和8.5%。在实际应用中，CAG将反事实失败减少了9.4%，平均提高了任务成功率17.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of counterfactual failures in Vision-Language-Action models (VLAs), where models act based on visual biases rather than language instructions. To evaluate and mitigate these failures, the authors introduce LIBERO-CF, a benchmark that assigns alternative instructions under visually plausible scenarios. They propose Counterfactual Action Guidance (CAG), a dual-branch inference scheme that improves language following accuracy and task success, especially on under-observed tasks, without requiring additional training or modifications to existing models.</div>
<div class="mono" style="margin-top:8px">本文探讨了Vision-Language-Action模型（VLAs）中的反事实失败问题，即模型基于视觉偏见而非语言指令行动。为了解决这一问题，作者引入了LIBERO-CF基准，该基准在视觉上合理的场景下分配替代指令。他们提出了一种名为Counterfactual Action Guidance（CAG）的双分支推理方案，该方案在不需额外训练或修改现有模型的情况下提高了模型在未观察任务上的鲁棒性。实验表明，CAG在语言跟随准确性和任务成功率方面取得了显著提升，特别是在未观察任务上，且在各种VLAs的模拟和实际环境中都表现出一致的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Step Duration for Accurate Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds</div>
<div class="meta-line">Authors: Zhaoyang Xiang, Victor Paredes, Guillermo A. Castillo, Ayonga Hereid</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2024-03-25T19:18:25+00:00 · Latest: 2026-02-19T18:19:15+00:00</div>
<div class="meta-line">Comments: 7 pages, 7 figures. Accepted to IEEE/RSJ IROS 2025. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.17136v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.17136v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional one-step preview planning algorithms for bipedal locomotion struggle to generate viable gaits when walking across terrains with restricted footholds, such as stepping stones. To overcome such limitations, this paper introduces a novel multi-step preview foot placement planning algorithm based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of walking robots. Our proposed approach adaptively changes the step duration and the swing foot trajectory for optimal foot placement under constraints, thereby enhancing the long-term stability of the robot and significantly improving its ability to navigate environments with tight constraints on viable footholds. We demonstrate its effectiveness through various simulation scenarios with complex stepping-stone configurations and external perturbations. These tests underscore its improved performance for navigating foothold-restricted terrains, even with external disturbances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应性步长以实现准确的足部放置：在受限立足点地形上实现稳健的双足运动</div>
<div class="mono" style="margin-top:8px">传统的双足运动一步预览规划算法在跨越受限立足点地形（如踏石）时难以生成可行的步态。为克服这些限制，本文提出了一种基于行走机器人步行Divergent Component of Motion (DCM) 的步到步离散演变的新型多步预览足部放置规划算法。我们提出的方法适应性地改变步长和摆动腿轨迹，以在约束条件下实现最佳足部放置，从而增强机器人的长期稳定性和显著提高其在受限立足点地形环境中导航的能力。我们通过各种具有复杂踏石配置和外部干扰的仿真场景展示了其有效性。这些测试强调了其在外部干扰下导航受限立足点地形的改进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of bipedal locomotion on terrains with restricted footholds by proposing a multi-step preview foot placement planning algorithm. The method adaptively adjusts step duration and swing foot trajectory based on the Divergent Component of Motion (DCM) to enhance stability and navigation. Experimental results show improved performance in navigating complex stepping-stone configurations and handling external perturbations.</div>
<div class="mono" style="margin-top:8px">本文提出了一种多步预览足部放置规划算法，以解决在受限 foothold 地形上的双足行走问题。该算法根据行走机器人的发散运动分量 (DCM) 调整步长和摆动腿轨迹，以增强长期稳定性。各种复杂踏石地形的仿真测试结果表明，即使在外部干扰下，该算法也能提高导航能力。</div>
</details>
</div>
<div class="card">
<div class="title">IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control</div>
<div class="meta-line">Authors: Qilong Cheng, Matthew Mackay, Ali Bereyhi</div>
<div class="meta-line">First: 2026-02-19T16:50:31+00:00 · Latest: 2026-02-19T16:50:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17537v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IRIS：基于学习的任务特定电影机器人手臂用于视动运动控制</div>
<div class="mono" style="margin-top:8px">机器人摄像系统能够实现超越人类能力的动态、可重复运动，但其采用受限于工业级平台的高成本和操作复杂性。我们介绍了智能机器人成像系统（IRIS），这是一种专为自主、基于学习的电影运动控制设计的6-DOF操作臂。IRIS 结合了轻量级的全3D打印硬件设计和基于动作分块与变换器（ACT）的目标条件视动模仿学习框架。该系统直接从人类示范中学习对象感知和感知平滑的摄像机轨迹，消除了显式几何编程的需要。整个平台成本低于1000美元，支持1.5公斤负载，并实现约1毫米的重复性。实际实验表明，该系统能够准确跟踪轨迹、可靠自主执行，并在多种电影运动中泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a cost-effective and easy-to-use robotic camera system for cinematic motion control. IRIS, a 6-DOF manipulator, integrates a lightweight 3D-printed hardware with a learning-based visuomotor imitation framework. The system learns from human demonstrations to generate object-aware and smooth camera trajectories without explicit programming, achieving accurate trajectory tracking and reliable autonomous execution. Key findings include a platform cost under $1,000, a 1.5 kg payload capacity, and approximately 1 mm repeatability. Experiments show successful generalization across various cinematic motions.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种低成本且易于使用的机器人摄像系统，用于电影运动控制。IRIS 是一个6-DOF机械臂，结合了轻量级3D打印硬件和基于Action Chunking with Transformers的基于学习的视觉-运动模仿框架。系统通过人类演示学习生成对象感知和平滑的摄像机轨迹，无需显式编程，实现了准确的轨迹跟踪和可靠的自主执行。关键发现包括平台成本低于1000美元，1.5公斤负载能力，以及约1毫米的重复精度。实验表明，系统在各种电影运动中表现出良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Proximal powered knee placement: a case study</div>
<div class="meta-line">Authors: Kyle R. Embry, Lorenzo Vianello, Jim Lipsey, Frank Ursetta, Michael Stephens, Zhi Wang, Ann M. Simon, Andrea J. Ikeda, Suzanne B. Finucane, Shawana Anarwala, Levi J. Hargrove</div>
<div class="meta-line">First: 2026-02-19T16:16:20+00:00 · Latest: 2026-02-19T16:16:20+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE RAS/EMBS 11th International Conference on Biomedical Robotics and Biomechatronics (BioRob 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17502v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17502v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>近端动力膝关节安装：案例研究</div>
<div class="mono" style="margin-top:8px">下肢截肢影响全球数百万人，导致行动能力下降、行走速度减慢和日常及社交活动参与度降低。动力假肢膝关节可以通过主动辅助膝关节扭矩部分恢复行动能力，改善步态对称性、坐起至站立转换和行走速度。然而，动力组件增加的重量可能会削弱这些益处，负面影响步态力学并增加代谢成本。因此，优化质量分布，而不是简单地减少总质量，可能提供更有效和实用的解决方案。在本探索性研究中，我们评估了在小样本组中将膝关节动力传动装置安装在上方的可行性。与下方安装相比，上方配置显示出行走速度（一名参与者提高9.2%）和步频（提高3.6%）的改善，步态对称性则表现出混合效果。运动学测量表明，两种配置下的膝关节活动范围和峰值速度相似。在斜坡和楼梯上的额外测试证实了控制策略在多种运动任务中的稳健性。初步结果显示，上方安装功能可行，精心的质量分布可以保持动力辅助的益处，同时减轻增加重量的负面影响。需要进一步研究来确认这些趋势并指导设计和临床建议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study evaluates the feasibility of placing a powered prosthetic knee above the knee in a small cohort, comparing it to below-knee placement. The above-knee configuration improved walking speed and cadence, while maintaining similar knee range of motion and peak velocity. The control strategy was robust across various tasks, suggesting that above-knee placement can preserve the benefits of powered assistance while mitigating the negative effects of added weight.</div>
<div class="mono" style="margin-top:8px">本研究探索了在小样本组中将假肢膝关节动力装置置于膝关节上方的可行性，旨在通过优化质量分布改善步态力学。与膝关节下方放置相比，膝关节上方配置提高了行走速度和步频，同时保持了相似的膝关节活动范围和峰值速度。控制策略在各种运动任务中表现出良好的鲁棒性，表明膝关节上方放置可以保留动力辅助的好处，同时减轻附加重量的负面影响。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection</div>
<div class="meta-line">Authors: Yichen Lu, Siwei Nie, Minlong Lu, Xudong Yang, Xiaobo Zhang, Peng Zhang</div>
<div class="meta-line">First: 2026-02-19T15:54:55+00:00 · Latest: 2026-02-19T15:54:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17484v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17484v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace&#x27;s verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追踪复制像素和正则化块亲和性在复制检测中的应用</div>
<div class="mono" style="margin-top:8px">图像复制检测（ICD）旨在通过稳健的特征表示学习来识别图像对之间的篡改内容。虽然自监督学习（SSL）已经提升了ICD系统的性能，但现有的视图级对比方法由于缺乏细粒度对应学习，难以应对复杂的编辑。我们通过两个关键创新解决了这一限制。首先，我们提出了PixTrace——一个像素坐标跟踪模块，用于在编辑变换中保持显式的空间映射。其次，我们引入了CopyNCE，这是一种几何引导的对比损失，通过从PixTrace验证的映射中提取的重叠比来正则化块亲和性。我们的方法将像素级的可追踪性与块级相似性学习相结合，抑制了SSL训练中的监督噪声。广泛的实验不仅展示了最先进的性能（匹配器88.7% uAP / 83.9% RP90，描述符72.6% uAP / 68.4% RP90，DISC21数据集），还比现有方法具有更好的可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of detecting copied pixels in image pairs by proposing PixTrace, a pixel coordinate tracking module, and CopyNCE, a geometrically-guided contrastive loss. These innovations enhance the fine-grained correspondence learning, leading to improved performance in image copy detection. The method achieves state-of-the-art results with 88.7% uAP and 83.9% RP90 for the matcher, and 72.6% uAP and 68.4% RP90 for the descriptor on the DISC21 dataset, while also offering better interpretability compared to existing methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有视图级对比方法的局限性来提升图像复制检测。引入了PixTrace像素坐标跟踪模块和CopyNCE几何导向对比损失，以保持空间映射并正则化块相似性。该方法在DISC21数据集上实现了最先进的性能，匹配器的uAP为88.7%，RP90为83.9%，描述符的uAP为72.6%，RP90为68.4%，同时增强了可解释性，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models</div>
<div class="meta-line">Authors: Clemence Grislain, Hamed Rahimi, Olivier Sigaud, Mohamed Chetouani</div>
<div class="meta-line">First: 2025-09-19T15:19:38+00:00 · Latest: 2026-02-19T15:45:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16072v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.16072v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://clemgris.github.io/I-FailSense/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-conditioned robotic manipulation in open-world settings requires not only accurate task execution but also the ability to detect failures for robust deployment in real-world environments. Although recent advances in vision-language models (VLMs) have significantly improved the spatial reasoning and task-planning capabilities of robots, they remain limited in their ability to recognize their own failures. In particular, a critical yet underexplored challenge lies in detecting semantic misalignment errors, where the robot executes a task that is semantically meaningful but inconsistent with the given instruction. To address this, we propose a method for building datasets targeting Semantic Misalignment Failures detection, from existing language-conditioned manipulation datasets. We also present I-FailSense, an open-source VLM framework with grounded arbitration designed specifically for failure detection. Our approach relies on post-training a base VLM, followed by training lightweight classification heads, called FS blocks, attached to different internal layers of the VLM and whose predictions are aggregated using an ensembling mechanism. Experiments show that I-FailSense outperforms state-of-the-art VLMs, both comparable in size and larger, in detecting semantic misalignment errors. Notably, despite being trained only on semantic misalignment detection, I-FailSense generalizes to broader robotic failure categories and effectively transfers to other simulation environments and real-world with zero-shot or minimal post-training. The datasets and models are publicly released on HuggingFace (Webpage: https://clemgris.github.io/I-FailSense/).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>I-FailSense：基于视觉语言模型的通用机器人故障检测</div>
<div class="mono" style="margin-top:8px">开放世界中的语言条件化机器人操作不仅需要准确的任务执行，还需要检测故障的能力，以在真实环境中实现稳健部署。尽管近期视觉语言模型（VLMs）在空间推理和任务规划方面取得了显著进步，但在识别自身故障方面仍有限制。特别是，一个关键但尚未充分探索的挑战是检测语义对齐错误，即机器人执行的任务在语义上是有意义的，但与给定的指令不一致。为解决这一问题，我们提出了一种构建针对语义对齐错误检测的数据集的方法，从现有的语言条件化操作数据集中获取。我们还介绍了I-FailSense，一个具有基于地面仲裁的开源VLM框架，专门用于故障检测。我们的方法依赖于在基VLM上进行后训练，然后训练附加在VLM不同内部层上的轻量级分类头FS块，并使用集成机制聚合其预测。实验表明，I-FailSense在检测语义对齐错误方面优于现有VLM，无论是大小相当还是更大的模型。值得注意的是，尽管仅在语义对齐检测上进行训练，I-FailSense仍能泛化到更广泛的机器人故障类别，并有效转移到其他模拟环境和真实世界中，无需或只需少量后训练。数据集和模型已公开发布在HuggingFace（网址：https://clemgris.github.io/I-FailSense/）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces I-FailSense, a method for detecting semantic misalignment errors in robotic manipulation tasks using vision-language models. It addresses the challenge of recognizing when a robot&#x27;s actions are semantically meaningful but inconsistent with the given instruction. The approach involves creating datasets for semantic misalignment failures and training a lightweight classification framework called FS blocks within a vision-language model. Experiments demonstrate that I-FailSense outperforms existing VLMs in detecting these errors and generalizes well to other failure categories and environments with minimal training.</div>
<div class="mono" style="margin-top:8px">I-FailSense通过提出构建针对语义不匹配错误的数据集方法，并引入一个名为I-FailSense的开源VLM框架来解决语言条件下的机器人操作中检测语义不匹配错误的挑战。该框架在基VLM上进行后训练，并添加了轻量级分类头FS块，这些预测使用集成机制进行聚合。实验表明，I-FailSense在检测语义不匹配错误方面优于现有VLM，并且能够很好地泛化到其他故障类别和环境，只需少量努力即可实现迁移。</div>
</details>
</div>
<div class="card">
<div class="title">2Mamba2Furious: Linear in Complexity, Competitive in Accuracy</div>
<div class="meta-line">Authors: Gabriel Mongaras, Eric C. Larson</div>
<div class="meta-line">First: 2026-02-19T13:45:23+00:00 · Latest: 2026-02-19T13:45:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>2Mamba2狂热: 线性在复杂性上，竞争在准确性上</div>
<div class="mono" style="margin-top:8px">线性注意力变换器由于其效率已成为softmax注意力的强大替代方案。然而，线性注意力在表达能力上较弱，导致准确性低于softmax注意力。为了弥合softmax注意力和线性注意力之间的准确性差距，我们对Mamba-2进行了操作，这是一种非常强大的线性注意力变体。我们首先将Mamba-2简化为其最基本和最重要的组成部分，评估哪些具体选择使其最准确。从简化后的Mamba变体（Mamba-2S）中，我们改进了A-掩码并增加了隐藏状态的阶数，从而提出了一种名为2Mamba的方法，该方法在准确性上几乎与softmax注意力相当，但在长上下文长度下具有更高的内存效率。我们还研究了有助于超越softmax注意力准确性的Mamba-2元素。所有实验的代码均已提供</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the accuracy of linear attention transformers, which are more efficient than softmax attention but less expressive. The study simplifies Mamba-2 to its core components, then improves the A-mask and increases the hidden state order, leading to 2Mamba, a method that nearly matches softmax attention accuracy while being more memory efficient for long context lengths. Key findings include a nearly equal accuracy to softmax attention with significant memory savings.</div>
<div class="mono" style="margin-top:8px">研究旨在通过简化和增强Mamba-2（一种线性注意力变体）来提高线性注意力变换器的准确性。方法包括将Mamba-2简化为其核心组件，然后改进A-mask并增加隐藏状态的阶数，从而得到一种名为2Mamba的新方法。该方法在长上下文长度下具有接近softmax注意力的准确性，同时更具内存效率。</div>
</details>
</div>
<div class="card">
<div class="title">Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises</div>
<div class="meta-line">Authors: Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbin Li, Yiming Li</div>
<div class="meta-line">First: 2025-04-30T15:21:25+00:00 · Latest: 2026-02-19T12:16:56+00:00</div>
<div class="meta-line">Comments: To appear in TIFS 2026. 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21730v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.21730v2">PDF</a> · <a href="https://github.com/NcepuQiaoTing/Cert-SSB">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample&#x27;s certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cert-SSBD: 经认证的样本特定平滑噪声后门防御</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）容易受到后门攻击的影响，攻击者通过操纵一小部分训练数据植入隐藏的后门。受攻击的模型在干净样本上表现正常，但在后门样本上将其错误分类为攻击者指定的目标类别，对实际应用中的DNN构成了重大威胁。目前，已经提出了几种经验防御方法来缓解后门攻击，但这些方法往往被更先进的后门技术绕过。相比之下，基于随机平滑的认证防御通过在训练和测试样本中添加随机噪声来对抗后门攻击，显示出一定的前景。本文揭示了现有随机平滑防御隐含地假设所有样本与决策边界等距，但在实践中可能不成立，导致认证性能不佳。为解决这一问题，我们提出了一种样本特定的认证后门防御方法，称为Cert-SSB。Cert-SSB首先使用随机梯度上升优化每个样本的噪声幅度，确保样本特定的噪声水平，然后应用于多个受污染的训练集以重新训练多个平滑模型。之后，Cert-SSB将多个平滑模型的预测聚合生成最终的鲁棒预测。特别是，在这种情况下，现有的认证方法变得不再适用，因为优化的噪声在不同样本之间变化。为克服这一挑战，我们引入了一种基于存储更新的认证方法，该方法动态调整每个样本的认证区域以提高认证性能。我们在多个基准数据集上进行了广泛的实验，证明了我们提出方法的有效性。我们的代码可在https://github.com/NcepuQiaoTing/Cert-SSB/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of deep neural networks to backdoor attacks by proposing Cert-SSB, a certified backdoor defense method. It optimizes noise magnitude for each sample using stochastic gradient ascent and applies it to multiple poisoned training sets to retrain smoothed models. The method then aggregates predictions to enhance robustness. Experiments show improved certification performance and effectiveness against backdoor attacks on benchmark datasets.</div>
<div class="mono" style="margin-top:8px">该论文通过提出Cert-SSBD方法来解决深度神经网络对后门攻击的脆弱性问题。该方法引入了针对每个样本的特定平滑噪声，以提高模型对后门攻击的鲁棒性。该方法优化了每个样本的噪声幅度，并通过聚合多个平滑模型的预测来增强认证性能。实验表明，Cert-SSBD在多种基准数据集上有效防御了后门攻击。</div>
</details>
</div>
<div class="card">
<div class="title">FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment</div>
<div class="meta-line">Authors: Han Zhao, Jingbo Wang, Wenxuan Song, Shuai Chen, Yang Liu, Yan Wang, Haoang Li, Donglin Wang</div>
<div class="meta-line">First: 2026-02-19T11:00:46+00:00 · Latest: 2026-02-19T11:00:46+00:00</div>
<div class="meta-line">Comments: Project Website: https://h-zhao1997.github.io/frappe</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://h-zhao1997.github.io/frappe">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FRAPPE：通过多未来表示对齐将世界建模融入通用政策</div>
<div class="mono" style="margin-top:8px">使VLA模型能够预测环境动态，即世界建模，已被认为是提高机器人推理和泛化的关键。然而，当前的方法面临两个主要问题：1. 训练目标迫使模型过度强调像素级重建，限制了语义学习和泛化；2. 推理过程中依赖预测的未来观察通常会导致误差累积。为了解决这些挑战，我们提出了未来表示对齐通过并行渐进扩展（FRAPPE）。我们的方法采用两阶段微调策略：在中期训练阶段，模型学习预测未来观察的潜在表示；在后期训练阶段，我们并行扩展计算负载并同时与多个不同的视觉基础模型对齐表示。通过显著提高微调效率并减少对标注动作数据的依赖，FRAPPE提供了一种可扩展且数据高效的途径，以增强通用机器人政策的世界意识。在RoboTwin基准测试和实际任务上的实验表明，FRAPPE优于现有最佳方法，并在长时序和未见过的场景中表现出强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FRAPPE addresses the limitations of current world modeling approaches by introducing a two-stage fine-tuning strategy. In the mid-training phase, the model learns to predict the latent representations of future observations, and in the post-training phase, it aligns these representations with multiple visual foundation models in parallel. This method improves fine-tuning efficiency and reduces the need for action-annotated data, leading to better generalization in long-horizon and unseen scenarios compared to state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">FRAPPE通过引入两阶段微调策略来解决当前世界建模方法的局限性。在中期训练阶段，模型学习预测未来观察的潜在表示；在后期训练阶段，它并行扩展计算工作量并同时与多个视觉基础模型对齐表示。这种方法提高了微调效率，减少了对标注动作数据的依赖，使得在长时序和未见过的场景中表现出更强的泛化能力，优于现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place</div>
<div class="meta-line">Authors: Antonio Rapuano, Yaolei Shen, Federico Califano, Chiara Gabellieri, Antonio Franchi</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-19T09:38:32+00:00 · Latest: 2026-02-19T09:38:32+00:00</div>
<div class="meta-line">Comments: Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17199v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>悬索的连续和混合动力学的非线性预测控制研究：用于空中抓取和放置的可伸缩悬索</div>
<div class="mono" style="margin-top:8px">本文提出了一种框架，用于基于偏微分方程（PDEs）的高保真模型与适合实时控制的降阶表示相结合的空中操作可伸缩悬索。PDEs 使用有限差分法离散化，并使用适当的正交分解提取降阶模型（ROM），该模型保留了主导变形模式的同时显著降低了计算复杂度。基于此 ROM，提出了一种非线性模型预测控制方案，能够稳定悬索振荡并处理混合过渡，如载荷的附着和脱离。仿真结果证实了 ROM 的稳定性和效率，以及控制器在各种操作条件下调节悬索动力学的有效性。附加仿真展示了 ROM 在受限环境中的轨迹规划应用，证明了所提方法的灵活性。总体而言，该框架使无人驾驶航空器（UAV）携带悬索的实时、动力学感知控制成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a framework for controlling the dynamics of a suspended deformable cable in aerial manipulation, using a high-fidelity model based on PDEs and a reduced-order model (ROM) derived via proper orthogonal decomposition. A nonlinear model predictive control scheme is formulated to stabilize cable oscillations and manage hybrid transitions. Simulation results show the stability, efficiency, and robustness of the ROM and the controller&#x27;s effectiveness under various conditions, highlighting the framework&#x27;s versatility for real-time UAV control with suspended flexible cables.</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于悬吊柔性缆线空中操作的动力学控制框架，结合了基于偏微分方程（PDEs）的高保真模型和通过正交分解提取的降阶模型（ROM）。通过非线性模型预测控制方案来稳定缆线振荡并处理混合过渡。仿真结果表明ROM和控制器在各种条件下的稳定性和效率，并展示了该框架在受限环境中的轨迹规划能力，突显了其实时无人机控制的灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">The Bots of Persuasion: Examining How Conversational Agents&#x27; Linguistic Expressions of Personality Affect User Perceptions and Decisions</div>
<div class="meta-line">Authors: Uğur Genç, Heng Gu, Chadha Degachi, Evangelos Niforatos, Senthil Chandrasegaran, Himanshu Verma</div>
<div class="meta-line">First: 2026-02-19T09:10:41+00:00 · Latest: 2026-02-19T09:10:41+00:00</div>
<div class="meta-line">Comments: Accepted to be presented at CHI&#x27;26 in Barcelona</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA&#x27;s composite personality did not affect participants&#x27; decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>说服性机器人：探讨语言表达个性的对话代理如何影响用户感知和决策</div>
<div class="mono" style="margin-top:8px">由大型语言模型驱动的对话代理（CAs）越来越能够通过语言表现出复杂的人格特质，但这些表现如何影响用户尚不清楚。因此，我们探讨了语言表达的CA人格如何在慈善捐赠的背景下影响用户的决策和感知。在一项众包研究中，360名参与者与八个CA之一互动，每个CA表现出由三种语言方面组成的人格特质：态度（乐观/悲观）、权威（权威/顺从）和推理（情感/理性）。虽然CA的整体人格没有影响参与者的决策，但它确实影响了他们的感知和情绪反应。特别是，与悲观CA互动的参与者感到情绪状态较低，对活动的认同感较低，认为CA不够可信和不那么有能力，但倾向于向慈善机构捐款更多。信任、能力和情境同理心的感知显著预测了捐款决策。我们的研究强调了CA作为操纵工具的风险，它们会微妙地影响用户的感知和决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how conversational agents&#x27; linguistic expressions of personality impact user perceptions and decisions in the context of charitable giving. Participants interacted with eight conversational agents, each embodying different combinations of optimism/pessimism, authority, and emotional/rational reasoning. While the CA&#x27;s personality did not influence donation decisions, it affected users&#x27; emotional states and perceptions, with pessimistic CAs leading to lower affinity towards the cause and perceived trustworthiness and competence, yet paradoxically resulting in higher donations. Trust, competence, and empathy were key predictors of donation behavior.</div>
<div class="mono" style="margin-top:8px">本研究探讨了对话代理人在慈善捐赠情境下通过语言表达个性如何影响用户感知和决策。参与者与八个具有不同乐观/悲观、权威性和情感/理性推理个性的对话代理互动。虽然代理人的个性没有影响捐赠决策，但它影响了用户的感知和情绪状态，悲观的代理使用户对活动的认同感和信任度、专业度降低，但反而导致了更多的捐赠。信任、能力和情境同理心是预测捐赠行为的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy</div>
<div class="meta-line">Authors: Huishi Huang, Jack Klusmann, Haozhe Wang, Shuchen Ji, Fengkang Ying, Yiyuan Zhang, John Nassour, Gordon Cheng, Daniela Rus, Jun Liu, Marcelo H Ang, Cecilia Laschi</div>
<div class="meta-line">First: 2026-02-19T06:56:47+00:00 · Latest: 2026-02-19T06:56:47+00:00</div>
<div class="meta-line">Comments: Camera-ready version for RoboSoft 2026. 8 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system&#x27;s response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理人机交互在增强现实中的抓取应用：刚柔机器人协同</div>
<div class="mono" style="margin-top:8px">混合刚柔机器人结合了刚性操作臂的精确性和柔性臂的顺应性和适应性，为不规则环境中的多功能抓取提供了有前景的方法。然而，协调混合机器人仍然具有挑战性，由于建模、感知和跨域运动学的困难。在本文中，我们提出了一种新颖的基于增强现实(AR)的物理人机交互框架，该框架使用户能够直接远程操作混合刚柔机器人执行简单的接近和抓取任务。通过AR头显，用户可以与集成到通用物理引擎中的机器人系统模拟模型进行交互，该物理引擎叠加在真实系统上，允许在实际部署之前进行模拟执行。为了确保虚拟和物理机器人之间的一致行为，我们引入了一种基于软机器人固有几何特性的实际到模拟参数识别管道，使我们能够准确地建模其静态和动态行为以及控制系统响应。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success</div>
<div class="meta-line">Authors: Varun Burde, Pavel Burget, Torsten Sattler</div>
<div class="meta-line">First: 2026-02-19T05:55:01+00:00 · Latest: 2026-02-19T05:55:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17101v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17101v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物体姿态估计与重建效果的机器人抓取成功率基准测试</div>
<div class="mono" style="margin-top:8px">3D重建是许多机器人感知任务的基础层，包括6D物体姿态估计和抓取姿态生成。现代物体的3D重建方法可以从多视角图像中生成视觉和几何上令人印象深刻的网格，但标准的几何评估并不能反映重建质量如何影响下游任务，如机器人操作性能。本文通过引入一个大规模的基于物理的基准测试来填补这一空白，该基准测试根据其在抓取中的功能有效性评估6D姿态估计器和3D网格模型。我们通过在各种重建的3D网格上生成抓取并执行它们，模拟使用不完美的模型生成的抓取姿态如何影响与真实物体的交互。这评估了3D重建中的姿态误差、抓取鲁棒性和几何不准确性对抓取性能的综合影响。我们的结果显示，重建伪影显著减少了抓取姿态候选的数量，但在姿态准确估计的情况下，对抓取性能的影响微乎其微。我们的结果还表明，抓取成功与姿态误差之间的关系主要由空间误差主导，即使是简单的平移误差也能揭示对称物体抓取姿态的成功。本文为如何通过机器人进行物体操作的感知系统提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper benchmarks the effects of 3D object reconstruction on robotic grasping success by introducing a large-scale, physics-based evaluation. It analyzes how reconstruction quality impacts 6D pose estimation and grasp generation, showing that while reconstruction artifacts reduce the number of grasp candidates, accurate pose estimation mitigates their negative effect on grasping performance. The study reveals that spatial errors are more critical than geometric inaccuracies in determining grasp success, even for symmetric objects.</div>
<div class="mono" style="margin-top:8px">该论文通过引入大规模的物理基准评估，研究了物体姿态估计和重建对机器人抓取成功率的影响。它分析了模型保真度对抓取性能的影响，生成各种3D网格上的抓取，并模拟实际交互。研究结果表明，重建中的错误会减少可抓取姿态候选的数量，但如果姿态估计准确，则对抓取性能影响甚微。此外，研究还发现，空间误差比姿态误差对抓取成功率的影响更大，尤其是对对称物体而言。</div>
</details>
</div>
<div class="card">
<div class="title">RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation</div>
<div class="meta-line">Authors: Yixue Zhang, Kun Wu, Zhi Gao, Zhen Zhao, Pei Ren, Zhiyuan Xu, Fei Liao, Xinhua Wang, Shichao Fan, Di Wu, Qiuxuan Feng, Meng Li, Zhengping Che, Chang Liu, Jian Tang</div>
<div class="meta-line">First: 2026-02-18T13:29:43+00:00 · Latest: 2026-02-19T04:26:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16444v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robogene-boost-vla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboGene：通过多样性驱动的代理框架增强VLA预训练以实现现实世界任务生成</div>
<div class="mono" style="margin-top:8px">通用机器人操作的追求受到多样化的现实世界交互数据稀缺性的阻碍。与视觉或语言中的网页数据收集不同，机器人数据收集是一个涉及高昂物理成本的主动过程。因此，自动化任务策展以最大化数据价值仍然是一个关键但尚未充分探索的挑战。现有的手动方法不可扩展且偏向于常见任务，而现成的基础模型往往会产生物理上不可行的指令。为了解决这个问题，我们引入了RoboGene，这是一种代理框架，旨在自动化生成单臂、双臂和移动机器人广泛物理可行的操作任务。RoboGene 结合了三个核心组件：多样性驱动的采样以实现广泛的任务覆盖、自我反思机制以强制执行物理约束以及人工在环的细化以实现持续改进。我们进行了广泛的定量分析和大规模的现实世界实验，收集了18000个轨迹的数据集，并引入了新的指标来评估任务的质量、可行性和多样性。结果表明，RoboGene 显著优于最先进的基础模型（例如GPT-4o、Gemini 2.5 Pro）。此外，现实世界实验表明，使用RoboGene预训练的VLA模型在成功率和泛化能力方面表现更优，突显了高质量任务生成的重要性。我们的项目可在https://robogene-boost-vla.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboGene is an agentic framework designed to automate the generation of diverse and physically plausible manipulation tasks for robotic manipulation. It integrates diversity-driven sampling, self-reflection mechanisms, and human-in-the-loop refinement. Extensive experiments and real-world datasets show that RoboGene outperforms existing methods and improves the success rates and generalization of VLA models pre-trained with its tasks.</div>
<div class="mono" style="margin-top:8px">RoboGene 是一个自动化生成多样化且物理上可行的机器人操作任务的框架，它结合了多样性的采样、自我反思机制和人工循环改进。实验结果表明，RoboGene 超过了最先进的基础模型，并且使用 RoboGene 预训练的 VLA 模型在实际任务中具有更高的成功率和更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation</div>
<div class="meta-line">Authors: Yejin Kim, Wilbert Pumacay, Omar Rayyan, Max Argus, Winson Han, Eli VanderBilt, Jordi Salvador, Abhay Deshpande, Rose Hendrix, Snehal Jauhri, Shuo Liu, Nur Muhammad Mahi Shafiullah, Maya Guru, Ainaz Eftekhar, Karen Farley, Donovan Clay, Jiafei Duan, Arjun Guru, Piper Wolters, Alvaro Herrasti, Ying-Chun Lee, Georgia Chalvatzaki, Yuchen Cui, Ali Farhadi, Dieter Fox, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-11T20:16:31+00:00 · Latest: 2026-02-19T00:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11337v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11337v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolmoSpaces：大规模开放生态系统，用于机器人导航和操作</div>
<div class="mono" style="margin-top:8px">大规模部署机器人需要应对日常情况的长尾效应。真实环境中的场景布局、物体几何形状和任务规范的无数变化是庞大且在现有机器人基准测试中严重不足的。衡量这种程度的泛化需要物理评估无法提供的规模和多样性基础设施。我们引入了MolmoSpaces，一个完全开放的生态系统，用于支持大规模机器人策略基准测试。MolmoSpaces 包含超过23万个多样化的室内环境，从手工制作的家庭场景到程序生成的多房间房屋，拥有13万个丰富的注释物体资产，包括4.8万个可操作物体及其4200万个稳定抓取。这些环境对模拟器是通用的，支持MuJoCo、Isaac和ManiSkill等流行选项。该生态系统支持所有类型的实体任务：静态和移动操作、导航以及需要跨整个室内环境协调感知、规划和交互的多房间长期任务。我们还设计了MolmoSpaces-Bench，一个包含8个任务的基准套件，机器人与我们的多样化场景和丰富注释物体进行交互。我们的实验表明，MolmoSpaces-Bench 展现出强大的模拟到现实的关联性（R = 0.96，ρ = 0.98），确认了较新的和更强的零样本策略在我们的基准测试中优于早期版本，并识别了提示措辞、初始关节位置和相机遮挡的关键敏感性。通过MolmoSpaces及其开源资产和工具，我们为机器人学习研究提供了可扩展的数据生成、策略训练和基准创建的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MolmoSpaces is designed to support large-scale benchmarking of robot policies by providing a diverse ecosystem of over 230k indoor environments and 130k annotated objects. The system is simulator-agnostic and includes a wide range of tasks such as manipulation, navigation, and multiroom tasks. MolmoSpaces-Bench, a benchmark suite of 8 tasks, demonstrates strong sim-to-real correlation and confirms the performance improvement of newer policies. Key sensitivities include prompt phrasing, initial joint positions, and camera occlusion.</div>
<div class="mono" style="margin-top:8px">MolmoSpaces 提供了一个包含超过 23 万个室内环境和 13 万个标注物体资产的多样化开放生态系统，以支持大规模的机器人策略基准测试。该系统包括支持各种任务（如操作、导航和多房间任务）的模拟器无关环境。MolmoSpaces-Bench 是一个包含 8 个任务的基准套件，展示了强大的模拟到现实的关联性，并确认了新版本零样本策略的性能提升。关键的敏感性包括提示措辞、初始关节位置和相机遮挡。</div>
</details>
</div>
<div class="card">
<div class="title">Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement</div>
<div class="meta-line">Authors: Minku Kim, Kuan-Chia Chen, Aayam Shrestha, Li Fuxin, Stefan Lee, Alan Fern</div>
<div class="meta-line">First: 2026-02-14T19:11:02+00:00 · Latest: 2026-02-18T23:55:04+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13850v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人哈诺伊：探究基于技能的整体身体控制技能组合方法</div>
<div class="mono" style="margin-top:8px">我们探究了一种基于技能的框架，用于类人方块重组，该框架通过在任务级别按顺序使用可重用技能来实现长期执行。在我们的架构中，所有技能都通过一个共享的任务无关的整体身体控制器（WBC）执行，提供了一致的闭环接口用于技能组合，而不同于使用每个技能的单独低级控制器的非共享设计。我们发现，简单地重复使用相同的预训练WBC在长期执行中会降低鲁棒性，因为新技能及其组合会诱导状态和命令分布的变化。我们通过一个简单的数据聚合程序来解决这一问题，该程序通过在域随机化下闭环技能执行的回放来增强共享-WBC的训练。为了评估该方法，我们引入了“类人哈诺伊”长周期的塔式方块重组基准，并在模拟和Digit V3类人机器人上报告了结果，展示了完全自主的长期重组，并量化了共享-WBC方法相对于非共享基线的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates a skill-based framework for humanoid box rearrangement, enabling long-term execution by sequencing reusable skills through a shared whole-body controller. It finds that reusing the same pretrained controller can reduce robustness over time, and proposes a data aggregation procedure to improve performance. The approach is evaluated on a long-horizon Tower-of-Hanoi benchmark, showing successful autonomous rearrangement and better performance compared to non-shared baselines on both simulation and a Digit V3 humanoid robot.</div>
<div class="mono" style="margin-top:8px">研究探索了一种基于技能的框架，用于使类人机器人在任务级别上通过序列化可重用技能来执行长时间的箱子重新排列任务。所有技能通过共享的整体身体控制器（WBC）进行一致的闭环技能组合。研究发现，重复使用相同的预训练WBC在长时间内会降低鲁棒性，因为新技能及其组合会改变状态和命令分布。为了改进这一点，作者引入了一种数据聚合程序，通过在域随机化下进行闭环技能执行来增强共享-WBC的训练。该方法通过Humanoid Hanoi基准进行评估，展示了在长时间范围内实现自主重新排列，并强调了共享-WBC方法相对于非共享基线的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming</div>
<div class="meta-line">Authors: Philip Sosnin, Jodie Knapp, Fraser Kennedy, Josh Collyer, Calvin Tsay</div>
<div class="meta-line">First: 2026-02-18T23:18:45+00:00 · Latest: 2026-02-18T23:18:45+00:00</div>
<div class="meta-line">Comments: Accepted to the 23rd International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16944v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16944v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces a verification framework that provides both sound and complete guarantees for data poisoning attacks during neural network training. We formulate adversarial data manipulation, model training, and test-time evaluation in a single mixed-integer quadratic programming (MIQCP) problem. Finding the global optimum of the proposed formulation provably yields worst-case poisoning attacks, while simultaneously bounding the effectiveness of all possible attacks on the given training pipeline. Our framework encodes both the gradient-based training dynamics and model evaluation at test time, enabling the first exact certification of training-time robustness. Experimental evaluation on small models confirms that our approach delivers a complete characterization of robustness against data poisoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用混合整数规划的确切认证数据投毒攻击</div>
<div class="mono" style="margin-top:8px">本研究引入了一种验证框架，为神经网络训练期间的数据投毒攻击提供既准确又完整的保证。我们将对抗性数据操纵、模型训练和测试时评估统一在一个混合整数二次规划（MIQCP）问题中。找到所提出形式的全局最优解可以证明产生最坏情况的投毒攻击，同时同时限制所有可能的攻击在给定训练管道中的有效性。我们的框架编码了基于梯度的训练动力学以及测试时的模型评估，使我们能够首次实现训练时鲁棒性的精确认证。实验评估表明，我们的方法可以完全表征数据投毒攻击下的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work presents a verification framework that ensures both sound and complete guarantees for data poisoning attacks during neural network training. By formulating adversarial data manipulation, model training, and test-time evaluation in a mixed-integer quadratic programming problem, the framework can find the worst-case poisoning attacks and bound the effectiveness of all possible attacks. The method encodes gradient-based training dynamics and model evaluation, providing the first exact certification of training-time robustness. Experiments on small models confirm that the approach fully characterizes robustness against data poisoning.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种通过将问题形式化为混合整数二次规划（MIQCP）问题来验证神经网络训练期间的数据投毒攻击的框架。该框架保证了对最坏情况投毒攻击的完整认证，并界定了所有可能攻击的有效性。它编码了基于梯度的训练动态和模型评估，提供了首个训练时鲁棒性的精确认证。实验表明，该方法完全界定了小模型对数据投毒的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reinforcement Learning-Based Locomotion for Resource-Constrained Quadrupeds with Exteroceptive Sensing</div>
<div class="meta-line">Authors: Davide Plozza, Patricia Apostol, Paul Joseph, Simon Schläpfer, Michele Magno</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2025-05-18T20:29:23+00:00 · Latest: 2026-02-18T22:43:05+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at the IEEE International Conference on Robotics and Automation (ICRA), Atlanta 2025. The code is available at github.com/ETH-PBL/elmap-rl-controller</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12537v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12537v2">PDF</a> · <a href="http://github.com/ETH-PBL/elmap-rl-controller">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compact quadrupedal robots are proving increasingly suitable for deployment in real-world scenarios. Their smaller size fosters easy integration into human environments. Nevertheless, real-time locomotion on uneven terrains remains challenging, particularly due to the high computational demands of terrain perception. This paper presents a robust reinforcement learning-based exteroceptive locomotion controller for resource-constrained small-scale quadrupeds in challenging terrains, which exploits real-time elevation mapping, supported by a careful depth sensor selection. We concurrently train both a policy and a state estimator, which together provide an odometry source for elevation mapping, optionally fused with visual-inertial odometry (VIO). We demonstrate the importance of positioning an additional time-of-flight sensor for maintaining robustness even without VIO, thus having the potential to free up computational resources. We experimentally demonstrate that the proposed controller can flawlessly traverse steps up to 17.5 cm in height and achieve an 80% success rate on 22.5 cm steps, both with and without VIO. The proposed controller also achieves accurate forward and yaw velocity tracking of up to 1.0 m/s and 1.5 rad/s respectively. We open-source our training code at github.com/ETH-PBL/elmap-rl-controller.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于环境感知的资源受限四足机器人鲁棒强化学习行走控制</div>
<div class="mono" style="margin-top:8px">紧凑型四足机器人在实际应用场景中越来越适合部署。它们较小的尺寸促进了与人类环境的轻松集成。然而，在不平坦地形上的实时行走仍然具有挑战性，特别是由于地形感知的高计算需求。本文提出了一种适用于资源受限的小型四足机器人在挑战性地形上的鲁棒强化学习环境感知行走控制器，该控制器利用实时高程映射，并通过仔细选择深度传感器加以支持。我们同时训练了一个策略和一个状态估计器，它们一起提供高程映射的里程计来源，可选地与视觉惯性里程计（VIO）融合。我们证明了额外放置一个飞行时间传感器对于保持鲁棒性的重要性，即使没有VIO，也能释放计算资源。实验表明，所提出的控制器可以完美地跨越高达17.5厘米的台阶，并在有和没有VIO的情况下，22.5厘米台阶的成功率达到80%。所提出的控制器还实现了高达1.0米/秒的前向和1.5弧度/秒的偏航速度跟踪。我们在github.com/ETH-PBL/elmap-rl-controller上开源了训练代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of real-time locomotion for small quadruped robots on uneven terrains by developing a robust reinforcement learning-based exteroceptive locomotion controller. The method uses real-time elevation mapping supported by a carefully selected depth sensor and concurrently trains a policy and a state estimator to provide odometry. The controller demonstrates successful traversal of steps up to 17.5 cm in height and a 80% success rate on 22.5 cm steps, both with and without visual-inertial odometry, and achieves accurate velocity tracking. The code is open-sourced for further research.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于强化学习的鲁棒外部感知运动控制器，以解决小型四足机器人在崎岖地形上实时运动的挑战。该方法利用精心选择的深度传感器进行实时地形高度映射，并训练策略和状态估计器提供航位推算。实验表明，该控制器可以跨越高达17.5厘米的台阶，并在22.5厘米的台阶上保持80%的成功率，无论是否有视觉惯性航位推算，同时实现准确的速度跟踪。代码已开源。</div>
</details>
</div>
<div class="card">
<div class="title">SparTa: Sparse Graphical Task Models from a Handful of Demonstrations</div>
<div class="meta-line">Authors: Adrian Röfer, Nick Heppert, Abhinav Valada</div>
<div class="meta-line">First: 2026-02-18T21:54:35+00:00 · Latest: 2026-02-18T21:54:35+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16911v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16911v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method&#x27;s demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SparTa: 稀疏图形任务模型从少量演示中学习</div>
<div class="mono" style="margin-top:8px">高效学习长期操作任务是机器人演示学习中的一个核心挑战。与最近专注于直接在动作域中学习任务的努力不同，我们关注的是推断机器人在任务中应实现什么，而不是如何实现。为此，我们使用一系列图形对象关系来表示不断变化的场景状态。我们提出了一种演示分割和聚合方法，提取一系列操作图，并估计任务阶段中对象状态的概率分布。与仅捕获部分交互或短暂时间窗口的先前基于图的方法不同，我们的方法捕获从控制开始到操作结束的完整对象交互。为了在学习多个演示时提高鲁棒性，我们还使用预训练的视觉特征进行对象匹配。在广泛的实验中，我们评估了我们方法的演示分割准确性以及从多个演示中学习找到所需最小任务模型的实用性。最后，我们在仿真和真实机器人上部署了拟合模型，证明了由此产生的任务表示支持跨环境的可靠执行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to learn long-horizon manipulation tasks efficiently by focusing on inferring the robot&#x27;s goals rather than the specific actions. It proposes SparTa, which uses graphical object relationships to represent evolving scene states and segments demonstrations to estimate distributions over object states. The method captures complete object interactions from the start of control to the end of manipulation and uses pre-trained visual features for robust object matching. Experiments show high segmentation accuracy and the effectiveness of learning from multiple demonstrations to find a minimal task model, with successful deployment in simulation and on a real robot.</div>
<div class="mono" style="margin-top:8px">研究旨在通过关注推断机器人的目标，而不是具体的动作来高效地学习长周期的操纵任务。方法包括用图形对象关系表示场景状态，并使用演示片段分割和聚合方法来提取操纵图并估计对象状态分布。该方法捕捉从控制开始到操纵结束的完整对象交互，而之前的许多方法仅关注部分交互或短暂的时间窗口。实验表明，该方法能够准确地分割演示，并通过从多个演示中学习提高鲁棒性，从而在仿真和真实机器人中实现可靠的执行。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-18T20:42:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向对象的零样本灵巧工具操作策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可以执行的任务集。然而，工具操作代表了一类具有挑战性的灵巧性，需要抓取细长物体、手持物体旋转以及进行有力的交互。由于收集这些行为的遥操作数据具有挑战性，因此模拟到现实的强化学习（RL）是一种有前途的替代方案。然而，先前的方法通常需要大量的工程努力来建模物体并调整每个任务的奖励函数。在本工作中，我们提出了SimToolReal，朝着为工具操作生成通用的模拟到现实的RL策略迈出了一步。我们不是专注于单一物体和任务，而是通过模拟程序生成大量工具样物体素，并训练一个具有通用目标的单一RL策略，即操纵每个物体到随机目标姿态。这种方法使SimToolReal能够在测试时进行通用灵巧工具操作，而无需任何物体或任务特定的训练。我们证明SimToolReal在120个跨越24个任务、12个物体实例和6个工具类别的现实世界操作中，比先前的重新瞄准和固定抓取方法高出37%，并且与特定目标物体和任务训练的专家RL策略的性能相当。最后，我们展示了SimToolReal在一系列日常工具中具有良好的泛化能力，在120个现实世界操作中实现了强大的零样本性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SimToolReal, a policy designed for zero-shot dexterous tool manipulation in robotics. It generates a variety of tool-like objects in simulation and trains a single RL policy to manipulate these objects to random goal poses. This approach allows the policy to perform general dexterous manipulation without specific training for each object or task. SimToolReal outperforms previous methods by 37% and matches the performance of specialist policies trained on specific objects and tasks, demonstrating strong zero-shot performance across 120 real-world rollouts involving 24 tasks, 12 object instances, and 6 tool categories.</div>
<div class="mono" style="margin-top:8px">该论文提出了SimToolReal，一种用于机器人零样本灵巧工具操作的策略。它通过在模拟中程序生成各种工具样物体并训练一个通用策略来解决模拟到现实的强化学习挑战。该策略能够在无需额外训练的情况下将不同物体移动到随机目标位置。实验结果显示，SimToolReal 的性能比之前的方法高出 37%，并在涵盖 24 任务、12 个物体实例和 6 个工具类别在内的 120 个真实世界操作中表现出强大的零样本性能。</div>
</details>
</div>
<div class="card">
<div class="title">One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation</div>
<div class="meta-line">Authors: Zhenyu Wei, Yunchao Yao, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-18T18:59:57+00:00 · Latest: 2026-02-18T18:59:57+00:00</div>
<div class="meta-line">Comments: Project Page: https://zhenyuwei2003.github.io/OHRA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16712v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhenyuwei2003.github.io/OHRA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一掌统治一切：统一灵巧操作的规范表示</div>
<div class="mono" style="margin-top:8px">当前的灵巧操作策略大多假定固定的手部设计，严重限制了它们对具有不同运动学和结构布局的新实体的泛化能力。为克服这一限制，我们引入了一个参数化的规范表示，统一了广泛的灵巧手架构。它包括一个统一的参数空间和一个规范的URDF格式，提供三个关键优势。1) 参数空间捕捉了有效学习算法中所需的关键形态和运动学变化。2) 可以在我们的空间中学习一个结构化的潜在流形，其中不同实体之间的插值会产生平滑且物理上合理的形态过渡。3) 规范的URDF标准化了动作空间，同时保留了原始URDF的动力学和功能特性，使跨实体学习高效可靠。我们通过广泛的分析和实验验证了这些优势，包括抓取策略回放、VAE潜在编码和跨实体零样本转移。具体而言，我们在统一表示上训练了一个VAE，以获得一个紧凑且语义丰富的潜在嵌入，并开发了一个基于规范表示的抓取策略，该策略在灵巧手之间具有泛化能力。我们通过模拟和在未见过的形态上的实际任务（例如，3指LEAP手的零样本成功率高达81.9%）展示了我们的框架如何统一结构多样手的表示空间和动作空间，为跨手学习提供了一个可扩展的基础，以实现通用灵巧操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of fixed hand designs in dexterous manipulation policies by introducing a parameterized canonical representation that unifies various hand architectures. The method includes a unified parameter space and a canonical URDF format, which captures morphological and kinematic variations, enables smooth transitions between embodiments, and standardizes the action space while preserving dynamic properties. Key experimental findings show that a VAE trained on this representation achieves a 3-finger LEAP Hand zero-shot success rate of 81.9%, demonstrating the framework&#x27;s effectiveness for cross-hand learning and universal dexterous manipulation.</div>
<div class="mono" style="margin-top:8px">论文通过引入一个参数化的统一表示来解决当前灵巧操作策略中固定手部设计的局限性，该表示统一了各种手部架构。该表示包括统一的参数空间和标准的URDF格式，增强了不同手部形态之间的泛化能力。实验结果表明，基于这种统一表示训练的VAE在3指LEAP手上的零样本成功率达到了81.9%，展示了该框架在跨手部学习中实现通用灵巧操作的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data</div>
<div class="meta-line">Authors: Ruijie Zheng, Dantong Niu, Yuqi Xie, Jing Wang, Mengda Xu, Yunfan Jiang, Fernando Castañeda, Fengyuan Hu, You Liang Tan, Letian Fu, Trevor Darrell, Furong Huang, Yuke Zhu, Danfei Xu, Linxi Fan</div>
<div class="meta-line">First: 2026-02-18T18:59:05+00:00 · Latest: 2026-02-18T18:59:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoScale：通过多样化第一人称人类数据扩展灵巧操作</div>
<div class="mono" style="margin-top:8px">人类行为是学习物理智能最具扩展性的数据来源之一，但如何有效地利用它来实现灵巧操作仍不清楚。尽管先前的工作在受限环境中展示了从人类到机器人的转移，但大规模人类数据是否能支持精细的、高自由度的灵巧操作尚不清楚。我们提出了EgoScale，这是一种基于大规模第一人称人类数据的从人类到灵巧操作的转移框架。我们在一个超过20,854小时的动作标注第一人称人类视频上训练了一个视觉语言动作（VLA）模型，数据量超过先前努力的20倍，并发现人类数据规模与验证损失之间存在对数线性关系。这种验证损失与下游真实机器人性能高度相关，确立了大规模人类数据作为可预测的监督来源。除了规模，我们引入了一个简单的两阶段转移配方：大规模人类预训练后，进行轻量级对齐的人机中期训练。这使得在最少的机器人监督下实现强大的长时灵巧操作和一次性的任务适应成为可能。我们的最终策略在使用22个自由度的灵巧机器人手中将平均成功率提高了54%，并且能够有效地转移到具有较少自由度的手上，表明大规模的人类运动提供了可重复使用、与身体无关的运动先验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EgoScale is a framework for transferring human dexterity to robots using large-scale egocentric human data. It trains a Vision Language Action model on over 20,854 hours of video, showing a log-linear relationship between data scale and validation loss, which correlates with robot performance. The method involves two stages: pretraining on human data and mid-training with aligned human-robot data, enabling strong dexterous manipulation and one-shot task adaptation with minimal robot supervision. The final policy improves the average success rate by 54% compared to a baseline with no pretraining.</div>
<div class="mono" style="margin-top:8px">EgoScale 是一个框架，利用大规模的第一人称人类数据将人类的灵巧性转移到机器人上以执行精细的操纵任务。通过在超过20,854小时的动作标注的人类视频上训练Vision Language Action模型，研究人员发现人类数据规模与验证损失之间存在对数线性关系，这与实际机器人性能相关。该框架采用两阶段转移方法，包括大规模的人类预训练和轻量级的人机对齐，能够实现长时间的灵巧操纵和一次性的任务适应。最终策略在使用22自由度的灵巧机器人手时将平均成功率提高了54%，并在自由度较低的机器人手上也表现出有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</div>
<div class="meta-line">Authors: Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</div>
<div class="meta-line">First: 2026-02-18T18:55:02+00:00 · Latest: 2026-02-18T18:55:02+00:00</div>
<div class="meta-line">Comments: Project page: https://hero-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16705v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hero-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人机器人开放词汇视觉移动物体末端执行器控制学习</div>
<div class="mono" style="margin-top:8px">使用类人机器人在野外对任意物体进行视觉移动物体操作需要精确的末端执行器（EE）控制和通过视觉输入（例如RGB-D图像）对场景的广泛理解。现有方法基于现实世界的模仿学习，由于难以收集大规模训练数据集，因此表现出有限的泛化能力。本文提出了一种新的范式HERO，用于类人机器人物体移动物体操作，结合了大型视觉模型的强大泛化能力和开放词汇理解，以及模拟训练中的强大控制性能。我们通过设计一种准确的残差感知末端执行器跟踪策略来实现这一点。该末端执行器跟踪策略结合了经典机器人学和机器学习。它使用a) 逆运动学将残差末端执行器目标转换为参考轨迹，b) 用于准确前向运动学的已学习神经前向模型，c) 目标调整，以及d) 重新规划。这些创新共同帮助我们将末端执行器跟踪误差降低了3.2倍。我们使用这种准确的末端执行器跟踪器构建了一个模块化移动物体系统，其中使用开放词汇大型视觉模型实现强大的视觉泛化。我们的系统能够在从办公室到咖啡馆等多样化的现实环境中操作，机器人能够可靠地操作各种日常物体（例如茶杯、苹果、玩具），这些物体位于43cm到92cm高度的表面上。在模拟和现实世界中的系统模块化和端到端测试表明我们提出的设计的有效性。我们认为本文中的进展可以为训练类人机器人与日常物体交互开辟新的途径。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to unfold cloth: Scaling up world models to deformable object manipulation</div>
<div class="meta-line">Authors: Jack Rome, Stephen James, Subramanian Ramamoorthy</div>
<div class="meta-line">First: 2026-02-18T18:14:41+00:00 · Latest: 2026-02-18T18:14:41+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习展开布料：将世界模型扩展到可变形物体操作</div>
<div class="mono" style="margin-top:8px">学习操作布料既是机器人研究中的一个典型问题，也是从辅助护理到服务业等多个应用领域的即时相关问题。可变形物体的复杂物理特性使得布料操作问题变得非平凡。为了创建一个能够应对各种形状、大小、折叠和皱纹模式的通用操作策略，除了通常的外观变化问题，仔细考虑模型结构及其对泛化性能的影响变得至关重要。在本文中，我们提出了一种使用最近提出的强化学习架构DreamerV2变体的方法，用于空中布料操作。我们的实现修改了该架构，使其能够利用表面法线输入，并修改了回放缓冲区和数据增强程序。这些修改共同增强了机器人使用的世界模型，解决了机器人操作对象的物理复杂性。我们在模拟中进行了评估，并在物理机器人设置中进行了零样本部署，展示了对不同布料类型的空中展开，证明了我们提出架构的泛化优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of cloth manipulation by developing a reinforcement learning approach using DreamerV2, which is enhanced with surface normals input and modified replay buffer and data augmentation. The method is evaluated in both simulation and a physical robot setup, showing the ability to unfold various types of cloth, highlighting the generalization benefits of the proposed architecture.</div>
<div class="mono" style="margin-top:8px">该论文解决了布料操作的挑战，这对于机器人研究和各种应用至关重要。作者使用修改后的DreamerV2强化学习架构创建了一个包含表面法线的世界模型，并增强了回放缓冲区和数据增强技术。该模型在模拟和物理机器人设置中进行了评估，成功展示了对不同类型的布料进行展开的能力，突显了他们方法的泛化优势。</div>
</details>
</div>
<div class="card">
<div class="title">Elements of Robot Morphology: Supporting Designers in Robot Form Exploration</div>
<div class="meta-line">Authors: Amy Koike, Serena Ge Guo, Xinning He, Callie Y. Kim, Dakota Sullivan, Bilge Mutlu</div>
<div class="meta-line">First: 2026-02-09T21:13:20+00:00 · Latest: 2026-02-18T17:41:00+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, Proceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09203v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09203v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人形态学要素：支持设计师进行机器人形态探索</div>
<div class="mono" style="margin-top:8px">机器人形态，即机器人的形状、结构，是人机交互（HRI）中的关键设计空间，影响着机器人的功能、表达方式以及与人的互动。尽管其重要性不言而喻，但关于设计框架如何指导系统性形态探索的研究却鲜有涉及。为填补这一空白，我们提出了机器人形态学要素这一框架，识别出五个基本要素：感知、关节、末端执行器、移动方式和结构。该框架源自对现有机器人的分析，支持对多样化机器人形态的结构化探索。为实现该框架的实用化，我们开发了形态探索模块（MEB），一套实体模块，使人们能够进行动手、协作的机器人形态实验。通过案例研究和设计研讨会，我们评估了该框架和工具包，展示了它们如何支持分析、创意生成、反思以及协作机器人设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces Elements of Robot Morphology, a framework that identifies five fundamental elements of robot morphology: perception, articulation, end effectors, locomotion, and structure. This framework supports systematic exploration of diverse robot forms. To operationalize the framework, Morphology Exploration Blocks (MEB) were developed, enabling hands-on, collaborative experimentation. The framework and toolkit were evaluated through a case study and design workshops, demonstrating their effectiveness in analysis, ideation, reflection, and collaborative robot design.</div>
<div class="mono" style="margin-top:8px">论文提出了机器人形态学框架，识别了感知、运动、末端执行器、移动和结构五个基本要素。该框架支持对多样化机器人形态的结构化探索。为了实现这一框架，开发了形态探索块（MEB），支持动手和协作的机器人形态实验。通过案例研究和设计研讨会评估了该框架和工具包，展示了它们在分析、创意生成、反思和协作机器人设计中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency</div>
<div class="meta-line">Authors: Yifei Su, Ning Liu, Dong Chen, Zhen Zhao, Kun Wu, Meng Li, Zhiyuan Xu, Zhengping Che, Jian Tang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T14:12:53+00:00 · Latest: 2026-02-18T13:54:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08822v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08822v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approaches accelerate sampling in generative modeling-based visuomotor policies by adapting techniques originally developed to speed up image generation. However, a major distinction exists: image generation typically produces independent samples without temporal dependencies, while robotic manipulation requires generating action trajectories with continuity and temporal coherence. To this end, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. Concretely, we introduce a frequency consistency constraint objective that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on 40 tasks of LIBERO. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreqPolicy: 基于频率一致性的高效流基视动策略</div>
<div class="mono" style="margin-top:8px">基于生成建模的视动策略在机器人操作中得到了广泛应用，这得益于它们能够建模多模态动作分布的能力。然而，多步采样的高推理成本限制了其在实时机器人系统中的应用。现有方法通过适应原本用于加速图像生成的技术来加速基于生成建模的视动策略的采样。然而，一个主要区别在于：图像生成通常产生独立样本且没有时间依赖性，而机器人操作需要生成具有连续性和时间一致性的动作轨迹。为此，我们提出了一种名为FreqPolicy的新方法，首先在流基视动策略上施加频率一致性约束。我们的工作使动作模型能够有效地捕捉时间结构，同时支持高效、高质量的一步动作生成。具体而言，我们引入了一个频率一致性约束目标，该目标在流的不同时间步沿频率域动作特征上强制执行对齐，从而促进一步动作生成向目标分布收敛。此外，我们设计了一种自适应一致性损失来捕捉机器人操作任务中固有的结构时间变化。我们在3个仿真基准上的53个任务上评估了FreqPolicy，证明了它在现有一步动作生成器中的优越性。我们进一步将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在LIBERO的40个任务上实现了加速且未降低性能。此外，我们在真实世界机器人场景中展示了其高效性和有效性，推理频率为93.5 Hz。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FreqPolicy is designed to improve the efficiency of flow-based visuomotor policies in robotic manipulation by imposing frequency consistency constraints, which enable effective temporal structure capture and high-quality one-step action generation. The approach introduces a frequency consistency constraint objective and an adaptive consistency loss to align action features across timesteps and capture structural temporal variations. Experiments on 53 tasks across three simulation benchmarks and 40 tasks of LIBERO in the VLA model demonstrate FreqPolicy&#x27;s superiority over existing methods, with an inference frequency of 93.5 Hz in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">FreqPolicy 是一种通过频率一致性约束来优化流基运动视觉策略的方法，以实现高效且高质量的一步动作生成，适用于机器人操作。它引入了频率一致性约束目标和自适应一致性损失，以促进动作轨迹的时间连贯性。FreqPolicy 在 3 个仿真基准的 53 个任务中优于现有的一步动作生成器，并且与视觉-语言-动作模型集成良好，在 LIBERO 的 40 个任务中保持性能，同时在真实世界场景中的推理频率达到 93.5 Hz。</div>
</details>
</div>
<div class="card">
<div class="title">Reactive Motion Generation With Particle-Based Perception in Dynamic Environments</div>
<div class="meta-line">Authors: Xiyuan Zhao, Huijun Li, Lifeng Zhu, Zhikai Wei, Xianyi Zhu, Aiguo Song</div>
<div class="meta-line">First: 2026-02-18T13:48:54+00:00 · Latest: 2026-02-18T13:48:54+00:00</div>
<div class="meta-line">Comments: This paper has 20 pages, 15 figures, and 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16462v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reactive motion generation in dynamic and unstructured scenarios is typically subject to essentially static perception and system dynamics. Reliably modeling dynamic obstacles and optimizing collision-free trajectories under perceptive and control uncertainty are challenging. This article focuses on revealing tight connection between reactive planning and dynamic mapping for manipulators from a model-based perspective. To enable efficient particle-based perception with expressively dynamic property, we present a tensorized particle weight update scheme that explicitly maintains obstacle velocities and covariance meanwhile. Building upon this dynamic representation, we propose an obstacle-aware MPPI-based planning formulation that jointly propagates robot-obstacle dynamics, allowing future system motion to be predicted and evaluated under uncertainty. The model predictive method is shown to significantly improve safety and reactivity with dynamic surroundings. By applying our complete framework in simulated and noisy real-world environments, we demonstrate that explicit modeling of robot-obstacle dynamics consistently enhances performance over state-of-the-art MPPI-based perception-planning baselines avoiding multiple static and dynamic obstacles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于粒子感知的动力学环境下的反应式运动生成</div>
<div class="mono" style="margin-top:8px">在动态和非结构化的场景中，反应式运动生成通常依赖于基本静态的感知和系统动力学。准确建模动态障碍物并在感知和控制不确定性下优化无碰撞轨迹是具有挑战性的。本文从模型的角度重点揭示了反应式规划与动态制图之间的紧密联系。为了实现高效的粒子感知并具有动态特性，我们提出了一种张量化的粒子权重更新方案，该方案明确地维护了障碍物的速度和协方差。基于这种动态表示，我们提出了一种障碍物感知的MPPI基规划公式，该公式联合传播了机器人-障碍物动力学，使得未来系统的运动可以在不确定性下被预测和评估。模型预测方法被证明可以显著提高在动态环境中的安全性和反应性。通过在模拟和嘈杂的真实环境中应用我们完整的框架，我们证明了对机器人-障碍物动力学的显式建模在避免多个静态和动态障碍物方面始终优于最先进的MPPI基感知-规划基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reactive motion generation in dynamic environments by integrating dynamic perception and planning. It introduces a tensorized particle weight update scheme that maintains obstacle velocities and covariance, and proposes an obstacle-aware Model Predictive Path Integral (MPPI) planning method that jointly propagates robot-obstacle dynamics. The results show significant improvements in safety and reactivity compared to state-of-the-art methods in both simulated and real-world environments with dynamic obstacles.</div>
<div class="mono" style="margin-top:8px">本文通过结合动态感知和规划解决了动态环境下的反应性运动生成问题。它引入了一种张量化的粒子权重更新方案，以维持障碍物的速度和协方差，并提出了一种基于模型预测路径积分（MPPI）的障碍感知规划方法，该方法联合传播了机器人和障碍物的动力学。结果显示，在包含静态和动态障碍物的模拟和真实环境中，这种方法在安全性和反应性方面显著优于最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</div>
<div class="meta-line">Authors: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi</div>
<div class="meta-line">First: 2026-02-12T17:46:52+00:00 · Latest: 2026-02-18T11:55:37+00:00</div>
<div class="meta-line">Comments: VIRENA is under active development and currently in use at the University of Zurich. This preprint will be updated as new features are released. For the latest version and to inquire about demos or pilot collaborations, contact the authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12207v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12207v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA&#x27;s no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRENA：虚拟竞技场，用于研究、教育和民主创新</div>
<div class="mono" style="margin-top:8px">数字平台塑造了人们的沟通、讨论和形成观点的方式。由于数据访问受限、现实世界实验的伦理限制以及现有研究工具的局限性，研究这些动态变得越来越困难。VIRENA（虚拟竞技场）是一个平台，它能够在现实社交媒体环境中进行受控实验。多个参与者可以同时在基于信息流的平台（Instagram、Facebook、Reddit）和即时通讯应用（WhatsApp、Messenger）的现实复制品中互动。由大型语言模型驱动的AI代理可以与人类一起参与，具有可配置的人格和现实行为。研究人员可以通过无需编程技能的可视化界面操控内容审核方法、预排定刺激内容，并在不同条件下运行实验。VIRENA 使以前不切实际的研究设计成为可能：在现实社会环境中研究人类与AI的互动、实验性地比较干预措施的效果以及观察小组讨论的展开过程。VIRENA 建立在开源技术之上，确保数据保留在机构控制之下并符合数据保护要求，目前在苏黎世大学使用中，并可供试点合作。VIRENA 的无代码界面使其跨学科和行业中的受控社交媒体模拟变得可行。本文记录了其设计、架构和功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VIRENA is a platform designed to enable controlled experimentation in realistic social media environments, addressing the challenges of restricted data access and ethical constraints. Researchers can manipulate content moderation, pre-schedule stimulus content, and run experiments through a no-code visual interface. Key findings include the ability to study human-AI interaction, experimentally compare moderation interventions, and observe group deliberation in realistic social contexts, making previously impractical research designs possible.</div>
<div class="mono" style="margin-top:8px">VIRENA 是一个平台，旨在通过现实的社交媒体环境进行受控实验，解决数据访问受限和伦理约束的问题。研究人员可以通过无代码的可视化界面操纵内容审核、预排刺激内容并运行实验。主要发现包括能够研究人-AI 交互、实验性比较审核干预措施以及观察群体讨论在现实社会环境中的演变，使以前不可行的研究设计成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">AMBER: A tether-deployable gripping crawler with compliant microspines for canopy manipulation</div>
<div class="meta-line">Authors: P. A. Wigner, L. Romanello, A. Hammad, P. H. Nguyen, T. Lan, S. F. Armanini, B. B. Kocer, M. Kovac</div>
<div class="meta-line">First: 2025-12-08T16:17:56+00:00 · Latest: 2026-02-18T11:42:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07680v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.07680v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90$^\circ$ body roll and inclination, while effective climbing on branches inclined up to 67.5$^\circ$, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10$^\circ$, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. The crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing. The aerial deployment is demonstrated at a conceptual and feasibility level, while full drone-crawler integration is left as future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMBER：一种用于树冠操作的可伸展攀爬器，配备顺应式微钩</div>
<div class="mono" style="margin-top:8px">本文介绍了一种可空中部署的攀爬器，用于树冠内的适应性移动和操作。该系统结合了顺应式微钩履带、双轨旋转夹持器和弹性尾部，使其能够安全地附着并稳定地穿越不同曲率和倾斜角度的树枝。实验表明，该攀爬器在90°身体滚转和倾斜角度下仍能可靠地抓握，有效攀爬倾斜角度达67.5°的树枝，最大速度为每秒0.55个身体长度。顺应式履带允许最大10°的偏航转向，提高其在不规则表面的机动性。功率测量显示，该攀爬器的无量纲运输成本比典型悬停功率消耗低一个数量级，提供了一个坚固、低功耗的环境采样和树冠内传感平台。空中部署在概念和可行性层面进行了演示，而完整的无人机-攀爬器集成留作未来工作。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped</div>
<div class="meta-line">Authors: Saumya Karan, Neerav Maram, Suraj Borate, Madhu Vadali</div>
<div class="meta-line">First: 2026-02-18T11:14:22+00:00 · Latest: 2026-02-18T11:14:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16371v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16371v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">SLOT (Soft Legged Omnidirectional Tetrapod), a tendon-driven soft quadruped robot with 3D-printed TPU legs, is presented to study physics-informed modeling and control of compliant legged locomotion using only four actuators. Each leg is modeled as a deformable continuum using discrete Cosserat rod theory, enabling the capture of large bending deformations, distributed elasticity, tendon actuation, and ground contact interactions. A modular whole-body modeling framework is introduced, in which compliant leg dynamics are represented through physically consistent reaction forces applied to a rigid torso, providing a scalable interface between continuum soft limbs and rigid-body locomotion dynamics. This formulation allows efficient whole-body simulation and real-time control without sacrificing physical fidelity. The proposed model is embedded into a convex model predictive control framework that optimizes ground reaction forces over a 0.495 s prediction horizon and maps them to tendon actuation through a physics-informed force-angle relationship. The resulting controller achieves asymptotic stability under diverse perturbations. The framework is experimentally validated on a physical prototype during crawling and walking gaits, achieving high accuracy with less than 5 mm RMSE in center of mass trajectories. These results demonstrate a generalizable approach for integrating continuum soft legs into model-based locomotion control, advancing scalable and reusable modeling and control methods for soft quadruped robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>腱驱动软四足动物的动态建模与MPC行走研究</div>
<div class="mono" style="margin-top:8px">SLOT（软腿全方位四足动物），一种使用3D打印TPU腿的腱驱动软四足动物机器人，用于研究仅使用四个执行器的顺应性腿足运动的物理启发式建模和控制。每条腿被建模为可变形连续体，使用离散柯西尔杆理论，能够捕捉到大弯曲变形、分布弹性、腱驱动和地面接触相互作用。引入了一种模块化的全身建模框架，在该框架中，通过在刚性躯干上施加物理一致的反作用力来表示顺应腿的动力学，提供了一种连续软肢和刚体运动动力学之间的可扩展接口。该公式允许高效的整体身体仿真和实时控制，而不牺牲物理精度。提出的模型嵌入到凸模型预测控制框架中，该框架在0.495秒的预测窗口内优化地面反作用力，并通过物理启发的力-角关系将其映射到腱驱动。所得到的控制器在各种扰动下实现了渐近稳定性。该框架在爬行和行走步态的物理原型上进行了实验验证，实现了高精度，中心质量轨迹的RMSE小于5毫米。这些结果展示了将连续软腿整合到基于模型的运动控制中的通用方法，推进了软四足动物机器人可扩展和可重用建模与控制方法的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study presents SLOT, a tendon-driven soft quadruped robot, to investigate compliant legged locomotion using discrete Cosserat rod theory for modeling each leg. The approach introduces a modular whole-body modeling framework that represents compliant leg dynamics through reaction forces on a rigid torso, enabling efficient simulation and real-time control. The model is integrated into a convex model predictive control framework, achieving asymptotic stability and high accuracy in center of mass trajectories during crawling and walking gaits, with less than 5 mm RMSE. This work advances scalable and reusable modeling and control methods for soft quadruped robots.</div>
<div class="mono" style="margin-top:8px">该研究介绍了使用离散柯西尔杆理论建模每个腿的腱驱动软四足机器人SLOT，以研究柔顺腿足运动。引入了一种模块化的整体建模框架，实现了高效的仿真和实时控制。提出的模型嵌入到凸模型预测控制框架中，该框架优化地面反作用力并将其映射到腱驱动。在物理原型上的实验验证显示，在爬行和行走过程中，中心质量轨迹的准确性非常高，展示了将连续柔顺腿足整合到基于模型的运动控制中的通用方法，推进了软四足机器人的可扩展和可重用建模与控制方法的发展。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
