<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-12 04:04</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260212_0404</div>
    <div class="row"><div class="card">
<div class="title">ST4VLA: Spatially Guided Training for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Jinhui Ye, Fangjing Wang, Ning Gao, Junqiu Yu, Yangkun Zhu, Bin Wang, Jinyu Zhang, Weiyang Jin, Yanwei Fu, Feng Zheng, Yilun Chen, Jiangmiao Pang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-10T18:59:17+00:00 · Latest: 2026-02-10T18:59:17+00:00</div>
<div class="meta-line">Comments: Spatially Training for VLA, Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10109v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10109v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://internrobotics.github.io/internvla-m1.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -&gt; 84.6 on Google Robot and from 54.7 -&gt; 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ST4VLA：空间引导训练的视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">大型多模态模型（VLMs）在多模态理解方面表现出色，但在扩展到具身任务时却力不从心，因为指令必须转化为低级的运动动作。我们提出了ST4VLA，这是一种双系统视觉-语言-行动框架，利用空间引导训练来使动作学习与VLM中的空间先验相一致。ST4VLA包括两个阶段：（i）空间定位预训练，通过从大规模网络和机器人特定数据中进行可扩展的点、框和轨迹预测，为VLM提供可转移的先验；（ii）空间引导的动作后训练，通过空间提示鼓励模型产生更丰富的空间先验，以指导动作生成。这种设计在策略学习过程中保持了空间定位的一致性，并在空间和动作目标之间实现了优化的一致性。实验结果显示，ST4VLA在Google Robot上的性能从66.1提高到84.6，在WidowX Robot上的性能从54.7提高到73.2，建立了SimplerEnv上的新最佳结果。它还展示了对未见过的对象和重新表述的指令的更强泛化能力，以及在真实世界环境中对长时间扰动的鲁棒性。这些结果突显了可扩展的空间引导训练是实现鲁棒和可泛化的机器人学习的一个有前景的方向。源代码、数据和模型可在https://internrobotics.github.io/internvla-m1.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ST4VLA is a dual-system Vision-Language-Action framework that uses spatially guided training to enhance action learning in VLMs. It consists of two stages: spatial grounding pre-training to equip the model with transferable priors, and spatially guided action post-training to encourage richer spatial priors for action generation. ST4VLA significantly improves performance on Google Robot and WidowX Robot, achieving new state-of-the-art results on SimplerEnv and demonstrating better generalization and robustness to unseen objects and long-horizon perturbations.</div>
<div class="mono" style="margin-top:8px">ST4VLA 是一种双系统视觉-语言-动作框架，通过将动作学习与大型视觉-语言模型中的空间先验对齐，使用空间引导训练来提高执行任务的表现。它包括两个阶段：空间接地预训练和空间引导动作后训练。ST4VLA 在 Google Robot 和 WidowX Robot 上显著提高了性能，达到了新的最佳结果，并展示了更好的泛化能力和对未见过的对象和长时扰动的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration</div>
<div class="meta-line">Authors: Modi Shi, Shijia Peng, Jin Chen, Haoran Jiang, Yinghui Li, Di Huang, Ping Luo, Hongyang Li, Li Chen</div>
<div class="meta-line">First: 2026-02-10T18:59:03+00:00 · Latest: 2026-02-10T18:59:03+00:00</div>
<div class="meta-line">Comments: Project page: https://opendrivelab.com/EgoHumanoid</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10106v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10106v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoHumanoid：利用机器人无介入的自我中心演示解锁野外移动操作</div>
<div class="mono" style="margin-top:8px">人类演示提供了丰富的环境多样性，并且能够自然地扩展，使其成为机器人遥操作的有吸引力的替代方案。尽管这一范式已经推动了机器人臂操作的发展，但其在更具挑战性、数据需求更大的人形移动操作问题上的潜力仍然未被充分探索。我们提出了EgoHumanoid，这是第一个使用丰富的自我中心人类演示数据与有限的机器人数据共同训练视觉-语言-动作策略的框架，使机器人能够在多种真实环境中的进行移动操作。为了弥合人类与机器人之间的实体差距，包括物理形态和视角的差异，我们引入了一个从硬件设计到数据处理的系统对齐流水线。开发了一个便携式系统以实现大规模的人类数据收集，并建立了实用的收集协议以提高可移植性。我们人到人形对齐流水线的核心包括两个关键组件。视图对齐减少了由于摄像头高度和视角差异引起的视觉领域差异。动作对齐将人类动作映射到统一的、动力学可行的动作空间，以供人形控制。广泛的实地实验表明，结合机器人无介入的自我中心数据显著优于仅基于机器人的基线，特别是在未见过的环境中。我们的分析进一步揭示了哪些行为能够有效转移以及人类数据扩展的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EgoHumanoid addresses the challenge of humanoid loco-manipulation by leveraging abundant egocentric human demonstrations and limited robot data. It introduces a systematic alignment pipeline for view and action alignment to bridge the embodiment gap. Extensive experiments show that EgoHumanoid outperforms robot-only baselines by 51% in unseen environments, highlighting the effectiveness of human demonstrations for this task.</div>
<div class="mono" style="margin-top:8px">EgoHumanoid通过利用丰富的第一人称人类演示和少量的机器人数据来解决人形机器人在移动和操作方面的挑战。它引入了一种系统对齐流水线，用于视图和动作对齐，以弥合人类与机器人的体态差异。广泛的实验表明，EgoHumanoid在未见过的环境中比仅使用机器人的基线高出51%，突显了人类演示对这一任务的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos</div>
<div class="meta-line">Authors: Juncheng Mu, Sizhe Yang, Yiming Bao, Hojin Bae, Tianming Wei, Linning Xu, Boyi Li, Huazhe Xu, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-02-10T18:59:02+00:00 · Latest: 2026-02-10T18:59:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10105v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10105v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DexImit：从单目人类视频学习双臂灵巧操作</div>
<div class="mono" style="margin-top:8px">数据稀缺从根本上限制了双臂灵巧操作的泛化能力，因为灵巧手的现实世界数据收集既昂贵又费时费力。人类操作视频作为操作知识的直接载体，为机器人学习的规模化提供了巨大潜力。然而，人类手与机器人灵巧手之间巨大的实体差距使得直接从人类视频进行预训练极其具有挑战性。为了弥合这一差距并释放大规模人类操作视频数据的潜力，我们提出了一种名为DexImit的自动化框架，该框架能够将单目人类操作视频转换为物理上合理的机器人数据，无需任何额外信息。DexImit采用四阶段生成流水线：（1）以近米尺度从任意视角重建手-物交互；（2）执行子任务分解和双臂调度；（3）合成与演示交互一致的机器人轨迹；（4）进行全面的数据增强以实现零样本现实世界部署。基于这些设计，DexImit可以根据互联网或视频生成模型的人类视频生成大规模的机器人数据。DexImit能够处理各种操作任务，包括工具使用（例如，切苹果）、长期任务（例如，制作饮料）和精细操作（例如，堆叠杯子）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of data scarcity in learning bimanual dexterous manipulation by proposing DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data. DexImit uses a four-stage pipeline to reconstruct hand-object interactions, perform subtask decomposition, synthesize robot trajectories, and augment the data for real-world deployment. The framework successfully generates large-scale robot data for various manipulation tasks, including tool use, long-horizon tasks, and fine-grained manipulations.</div>
<div class="mono" style="margin-top:8px">DexImit 是一个自动化框架，能够将单目人类操作视频转换为物理上合理的机器人数据，无需额外信息。它通过四个阶段的生成管道来解决双手灵巧操作中的数据稀缺问题：重建手-物体交互、执行子任务分解和双臂调度、合成机器人轨迹以及全面的数据增强。DexImit 可以生成用于各种操作任务的大规模机器人数据，包括使用工具、长期任务和精细操作。</div>
</details>
</div>
<div class="card">
<div class="title">Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction</div>
<div class="meta-line">Authors: Sizhe Yang, Linning Xu, Hao Li, Juncheng Mu, Jia Zeng, Dahua Lin, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-02-10T18:58:15+00:00 · Latest: 2026-02-10T18:58:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10101v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10101v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robo3R：通过精确的前馈3D重建提升机器人操作</div>
<div class="mono" style="margin-top:8px">三维空间感知是通用机器人操作的基础，但获得可靠且高质量的3D几何结构仍然具有挑战性。深度传感器受到噪声和材料敏感性的影响，而现有的重建模型缺乏进行物理交互所需的精度和度量一致性。我们引入了Robo3R，这是一种前馈、操作准备就绪的3D重建模型，能够直接从RGB图像和机器人状态实时预测准确的、度量级的场景几何结构。Robo3R 联合推断尺度不变的局部几何结构和相对相机姿态，并通过学习到的全局相似性变换统一到以机器人为中心的场景表示中。为了满足操作所需的精度要求，Robo3R 使用掩码点头生成锐利、细粒度的点云，并使用基于关键点的视角-n-点（PnP）公式来细化相机外参和全局对齐。Robo3R 在包含四百万高质量标注帧的Robo3R-4M这一精心策划的大规模合成数据集上进行训练，Robo3R 一致地优于最先进的重建方法和深度传感器。在包括模仿学习、模拟到现实的转移、抓取合成和无碰撞运动规划在内的下游任务中，我们观察到一致的性能提升，这表明这种替代的3D传感模块对于机器人操作具有潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Robo3R is a feed-forward 3D reconstruction model designed for robotic manipulation, which predicts accurate metric-scale scene geometry from RGB images and robot states in real time. It jointly infers local geometry and camera poses, and unifies them into a scene representation in the robot’s canonical frame. Robo3R outperforms state-of-the-art methods and depth sensors across various downstream tasks, including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, demonstrating its potential as an alternative 3D sensing module for robotic manipulation.</div>
<div class="mono" style="margin-top:8px">Robo3R 通过直接从 RGB 图像和机器人状态生成实时的准确 3D 重建来增强机器人操作。它联合推断局部几何和相机姿态，并将它们统一到机器人框架中的场景表示中。Robo3R 在精度和度量一致性方面优于现有方法和深度传感器，导致在下游任务如模仿学习、仿真到现实世界的转移、抓取合成和碰撞自由运动规划中表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model</div>
<div class="meta-line">Authors: Jingwen Sun, Wenyao Zhang, Zekun Qi, Shaojie Ren, Zezhi Liu, Hanxin Zhu, Guangzhong Sun, Xin Jin, Zhibo Chen</div>
<div class="meta-line">First: 2026-02-10T18:58:01+00:00 · Latest: 2026-02-10T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLA-JEPA：通过潜在世界模型增强视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">在互联网规模的视频上预训练视觉-语言-动作（VLA）策略是诱人的，但当前的潜在动作目标往往学到错误的东西：它们仍然锚定在像素变化上，而不是动作相关的状态转换，这使它们容易受到外观偏差、无关运动和信息泄露的影响。我们引入了VLA-JEPA，这是一种JEPA风格的预训练框架，通过设计避免了这些陷阱。关键思想是“无泄漏状态预测”：目标编码器从未来帧中生成潜在表示，而学生路径仅看到当前观察——未来信息仅用作监督目标，从不作为输入。通过在潜在空间而不是像素空间中预测，VLA-JEPA学会了对摄像机运动和无关背景变化具有鲁棒性的动力学抽象。这提供了一个简单的两阶段食谱——JEPA预训练后跟动作头微调——而无需先前潜在动作管道的多阶段复杂性。在LIBERO、LIBERO-Plus、SimplerEnv和真实世界的操作任务上的实验表明，VLA-JEPA在现有方法上实现了一致的泛化和鲁棒性提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve VLA policies by addressing the limitations of current latent-action objectives, which are prone to appearance bias and irrelevant motion. VLA-JEPA introduces a leakage-free state prediction framework, where the target encoder generates latent representations from future frames, while the student pathway only observes the current frame. This method enhances robustness and generalization, as demonstrated by experiments on LIBERO, LIBERO-Plus, SimplerEnv, and real-world manipulation tasks, showing consistent gains over existing methods.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决当前潜动作目标的局限性，如外观偏差和无关运动问题，来提升VLA策略。VLA-JEPA提出了一种无泄漏状态预测框架，其中目标编码器从未来帧生成潜空间表示，而学生路径仅观察当前帧。这种方法增强了鲁棒性和泛化能力，实验结果表明，在LIBERO、LIBERO-Plus、SimplerEnv和真实世界的操作任务上，VLA-JEPA相较于现有方法取得了持续的改进。</div>
</details>
</div>
<div class="card">
<div class="title">UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking</div>
<div class="meta-line">Authors: Baijun Chen, Weijie Wan, Tianxing Chen, Xianda Guo, Congsheng Xu, Yuanyang Qi, Haojie Zhang, Longyan Wu, Tianling Xu, Zixuan Li, Yizhe Wu, Rui Li, Xiaokang Yang, Ping Luo, Wei Sui, Yao Mu</div>
<div class="meta-line">First: 2026-02-10T18:57:00+00:00 · Latest: 2026-02-10T18:57:00+00:00</div>
<div class="meta-line">Comments: Website: https://univtac.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10093v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10093v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://univtac.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniVTAC：统一的视觉-触觉模拟平台，用于生成、学习和基准测试操纵数据</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）策略的快速发展推动了机器人操纵的进步。然而，对于接触丰富的操纵任务，如插入，仅依靠视觉难以实现稳健完成，触觉感知至关重要。同时，在物理世界中获取大规模可靠的触觉数据仍然成本高昂且具有挑战性，缺乏统一的评估平台进一步限制了策略学习和系统分析。为了解决这些挑战，我们提出了UniVTAC，这是一个基于模拟的视觉-触觉数据合成平台，支持三种常用的视觉-触觉传感器，能够实现可扩展且可控的生成具有信息性的接触交互。基于此平台，我们引入了UniVTAC编码器，这是一种在大规模模拟合成数据上训练的视觉-触觉编码器，带有设计的监督信号，为下游操纵任务提供以触觉为中心的视觉-触觉表示。此外，我们还提出了UniVTAC基准测试，其中包括八个代表性的视觉-触觉操纵任务，用于评估触觉驱动策略。实验结果表明，结合UniVTAC编码器在UniVTAC基准测试上的平均成功率提高了17.1%，而实际机器人实验进一步证明了任务成功率提高了25%。我们的网页地址为：https://univtac.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniVTAC is a simulation platform designed to generate, learn, and benchmark visuo-tactile manipulation data. It supports three tactile sensors and enables scalable and controllable generation of contact interactions. The platform includes a UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation data, which improves success rates by 17.1% on the UniVTAC Benchmark. Real-world experiments show a 25% improvement in task success. The UniVTAC Benchmark consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies.</div>
<div class="mono" style="margin-top:8px">UniVTAC是一个用于生成操纵任务中视觉-触觉数据的模拟平台，支持三种触觉传感器，并能够实现大规模和可控的数据生成。它包括一个在大规模模拟数据上训练的视觉-触觉编码器和一个由八个代表性触觉驱动的操纵任务组成的基准。实验结果显示，在UniVTAC基准上的成功率提高了17.1%，而在真实世界任务中的成功率提高了25%。</div>
</details>
</div>
<div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2026-02-10T18:32:44+00:00</div>
<div class="meta-line">Comments: ICLR 2026, Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17439v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从空间到行动：在空间基础先验中接地的视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-行动（VLA）模型在三维真实世界中行动，但通常基于二维编码器，这导致了空间推理的缺口，限制了泛化能力和适应性。最近的三维集成技术要么需要专门的传感器并且在不同模态间迁移效果差，要么注入弱线索，缺乏几何信息并降低视觉-语言对齐。在本工作中，我们引入了FALCON（从空间到行动），这是一种新颖的范式，将丰富的三维空间令牌注入到行动头中。FALCON 利用空间基础模型仅从RGB图像中提供强大的几何先验，并包含一个可选地融合深度或姿态的体感空间模型，以提高精度，而无需重新训练或架构更改。为了保持语言推理，空间令牌被空间增强的行动头消费而不是被连接到视觉-语言主干中。这些设计使FALCON能够解决空间表示、模态可迁移性和对齐的限制。在三个模拟基准和十一个真实世界任务的全面评估中，我们提出的FALCON达到了最先进的性能，始终超越了竞争性基线，并且在杂乱、空间提示条件、物体尺度和高度变化下保持了鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FALCON is designed to bridge the spatial reasoning gap in vision-language-action models by injecting rich 3D spatial tokens into the action head, leveraging spatial foundation models to provide strong geometric priors. It includes an Embodied Spatial Model that can optionally fuse depth or pose for higher fidelity. Comprehensive evaluations across various benchmarks show that FALCON outperforms existing methods, demonstrating superior performance and robustness under different conditions.</div>
<div class="mono" style="margin-top:8px">研究旨在通过将3D空间标记集成到动作头中，解决视觉-语言-动作模型的空间推理缺口。FALCON 利用空间基础模型从RGB输入中提供强大的几何先验，并可选地融合深度或姿态数据以提高精度。在各种基准测试中的评估表明，FALCON 的性能优于现有模型，并且在不同条件下保持了稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments</div>
<div class="meta-line">Authors: Dharmendra Sharma, Archit Sharma, John Reberio, Vaibhav Kesharwani, Peeyush Thakur, Narendra Kumar Dhar, Laxmidhar Behera</div>
<div class="meta-line">First: 2026-02-10T17:37:35+00:00 · Latest: 2026-02-10T17:37:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10015v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10015v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboSubtaskNet：在实际环境中的类人到机器人技能转移的时序子任务分割</div>
<div class="mono" style="margin-top:8px">在长的未剪辑视频中，准确地定位和分类细粒度的子任务片段对于安全的人机协作至关重要。与通用活动识别不同，协作操作需要可以直接由机器人执行的子任务标签。我们提出了RoboSubtaskNet，这是一种多阶段的类人到机器人子任务分割框架，该框架结合了注意力增强的I3D特征（RGB加上光流）与一个采用斐波那契扩张计划修改的MS-TCN，以捕捉更好的短期过渡，如伸手-拾取-放置。该网络通过一个复合目标进行训练，该目标包括交叉熵和时间正则化（截断均方误差和一个过渡感知项），以减少过度分割并鼓励有效的子任务进展。为了缩小视觉基准与控制之间的差距，我们引入了RoboSubtask数据集，该数据集包含医疗保健和工业演示，并在子任务级别进行了注释，旨在确定性地映射到操作器原语。实验中，RoboSubtaskNet在GTEA和我们的RoboSubtask基准（边界敏感和序列指标）上优于MS-TCN和MS-TCN++，在长时序的Breakfast基准上保持竞争力。具体而言，RoboSubtaskNet在GTEA上达到F1 @ 50 = 79.5%，Edit = 88.6%，Acc = 78.9%；在Breakfast上达到F1 @ 50 = 30.4%，Edit = 52.0%，Acc = 53.5%；在RoboSubtask上达到F1 @ 50 = 94.2%，Edit = 95.6%，Acc = 92.2%。我们进一步在7自由度Kinova Gen3操作器上验证了完整的感知到执行管道，在物理试验中实现了可靠的端到端行为（总体任务成功率约为91.25%）。这些结果表明，从子任务级别视频理解到实际环境中的部署机器人操作的实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a framework for segmenting fine-grained sub-tasks in long videos to enable safe human-robot collaboration. RoboSubtaskNet uses a multi-stage approach combining attention-enhanced I3D features with a modified MS-TCN to accurately segment sub-tasks. It is trained with a composite objective to reduce over-segmentation and encourage valid sub-task progressions. The framework outperforms MS-TCN and MS-TCN++ on GTEA and RoboSubtask benchmarks, and achieves reliable end-to-end behavior on a 7-DoF Kinova Gen3 manipulator in physical trials.</div>
<div class="mono" style="margin-top:8px">RoboSubtaskNet 是一个多阶段框架，用于在长视频中分割细粒度的子任务，对于安全的人机协作至关重要。它使用注意力增强的 I3D 特征结合修改后的 MS-TCN 来捕捉短期过渡。该网络通过复合目标进行训练，以减少过度分割并鼓励有效的子任务进展。RoboSubtaskNet 在 GTEA 和 RoboSubtask 基准测试中表现出色，实现了高 F1 分数和编辑距离。它还在 7-DoF Kinova Gen3 操作器的物理试验中展示了可靠的端到端行为。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper</div>
<div class="meta-line">Authors: Xuhui Kang, Tongxuan Tian, Sung-Wook Lee, Binghao Huang, Yunzhu Li, Yen-Ling Kuo</div>
<div class="meta-line">First: 2026-02-10T17:36:33+00:00 · Latest: 2026-02-10T17:36:33+00:00</div>
<div class="meta-line">Comments: 10 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10013v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://force-gripper.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用低成本触觉力控制夹爪进行力调节操作学习</div>
<div class="mono" style="margin-top:8px">成功操作许多日常物品，如薯片，需要精确的力调节。未能调节力可能导致任务失败或对物品造成不可逆的损坏。人类可以通过适应触觉反馈在短时间内精确实现这一点。我们旨在赋予机器人这种能力。然而，商用夹爪成本高或最小力高，使其不适合研究日常力敏感物体的力控制策略学习。我们引入了TF-夹爪，这是一种成本约为150美元的力控制并指夹爪，集成了触觉传感作为反馈。它的有效力范围为0.45-45N，并且兼容不同的机器人手臂。此外，我们设计了一种与TF-夹爪配对的远程操作设备，用于记录人类施加的抓取力。虽然标准低频策略可以在这些数据上进行训练，但它们难以应对力调节的反应性和接触依赖性。为了解决这个问题，我们提出了RETAF（反应性触觉力适应），这是一种将抓取力控制与臂姿态预测解耦的框架。RETAF使用手腕图像和触觉反馈以高频率调节力，而基础策略预测末端执行器姿态和夹爪开闭动作。我们在五项需要精确力调节的现实任务中评估了TF-夹爪和RETAF。结果表明，与位置控制相比，直接力控制显著提高了抓取稳定性和任务性能。我们进一步表明，触觉反馈对于力调节至关重要，RETAF始终优于基线，并且可以与各种基础策略集成。我们希望这项工作为在机器人操作中扩展力控制策略的学习开辟一条途径。项目页面：https://force-gripper.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable robots to precisely regulate force for manipulating everyday objects, which is crucial for tasks like handling potato chips. To address the limitations of commercial grippers, the authors developed TF-Gripper, a low-cost ($150) force-controlled gripper with tactile sensing. They also proposed RETAF, a framework that decouples force control from arm pose prediction, to handle the reactive nature of force regulation. Experimental results across five tasks demonstrated that direct force control and tactile feedback significantly improved grasp stability and task performance, with RETAF outperforming baseline methods.</div>
<div class="mono" style="margin-top:8px">研究旨在使机器人能够精确调节力以处理日常物品，这对于处理薯片等任务至关重要。为解决商用夹爪的局限性，作者开发了TF-Gripper，这是一种低成本的力控制并联夹爪，集成了触觉传感。他们还提出了RETAF框架，将力控制与臂姿态预测分离，允许高频率的力调节。在五个任务中的实验表明，直接力控制可以提高抓握稳定性和任务性能，触觉反馈对于有效的力调节至关重要。RETAF优于基线方法，并且可以与各种基爪策略集成。</div>
</details>
</div>
<div class="card">
<div class="title">Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity</div>
<div class="meta-line">Authors: Yunhai Han, Linhao Bai, Ziyu Xiao, Zhaodong Yang, Yogita Choudhary, Krishna Jha, Chuizheng Kong, Shreyas Kousik, Harish Ravichandar</div>
<div class="meta-line">First: 2026-02-07T07:18:00+00:00 · Latest: 2026-02-10T17:20:36+00:00</div>
<div class="meta-line">Comments: Codes can be found at https://github.com/GT-STAR-Lab/K-UBM/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07413v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07413v2">PDF</a> · <a href="https://github.com/GT-STAR-Lab/K-UBM/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There has been rapid and dramatic progress in learning complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it computes the desired robot behavior with the resulting flow of visual features over the entire skill horizon. To enable reactivity, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge. Across seven simulated and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of SOTA baselines, while offering faster inference, smooth execution, robustness to occlusions, and flexible replanning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随波逐流：Koopman行为模型作为隐式规划者用于视知觉-运动灵巧性</div>
<div class="mono" style="margin-top:8px">从演示中学习复杂的视知觉-运动操作技能方面取得了快速而显著的进步，部分归功于使用扩散和变换器为基础架构的表达性策略类。然而，这些设计选择需要大量的数据和计算资源，并且在多指灵巧操作的背景下远未可靠。从根本上说，它们将技能建模为反应性映射，并依赖固定时间窗的动作分块来减轻抖动，从而在时间连贯性和反应性之间形成刚性权衡。在本文中，我们引入了统一行为模型（UBMs），这是一种框架，用于学习将灵巧技能表示为耦合动力系统，这些系统捕捉环境视觉特征（视觉流）和机器人本体感受状态（动作流）的共同进化。通过捕捉这些行为动力学，UBMs可以通过构造确保时间连贯性，而不是通过启发式平均。为了实现这些模型的操作化，我们提出了Koopman-UBM，这是UBMs的第一个实例，它利用Koopman算子理论有效地学习一个统一的表示，在其中联合的视觉和本体感受特征流由结构化的线性系统支配。我们证明Koopman-UBM可以被视为一个隐式规划者：给定初始条件，它计算出期望的机器人行为，并计算整个技能时间窗内视觉特征的流动。为了实现反应性，我们引入了一种在线重规划策略，在这种策略中，模型充当自己的运行时监控器，当预测和观察到的视觉流发生分歧时自动触发重规划。在七个模拟任务和两个真实世界任务中，我们证明K-UBM的性能与SOTA基线相当或更优，同时提供更快的推理、平滑执行、对遮挡的鲁棒性和灵活的重规划。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of learning complex visuo-motor manipulation skills by introducing Unified Behavioral Models (UBMs) that represent dexterous skills as coupled dynamical systems. Koopman-UBM, a specific instantiation, uses Koopman Operator theory to learn a unified representation of visual and proprioceptive features. The model is shown to perform on par with state-of-the-art methods while offering faster inference, smoother execution, better robustness to occlusions, and flexible replanning capabilities. An online replanning strategy ensures reactivity by monitoring and adjusting the model&#x27;s predictions in real-time. Across various simulated and real-world tasks, Koopman-UBM demonstrates superior performance and reliability compared to existing approaches.</div>
<div class="mono" style="margin-top:8px">该研究通过引入统一行为模型（UBMs），将灵巧技能表示为耦合动力系统，以应对复杂视觉-运动操作技能的学习挑战。Koopman-UBM 特定实例利用Koopman算子理论学习视觉和 proprioceptive 特征的统一表示。该模型在多项模拟和实际任务中表现出色，与最先进的基线相当或超越，具有更快的推理速度、更平滑的执行和对遮挡的鲁棒性。通过自动触发重新规划来增强反应性，当预测和观察到的视觉流出现偏差时，模型会作为自己的运行时监控器进行重新规划。</div>
</details>
</div>
<div class="card">
<div class="title">RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation</div>
<div class="meta-line">Authors: Hao Li, Ziqin Wang, Zi-han Ding, Shuai Yang, Yilun Chen, Yang Tian, Xiaolin Hu, Tai Wang, Dahua Lin, Feng Zhao, Si Liu, Jiangmiao Pang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-10T17:01:54+00:00 · Latest: 2026-02-10T17:01:54+00:00</div>
<div class="meta-line">Comments: Published to ICLR 2026, 69 pages, 40 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09973v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboInter：面向机器人操作的综合中间表示套件</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）的进步激发了对视觉-语言-动作（VLA）系统在机器人操作中的兴趣。然而，现有的操作数据集在收集上成本高昂，高度具体化，覆盖范围和多样性不足，从而阻碍了VLA模型的泛化。最近的方法试图通过计划-执行范式来缓解这些限制，其中首先生成高层计划（例如子任务、轨迹），然后将其转换为低层动作，但它们严重依赖额外的中间监督，而现有数据集中这种监督几乎不存在。为了解决这一差距，我们引入了RoboInter操作套件，这是一个统一的资源，包括数据、基准和中间表示的模型。它包括RoboInter-Tool，一个轻量级的GUI，用于半自动标注多种表示，以及RoboInter-Data，一个包含超过23万集、跨越571个多样场景的大规模数据集，提供了超过10类中间表示的密集逐帧标注，规模和标注质量远超先前工作。在此基础上，RoboInter-VQA引入了9个空间和20个时间上的实体VQA类别，系统地评估和提升VLMs的实体推理能力。同时，RoboInter-VLA提供了一个集成的计划-执行框架，支持模块化和端到端的VLA变体，通过中间监督将高层规划与低层执行连接起来。总体而言，RoboInter为通过精细和多样化的中间表示推进稳健和可泛化的机器人学习奠定了实用基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces RoboInter, a comprehensive suite for robotic manipulation that addresses the limitations of existing datasets by providing a unified resource including data, benchmarks, and models of intermediate representations. It features RoboInter-Tool for semi-automatic annotation and RoboInter-Data, a large-scale dataset with over 230k episodes and dense per-frame annotations. RoboInter-VQA and RoboInter-VLA further enhance embodied reasoning and plan-then-execute frameworks, respectively, supporting modular and end-to-end VLA variants. The suite aims to advance robust and generalizable robotic learning through fine-grained and diverse intermediate representations.</div>
<div class="mono" style="margin-top:8px">该论文介绍了RoboInter，一个全面的机器人操作套件，包括中间表示的数据、基准和模型。它通过提供一个包含超过230k个场景的大型数据集和密集的每帧注释来解决现有数据集的局限性。RoboInter-VQA和RoboInter-VLA分别提供了新的体态VQA类别和集成的计划-执行框架，以系统地评估和增强视觉语言模型在机器人任务中的推理能力。主要发现包括创建了一个具有更大规模和注释质量的稳健数据集和框架，显著超越了先前的工作，促进了更通用的机器人学习。</div>
</details>
</div>
<div class="card">
<div class="title">Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation</div>
<div class="meta-line">Authors: Archit Sharma, Dharmendra Sharma, John Rebeiro, Peeyush Thakur, Narendra Dhar, Laxmidhar Behera</div>
<div class="meta-line">First: 2026-02-10T16:25:39+00:00 · Latest: 2026-02-10T16:25:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09940v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in &lt; 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Instruct2Act：从人类指令到动作序列和执行的机器人动作网络</div>
<div class="mono" style="margin-top:8px">机器人在现实世界环境中往往难以遵循自由形式的人类指令，这主要是由于计算能力和感知能力的限制。我们通过一种轻量级的全设备管道解决了这一问题，该管道将自然语言命令转换为可靠的操纵指令。我们的方法分为两个阶段：(i) 指令到动作模块（Instruct2Act），这是一个紧凑的双向长短期记忆网络（BiLSTM）与多头注意力自动编码器相结合，将指令解析为有序的原子动作序列（例如，接近、抓取、移动、放置）；(ii) 机器人动作网络（RAN），它使用动态自适应轨迹径向网络（DATRN）与基于视觉的环境分析器（YOLOv8）相结合，为每个子动作生成精确的控制轨迹。整个系统在一台普通的设备上运行，无需云服务。在我们自定义的专有数据集上，Instruct2Act 达到了 91.5% 的子动作预测准确率，同时保持了较小的体积。在四项任务（拾取放置、拾取倾倒、擦拭和拾取传递）的实地机器人评估中，总体成功率为 90%；子动作推理完成时间少于 3.8 秒，端到端执行时间为 30-60 秒，具体取决于任务的复杂性。这些结果表明，精细的指令到动作解析，结合基于 DATRN 的轨迹生成和基于视觉的定位，为资源受限、单摄像头环境下的确定性、实时操纵提供了一条实用的道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of robots following free-form human instructions by developing a lightweight on-device pipeline. It consists of an instruction-to-actions module (Instruct2Act) that converts natural-language commands into an ordered sequence of atomic actions using a compact BiLSTM with multi-head-attention autoencoder, and a robot action network (RAN) that generates precise control trajectories for each sub-action using DATRN and a vision-based environment analyzer. The system achieves 91.5% sub-action prediction accuracy and 90% success rate across four tasks, with sub-action inference completing in less than 3.8 seconds and end-to-end executions in 30-60 seconds depending on task complexity.</div>
<div class="mono" style="margin-top:8px">研究通过开发一个轻量级的端到端管道来解决机器人遵循自由形式的人类指令的问题。该管道包括一个指令到动作模块（Instruct2Act），使用紧凑的BiLSTM与多头注意力自编码器将自然语言命令转换为有序的原子动作序列，以及一个机器人动作网络（RAN），使用DATRN和基于视觉的环境分析器生成每个子动作的精确控制轨迹。该系统在四个任务（拾取放置、拾取倾倒、擦拭和拾取传递）上实现了91.5%的子动作预测准确率和90%的成功率，子动作推理在不到3.8秒内完成，端到端执行在30-60秒之间，具体取决于任务的复杂性。</div>
</details>
</div>
<div class="card">
<div class="title">TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback</div>
<div class="meta-line">Authors: Zihao Li, Yanan Zhou, Ranpeng Qiu, Hangyu Wu, Guoqiang Ren, Weiming Zhi</div>
<div class="meta-line">First: 2026-02-10T15:26:42+00:00 · Latest: 2026-02-10T15:26:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09888v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09888v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot&#x27;&#x27; the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TriPilot-FF：协调全身远程操作与力反馈</div>
<div class="mono" style="margin-top:8px">移动操作臂扩展了机器人操作的范围。然而，这种机器人的全身远程操作仍然是一个问题：操作员必须协调轮式底盘和两臂，同时考虑障碍物和接触。现有的界面主要以手为中心（例如，VR控制器和摇杆），而脚操作的连续底盘控制渠道则被忽视。我们提出了TriPilot-FF，这是一种开源的全身远程操作系统，用于一种定制的双臂移动操作臂，引入了一个脚踏板，结合了基于激光雷达的脚踏板触觉，以及上身双臂领导者-跟随者远程操作。仅使用低成本的底盘安装激光雷达，TriPilot-FF 从命令方向的接近障碍物信号中生成一个阻力脚踏板提示，通过形成操作员命令以避免碰撞的行为，而不使用显式的碰撞避免控制器。该系统还支持手臂侧力反馈，以提高接触意识，并提供实时双臂操作可操作性力和视觉指导，以提示移动底盘重新定位，从而提高可达性。我们展示了TriPilot-FF在长时间段和需要精确移动底盘和协调的任务中有效“协同操作”人类操作员的能力。最后，我们将远程操作反馈信号整合到Action Chunking with Transformers (ACT)策略中，并展示了当额外信息可用时性能的提升。我们发布了脚踏板设备设计、完整的软件堆栈，并在双臂轮式平台上进行了广泛的实地评估。TriPilot-FF的项目页面为http://bit.ly/46H3ZJT。</div>
</details>
</div>
<div class="card">
<div class="title">BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation</div>
<div class="meta-line">Authors: Yucheng Hu, Jianke Zhang, Yuanfei Luo, Yanjiang Guo, Xiaoyu Chen, Xinshu Sun, Kun Feng, Qingzhou Lu, Sheng Chen, Yangang Zhang, Wei Li, Jianyu Chen</div>
<div class="meta-line">First: 2026-02-10T14:54:01+00:00 · Latest: 2026-02-10T14:54:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09849v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09849v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BagelVLA：通过交错的视觉-语言-行动生成增强长时程操作</div>
<div class="mono" style="margin-top:8px">为使嵌入式代理具备关于任务推理、预见物理结果和生成精确行动的能力，对于通用操作来说是必不可少的。虽然最近的视觉-语言-行动（VLA）模型利用了预训练的基础模型，但它们通常侧重于语言规划或视觉预测中的一个方面。这些方法很少同时整合这两种能力来指导行动生成，导致在复杂的长时程操作任务中表现不佳。为了弥合这一差距，我们提出了一种名为BagelVLA的统一模型，该模型在单一框架中整合了语言规划、视觉预测和行动生成。BagelVLA从一个预训练的统一理解和生成模型初始化，并训练其直接将文本推理和视觉预测交织到行动执行循环中。为了高效地结合这些模态，我们引入了残差流引导（RFG），该方法从当前观察开始，并利用单步去噪来提取预测的视觉特征，以最小的延迟指导行动生成。广泛的实验表明，BagelVLA在多个模拟和真实世界基准测试中显著优于现有基线，特别是在需要多阶段推理的任务中表现尤为突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance long-horizon manipulation tasks by integrating linguistic planning, visual forecasting, and action generation in a unified model called BagelVLA. It leverages a pretrained unified understanding and generative model, and introduces Residual Flow Guidance (RFG) to efficiently couple these modalities during action execution. Experiments show that BagelVLA significantly outperforms existing methods on various simulated and real-world benchmarks, especially in tasks requiring multi-stage reasoning.</div>
<div class="mono" style="margin-top:8px">研究旨在通过将语言规划、视觉预测和动作生成统一在一个名为BagelVLA的模型中，来提升体态代理的长期操作任务。该模型从一个预训练的统一理解和生成模型初始化，并直接将文本推理和视觉预测耦合到动作执行循环中。主要实验发现是，BagelVLA在各种模拟和真实世界基准测试中显著优于现有基线，特别是在需要多阶段推理的任务中表现尤为突出。</div>
</details>
</div>
<div class="card">
<div class="title">Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation</div>
<div class="meta-line">Authors: Tim Engelbracht, René Zurbrügg, Matteo Wohlrapp, Martin Büchner, Abhinav Valada, Marc Pollefeys, Hermann Blum, Zuria Bauer</div>
<div class="meta-line">First: 2025-12-04T15:11:48+00:00 · Latest: 2026-02-10T14:42:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04884v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04884v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a dataset for force-grounded, cross-view articulated manipulation that couples what is seen with what is done and what is felt during real human interaction. The dataset contains 3048 sequences across 381 articulated objects in 38 environments. Each object is operated under four embodiments - (i) human hand, (ii) human hand with a wrist-mounted camera, (iii) handheld UMI gripper, and (iv) a custom Hoi! gripper - where the tool embodiment provides synchronized end-effector forces and tactile sensing. Our dataset offers a holistic view of interaction understanding from video, enabling researchers to evaluate how well methods transfer between human and robotic viewpoints, but also investigate underexplored modalities such as force sensing and prediction. Further information can be found on the Website.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>嗨！——一种基于力的跨视角 articulated 操作数据集</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于力的跨视角 articulated 操作数据集，将所见、所做和所感在真实的人机交互中结合起来。数据集包含3048个序列，涉及381个 articulated 物体在38个环境中的操作。每个物体在四种不同的身体形式下操作——(i) 人类手，(ii) 人类手配戴腕部摄像头，(iii) 手持UMI夹爪，和(iv) 一种自定义的 Hoi! 夹爪——其中工具形式提供了同步的末端执行器力和触觉传感。我们的数据集提供了从视频中理解交互的全面视角，使研究人员能够评估方法在人类和机器人视角之间的转移能力，同时也调查了未充分探索的模态，如力感知和预测。更多信息请参见网站。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a dataset for force-grounded, cross-view articulated manipulation that integrates visual, tactile, and force data during human interaction. The dataset includes 3048 sequences involving 381 articulated objects in 38 environments, with each object manipulated by four different embodiments: human hand, human hand with a wrist-mounted camera, handheld UMI gripper, and a custom Hoi! gripper. Key findings show that the dataset provides a comprehensive view of interaction understanding, allowing researchers to evaluate the transferability of methods between human and robotic viewpoints and to explore force sensing and prediction.</div>
<div class="mono" style="margin-top:8px">论文介绍了一个用于力导向、跨视角灵巧操作的数据集，该数据集整合了人类互动过程中的视觉、触觉和力数据。它包含3048个序列，涉及381个物体在38个环境中的操作，由四种不同的装置执行：人类手、带有摄像头的人类手、手持UMI夹爪和一个自定义的Hoi!夹爪。该数据集允许研究人员评估方法在人类和机器人视角之间的可转移性，并探索力感知和预测等未充分研究的模态。</div>
</details>
</div>
<div class="card">
<div class="title">Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning</div>
<div class="meta-line">Authors: Ruopeng Cui, Yifei Bi, Haojie Luo, Wei Li</div>
<div class="meta-line">First: 2026-02-10T13:28:13+00:00 · Latest: 2026-02-10T13:28:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09767v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09767v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>四足机器人通过无监督学习发现多样化技能</div>
<div class="mono" style="margin-top:8px">强化学习需要专家精心塑造奖励以激发目标行为，而模仿学习则依赖于昂贵的任务特定数据。相比之下，无监督技能发现有可能通过内在动机学习多样化的有用技能从而减轻这些负担。然而，现有方法存在两个关键局限：它们通常依赖单一策略掌握多样化的行为，而不建模它们之间的共享结构或区别，导致学习效率低下；此外，它们容易受到奖励作弊的影响，即奖励信号迅速增加并收敛，而学到的技能实际多样性不足。在本工作中，我们引入了一种正交混合专家（OMoE）架构，防止多样化行为坍缩为重叠的表示，使单一策略能够掌握广泛的运动技能。此外，我们设计了一个多判别器框架，其中不同的判别器在不同的观测空间上操作，有效缓解了奖励作弊。我们在12-DOF的Unitree A1四足机器人上评估了该方法，展示了多样化的运动技能。我们的实验表明，所提出的框架提高了训练效率，并且与基线相比，状态空间覆盖范围扩大了18.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the limitations of existing unsupervised skill discovery methods for quadruped robots by introducing an Orthogonal Mixture-of-Experts (OMoE) architecture and a multi-discriminator framework. The OMoE architecture prevents diverse behaviors from collapsing into overlapping representations, allowing a single policy to master a wide spectrum of locomotion skills. The multi-discriminator framework mitigates reward hacking by operating on distinct observation spaces. Experiments on the Unitree A1 quadruped robot show that this approach improves training efficiency and expands state-space coverage by 18.3% compared to the baseline.</div>
<div class="mono" style="margin-top:8px">该研究通过引入正交混合专家（OMoE）架构和多判别器框架，解决了现有四足机器人无监督技能发现方法的局限性。OMoE架构防止行为在表示上相互重叠，而多判别器框架有效缓解了奖励作弊问题。实验表明，该方法在12-DOF的Unitree A1四足机器人上展示了多样化的运动技能，提高了训练效率，并且相比基线方法在状态空间覆盖上增加了18.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion</div>
<div class="meta-line">Authors: Tianyang Wu, Hanwei Guo, Yuhang Wang, Junshu Yang, Xinyang Sui, Jiayi Xie, Xingyu Chen, Zeyang Liu, Xuguang Lan</div>
<div class="meta-line">First: 2026-01-31T11:50:16+00:00 · Latest: 2026-02-10T13:18:08+00:00</div>
<div class="meta-line">Comments: fix RoboGauge linear velocity metric, friction error; update radar fig (page 1), score tables (page 6,7); fix obs stack bug</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00678v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00678v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>朝基于MoE的稳健四足运动向可靠的仿真实际预测迈进</div>
<div class="mono" style="margin-top:8px">强化学习在四足敏捷运动方面显示出强大的潜力，即使仅使用本体感觉感知。然而，在实践中，仿真实际差距和复杂地形中的奖励过拟合可能导致无法转移的策略，而物理验证则存在风险且效率低下。为解决这些挑战，我们引入了一个统一框架，包括一个混合专家（MoE）运动策略，用于稳健的多地形表示，以及RoboGauge，这是一个预测评估套件，量化仿真实际转移性。MoE策略采用门控的专家集合来分解潜在地形和命令建模，通过本体感觉单独部署实现卓越的稳健性和泛化能力。RoboGauge进一步通过地形、难度级别和领域随机化进行的仿真实际测试，提供多维度的本体感觉基线度量，无需大量物理试验即可可靠地选择MoE策略。在Unitree Go2上的实验表明，该策略在未见过的挑战性地形上实现了稳健的运动，包括雪地、沙地、楼梯、斜坡和30厘米障碍物。在专门的高速测试中，机器人达到了4米/秒的速度，并表现出与高速稳定性相关的狭窄步态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenges of transferring quadrupedal locomotion policies from simulation to reality using a Mixture-of-Experts (MoE) policy and a predictive assessment suite called RoboGauge. The MoE policy uses a set of specialized experts to model latent terrain and commands, improving robustness and generalization with proprioception alone. RoboGauge evaluates the sim-to-real transferability through multi-dimensional metrics, reducing the need for extensive physical validation. Experiments on a Unitree Go2 robot show robust performance on various challenging terrains and high-speed locomotion capabilities.</div>
<div class="mono" style="margin-top:8px">本文旨在解决将用于四足行走的策略从仿真转移到现实世界中的挑战，特别是在复杂地形中。它引入了一个统一框架，包含一个混合专家（MoE）策略和一个预测评估套件RoboGauge。MoE策略使用一组专门的专家来处理潜在地形和命令建模，提高鲁棒性和泛化能力。RoboGauge通过多维度指标评估策略的仿真到现实世界的转移性，减少了大量物理试验的需要。实验表明，Unitree Go2机器人在各种具有挑战性的地形上表现出稳健的性能，最高可达4 m/s的速度，并且在高速时表现出更好的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving</div>
<div class="meta-line">Authors: Ziang Guo, Zufeng Zhang</div>
<div class="meta-line">First: 2025-10-17T09:02:18+00:00 · Latest: 2026-02-10T12:33:34+00:00</div>
<div class="meta-line">Comments: WIP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.15446v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.15446v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In autonomous driving, dynamic environment and corner cases pose significant challenges to the robustness of ego vehicle&#x27;s state understanding and decision making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving that explicitly models state-action mapping to address these challenges, enabling interpretable and robust decision making. By leveraging the advancement of the state understanding of the Vision Language Action Model (VLA) with generative diffusion policy-based action head, our VDRive guides the driving contextually and geometrically. Contextually, VLA predicts future observations through token generation pre-training, where the observations are represented as discrete codes by a Conditional Vector Quantized Variational Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning fine-tuning of the VLA to predict future trajectories and actions based on current driving conditions. VLA supplies the current state tokens and predicted state tokens for the action policy head to generate hierarchical actions and trajectories. During policy training, a learned critic evaluates the actions generated by the policy and provides gradient-based feedback, forming an actor-critic framework that enables a reinforcement-based policy learning pipeline. Experiments show that our VDRive achieves state-of-the-art performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VDRive：利用强化VLA和扩散策略实现端到端自动驾驶</div>
<div class="mono" style="margin-top:8px">在自动驾驶中，动态环境和边缘情况对自主车辆状态理解和决策的鲁棒性构成了重大挑战。我们提出了VDRive，一种新颖的端到端自动驾驶管道，明确建模状态-动作映射以应对这些挑战，从而实现可解释和鲁棒的决策。通过利用Vision Language Action模型（VLA）对状态理解的进展以及基于生成扩散策略的动作头，我们的VDRive在上下文和几何上引导驾驶。上下文中，VLA通过标记生成预训练来预测未来的观察结果，其中观察结果由条件向量量化变分自编码器（CVQ-VAE）表示为离散代码。几何上，我们对VLA进行强化学习微调，以根据当前驾驶条件预测未来轨迹和动作。VLA提供当前状态标记和预测状态标记给动作策略头以生成分层动作和轨迹。在策略训练期间，学习到的评论家评估由策略生成的动作并提供基于梯度的反馈，形成一个演员-评论家框架，使基于强化学习的策略学习管道成为可能。实验表明，我们的VDRive在Bench2Drive闭环基准和nuScenes开环规划中达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VDRive is an end-to-end autonomous driving pipeline that addresses the challenges of dynamic environments and corner cases by explicitly modeling state-action mapping. It uses a Vision Language Action (VLA) model for state understanding and a generative diffusion policy for action prediction. The VLA predicts future observations and trajectories through token generation and a Conditional Vector Quantized Variational Autoencoder (CVQ-VAE), while the diffusion policy guides actions based on current driving conditions. Experiments demonstrate that VDRive outperforms existing methods in both closed-loop and open-loop scenarios.</div>
<div class="mono" style="margin-top:8px">VDRive 通过结合 Vision Language Action 模型（VLA）和生成扩散策略，旨在解决自动驾驶中状态理解和决策的鲁棒性问题。它使用条件向量量化变分自编码器（CVQ-VAE）将观察表示为离散代码，并通过强化学习预测未来的轨迹和动作。实验表明，VDRive 在闭环和开环场景中均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization</div>
<div class="meta-line">Authors: Ye Wang, Sipeng Zheng, Hao Luo, Wanpeng Zhang, Haoqi Yuan, Chaoyi Xu, Haiweng Xu, Yicheng Feng, Mingyang Yu, Zhiyu Kang, Zongqing Lu, Qin Jin</div>
<div class="meta-line">First: 2026-02-10T12:25:43+00:00 · Latest: 2026-02-10T12:25:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09722v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09722v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard &quot;scale data&quot; recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重思视觉-语言-行动模型的扩展：对齐、混合和正则化</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-行动（VLA）模型在通用机器人控制方面显示出强大的前景，但尚不清楚——以及在什么条件下——标准的“扩展数据”配方是否适用于机器人领域，其中训练数据在不同机器人、传感器和行动空间之间本质上是异质的。我们进行了一项系统性的控制研究，重新审视了跨多种机器人预训练的核心训练选择。使用一个代表性的VLA框架，结合视觉-语言骨干和流匹配，我们在匹配条件下消减了关键设计决策，并在广泛的模拟和真实机器人实验中进行了评估。为了提高现实世界结果的可靠性，我们引入了一种分组盲人集成协议，使操作员无法识别模型身份，并将策略执行与结果判断分离，从而减少实验者偏见。我们的分析针对VLA扩展的三个维度。1）物理对齐：我们表明，统一的末端执行器（EEF）相对动作表示对于跨机器人种群的稳健转移至关重要。2）机器人种群混合：我们发现，简单地合并异质机器人数据集往往会导致负迁移而不是收益，突显了盲目数据扩展的脆弱性。3）训练正则化：我们观察到，直观的策略，如感觉下采样和多阶段微调，并不一致地在大规模下提高性能。这项研究挑战了一些关于嵌入式扩展的常见假设，并为从多样化的机器人数据训练大规模VLA策略提供了实用指导。项目网站：https://research.beingbeyond.com/rethink_vla</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the scaling of Vision-Language-Action (VLA) models for robotics, addressing the challenges of heterogeneous training data across different robots. By systematically evaluating key design choices and using a Grouped Blind Ensemble protocol to reduce experimenter bias, the research finds that a unified end-effector relative action representation is crucial for cross-embodiment transfer, naive pooling of heterogeneous datasets often leads to negative transfer, and common regularization strategies do not consistently improve performance at scale. These findings challenge common assumptions and provide practical guidance for training VLA policies from diverse robotic data.</div>
<div class="mono" style="margin-top:8px">该研究通过考察物理对齐、体态混合和训练正则化，探讨了Vision-Language-Action (VLA)模型在机器人领域的扩展。使用流匹配框架，研究者消融了关键设计选择，并进行了广泛的仿真和真实机器人实验。主要发现包括统一末端执行器动作表示对于跨体态转移的重要性、混合异构机器人数据集的脆弱性以及常见正则化策略在大规模下的不一致性。这项工作挑战了VLA扩展的一些常见假设，并为从多样化的机器人数据中训练大规模VLA策略提供了实用指导。</div>
</details>
</div>
<div class="card">
<div class="title">AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild</div>
<div class="meta-line">Authors: Xiaolou Sun, Wufei Si, Wenhui Ni, Yuntian Li, Dongming Wu, Fei Xie, Runwei Guan, He-Yang Xu, Henghui Ding, Yuan Wu, Yutao Yue, Yongming Huang, Hui Xiong</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-10T11:08:07+00:00 · Latest: 2026-02-10T11:08:07+00:00</div>
<div class="meta-line">Comments: Acceped by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09657v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09657v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoFly：无人机自主导航的视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">视觉-语言导航（VLN）要求智能代理通过解释语言指令并结合视觉观察来导航环境，是嵌入式人工智能中的核心任务。当前针对无人机的VLN研究依赖详细的预指定指令来引导无人机沿预定路线飞行。然而，现实世界中的户外探索通常发生在未知环境中，详细的导航指令不可用。相反，只能提供粗略的位置或方向指导，要求无人机通过持续规划和障碍物规避来自主导航。为解决这一问题，我们提出了AutoFly，一种端到端的视觉-语言-行动（VLA）模型，用于自主无人机导航。AutoFly结合了伪深度编码器，从RGB输入中提取深度感知特征以增强空间推理，并采用逐步两阶段训练策略，有效对齐视觉、深度和语言表示与行动策略。此外，现有的VLN数据集在真实世界自主导航方面存在根本局限，因为它们过度依赖明确的指令遵循而忽视了自主决策，并且缺乏真实世界数据。为解决这些问题，我们构建了一个新的自主导航数据集，通过：（1）强调持续障碍规避、自主规划和识别流程的轨迹收集；（2）全面整合真实世界数据，来从指令遵循转向自主行为建模。实验结果表明，AutoFly在模拟和真实环境中的一致性能中，比最先进的VLA基线高出3.9%的成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AutoFly is an end-to-end Vision-Language-Action model designed for autonomous UAV navigation in unknown environments. It uses a pseudo-depth encoder to improve spatial reasoning and a two-stage training strategy to align visual, depth, and linguistic representations with action policies. Experimental results show that AutoFly outperforms existing VLA baselines by 3.9% in success rate across both simulated and real environments.</div>
<div class="mono" style="margin-top:8px">AutoFly 是一种端到端的视觉-语言-动作模型，用于在未知环境中自主无人机导航。它使用伪深度编码器来提高空间推理能力，并采用两阶段训练策略来对齐视觉、深度和语言表示与动作策略。实验结果表明，AutoFly 在模拟和真实环境中均优于现有 VLA 基线，成功率高出 3.9%。</div>
</details>
</div>
<div class="card">
<div class="title">CoLA-Flow Policy: Temporally Coherent Imitation Learning via Continuous Latent Action Flow Matching for Robotic Manipulation</div>
<div class="meta-line">Authors: Wu Songwei, Jiang Zhiduo, Xie Guanghu, Sun Wandong, Liu Hong, Liu Yang</div>
<div class="meta-line">First: 2026-01-30T15:36:43+00:00 · Latest: 2026-02-10T10:16:20+00:00</div>
<div class="meta-line">Comments: 8 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23087v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.23087v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference latency, while flow matching enables fast one-step generation yet often leads to unstable execution when applied directly in the raw action space.
  We propose LG-Flow Policy, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space. By encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow, the proposed approach decouples global motion structure from low-level control noise, resulting in smooth and reliable long-horizon execution.
  LG-Flow Policy further incorporates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues evaluated as a representative modality in real-world settings. Experimental results in simulation and on physical robot platforms demonstrate that LG-Flow Policy achieves near single-step inference, substantially improves trajectory smoothness and task success over flow-based baselines operating in the raw action space, and remains significantly more efficient than diffusion-based policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoLA-Flow策略：通过连续潜在动作流匹配实现时间上一致的模仿学习以进行机器人操作</div>
<div class="mono" style="margin-top:8px">学习长时程机器人操作需要同时实现表达性行为建模、实时推理和稳定执行，这对现有的生成性策略来说仍然是具有挑战性的。基于扩散的方法提供了强大的建模能力，但通常会导致较高的推理延迟，而流匹配能够实现快速的一步生成，但在直接应用于原始动作空间时，往往会导致执行不稳定。
  我们提出了一种轨迹级的模仿学习框架LG-Flow策略，在连续的潜在动作空间中进行流匹配。通过将动作序列编码为时间上正则化的潜在轨迹，并学习一个显式的潜在空间流，该方法将全局运动结构与低级控制噪声解耦，从而实现平滑可靠的长时程执行。
  LG-Flow策略进一步结合了几何感知的点云条件和执行时的多模态调制，在实际场景中视觉线索被评估为一种代表性的模态。在模拟和物理机器人平台上的实验结果表明，LG-Flow策略实现了接近单步推理，显著提高了轨迹平滑度和任务成功率，并且在效率上远优于基于流的基线策略，后者在原始动作空间中操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenges of learning long-horizon robotic manipulation by proposing LG-Flow Policy, which performs flow matching in a continuous latent action space. This approach decouples global motion structure from low-level control noise, enabling smooth and reliable long-horizon execution. Key experimental results show that LG-Flow Policy achieves near single-step inference, improves trajectory smoothness and task success compared to flow-based baselines, and is more efficient than diffusion-based policies.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出LG-Flow Policy解决长期机器人操作的学习挑战，该方法在连续的潜在动作空间中进行流匹配，将动作序列编码为时空正则化的潜在轨迹，并学习显式的潜在空间流，从而将全局运动结构与低级控制噪声解耦。实验结果表明，LG-Flow Policy实现了接近单步推理，提高了轨迹平滑度和任务成功率，相比基于流的基线方法更为高效，且优于基于扩散的方法。</div>
</details>
</div>
<div class="card">
<div class="title">AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception</div>
<div class="meta-line">Authors: Ruoxuan Feng, Yuxuan Zhou, Siyu Mei, Dongzhan Zhou, Pengwei Wang, Shaowei Cui, Bin Fang, Guocai Yao, Di Hu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-10T10:05:53+00:00 · Latest: 2026-02-10T10:05:53+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09617v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09617v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTouch 2：通用光学触觉表示学习以实现动态触觉感知</div>
<div class="mono" style="margin-top:8px">现实世界中的接触丰富操作要求机器人感知时间触觉反馈，捕捉细微的表面变形，并推理物体属性以及力动态。尽管光学触觉传感器能够提供如此丰富的信息，但现有的触觉数据集和模型仍然有限。这些资源主要关注物体级别的属性（例如，材料），而很大程度上忽略了物理互动过程中精细的触觉时间动态。我们认为，推进动态触觉感知需要一种系统性的动态感知能力层次结构来指导数据收集和模型设计。为了解决缺乏富含动态信息的触觉数据，我们提出了ToucHD，这是一个大规模的分层触觉数据集，涵盖了触觉原子动作、真实世界的操作以及触觉-力配对数据。除了规模，ToucHD还建立了一个全面的触觉动态数据生态系统，从数据角度明确支持分层感知能力。在此基础上，我们提出了AnyTouch 2，这是一种通用的触觉表示学习框架，适用于各种光学触觉传感器，统一了物体级别的理解与细粒度、力感知的动态感知。该框架捕捉了帧间像素级和动作特定的变形，同时明确建模物理力动态，从而从模型角度学习多级动态感知能力。我们在涵盖静态物体属性和动态物理属性的基准测试以及跨越多个动态感知能力层级的真实世界操作任务上评估了我们的模型。实验结果表明，该模型在传感器和任务上表现出一致且强大的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the need for robots to perceive dynamic tactile feedback during manipulation tasks. It introduces ToucHD, a large hierarchical tactile dataset, and AnyTouch 2, a framework for learning tactile representations from optical tactile sensors. The framework captures both pixel-level and action-specific deformations, and explicitly models force dynamics, enabling multi-level dynamic perception. Experiments show consistent performance across different sensors and tasks, including static and dynamic attributes, and manipulation tasks requiring force-aware dexterity.</div>
<div class="mono" style="margin-top:8px">论文旨在解决机器人在执行操作任务时需要感知动态触觉反馈的需求。它引入了ToucHD，一个大型分层触觉数据集，以及AnyTouch 2，一种从光学触觉传感器学习触觉表示的框架。该框架捕捉像素级和动作特定的变形，并明确建模物理力动态，从而实现多级动态感知。实验结果显示，该模型在不同传感器和任务上表现一致，包括静态和动态属性，以及需要力感知灵巧操作的任务。</div>
</details>
</div>
<div class="card">
<div class="title">The hidden risks of temporal resampling in clinical reinforcement learning</div>
<div class="meta-line">Authors: Thomas Frost, Hrisheekesh Vaidya, Steve Harris</div>
<div class="meta-line">First: 2026-02-06T11:02:06+00:00 · Latest: 2026-02-10T09:51:38+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures. v2 fixes missing acknowledgements</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06603v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06603v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline reinforcement learning (ORL) has shown potential for improving decision-making in healthcare. However, contemporary research typically aggregates patient data into fixed time intervals, simplifying their mapping to standard ORL frameworks. The impact of these temporal manipulations on model safety and efficacy remains poorly understood. In this work, using both a gridworld navigation task and the UVA/Padova clinical diabetes simulator, we demonstrate that temporal resampling significantly degrades the performance of offline reinforcement learning algorithms during live deployment. We propose three mechanisms that drive this failure: (i) the generation of counterfactual trajectories, (ii) the distortion of temporal expectations, and (iii) the compounding of generalisation errors. Crucially, we find that standard off-policy evaluation metrics can fail to detect these drops in performance. Our findings reveal a fundamental risk in current healthcare ORL pipelines and emphasise the need for methods that explicitly handle the irregular timing of clinical decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>临床强化学习中时间重采样的隐含风险</div>
<div class="mono" style="margin-top:8px">离线强化学习（ORL）在改善医疗保健中的决策方面显示出潜力。然而，当前研究通常将患者数据聚合到固定的时间间隔中，简化了它们与标准ORL框架的映射。这些时间操作对模型安全性和有效性的影响仍然知之甚少。在本研究中，我们使用网格世界导航任务和UVA/帕多瓦临床糖尿病模拟器，证明了时间重采样在实际部署中显著降低了离线强化学习算法的性能。我们提出了三个导致这种失败的机制：(i) 生成反事实轨迹，(ii) 扭曲时间预期，(iii) 累积泛化误差。关键的是，我们发现标准的离策评估指标可能无法检测到这些性能下降。我们的研究揭示了当前医疗保健ORL管道中的一个基本风险，并强调了需要处理临床决策不规则时间的方法的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the risks associated with temporal resampling in offline reinforcement learning for healthcare applications. It uses both a gridworld navigation task and a clinical diabetes simulator to show that temporal resampling can significantly reduce the performance of reinforcement learning algorithms during live deployment. The study identifies three mechanisms contributing to this failure: the creation of counterfactual trajectories, distortion of temporal expectations, and compounding of generalization errors. Importantly, standard off-policy evaluation metrics may not detect these performance drops, highlighting a critical risk in current healthcare ORL pipelines.</div>
<div class="mono" style="margin-top:8px">该研究探讨了时间重采样在医疗保健应用中离线强化学习中的风险。通过使用网格世界导航任务和临床糖尿病模拟器，研究显示时间重采样在实际部署中会显著降低强化学习算法的性能。研究识别了三个导致这一失败的机制：生成反事实轨迹、时间预期的扭曲以及泛化误差的累积。重要的是，标准的离策评估指标可能无法检测到这些性能下降，突显了当前医疗保健ORL管道中的一个基本风险。</div>
</details>
</div>
<div class="card">
<div class="title">Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation</div>
<div class="meta-line">Authors: Marco Moletta, Michael C. Welle, Danica Kragic</div>
<div class="meta-line">First: 2026-02-10T09:35:22+00:00 · Latest: 2026-02-10T09:35:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09583v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09583v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向偏好的视觉-运动扩散策略在可变形物体操作中的应用</div>
<div class="mono" style="margin-top:8px">人类自然会发展出对如何执行操作任务的偏好，这些偏好往往是微妙的、个人化的且难以表达。虽然机器人需要考虑这些偏好以增加个性化和用户满意度，但在机器人操作中，特别是在处理如衣物和织物等可变形物体时，这些偏好仍然很大程度上未被探索。在本研究中，我们研究如何通过有限的演示来调整预训练的视觉-运动扩散策略以反映偏好行为。我们引入了RKO，这是一种新颖的偏好对齐方法，结合了两种最近框架的优点：RPO和KTO。我们在多种衣物和偏好设置下的真实世界布料折叠任务上，将RKO与常见的偏好学习框架，包括这两种框架，以及一个基线的纯扩散策略进行了评估。结果显示，偏好对齐策略（特别是RKO）在性能和采样效率方面优于标准的扩散策略微调。这些结果突显了结构化偏好学习在复杂可变形物体操作任务中实现个性化机器人行为的重要性与可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of incorporating human preferences into robotic manipulation of deformable objects, such as garments and fabrics. The authors propose RKO, a preference-alignment method that combines elements of RPO and KTO to adapt pretrained visuomotor diffusion policies. Evaluations on real-world cloth-folding tasks demonstrate that preference-aligned policies, especially RKO, outperform standard diffusion policy fine-tuning in terms of both performance and sample efficiency.</div>
<div class="mono" style="margin-top:8px">该研究探讨了如何将人类偏好融入对变形物体（如衣物和织物）的机器人操作中。作者提出了一种名为RKO的偏好对齐方法，该方法结合了RPO和KTO的优点来适应预训练的视觉-运动扩散策略。在实际布料折叠任务上的评估表明，偏好对齐策略，尤其是RKO，在性能和样本效率上都优于标准的扩散策略微调。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating the Likelihood Paradox in Flow-based OOD Detection via Entropy Manipulation</div>
<div class="meta-line">Authors: Donghwan Kim, Hyunsoo Yoon</div>
<div class="meta-line">First: 2026-02-10T09:31:03+00:00 · Latest: 2026-02-10T09:31:03+00:00</div>
<div class="meta-line">Comments: 28 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09581v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09581v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep generative models that can tractably compute input likelihoods, including normalizing flows, often assign unexpectedly high likelihoods to out-of-distribution (OOD) inputs. We mitigate this likelihood paradox by manipulating input entropy based on semantic similarity, applying stronger perturbations to inputs that are less similar to an in-distribution memory bank. We provide a theoretical analysis showing that entropy control increases the expected log-likelihood gap between in-distribution and OOD samples in favor of the in-distribution, and we explain why the procedure works without any additional training of the density model. We then evaluate our method against likelihood-based OOD detectors on standard benchmarks and find consistent AUROC improvements over baselines, supporting our explanation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过熵操控缓解流基OOD检测中的似然悖论</div>
<div class="mono" style="margin-top:8px">能够计算输入似然性的深度生成模型，包括归一化流，通常会对离分布（OOD）输入赋予意外高的似然值。我们通过基于语义相似性操控输入熵来缓解这一似然悖论，对与分布内记忆库相似度较低的输入施加更强的扰动。我们提供了一种理论分析，表明熵控制增加了分布内和OOD样本之间的期望对数似然值差距，有利于分布内样本，并解释了为何该过程无需额外训练密度模型即可生效。然后，我们在标准基准上将我们的方法与基于似然性的OOD检测器进行评估，发现与基线相比的一致性AUROC改进，支持了我们的解释。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of deep generative models assigning high likelihoods to out-of-distribution (OOD) inputs. The authors propose a method to manipulate input entropy based on semantic similarity, applying stronger perturbations to less similar inputs. Theoretical analysis shows that this approach increases the expected log-likelihood gap between in-distribution and OOD samples. Experiments on standard benchmarks demonstrate consistent improvements in AUROC over baseline methods, validating the effectiveness of the proposed technique.</div>
<div class="mono" style="margin-top:8px">论文解决了深度生成模型，尤其是归一化流，对离分布（OOD）输入赋予过高似然值的问题。为了解决这一问题，作者基于语义相似度操纵输入熵，对不相似的输入施加更强的扰动。理论分析表明，这种方法可以增加在分布和OOD样本之间的预期对数似然值差距，有利于在分布样本。实验结果在标准基准上显示，与基线方法相比，AUROC指标有持续的改进，验证了理论解释。</div>
</details>
</div>
<div class="card">
<div class="title">Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows</div>
<div class="meta-line">Authors: Chenyu Yang, Denis Tarasov, Davide Liconti, Hehui Zheng, Robert K. Katzschmann</div>
<div class="meta-line">First: 2026-02-10T09:28:20+00:00 · Latest: 2026-02-10T09:28:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09580v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09580v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy&#x27;s temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于动作分块评论家和归一化流的样本高效现实世界灵巧策略微调</div>
<div class="mono" style="margin-top:8px">由于有限的现实世界交互预算和高度多模态的动作分布，现实世界中的灵巧操作策略微调仍然具有挑战性。基于扩散的策略虽然具有表现力，但在微调过程中由于动作概率难以计算，无法进行保守的似然更新。相比之下，传统的高斯策略在多模态下会失效，尤其是在动作分块执行时，标准的逐步评论家无法与分块执行对齐，导致信用分配不佳。我们提出了SOFT-FLOW，这是一种基于样本高效离策略微调框架，使用归一化流（NF）来解决这些问题。归一化流策略为多模态动作分块提供了精确的似然性，通过似然正则化进行保守、稳定的策略更新，从而提高样本效率。动作分块评论家评估整个动作序列，与策略的时间结构对齐价值估计，改善长期信用分配。据我们所知，这是首次在真实机器人硬件上展示基于似然的、多模态生成策略与分块级价值学习的结合。我们在现实世界中的两个具有挑战性的灵巧操作任务上评估了SOFT-FLOW：用从箱子里取出的剪刀剪胶带，以及手掌向下握持的立方体内部旋转——这两个任务都需要长时间的精确、灵巧控制。在这些任务上，SOFT-FLOW实现了标准方法难以实现的稳定、样本高效适应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of fine-tuning dexterous manipulation policies in real-world settings with limited interaction data and multimodal action distributions. It introduces SOFT-FLOW, a framework that uses normalizing flows to provide exact likelihoods for multimodal action chunks, enabling conservative policy updates. An action-chunked critic evaluates entire sequences, improving credit assignment for long-horizon tasks. On two real-world dexterous manipulation tasks, SOFT-FLOW demonstrates stable and sample-efficient adaptation, outperforming standard methods.</div>
<div class="mono" style="margin-top:8px">研究解决了在有限交互数据和多模态动作分布下，现实世界中精细操作策略微调的挑战。提出了SOFT-FLOW框架，使用正则化流为多模态动作片段提供精确的似然性，并使用动作片段批评家使价值估计与策略的时间结构对齐。在需要精确长时间控制的任务上，实验证明SOFT-FLOW实现了稳定且样本高效的适应，优于标准方法。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization</div>
<div class="meta-line">Authors: Lucas Palazzolo, Mickaël Binois, Laëtitia Giraldi</div>
<div class="meta-line">First: 2026-02-10T09:14:32+00:00 · Latest: 2026-02-10T09:14:32+00:00</div>
<div class="meta-line">Comments: 16 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09563v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09563v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微游动器轨迹跟踪的最优控制方法研究：基于贝叶斯优化</div>
<div class="mono" style="margin-top:8px">在微机器人领域，微游动器的轨迹跟踪仍然是一个关键挑战，低雷诺数动力学使得控制设计特别复杂。本文将轨迹跟踪问题表述为最优控制问题，并通过贝叶斯优化与B-样条参数化相结合的方法解决，从而在不进行复杂梯度计算的情况下处理高计算成本。该方法应用于鞭毛磁性游动器，能够重现多种目标轨迹，包括实验研究中观察到的生物启发路径。进一步在三个球体游动器模型上评估该方法，证明其能够适应并部分补偿壁诱导的水动力效应。所提出的优化策略可以在不同保真度的模型上一致应用，从低维ODE模型到高保真度PDE模拟，展示了其稳健性和通用性。这些结果突显了贝叶斯优化作为复杂流体-结构相互作用下微尺度运动最优控制策略的多功能工具的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of trajectory tracking for microswimmers in microrobotics by formulating the problem as an optimal control issue and solving it using B-spline parametrization combined with Bayesian optimization. The method successfully tracks various target trajectories for a flagellated magnetic swimmer and adapts to hydrodynamic effects in a three-sphere swimmer model, demonstrating its robustness across different models of fidelity.</div>
<div class="mono" style="margin-top:8px">该研究通过将轨迹跟踪问题表述为最优控制问题，并结合B样条参数化和贝叶斯优化方法解决此问题，成功实现了对鞭毛磁泳器的多种目标轨迹跟踪，并在三球泳器模型中适应了流体动力效应，展示了其在不同模型上的鲁棒性和通用性，证明了贝叶斯优化作为微尺度运动中复杂流体-结构相互作用下最优控制策略的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence</div>
<div class="meta-line">Authors: Jinxian Zhou, Ruihai Wu, Yiwei Liu, Yiwen Hou, Xunzhe Zhou, Checheng Yu, Licheng Zhong, Lin Shao</div>
<div class="meta-line">First: 2026-02-09T09:30:23+00:00 · Latest: 2026-02-10T07:21:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08425v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08425v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://biadapt-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bi-Adapt: 少量样本的双臂适应方法以实现新型3D物体类别下的语义对应</div>
<div class="mono" style="margin-top:8px">双臂操作对于机器人执行复杂任务至关重要但极具挑战性，需要两臂之间的协调合作。然而，现有的双臂操作方法往往依赖于昂贵的数据收集和训练，难以高效地将新类别未见过的物体进行泛化。在本文中，我们提出了一种名为Bi-Adapt的新框架，该框架通过语义对应实现跨类别操作映射。Bi-Adapt利用视觉基础模型的强大能力进行微调，仅使用有限数据即可在零样本情况下表现出显著的泛化能力。在仿真和真实环境中的广泛实验验证了我们方法的有效性，并展示了其高效性，即使在有限数据的情况下也能在不同基准任务中实现较高的成功率。项目网站：https://biadapt-project.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve robots&#x27; ability to perform complex bimanual manipulation tasks by addressing the challenge of generalizing to unseen objects in novel categories. Bi-Adapt, a novel framework, uses semantic correspondence and fine-tuning with restricted data to achieve cross-category affordance mapping. The framework demonstrates notable zero-shot generalization and high efficiency, achieving a high success rate on various benchmark tasks with limited data in both simulation and real-world environments.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决机器人在执行复杂双臂操作任务时对未见过的新类别物体的泛化能力不足的问题，提高其能力。Bi-Adapt框架利用语义对应和有限数据的微调来实现跨类别操作映射。该框架在零样本情况下表现出显著的泛化能力和高效率，在模拟和真实环境中的多种基准任务中，使用少量数据实现了较高的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit State Estimation via Video Replanning</div>
<div class="meta-line">Authors: Po-Chen Ko, Jiayuan Mao, Yu-Hsiang Fu, Hsien-Jeng Yeh, Chu-Rong Chen, Wei-Chiu Ma, Yilun Du, Shao-Hua Sun</div>
<div class="meta-line">First: 2025-10-20T09:02:25+00:00 · Latest: 2026-02-10T06:46:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17315v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video-based representations have gained prominence in planning and decision-making due to their ability to encode rich spatiotemporal dynamics and geometric relationships. These representations enable flexible and generalizable solutions for complex tasks such as object manipulation and navigation. However, existing video planning frameworks often struggle to adapt to failures at interaction time due to their inability to reason about uncertainties in partially observed environments. To overcome these limitations, we introduce a novel framework that integrates interaction-time data into the planning process. Our approach updates model parameters online and filters out previously failed plans during generation. This enables implicit state estimation, allowing the system to adapt dynamically without explicitly modeling unknown state variables. We evaluate our framework through extensive experiments on a new simulated manipulation benchmark, demonstrating its ability to improve replanning performance and advance the field of video-based decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过视频重规划进行隐式状态估计</div>
<div class="mono" style="margin-top:8px">基于视频的表示在规划和决策中因其能够编码丰富的时空动态和几何关系而受到重视。这些表示使对象操作和导航等复杂任务的灵活和通用解决方案成为可能。然而，现有的视频规划框架往往难以在交互时适应失败，因为它们无法处理部分观测环境中的不确定性。为克服这些限制，我们提出了一种新的框架，将交互时的数据整合到规划过程中。我们的方法在线更新模型参数，并在生成时过滤掉之前失败的计划。这使得隐式状态估计成为可能，从而使系统能够动态适应而无需显式建模未知状态变量。我们通过在新的模拟操作基准上的广泛实验评估了我们的框架，证明了其改进重规划性能并推动基于视频的决策领域发展的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance video-based planning frameworks by integrating interaction-time data to improve adaptability in partially observed environments. The method updates model parameters online and filters out failed plans, enabling implicit state estimation. Key findings show improved replanning performance in a new simulated manipulation benchmark, addressing limitations in existing frameworks that struggle with uncertainties in partially observed environments.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决现有视频规划框架在交互时间失败时难以适应的问题来改进视频规划。方法引入了一个新的框架，该框架在线更新模型参数并过滤掉之前失败的计划，从而实现隐式状态估计。关键实验结果表明，这种方法在模拟操作基准测试中提高了重新规划性能，使系统能够在部分观测环境中更灵活地适应，而无需明确建模未知状态变量。</div>
</details>
</div>
<div class="card">
<div class="title">VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference</div>
<div class="meta-line">Authors: Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao</div>
<div class="meta-line">First: 2025-11-20T15:16:09+00:00 · Latest: 2026-02-10T05:44:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16449v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.16449v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA&#x27;s intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLA-Pruner: 时空感知的双层视觉标记剪枝方法以实现高效的视觉-语言-行动推理</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型在具身人工智能方面展现了巨大的潜力，然而处理连续视觉流的高昂计算成本严重限制了其实时部署。标记剪枝（保留显著的视觉标记并丢弃冗余的标记）已成为加速视觉-语言模型（VLMs）的有效方法，为高效的VLA提供了解决方案。然而，这些针对VLM的特定标记剪枝方法仅基于语义显著性指标（例如预填充注意）选择标记，而忽视了VLA固有的双系统本质，即高层次语义理解和低层次行动执行。因此，这些方法偏向于语义线索，丢弃了用于生成行动的关键信息，显著降低了VLA的性能。为解决这一问题，我们提出了一种VLA-Pruner，这是一种通用的即插即用VLA特定标记剪枝方法，与VLA模型的双系统本质相一致，并利用机器人操作中的时间连续性。具体而言，VLA-Pruner采用双层重要性标准来保留视觉标记：视觉-语言预填充注意用于语义层面的相关性，通过时间平滑估计的动作解码注意用于行动层面的重要性。基于此标准，VLA-Pruner提出了一种新颖的双层标记选择策略，在给定计算预算的情况下，自适应地保留一套紧凑且信息丰富的视觉标记，以支持语义理解和行动执行。实验表明，VLA-Pruner在多种VLA架构和多样化的机器人任务中均实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VLA-Pruner is a token pruning method designed for efficient Vision-Language-Action (VLA) models by considering both semantic and action aspects. It uses a dual-level importance criterion combining prefill attention and action decode attention to retain visually salient tokens, ensuring both semantic understanding and action execution. Experiments demonstrate that VLA-Pruner outperforms existing methods across various VLA architectures and robotic tasks, significantly reducing computational costs without compromising performance.</div>
<div class="mono" style="margin-top:8px">VLA-Pruner 是一种针对视觉-语言-行动 (VLA) 模型的剪枝方法，同时考虑语义和行动两个方面。它使用结合预填充注意力和动作解码注意力的双层重要性标准来保留视觉上显著的令牌，确保既能进行语义理解又能执行行动。实验表明，VLA-Pruner 在各种 VLA 架构和机器人任务中表现出色，显著降低了计算成本而不影响性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
