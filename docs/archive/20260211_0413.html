<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-11 04:13</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260211_0413</div>
    <div class="row"><div class="card">
<div class="title">TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation</div>
<div class="meta-line">Authors: Qinwen Xu, Jiaming Liu, Rui Zhou, Shaojun Shi, Nuowei Han, Zhuoyang Liu, Chenyang Gu, Shuo Gu, Yang Yue, Gao Huang, Wenzhao Zheng, Sirui Han, Peng Jia, Shanghang Zhang</div>
<div class="meta-line">First: 2026-02-09T18:59:52+00:00 · Latest: 2026-02-09T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09023v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TwinRL-VLA：数字孪生驱动的强化学习在现实世界机器人操作中的应用</div>
<div class="mono" style="margin-top:8px">尽管具有强大的泛化能力，视觉-语言-行动（VLA）模型仍然受限于专家演示的高成本和现实世界互动的不足。虽然在线强化学习（RL）在提升基础模型的一般性方面显示出潜力，但在现实世界环境中将RL应用于VLA操作仍然受到探索效率低和探索空间受限的阻碍。通过系统性的现实世界实验，我们观察到在线RL的有效探索空间与监督微调（SFT）的数据分布密切相关。受此观察的启发，我们提出了TwinRL，这是一种数字孪生与现实世界协作的RL框架，旨在扩展和引导VLA模型的探索。首先，从智能手机拍摄的场景中高效重建高保真数字孪生，使现实和模拟环境之间能够进行现实的双向转移。在SFT预热阶段，我们引入了使用数字孪生扩展探索空间的策略，以扩大数据轨迹分布的支持。在此增强的初始化基础上，我们提出了一个从模拟到现实的引导探索策略，以进一步加速在线RL。具体而言，TwinRL在部署前在数字孪生中高效并行执行在线RL，有效地弥合了离线和在线训练阶段之间的差距。随后，我们利用高效的数字孪生采样来识别易出故障但具有信息性的配置，这些配置用于指导现实机器人上的目标化人机交互式滚动。在我们的实验中，TwinRL在由现实世界演示覆盖的分布内区域和分布外区域均达到了100%的成功率，比之前的现实世界RL方法至少快30%，并且在四个任务上平均只需约20分钟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of Vision-Language-Action (VLA) models in real-world robotic manipulation, such as high cost of expert demonstrations and low exploration efficiency. It proposes TwinRL, a digital twin-real-world collaborative reinforcement learning framework. TwinRL reconstructs high-fidelity digital twins from smartphone-captured scenes, expanding the exploration space during supervised fine-tuning and accelerating online RL through sim-to-real guided exploration. The method achieves 100% success in both in-distribution and out-of-distribution regions and provides at least a 30% speedup over previous methods.</div>
<div class="mono" style="margin-top:8px">研究旨在提高Vision-Language-Action（VLA）模型在真实世界机器人操作中的在线强化学习（RL）探索效率。提出了一个数字孪生-真实世界协作RL框架TwinRL来解决这一问题。该框架从智能手机拍摄的场景中高效重建高保真数字孪生，以实现双向转移，并在监督微调阶段扩展探索空间。TwinRL随后在数字孪生中进行高效的在线RL，之后进行针对真实机器人的人机在环回放。实验表明，TwinRL在分布内和分布外区域均实现了100%的成功率，比之前的方法快30%，并且在四个任务上平均只需20分钟。</div>
</details>
</div>
<div class="card">
<div class="title">$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies</div>
<div class="meta-line">Authors: Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai, Hongyang Li, Huijie Wang, Jin Chen, Kaiyang Wu, Li Chen, Lirui Zhao, Modi Shi, Ping Luo, Qingwen Bu, Shijia Peng, Tianyu Li, Yibo Yuan</div>
<div class="meta-line">First: 2026-02-09T18:59:45+00:00 · Latest: 2026-02-09T18:59:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09021v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09021v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$χ_{0}$: 资源感知鲁棒操作通过驯服分布不一致性</div>
<div class="mono" style="margin-top:8px">高可靠性的长期机器人操作传统上依赖大规模数据和计算来理解复杂的现实世界动力学。然而，我们发现现实世界鲁棒性的主要瓶颈不仅在于资源规模，还在于人类演示分布、策略学习的归纳偏见和测试时执行分布之间的分布偏移——这是一种系统性不一致性，导致多阶段任务中的累积错误。为了缓解这些不一致性，我们提出了$χ_{0}$，一种资源高效框架，具有专门设计的有效模块，以实现机器人操作的生产级鲁棒性。我们的方法基于三个技术支柱：(i) 模型算术，一种权重空间合并策略，能够高效地吸收不同演示的多样化分布，从物体外观到状态变化；(ii) 阶段优势，一种阶段感知的优势估计器，提供稳定、密集的进步信号，克服了先前非阶段方法的数值不稳定性；(iii) 训练部署对齐，通过时空增强、启发式DAgger修正和时间片段平滑来弥合分布差距。$χ_{0}$ 使两台双臂机器人能够协作执行长期的服装操作，涵盖从平整、折叠到挂不同衣物的任务。我们的方法展示了高可靠性的自主性；我们能够从任意初始状态连续运行系统24小时不间断。实验验证了$χ_{0}$ 在成功率上比最先进的$π_{0.5}$ 高出近250%，仅使用20小时数据和8个A100 GPU。代码、数据和模型将被发布以促进社区的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of achieving high-reliability long-horizon robotic manipulation by focusing on distributional inconsistencies among human demonstrations, policy inductive biases, and test-time execution. It introduces $χ_{0}$, a resource-efficient framework that includes Model Arithmetic, Stage Advantage, and Train-Deploy Alignment to mitigate these inconsistencies. The system successfully enables dual-arm robots to collaboratively manipulate garments for 24 hours, and experimental results show a 250% improvement in success rate over $π_{0.5}$ with limited data and computational resources.</div>
<div class="mono" style="margin-top:8px">该论文旨在通过解决人类演示、策略归纳偏见和测试时执行之间的分布不一致问题，实现高可靠性的长期机器人操作。它提出了一个资源高效的框架$χ_{0}$，包括模型算术、阶段优势和训练部署对齐，以缓解这些不一致。该方法使双臂机器人能够执行复杂的衣物整理任务，具有高度的可靠性。实验表明，$χ_{0}$在成功率上比最先进的$π_{0.5}$高出250%，且仅需较少的数据和计算资源。</div>
</details>
</div>
<div class="card">
<div class="title">Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models</div>
<div class="meta-line">Authors: Zichen Jeff Cui, Omar Rayyan, Haritheja Etukuru, Bowen Tan, Zavier Andrianarivo, Zicheng Teng, Yihang Zhou, Krish Mehta, Nicholas Wojno, Kevin Yuanbo Wu, Manan H Anjaria, Ziyuan Wu, Manrong Mao, Guangxun Zhang, Binit Shah, Yejin Kim, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah</div>
<div class="meta-line">First: 2026-02-09T18:58:50+00:00 · Latest: 2026-02-09T18:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09017v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cap-policy.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>接触锚定策略：接触条件化创建强大的机器人效用模型</div>
<div class="mono" style="margin-top:8px">机器人学习的主流范式试图通过运行时的语言提示在不同环境、不同身体和不同任务之间进行泛化。这一方法存在一个根本性的矛盾：语言往往过于抽象，无法引导所需的具体物理理解以实现稳健的操作。在本研究中，我们引入了接触锚定策略（CAP），用空间中的物理接触点取代语言条件化。同时，我们将CAP结构化为模块化的效用模型库，而不是一个单一的通用策略。这种分解允许我们实现从现实到模拟的迭代循环：我们构建了EgoGym，一个轻量级的模拟基准，以快速识别失败模式并改进我们的模型和数据集，以便在实际部署之前进行优化。我们展示了通过接触条件化和模拟迭代，CAP在三个基本操作技能上无需额外数据即可泛化到新的环境和身体，并在零样本评估中比大型最先进的视觉语言模型高出56%。所有模型检查点、代码库、硬件、模拟和数据集都将开源。项目页面：https://cap-policy.github.io/</div>
</details>
</div>
<div class="card">
<div class="title">Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction</div>
<div class="meta-line">Authors: Hongyi Chen, Tony Dong, Tiancheng Wu, Liquan Wang, Yash Jangir, Yaru Niu, Yufei Ye, Homanga Bharadhwaj, Zackory Erickson, Jeffrey Ichnowski</div>
<div class="meta-line">First: 2026-02-09T18:56:02+00:00 · Latest: 2026-02-09T18:56:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.09013v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从RGB人体视频中通过4D手-物轨迹重建学习灵巧操作策略</div>
<div class="mono" style="margin-top:8px">多指机器人手操作和抓取由于高维动作空间和大规模训练数据获取的困难而具有挑战性。现有方法主要依赖穿戴设备或专用传感设备的人类远程操作来捕捉手-物交互，这限制了其可扩展性。在本工作中，我们提出了一种无需设备的框架VIDEOMANIP，可以直接从RGB人体视频中学习灵巧操作。利用计算机视觉的最新进展，VIDEOMANIP通过估计人体手部姿态、物体网格并重新定位重建的人体动作到机器人手中来重建显式的4D机器人-物体轨迹，从而进行操作学习。为了使重建的机器人数据适合灵巧操作训练，我们引入了基于交互的手-物接触优化和以抓取为中心的手部建模，以及一种从单个视频生成多样化训练轨迹的演示合成策略，从而在无需额外机器人演示的情况下实现泛化策略学习。在仿真中，使用Inspire手学习的抓取模型在20种不同物体上实现了70.25%的成功率。在现实世界中，从RGB视频训练的操作策略在使用LEAP手执行七个任务时实现了平均62.86%的成功率，优于基于重新定位的方法15.87%。项目视频可在videomanip.github.io获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of multi-finger robotic hand manipulation by proposing VIDEOMANIP, a device-free framework that learns dexterous manipulation policies directly from RGB human videos. It reconstructs 4D hand-object trajectories by estimating human hand poses and object meshes, and retargets these motions to robotic hands. The framework includes hand-object contact optimization and a demonstration synthesis strategy to generate diverse training trajectories. Experimental results show that the learned grasping model achieves a 70.25% success rate in simulation and an average 62.86% success rate in real-world tasks, outperforming retargeting-based methods.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种无需设备的框架VIDEOMANIP，从RGB人体视频中学习灵巧的操作策略。该框架通过估计人体手部姿态和物体网格来重建4D手-物体轨迹，并将这些轨迹重新映射到机器人手上。该框架包括手-物体接触优化和演示合成策略，以生成多样化的训练数据。实验结果显示，抓取模型在模拟中的成功率达到了70.25%，在现实世界任务中的平均成功率达到了62.86%，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure</div>
<div class="meta-line">Authors: Zirui Li, Xuefeng Bai, Kehai Chen, Yizhi Li, Jian Yang, Chenghua Lin, Min Zhang</div>
<div class="meta-line">First: 2026-02-09T15:25:12+00:00 · Latest: 2026-02-09T15:25:12+00:00</div>
<div class="meta-line">Comments: 22 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在隐性推理过程中的动态：因果结构的实证研究</div>
<div class="mono" style="margin-top:8px">潜在或连续的隐性推理方法用一系列内部潜在步骤替代显式的文本推理，但这些中间计算难以通过基于相关性的探针进行评估。本文将潜在隐性推理视为表示空间中的可操控因果过程，通过结构因果模型（SCM）建模潜在步骤，并通过逐步的$\mathrm{do}$-干预分析其影响。我们研究了两种代表性范式（即Coconut和CODI）在数学和一般推理任务中的表现，以探讨三个关键问题：（1）哪些步骤对于正确性是因果必要的，以及何时答案可以早期决定；（2）影响如何在步骤之间传播，这种结构与显式CoT有何不同；（3）中间轨迹是否保留竞争答案模式，以及输出级承诺与表示级承诺在步骤之间有何不同。我们发现潜在步骤预算的行为更像是分阶段的功能性而非均匀的额外深度，并且早期输出偏差与晚期表示承诺之间存在持续的差距。这些结果促使我们采用条件模式和稳定性意识分析——以及相应的训练/解码目标——作为更可靠的隐性推理系统解释和改进工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates latent chain-of-thought methods by modeling them as a causal process in representation space using structural causal models (SCM). The study examines Coconut and CODI on mathematical and general reasoning tasks to explore the necessity of causal steps, influence propagation, and the persistence of competing answer modes. Key findings include that latent-step budgets exhibit staged functionality with non-local routing, and there is a persistent gap between early output bias and late representational commitment, suggesting the need for mode-conditional and stability-aware analyses in improving latent reasoning systems.</div>
<div class="mono" style="margin-top:8px">该论文通过使用结构因果模型（SCM）将潜隐链推理视为表示空间中的因果过程来研究潜隐链推理方法。作者在数学和一般推理任务上研究了Coconut和CODI，以理解必要的因果步骤、影响传播和答案模式。主要发现包括：潜隐步骤预算表现出阶段功能而非均匀深度，并且早期输出偏差与晚期表征承诺之间存在持续差距，这表明需要条件模式和稳定性意识分析来解释和改进这些系统。</div>
</details>
</div>
<div class="card">
<div class="title">Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch</div>
<div class="meta-line">Authors: Cuijie Xu, Shurui Zheng, Zihao Su, Yuanfan Xu, Tinghao Yi, Xudong Zhang, Jian Wang, Yu Wang, Jinchen Yu</div>
<div class="meta-line">First: 2026-02-09T15:18:12+00:00 · Latest: 2026-02-09T15:18:12+00:00</div>
<div class="meta-line">Comments: 14 pages, 9 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08776v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xucj98.github.io/mind-the-gap-page/}{project">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot&#x27;s executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to &quot;Intent Cloning&quot; (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator&#x27;s strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a &quot;virtual equilibrium point&quot;, effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意差距：通过意图-执行差异学习视觉运动策略中的隐式阻抗</div>
<div class="mono" style="margin-top:8px">远程操作本质上依赖于人类操作者作为闭环控制器主动补偿硬件缺陷，包括延迟、机械摩擦和缺乏明确的力反馈。标准行为克隆（BC）通过模仿机器人执行的轨迹，根本上忽略了这种补偿机制。在本工作中，我们提出了一种双状态条件框架，将学习目标转变为“意图克隆”（主命令）。我们认为，意图-执行差异，即主命令与从命令之间的差异，不是噪声，而是物理上编码隐式交互力的关键信号，并且算法上揭示了操作者克服系统动力学的策略。通过预测主意图，我们的策略学会生成一个“虚拟平衡点”，从而实现隐式阻抗控制。此外，通过明确地基于这种差异的历史进行条件处理，模型执行隐式系统识别，将跟踪误差视为外部力以关闭控制回路。为了弥合由于推理延迟造成的时序差距，我们进一步将策略形式化为轨迹修复者，以确保连续控制。我们在一个无传感器、低成本的双臂设置上验证了我们的方法。在涉及接触丰富操作和动态跟踪的任务中，实证结果显示出明显的差距：标准执行克隆由于无法克服接触刚度和跟踪滞后而失败，而我们的差异感知方法实现了稳健的成功。这为低成本硬件提供了一个简约的行为克隆框架，无需依赖显式力感知即可实现力感知和动态补偿。有关视频请参见项目页面：https://xucj98.github.io/mind-the-gap-page/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of teleoperation by proposing a Dual-State Conditioning framework that focuses on predicting the master command (intent cloning) rather than directly mimicking the robot&#x27;s executed trajectory. The method leverages the Intent-Execution Mismatch to learn implicit impedance control and perform system identification. Experiments on a sensorless, low-cost bi-manual setup show that the proposed approach outperforms standard execution-cloning methods in tasks requiring contact-rich manipulation and dynamic tracking, demonstrating robust success even in the presence of contact stiffness and tracking lag.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种双状态条件框架，专注于“意图克隆”来学习隐式阻抗控制。方法利用意图-执行差异来推断操作者的策略，并生成一个“虚拟平衡点”。在低成本无传感器的双手动设置下进行的实验表明，所提出的方法在需要接触丰富操作和动态跟踪的任务中优于标准的执行克隆方法，即使在存在接触刚度和跟踪滞后的情况下也能实现稳健的成功。</div>
</details>
</div>
<div class="card">
<div class="title">SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning</div>
<div class="meta-line">Authors: Hanzhen Wang, Jiaming Xu, Yushun Xiang, Jiayi Pan, Yongkang Zhou, Yong-Lu Li, Guohao Dai</div>
<div class="meta-line">First: 2025-09-06T06:22:19+00:00 · Latest: 2026-02-09T13:23:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05614v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.05614v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pruning is a typical acceleration technique for compute-bound models by removing computation on unimportant values. Recently, it has been applied to accelerate Vision-Language-Action (VLA) model inference. However, existing acceleration methods focus on local information from the current action step and ignore the global context, leading to &gt;20% success rate drop and limited speedup in some scenarios. In this paper, we point out spatial-temporal consistency in VLA tasks: input images in consecutive steps exhibit high similarity, and propose the key insight that token selection should combine local information with global context of the model. Based on this, we propose SpecPrune-VLA, a training-free, two-level pruning method with heuristic control. (1) Action-level static pruning. We leverage global history and local attention to statically reduce visual tokens per action. (2) Layer-level dynamic pruning. We prune tokens adaptively per layer based on layer-wise importance. (3) Lightweight action-aware controller: We classify actions as coarse- or fine-grained by the speed of the end effector and adjust pruning aggressiveness accordingly. Extensive experiments show that SpecPrune-VLA achieves up to 1.57$\times$ speedup in LIBERO simulation and 1.70$\times$ on real-world tasks, with negligible success rate degradation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpecPrune-VLA: 通过动作感知自推测剪枝加速视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">剪枝是一种通过移除不重要计算来加速计算密集型模型的典型技术。最近，它被应用于加速视觉-语言-动作（VLA）模型推理。然而，现有的加速方法仅关注当前动作步骤的局部信息，而忽略了全局上下文，导致成功率下降超过20%，并在某些场景中限制了加速效果。在本文中，我们指出了VLA任务中的时空一致性：连续步骤中的输入图像表现出高度相似性，并提出关键见解，即token选择应结合局部信息和模型的全局上下文。基于此，我们提出了SpecPrune-VLA，这是一种无需训练、具有启发式控制的两级剪枝方法。（1）动作级静态剪枝。我们利用全局历史和局部注意力静态减少每个动作的视觉token。（2）层级动态剪枝。我们基于每层的重要性自适应地剪枝token。（3）轻量级动作感知控制器：我们通过末端执行器的速度将动作分类为粗粒度或细粒度，并相应地调整剪枝的激进程度。广泛的实验表明，SpecPrune-VLA 在LIBERO仿真中实现了高达1.57倍的加速，在实际任务中实现了1.70倍的加速，且成功率下降可以忽略不计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of existing pruning methods for accelerating Vision-Language-Action (VLA) models by focusing solely on local information. It introduces SpecPrune-VLA, a two-level pruning method that incorporates both local and global context. The method includes action-level static pruning using global history and local attention, layer-level dynamic pruning based on layer-wise importance, and an action-aware controller that adjusts pruning aggressiveness. Experiments demonstrate that SpecPrune-VLA achieves significant speedups of up to 1.70 times without degrading the success rate significantly.</div>
<div class="mono" style="margin-top:8px">本文针对现有用于加速视觉-语言-动作（VLA）模型的剪枝方法仅关注局部信息的局限性，提出了一种结合局部和全局上下文的两级剪枝方法SpecPrune-VLA。该方法包括基于全局历史和局部注意力的动作级静态剪枝、基于层级重要性的层级动态剪枝以及根据末端执行器速度粗细调整剪枝强度的轻量级动作感知控制器。实验结果显示，该方法可实现最高1.70倍的加速，同时对成功率影响较小。</div>
</details>
</div>
<div class="card">
<div class="title">SafeLink: Safety-Critical Control Under Dynamic and Irregular Unsafe Regions</div>
<div class="meta-line">Authors: Songqiao Hu, Zidong Wang, Zeyi Liu, Zhen Shen, Xiao He</div>
<div class="meta-line">First: 2025-03-19T14:16:37+00:00 · Latest: 2026-02-09T13:17:56+00:00</div>
<div class="meta-line">Comments: 12 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16551v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.16551v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Control barrier functions (CBFs) provide a theoretical foundation for safety-critical control in robotic systems. However, most existing methods rely on explicit analytical expressions of unsafe state regions, which are often impractical for irregular and dynamic unsafe regions. This paper introduces SafeLink, a novel CBF construction method based on cost-sensitive incremental random vector functional-link (RVFL) neural networks. By designing a valid cost function, SafeLink assigns different sensitivities to safe and unsafe state points, thereby eliminating false negatives in classification of unsafe state points. Under the constructed CBF, theoretical guarantees are established regarding system safety and the Lipschitz continuity of the control inputs. Furthermore, incremental update theorems are provided, enabling precise real-time adaptation to changes in unsafe regions. An analytical expression for the gradient of SafeLink is also derived to facilitate control input computation. The proposed method is validated on the endpoint position control task of a nonlinear two-link manipulator. Experimental results demonstrate that the method effectively learns the unsafe regions and rapidly adapts as these regions change, achieving computational speeds significantly faster than baseline methods while ensuring the system safely reaches its target position.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeLink：在动态和不规则的不安全区域下的安全控制</div>
<div class="mono" style="margin-top:8px">控制屏障函数（CBFs）为机器人系统中的安全关键控制提供了理论基础。然而，大多数现有方法依赖于不安全状态区域的显式解析表达式，这对于不规则和动态的不安全区域往往是不切实际的。本文介绍了一种基于成本敏感增量随机向量函数链接（RVFL）神经网络的新颖CBF构建方法SafeLink。通过设计有效的成本函数，SafeLink 对安全和不安全状态点赋予不同的敏感度，从而在分类不安全状态点时消除假阴性。在构建的CBF下，建立了关于系统安全性和控制输入Lipschitz连续性的理论保证。此外，还提供了增量更新定理，使SafeLink 能够精确实时地适应不安全区域的变化。还推导了SafeLink 的梯度解析表达式，以方便控制输入的计算。所提出的方法在非线性双连杆末端位置控制任务上进行了验证。实验结果表明，该方法有效地学习了不安全区域，并能够快速适应这些区域的变化，计算速度显著快于基线方法，同时确保系统安全地到达目标位置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SafeLink is a novel method for constructing control barrier functions (CBFs) to ensure safety in robotic systems, especially when dealing with irregular and dynamic unsafe regions. It uses cost-sensitive incremental random vector functional-link neural networks to assign different sensitivities to safe and unsafe state points, improving the accuracy of unsafe state point classification. The method provides theoretical guarantees for system safety and the continuity of control inputs, and allows for real-time adaptation to changes in unsafe regions. Experiments on a nonlinear two-link manipulator show that SafeLink effectively learns and adapts to unsafe regions, achieving faster computational speeds than baseline methods while maintaining safety.</div>
<div class="mono" style="margin-top:8px">SafeLink 是一种新型的控制屏障函数（CBF）构建方法，解决了现有方法在处理不规则和动态的不安全区域时的局限性。它使用成本敏感的增量随机向量功能链接神经网络，对安全和不安全状态点赋予不同的敏感度，确保准确分类和安全保证。该方法提供了实时适应的增量更新定理，并推导了梯度的解析表达式以实现高效的控制输入计算。实验结果表明，SafeLink 能够有效学习和适应变化的不安全区域，计算速度显著快于基线方法，同时确保系统安全地到达目标位置。</div>
</details>
</div>
<div class="card">
<div class="title">Mimic Intent, Not Just Trajectories</div>
<div class="meta-line">Authors: Renming Huang, Chendong Zeng, Wenjing Tang, Jingtian Cai, Cewu Lu, Panpan Cai</div>
<div class="meta-line">First: 2026-02-09T12:44:35+00:00 · Latest: 2026-02-09T12:44:35+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08602v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories&#x27;&#x27; (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模仿意图，而不仅仅是轨迹</div>
<div class="mono" style="margin-top:8px">尽管生成建模和预训练在模仿学习（IL）中已经通过灵巧操作取得了显著成功，但最先进的方法如视觉-语言-动作（VLA）模型仍然难以适应环境变化和技能转移。我们认为这源于模仿原始轨迹而未能理解其背后的意图。为了解决这一问题，我们提出在端到端的IL中明确分离行为意图和执行细节：&quot;模仿意图，而不仅仅是轨迹&quot;（MINT）。我们通过多尺度频域分词实现这一目标，这强制执行了动作片段表示的频谱分解。我们学习具有多尺度粗细结构的动作令牌，并强制最粗的令牌捕捉低频全局结构，而更细的令牌编码高频细节。这产生了一个抽象的&quot;意图令牌&quot;，有助于规划和转移，以及多尺度&quot;执行令牌&quot;，使其能够精确适应环境动力学。基于这一层次结构，我们的策略通过&quot;逐级自回归&quot;生成轨迹，执行逐步的&quot;意图到执行推理&quot;，从而提高学习效率和泛化能力。关键的是，这种分离使得技能可以&quot;单次转移&quot;，只需将演示中的意图令牌注入自回归生成过程即可。在几个操作基准测试和真实机器人上的实验表明，该方法具有最先进的成功率、优越的推理效率、对干扰的鲁棒泛化能力以及有效的单次转移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of current imitation learning methods in adapting to environmental changes and transferring skills by focusing solely on trajectories. It proposes MINT, which explicitly disentangles behavior intent from execution details using multi-scale frequency-space tokenization. This approach enables the generation of abstract intent tokens and detailed execution tokens, facilitating efficient learning and robust generalization. Experiments show that MINT achieves state-of-the-art success rates, superior inference efficiency, and effective one-shot skill transfer.</div>
<div class="mono" style="margin-top:8px">研究旨在通过区分行为意图和执行细节来改进模仿学习（IL）以实现灵巧操作。方法MINT使用多尺度频域分词来分解动作表示为意图和执行令牌，从而实现高效的学习和转移。关键发现包括最先进的成功率、鲁棒的泛化能力和有效的单次转移技能。</div>
</details>
</div>
<div class="card">
<div class="title">A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation</div>
<div class="meta-line">Authors: Kenghou Hoi, Yuze Wu, Annan Ding, Junjie Wang, Anke Zhao, Chengqian Zhang, Fei Gao</div>
<div class="meta-line">First: 2026-02-09T12:40:34+00:00 · Latest: 2026-02-09T12:40:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08599v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08599v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种精确的实时力感知抓取系统以实现稳健的空中操作</div>
<div class="mono" style="margin-top:8px">空中操作需要力感知能力以实现安全有效的抓取和物理交互。以往的工作往往依赖于沉重且昂贵的力传感器，这些传感器不适合典型的四旋翼平台，或者在没有力反馈的情况下进行抓取，这可能会损坏脆弱的物体。为了解决这些限制，我们提出了一种新的力感知抓取框架，结合了六个低成本的敏感皮肤式触觉传感器。我们引入了一种基于磁性的触觉传感模块，提供了高精度的三维力测量。我们通过参考霍尔传感器消除了地磁干扰，并简化了与以往工作相比的校准过程。所提出的框架使精确的力感知抓取控制成为可能，允许安全地操作脆弱的物体，并实时测量抓取物品的重量。该系统通过全面的实地实验得到了验证，包括气球抓取、动态负载变化测试和消融研究，展示了其在各种空中操作场景中的有效性。我们的方法实现了完全的机载操作，无需外部运动捕捉系统，显著增强了力敏感空中操作的实用性。补充视频可在以下链接中查看：https://www.youtube.com/watch?v=mbcZkrJEf1I.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a precise real-time force-aware grasping system for aerial manipulation, addressing the limitations of previous heavy and expensive force sensors. The method involves using six low-cost tactile sensors and a magnetic-based tactile sensing module for high-precision force measurements. Key experimental findings include successful grasping of fragile objects, real-time weight measurement, and effective handling under dynamic load variations, validating the system&#x27;s practicality in various aerial manipulation scenarios without external motion capture systems.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种精确的实时力感知抓取系统，以解决以往重型和昂贵力传感器的局限性。方法包括使用六个低成本触觉传感器和基于磁性的触觉传感模块进行高精度力测量。关键实验发现包括成功抓取脆弱物体、实时重量测量以及在动态载荷变化下的有效处理，验证了该系统在各种空中操作场景中的实用性，无需外部运动捕捉系统。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Sampling to Guide Universal Manipulation RL</div>
<div class="meta-line">Authors: Marc Toussaint, Cornelius V. Braun, Eckart Cobo-Briesewitz, Sayantan Auddy, Armand Jordana, Justin Carpentier</div>
<div class="meta-line">First: 2026-02-09T11:54:45+00:00 · Latest: 2026-02-09T11:54:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08557v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08557v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>受限采样以引导通用操作RL</div>
<div class="mono" style="margin-top:8px">我们考虑如何利用基于模型的求解器来引导训练一个通用策略，以从任何可行的起始状态控制到任何可行的目标状态，在接触丰富的操作环境中。尽管强化学习（RL）在这样的环境中展示了其优势，但在稀疏奖励设置中，它可能难以充分探索和发现复杂的操作策略。我们的方法基于这样一个想法：在这样的操作过程中存在一个低维流形，包含所有可行且可能访问的状态，并使用来自该流形的采样器来引导RL。我们提出了采样引导的RL，该方法使用基于模型的约束求解器高效地采样满足不同碰撞、接触和力约束的可行配置，并利用这些配置来引导RL以生成通用（目标条件）操作策略。我们研究了直接使用这些数据来偏置状态访问，以及使用黑盒优化开放环轨迹之间的随机配置来施加状态偏置，并可选地添加行为克隆损失。在最小化的双球操作设置中，采样引导的RL发现了复杂的操作策略，并在达到任何静态稳定状态方面取得了高成功率。在更具挑战性的熊猫臂设置中，我们的方法在接近零的基线下实现了显著的成功率，并展示了广泛的复杂全身接触操作策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to leverage model-based solvers to guide the training of a universal policy for manipulation tasks, especially in contact-rich environments where RL may struggle with exploration. The method, Sample-Guided RL, uses a sampler to efficiently generate feasible configurations that satisfy constraints and guides RL to discover complex manipulation strategies. In experiments, the approach successfully discovers complex strategies and achieves high success rates in a double sphere setting and a significant success rate in a panda arm setting, demonstrating a range of whole-body-contact manipulation strategies.</div>
<div class="mono" style="margin-top:8px">研究旨在利用基于模型的求解器来引导通用策略的训练，以应对接触丰富的操作任务。方法Sample-Guided RL使用采样器高效生成满足多种约束的可行配置，并引导RL探索和发现复杂的操作策略。实验结果显示，该方法在双球设置中成功发现了复杂的策略，并实现了高成功率；在更具挑战性的熊猫臂设置中，该方法也展示了广泛的复杂全身接触操作策略，并取得了显著的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots</div>
<div class="meta-line">Authors: Mingqi Yuan, Tao Yu, Wenqi Ge, Xiuyong Yao, Huijiang Wang, Jiayu Chen, Bo Li, Wei Zhang, Wenjun Zeng, Hua Chen, Xin Jin</div>
<div class="meta-line">Venue: IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2025</div>
<div class="meta-line">First: 2025-06-25T14:35:33+00:00 · Latest: 2026-02-09T10:08:57+00:00</div>
<div class="meta-line">Comments: 19 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20487v5">Abs</a> · <a href="https://arxiv.org/pdf/2506.20487v5">PDF</a> · <a href="https://github.com/yuanmingqi/awesome-bfm-papers">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots are drawing significant attention as versatile platforms for complex motor control, human-robot interaction, and general-purpose physical intelligence. However, achieving efficient whole-body control (WBC) in humanoids remains a fundamental challenge due to sophisticated dynamics, underactuation, and diverse task requirements. While learning-based controllers have shown promise for complex tasks, their reliance on labor-intensive and costly retraining for new scenarios limits real-world applicability. To address these limitations, behavior(al) foundation models (BFMs) have emerged as a new paradigm that leverages large-scale pre-training to learn reusable primitive skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a wide range of downstream tasks. In this paper, we present a comprehensive overview of BFMs for humanoid WBC, tracing their development across diverse pre-training pipelines. Furthermore, we discuss real-world applications, current limitations, urgent challenges, and future opportunities, positioning BFMs as a key approach toward scalable and general-purpose humanoid intelligence. Finally, we provide a curated and regularly updated collection of BFM papers and projects to facilitate more subsequent research, which is available at https://github.com/yuanmingqi/awesome-bfm-papers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为基础模型综述：类人机器人全身控制系统的下一代控制体系</div>
<div class="mono" style="margin-top:8px">类人机器人作为复杂运动控制、人机交互和通用物理智能的多功能平台，正引起广泛关注。然而，由于复杂的动力学、欠驱动和多样的任务需求，实现类人机器人全身控制（WBC）仍然是一个基本挑战。尽管基于学习的控制器在复杂任务中显示出潜力，但它们依赖于劳动密集型和昂贵的新场景重新训练限制了其实用性。为了解决这些限制，行为基础模型（BFMs）作为一种新范式出现，通过大规模预训练学习可重用的原始技能和广泛的先验行为，使零样本或快速适应广泛下游任务成为可能。在本文中，我们对BFMs在类人WBC中的全面概述进行了介绍，追溯了它们在不同预训练管道中的发展。此外，我们讨论了实际应用、当前限制、紧迫挑战和未来机会，将BFMs定位为实现可扩展和通用类人智能的关键方法。最后，我们提供了一个精心挑选并定期更新的BFMs论文和项目集合，以促进后续研究，该集合可在https://github.com/yuanmingqi/awesome-bfm-papers获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores the development of behavior foundation models (BFMs) for whole-body control in humanoid robots, addressing the challenge of efficient control due to complex dynamics and underactuation. BFMs leverage large-scale pre-training to learn reusable skills and behavioral priors, enabling zero-shot or rapid adaptation to various tasks. Key findings include the potential of BFMs for real-world applications and their role in advancing humanoid intelligence, though current limitations and challenges are also discussed.</div>
<div class="mono" style="margin-top:8px">论文探讨了行为基础模型（BFMs）在人形机器人全身控制中的发展，解决了由于复杂动力学和欠驱动带来的高效控制挑战。BFMs通过大规模预训练学习可重用的技能和行为先验，实现零样本或快速适应多种任务。研究提供了BFM发展的概述，讨论了实际应用和局限性，并提供了一个包含BFM论文和项目的精选集合。</div>
</details>
</div>
<div class="card">
<div class="title">Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</div>
<div class="meta-line">Authors: Yuyang Li, Yinghan Chen, Zihang Zhao, Puhao Li, Tengyu Liu, Siyuan Huang, Yixin Zhu</div>
<div class="meta-line">First: 2025-12-10T17:35:13+00:00 · Latest: 2026-02-09T10:08:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09851v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09851v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of tactile policy(66.3%) and vision-only policy (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同时触觉-视觉感知学习多模态机器人操作</div>
<div class="mono" style="margin-top:8px">机器人操作需要丰富的多模态感知和有效的学习框架来处理复杂的现实世界任务。透明皮肤（STS）传感器结合了触觉和视觉感知，提供了有前景的传感能力，而现代模仿学习则提供了强大的策略获取工具。然而，现有的STS设计缺乏同时的多模态感知，并且触觉跟踪不可靠。此外，将这些丰富的多模态信号整合到基于学习的操作管道中仍然是一个开放的挑战。我们引入了TacThru，这是一种STS传感器，能够实现同时的视觉感知和稳健的触觉信号提取，以及TacThru-UMI，这是一种利用这些多模态信号进行操作的模仿学习框架。我们的传感器具有完全透明的弹性体、持续照明、新型关键线标记和高效的跟踪功能，而我们的学习系统则通过基于Transformer的扩散策略将这些信号整合在一起。在五个具有挑战性的现实世界任务上的实验表明，TacThru-UMI 的平均成功率达到了85.5%，显著优于触觉策略（66.3%）和仅视觉策略（55.4%）的基线。该系统在关键场景中表现出色，包括薄而柔软物体的接触检测和需要多模态协调的精确操作。这项工作表明，将同时的多模态感知与现代学习框架相结合，可以实现更精确和适应性强的机器人操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the need for simultaneous tactile and visual perception in robotic manipulation, introducing TacThru, a See-through-skin sensor that enables robust tactile signal extraction alongside visual perception. TacThru-UMI, an imitation learning framework, integrates these multimodal signals using a Transformer-based Diffusion Policy. Experiments on five real-world tasks show that TacThru-UMI achieves an 85.5% success rate, outperforming tactile and vision-only policies by 19.2% and 30.1%, respectively, in scenarios involving contact detection and precision manipulation.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用新型See-through-skin (STS)传感器TacThru，结合同时的触觉和视觉感知，提高机器人的操作能力，该传感器实现了可靠的触觉跟踪和持续照明。TacThru-UMI框架利用Transformer基的扩散策略来利用多模态信号进行模仿学习。在五个真实世界的任务中，TacThru-UMI的成功率达到了85.5%，分别比触觉策略和纯视觉策略高出19.2%和30.1%。该系统在检测薄软物体接触和需要多模态协调的精细操作方面表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios</div>
<div class="meta-line">Authors: Tian Gao, Celine Tan, Catherine Glossop, Timothy Gao, Jiankai Sun, Kyle Stachowicz, Shirley Wu, Oier Mees, Dorsa Sadigh, Sergey Levine, Chelsea Finn</div>
<div class="meta-line">First: 2026-02-09T09:54:02+00:00 · Latest: 2026-02-09T09:54:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08440v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08440v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://steervla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SteerVLA：在长尾驾驶场景中引导视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">自主驾驶中的一个基本挑战是将高层语义推理与低层反应控制相结合，以处理长尾事件。虽然在大规模网络数据上训练的视觉-语言模型（VLMs）提供了强大的常识推理能力，但它们缺乏安全车辆控制所需的实际经验。我们认为，有效的自主代理应该利用VLM的世界知识来引导可调节的驾驶策略，以实现低层策略的稳健控制。为此，我们提出了SteerVLA，该方法利用VLM的推理能力生成细粒度的语言指令，以引导视觉-语言-行动（VLA）驾驶策略。我们方法的关键在于VLM和VLA之间丰富的语言接口，这使得高层策略能够更有效地将其推理与低层策略的控制输出联系起来。为了提供与车辆控制对齐的细粒度语言监督，我们利用VLM对现有驾驶数据进行详细语言注解的增强，我们发现这对于有效的推理和可调节性至关重要。我们在一个具有挑战性的闭环基准上评估了SteerVLA，结果显示其总体驾驶得分比最先进的方法高出4.77分，在长尾子集上高出8.04分。项目网站可访问：https://steervla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of integrating high-level semantic reasoning with low-level reactive control in autonomous driving. It proposes SteerVLA, which uses vision-language models to generate detailed language instructions to guide a vision-language-action driving policy. The method significantly improves performance on a challenging closed-loop benchmark, achieving a 4.77-point increase in overall driving score and an 8.04-point increase on a long-tail subset compared to state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">研究旨在将高层次的语义推理与低层次的反应控制结合在自主驾驶中，特别是针对长尾事件。SteerVLA 使用视觉语言模型生成详细的语言指令来引导视觉语言行动驾驶策略。该方法显著提高了性能，在闭环基准测试中，与最先进的方法相比，总体驾驶得分提高了4.77分，长尾子集提高了8.04分。</div>
</details>
</div>
<div class="card">
<div class="title">Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence</div>
<div class="meta-line">Authors: Jinxian Zhou, Ruihai Wu, Yiwei Liu, Yiwen Hou, Xunzhe Zhou, Checheng Yu, Licheng Zhong, Lin Shao</div>
<div class="meta-line">First: 2026-02-09T09:30:23+00:00 · Latest: 2026-02-09T09:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08425v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08425v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://biadapt-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bi-Adapt: 少量样本双臂适应新类别的3D对象通过语义对应</div>
<div class="mono" style="margin-top:8px">双臂操作对于机器人执行复杂任务至关重要但极具挑战性，需要两臂之间的协调合作。然而，现有的双臂操作方法往往依赖于昂贵的数据收集和训练，难以高效地泛化到新类别的未见物体。本文提出了一种名为Bi-Adapt的新框架，通过语义对应实现跨类别操作映射。Bi-Adapt利用视觉基础模型的强大能力进行微调，仅使用有限数据对新类别进行少量样本适应，表现出显著的泛化能力。在仿真和真实环境中的广泛实验验证了我们方法的有效性和高效率，即使在有限数据下也能在不同基准任务的新类别上实现高成功率。项目网站: https://biadapt-project.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the efficiency of bimanual manipulation for robots, especially in handling unseen objects in novel categories. Bi-Adapt, a novel framework, uses semantic correspondence and vision foundation models for cross-category affordance mapping. The method requires limited fine-tuning data and shows notable zero-shot generalization. Experiments in both simulation and real-world settings confirm its effectiveness and high efficiency, achieving high success rates on various benchmark tasks with minimal data.</div>
<div class="mono" style="margin-top:8px">研究动机是使机器人能够高效地对未见过的新类别物体进行双臂操作。Bi-Adapt框架通过语义对应和视觉基础模型实现跨类别操作映射。实验结果表明，Bi-Adapt在模拟和真实环境中都能在有限数据下获得高成功率，展示了其显著的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models</div>
<div class="meta-line">Authors: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li</div>
<div class="meta-line">First: 2026-02-09T08:47:14+00:00 · Latest: 2026-02-09T08:47:14+00:00</div>
<div class="meta-line">Comments: 38 pages, 9 figures. Project page:https://bimanibench.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08392v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bimanibench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiManiBench：一种评估多模态大型语言模型双臂协调性的分层基准</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）在增强具身人工智能方面取得了显著进展，使用它们来评估机器人的智能已经成为一个关键趋势。然而，现有的框架主要局限于单臂操作，未能捕捉到像举起一个重锅这样的双臂任务所需的时空协调。为了解决这个问题，我们引入了BiManiBench，这是一种分层基准，评估MLLMs在三个层级上的表现：基本的空间推理、高级动作规划和低级末端执行器控制。我们的框架隔离了独特的双臂挑战，如手臂可达性和运动学约束，从而区分感知幻觉与规划失败。对超过30个最先进的模型的分析表明，尽管在高级推理方面表现出色，MLLMs在双臂空间定位和控制方面仍然存在困难，经常导致相互干扰和顺序错误。这些发现表明当前的范式缺乏对相互运动学约束的深刻理解，突显了未来研究需要关注双臂碰撞避免和细粒度的时间序列。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">BiManiBench is a hierarchical benchmark designed to evaluate the bimanual coordination capabilities of Multimodal Large Language Models (MLLMs). It assesses models across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. The study reveals that while MLLMs excel in high-level reasoning, they struggle with dual-arm spatial grounding and control, often leading to mutual interference and sequencing errors. This indicates a lack of understanding of mutual kinematic constraints and suggests a need for future research focusing on inter-arm collision-avoidance and fine-grained temporal sequencing.</div>
<div class="mono" style="margin-top:8px">BiManiBench 是一个层次化的基准，用于评估多模态大型语言模型（MLLMs）的双臂协调能力。它在三个层级上评估模型：基本的空间推理、高级的动作规划和低级的末端执行器控制。对超过30个最先进的模型的分析显示，尽管MLLMs在高级推理方面表现出色，但在双臂空间定位和控制方面仍存在困难，经常导致相互干扰和顺序错误。这表明当前的范式缺乏对相互运动约束的理解，强调未来研究需要关注双臂碰撞避免和精细的时间序列控制。</div>
</details>
</div>
<div class="card">
<div class="title">OpenGVL -- Benchmarking Visual Temporal Progress for Data Curation</div>
<div class="meta-line">Authors: Paweł Budzianowski, Emilia Wiśnios, Michał Tyrolski, Gracjan Góral, Igor Kulakov, Viktor Petrenko, Krzysztof Walas</div>
<div class="meta-line">Venue: CoRL 2025</div>
<div class="meta-line">First: 2025-09-22T02:52:55+00:00 · Latest: 2026-02-09T08:29:03+00:00</div>
<div class="meta-line">Comments: Workshop on Making Sense of Data in Robotics: Composition, Curation, and Interpretability at Scale at CoRL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17321v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.17321v4">PDF</a> · <a href="http://github.com/budzianowski/opengvl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \href{github.com/budzianowski/opengvl}{OpenGVL}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenGVL -- 评估视觉时间进程在数据整理中的基准</div>
<div class="mono" style="margin-top:8px">数据稀缺仍然是机器人技术进步的最大限制因素之一。然而，野外可用的机器人数据量正在呈指数级增长，为大规模数据利用创造了新的机会。可靠的阶段性任务完成预测可以帮助自动标注和整理这些数据。最近提出了生成价值学习（GVL）方法，利用视觉语言模型（VLMs）中的知识从视觉观察中预测任务进度。在此基础上，我们提出了OpenGVL，这是一个全面的基准，用于估计涉及机器人和人类主体的多种挑战性操作任务的进度。我们评估了公开可用的开源基础模型的能力，显示开源模型家族显著落后于闭源模型，仅在时间进程预测任务上达到其性能的大约70%。此外，我们展示了OpenGVL如何作为自动化数据整理和过滤的实用工具，使大规模机器人数据集的质量评估变得高效。我们将在GitHub（github.com/budzianowski/opengvl）上发布基准和完整的代码库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to address the challenge of data scarcity in robotics by leveraging visual temporal progress prediction to automatically annotate and curate large-scale robotics data. It introduces OpenGVL, a benchmark for estimating task progress across various manipulation tasks involving both robots and humans. The evaluation shows that open-source models perform about 70% as well as closed-source models in predicting temporal progress, and OpenGVL can be used for automated data curation and quality assessment of robotics datasets.</div>
<div class="mono" style="margin-top:8px">论文旨在通过利用视觉时间进度预测来自动注释和整理大规模的机器人数据，解决机器人领域数据稀缺的问题。研究引入了OpenGVL，一个用于估计各种操作任务进度的基准。研究评估了开源和闭源基础模型，发现开源模型在时间进度预测任务中的表现仅为闭源模型的约70%。OpenGVL还被证明可以作为自动数据整理和过滤的实用工具，提高大规模机器人数据集的质量评估。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Human-Like Badminton Skills for Humanoid Robots</div>
<div class="meta-line">Authors: Yeke Chen, Shihao Dong, Xiaoyu Ji, Jingkai Sun, Zeren Luo, Liu Zhao, Jiahui Zhang, Wanyue Li, Ji Ma, Bowen Xu, Yimin Han, Yudong Zhao, Peng Lu</div>
<div class="meta-line">First: 2026-02-09T08:09:52+00:00 · Latest: 2026-02-09T08:09:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08370v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08370v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a &quot;mimic&quot; to a capable &quot;striker.&quot; Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>赋予类人机器人类似人类的羽毛球技能</div>
<div class="mono" style="margin-top:8px">在高需求的体育项目如羽毛球中实现多变且类似人类的表现仍然是类人机器人技术的一大挑战。与标准的移动或静态操作不同，这项任务需要无缝整合全身爆发性协调和精确、时间敏感的拦截。尽管最近的进步已经实现了类似人类的运动模仿，但在不牺牲风格自然性的前提下，将运动模仿与功能性的、物理意识的击球结合起来仍然是一个难题。为了解决这个问题，我们提出了模仿到互动的渐进强化学习框架，旨在将机器人从“模仿者”进化为“击球手”。我们的方法从人类数据中建立了一个稳健的运动先验，将其提炼为紧凑的基于模型的状态表示，并通过对抗先验稳定动力学。至关重要的是，为了克服专家演示稀疏的问题，我们引入了一种流形扩展策略，将离散的击球点泛化为密集的互动体积。我们通过模拟中掌握各种技能，包括吊球和杀球，验证了我们的框架。此外，我们展示了首次在类人机器人上实现零样本模拟到现实世界的类人羽毛球技能的转移，成功在物理世界中复制了人类运动员的动能优雅和功能性精确。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable humanoid robots to perform human-like badminton skills, which require both explosive whole-body coordination and precise timing. The proposed Imitation-to-Interaction framework uses progressive reinforcement learning to evolve a robot from mimicking human movements to becoming a capable striker. Key findings include the robot mastering various skills like lifts and drop shots in simulation and successfully transferring anthropomorphic badminton skills to a real humanoid robot, demonstrating kinetic elegance and functional precision.</div>
<div class="mono" style="margin-top:8px">研究旨在使类人机器人能够执行类似人类的羽毛球技能，重点在于整合爆发性的全身协调和精确的时机。提出的Imitation-to-Interaction框架使用渐进式的强化学习使机器人从模仿人类动作进化为成为一个熟练的击球手。关键发现包括成功模拟各种羽毛球技能，并首次实现了将类人技能从模拟直接转移到实际的类人机器人上，展示了动能的优雅和功能的精确性。</div>
</details>
</div>
<div class="card">
<div class="title">Nimbus: A Unified Embodied Synthetic Data Generation Framework</div>
<div class="meta-line">Authors: Zeyu He, Yuchang Zhang, Yuanzhen Zhou, Miao Tao, Hengjie Li, Hui Wang, Yang Tian, Jia Zeng, Tai Wang, Wenzhe Cai, Yilun Chen, Ning Gao, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-01-29T09:27:31+00:00 · Latest: 2026-02-09T06:57:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21449v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21449v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Nimbus：统一的具身合成数据生成框架</div>
<div class="mono" style="margin-top:8px">扩大数据的体积和多样性对于具身智能的泛化至关重要。虽然合成数据生成提供了比昂贵的物理数据获取更具扩展性的替代方案，但现有的管道仍然碎片化且任务特定。这种隔离导致了重大的工程效率低下和系统不稳定，无法支持基础模型训练所需的持续、高吞吐量的数据生成。为了解决这些挑战，我们提出了Nimbus，这是一种统一的合成数据生成框架，旨在整合异构导航和操作管道。Nimbus 引入了一种模块化的四层架构，该架构通过异步阶段分离轨迹规划、渲染和存储。通过实现动态管道调度、全局负载均衡、分布式容错和后端特定的渲染优化，该系统最大限度地利用了CPU、GPU和I/O资源。我们的评估表明，与未优化的基线相比，Nimbus 在端到端吞吐量上实现了2-3倍的改进，并确保在大规模分布式环境中具有稳健且长期的运行。该框架是InternData套件的生产核心，使跨域数据合成变得无缝。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Nimbus is a unified synthetic data generation framework aimed at improving the efficiency and scalability of embodied intelligence training data. It addresses the fragmented and task-specific nature of existing pipelines by introducing a modular four-layer architecture with decoupled execution stages for trajectory planning, rendering, and storage. Nimbus enhances resource utilization through dynamic pipeline scheduling, global load balancing, and distributed fault tolerance, resulting in a 2-3X improvement in end-to-end throughput compared to unoptimized baselines. This framework supports robust, long-term operation in large-scale distributed environments and is the backbone for the InternData suite&#x27;s data synthesis needs.</div>
<div class="mono" style="margin-top:8px">Nimbus 是一个统一的合成数据生成框架，旨在提高体现智能训练数据的效率和可扩展性。它通过引入模块化的四层架构和解耦的执行阶段来解决现有管道的碎片化和任务特定性问题，这些阶段分别负责轨迹规划、渲染和存储。Nimbus 通过动态管道调度、全局负载均衡和分布式容错来提高资源利用率，从而在端到端吞吐量上比未优化的基线提高了2-3倍。该框架支持在大规模分布式环境中长期稳健运行，并作为 InternData 套件数据合成的基础。</div>
</details>
</div>
<div class="card">
<div class="title">ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects</div>
<div class="meta-line">Authors: Josh Pinskier, Sarah Baldwin, Stephen Rodan, David Howard</div>
<div class="meta-line">First: 2026-02-09T05:35:08+00:00 · Latest: 2026-02-09T05:35:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08285v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate change, invasive species and human activities are currently damaging the world&#x27;s coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReefFlex：一种用于有机和易碎物体软体机器人抓取的生成设计框架</div>
<div class="mono" style="margin-top:8px">气候变化、入侵物种和人类活动正在以前所未有的速度破坏世界的珊瑚礁，威胁其丰富的生物多样性和渔业，并减少沿海保护。解决这一巨大挑战需要可扩展的珊瑚再生技术，能够培育气候适应性强的物种并加速自然再生过程；这些行动受到缺乏安全和坚固的工具来处理脆弱珊瑚的阻碍。我们研究了ReefFlex，一种生成软手指设计方法，探索了软手指的多样化空间，以产生一组能够安全抓取在杂乱环境中脆弱且几何形状各异的珊瑚的候选方案。我们的关键见解是将异质抓取编码为一组简化且可处理的运动基元，创建了一个简化且可处理的多目标优化问题。为了评估该方法，我们设计了一种软机器人用于珊瑚礁恢复，在岸上水族养殖场中生长和操作珊瑚，以供未来珊瑚礁移植使用。我们证明ReefFlex提高了抓取成功率和抓取质量（抗干扰性、定位精度），并在珊瑚操作过程中减少了不良事件的发生。ReefFlex提供了一种设计复杂操作中通用软末端执行器的方法，并为以前无法实现的珊瑚处理恢复领域自动化铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ReefFlex is a generative design framework that explores diverse soft finger designs to safely grasp fragile and geometrically heterogeneous corals. By encoding grasping into motion primitives, it simplifies the optimization problem. The method was evaluated through a soft robot designed for reef rehabilitation, which demonstrated increased grasp success and quality, and reduced adverse events during coral manipulation compared to reference designs. This framework offers a general approach for designing soft end-effectors for complex handling tasks, particularly in coral restoration.</div>
<div class="mono" style="margin-top:8px">ReefFlex 是一种生成设计框架，探索多种软手指设计以安全抓取脆弱且几何形状各异的珊瑚。通过将异质抓取编码为运动基本模式，简化了优化问题。该方法通过一个用于珊瑚修复的软机器人进行了评估，该机器人展示了比参考设计更高的抓取成功率、抗干扰性和定位精度，为珊瑚处理自动化铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer</div>
<div class="meta-line">Authors: Ke Zhang, Lixin Xu, Chengyi Song, Junzhe Xu, Xiaoyi Lin, Zeyu Jiang, Renjing Xu</div>
<div class="meta-line">First: 2026-02-09T05:16:48+00:00 · Latest: 2026-02-09T05:16:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08278v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08278v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://davidlxu.github.io/DexFormer-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation remains one of the most challenging problems in robotics, requiring coherent control of high-DoF hands and arms under complex, contact-rich dynamics. A major barrier is embodiment variability: different dexterous hands exhibit distinct kinematics and dynamics, forcing prior methods to train separate policies or rely on shared action spaces with per-embodiment decoder heads. We present DexFormer, an end-to-end, dynamics-aware cross-embodiment policy built on a modified transformer backbone that conditions on historical observations. By using temporal context to infer morphology and dynamics on the fly, DexFormer adapts to diverse hand configurations and produces embodiment-appropriate control actions. Trained over a variety of procedurally generated dexterous-hand assets, DexFormer acquires a generalizable manipulation prior and exhibits strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand. Our results show that a single policy can generalize across heterogeneous hand embodiments, establishing a scalable foundation for cross-embodiment dexterous manipulation. Project website: https://davidlxu.github.io/DexFormer-web/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DexFormer: 历史条件下的变换器跨体态灵巧操作</div>
<div class="mono" style="margin-top:8px">灵巧操作仍然是机器人技术中最具挑战性的问题之一，需要在复杂的、接触丰富的动力学条件下对高自由度的手和臂进行协调控制。主要障碍是体态差异：不同的灵巧手具有不同的运动学和动力学，迫使先前的方法训练单独的策略或依赖于共享的动作空间，并为每个体态使用解码器头部。我们提出了DexFormer，这是一种基于修改后的变换器骨干的端到端、动力学感知的跨体态策略，它基于历史观察进行条件控制。通过使用时间上下文实时推断形态和动力学，DexFormer能够适应各种手部配置并产生适合体态的控制动作。DexFormer在多种程序生成的灵巧手资产上进行训练，获得了可泛化的操作先验，并在Leap Hand、Allegro Hand和Rapid Hand上表现出强大的零样本迁移能力。我们的结果表明，单一策略可以跨异构手部体态泛化，为跨体态灵巧操作奠定了可扩展的基础。项目网站：https://davidlxu.github.io/DexFormer-web/.</div>
</details>
</div>
<div class="card">
<div class="title">Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes</div>
<div class="meta-line">Authors: Seunghoon Jeong, Eunho Lee, Jeongyun Kim, Ayoung Kim</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-09T04:50:36+00:00 · Latest: 2026-02-09T04:50:36+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures, 4 tables, accepted to ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08266v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08266v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于对象的信息导向下一最佳视角在拥挤场景中对象感知3D 高斯点积中的应用</div>
<div class="mono" style="margin-top:8px">在不可避免的遮挡和不完整的观察条件下，选择信息丰富的视角对于构建可靠的表示至关重要。在此背景下，3D 高斯点积（3DGS）具有明显的优势，因为它可以明确指导后续视角的选择，并利用新观察结果进行表示的细化。然而，现有方法仅依赖几何线索，忽视了操作相关的语义，并倾向于优先利用信息而非探索。为解决这些局限性，我们提出了一种实例感知的下一最佳视角（NBV）策略，通过利用对象特征优先考虑未探索的区域。具体而言，我们的对象感知3DGS将实例级信息提炼为一热对象向量，用于计算置信加权信息增益，从而指导识别与错误和不确定的高斯分布相关的区域。此外，我们的方法可以轻松适应对象中心的NBV，将视角选择集中在目标对象上，从而提高对物体放置的重建鲁棒性。实验表明，与基线方法相比，我们的NBV策略在合成数据集上将深度误差降低了77.14%，在真实世界的GraspNet数据集上降低了34.10%。此外，与针对整个场景进行NBV相比，对特定对象进行NBV可以额外降低该对象25.60%的深度误差。我们还通过现实世界的机器人操作任务进一步验证了我们方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of selecting informative viewpoints in cluttered scenes for building reliable 3D representations using 3D Gaussian Splatting (3DGS). It introduces an object-aware Next Best View (NBV) policy that leverages object features to guide view selection, focusing on underexplored regions. Experiments show that this approach reduces depth error by up to 77.14% on synthetic data and 34.10% on real-world data compared to existing methods, and further reduces depth error by 25.60% when targeting a specific object. The method enhances reconstruction robustness to object placement in robotic manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文解决了在杂乱场景中选择信息性视点以使用3D高斯点积（3DGS）构建可靠3D表示的挑战。它引入了一种基于对象的Next Best View（NBV）策略，利用对象特征优先选择未探索区域，合成数据中深度误差最多减少77.14%，真实世界GraspNet数据中减少34.10%。此外，针对特定对象进行NBV进一步将该对象的深度误差减少25.60%。</div>
</details>
</div>
<div class="card">
<div class="title">Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control</div>
<div class="meta-line">Authors: Yuanzhu Zhan, Yufei Jiang, Muqing Cao, Junyi Geng</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-09T04:10:39+00:00 · Latest: 2026-02-09T04:10:39+00:00</div>
<div class="meta-line">Comments: 9 pages, 7 figures. Accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08251v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>接触感知在轨混合控制的空中操作</div>
<div class="mono" style="margin-top:8px">空中操作（AM）有望将无人驾驶航空器（UAV）从被动检查扩展到诸如抓取、组装和现场维护等接触丰富的任务。大多数先前的AM演示依赖于外部运动捕捉（MoCap）并强调粗略交互的位置控制，限制了其部署能力。我们提出了一种完全在轨感知-控制管道，无需MoCap即可实现准确的运动跟踪和调节接触力矩。主要组成部分包括（1）一种增强的视觉-惯性里程计（VIO）估计器，带有仅在交互期间激活的接触一致性因子，这会收紧接触框架周围的不确定性并减少漂移，以及（2）基于图像的视觉伺服（IBVS）以减轻感知-控制耦合，结合一种混合力-运动控制器，用于调节接触力矩和横向运动以实现稳定接触。实验表明，我们的方法仅使用机载传感即可闭合感知到力矩的环路，接触时的速度估计改进了66.01%，可靠地接近目标并稳定地保持力的控制点，朝着可部署的野外空中操作迈进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to advance aerial manipulation by enabling UAVs to perform contact-rich tasks such as grasping and assembly without relying on external motion capture. The key method involves an onboard perception-control pipeline with an augmented visual-inertial odometry estimator and a hybrid force-motion controller. Experiments demonstrate that the approach can accurately track motion and regulate contact forces using only onboard sensors, improving velocity estimation by 66.01% at contact and ensuring reliable target approach and stable force holding, making it suitable for deployable, real-world aerial manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文通过开发一种无需外部运动捕捉的机载感知控制管道，解决了执行空中操作任务的挑战，该管道能够实现准确的运动跟踪和调节接触力。关键组件包括增强的视觉惯性里程计估计器和混合力-运动控制器。实验表明，该方法在接触时将速度估计提高了66.01%，能够可靠地接近目标并实现稳定的力保持，使其适用于可部署的野外空中操作任务。</div>
</details>
</div>
<div class="card">
<div class="title">STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction</div>
<div class="meta-line">Authors: Jinhao Li, Yuxuan Cong, Yingqiao Wang, Hao Xia, Shan Huang, Yijia Zhang, Ningyi Xu, Guohao Dai</div>
<div class="meta-line">First: 2026-02-09T03:50:40+00:00 · Latest: 2026-02-09T03:50:40+00:00</div>
<div class="meta-line">Comments: 13 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08245v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STEP：时空一致性预测的预热视觉运动策略</div>
<div class="mono" style="margin-top:8px">扩散策略最近由于能够建模动作序列的分布并捕捉多模态性，在机器人操作中的视觉运动控制方面展现出了强大的范式。然而，迭代去噪会导致显著的推理延迟，限制了实时闭环系统中的控制频率。现有的加速方法要么减少采样步骤，要么通过直接预测绕过扩散，要么重用过去的动作，但往往难以同时保持动作质量并实现一致的低延迟。在本文中，我们提出了一种轻量级的时空一致性预测机制STEP，以构建既与目标动作在分布上接近又在时间上一致的高质量预热动作，而不牺牲原始扩散策略的生成能力。然后，我们提出了一种基于时间动作变化的加速度感知扰动注入机制，以适应性地调节动作激发，防止执行停滞，特别是在实际任务中。我们还提供了一个理论分析，表明提出的预测诱导了一个局部收缩映射，确保了在扩散细化过程中动作误差的收敛。我们在九个模拟基准和两个实际任务上进行了广泛的评估。值得注意的是，与BRIDGER和DDIM相比，STEP在RoboMimic基准和实际任务中分别实现了平均21.6%和27.5%更高的成功率。这些结果表明，STEP在推理延迟和成功率的帕累托前沿上始终优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the high inference latency issue in diffusion policies for real-time robotic manipulation. The authors propose STEP, which includes a spatiotemporal consistency prediction mechanism to generate high-quality warm-start actions and a velocity-aware perturbation injection mechanism to prevent execution stall. STEP achieves an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM, respectively, on simulated and real-world tasks, while maintaining low latency.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出STEP，利用时空一致性预测生成高质量的预热动作，以解决扩散策略在实时机器人操作中的高推理延迟问题。该方法确保了动作质量和低延迟，并通过理论分析展示了局部收敛映射。实验结果表明，STEP在九个模拟基准和两个真实世界任务上的成功率高于现有方法BRIDGER和DDIM，推动了推理延迟和成功率的帕累托前沿。</div>
</details>
</div>
<div class="card">
<div class="title">MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</div>
<div class="meta-line">Authors: Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath</div>
<div class="meta-line">First: 2025-12-18T18:59:03+00:00 · Latest: 2026-02-09T03:24:03+00:00</div>
<div class="meta-line">Comments: 25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16909v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16909v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hybridrobotics.github.io/MomaGraph/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MomaGraph：基于视觉语言模型的统一场景图及其在体感任务规划中的状态感知</div>
<div class="mono" style="margin-top:8px">家庭中的移动机械臂必须同时导航和操作。这需要一种紧凑且语义丰富的场景表示，能够捕捉物体的位置、功能以及哪些部分可以操作。场景图是一个自然的选择，但先前的工作往往将空间关系和功能关系分开处理，将场景视为静态快照，不包含物体状态或时间更新，也忽略了与当前任务相关的最重要信息。为了解决这些限制，我们引入了MomaGraph，这是一种将空间功能关系和部分级交互元素整合在一起的统一场景表示。然而，要推进这种表示需要合适的数据和严格的评估，这些方面目前仍然缺乏。因此，我们贡献了MomaGraph-Scenes，这是第一个包含丰富注释、任务驱动的场景图的大规模数据集，以及MomaGraph-Bench，这是一个涵盖从高层规划到细粒度场景理解的六个推理能力的系统评估套件。在此基础上，我们进一步开发了MomaGraph-R1，这是一种7B参数的视觉语言模型，通过强化学习在MomaGraph-Scenes上进行训练。MomaGraph-R1预测任务导向的场景图，并在Graph-then-Plan框架下作为零样本任务规划器。广泛的实验表明，我们的模型在开源模型中达到了最先进的结果，准确率达到71.6%（比最佳基线高11.4%），并且在公共基准测试中具有泛化能力，并且能够有效地转移到真实机器人实验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MomaGraph addresses the limitations of previous scene graph representations by integrating spatial-functional relationships and part-level interactive elements, and introduces MomaGraph-Scenes, a large-scale dataset of richly annotated, task-driven scene graphs for household environments. MomaGraph-R1, a 7B vision-language model, is trained with reinforcement learning on this dataset and demonstrates state-of-the-art results, achieving 71.6% accuracy on the benchmark and showing generalization and transfer capabilities.</div>
<div class="mono" style="margin-top:8px">MomaGraph通过整合空间-功能关系和部件级交互元素来解决先前场景图表示的局限性。它引入了MomaGraph-Scenes，这是一个包含丰富注释和任务驱动的场景图的大规模数据集，适用于家庭环境，以及MomaGraph-Bench，这是一个用于体态代理的评估套件。MomaGraph-R1是一个7B的视觉-语言模型，能够预测任务导向的场景图，并作为零样本任务规划器使用，其在基准测试中的准确率达到71.6%，超越了之前的模型11.4%。</div>
</details>
</div>
<div class="card">
<div class="title">AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act</div>
<div class="meta-line">Authors: Pengyuan Guo, Zhonghao Mai, Zhengtong Xu, Kaidi Zhang, Heng Zhang, Zichen Miao, Arash Ajoudani, Zachary Kingston, Qiang Qiu, Yu She</div>
<div class="meta-line">First: 2026-02-02T05:30:14+00:00 · Latest: 2026-02-09T03:21:20+00:00</div>
<div class="meta-line">Comments: Added appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01662v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups&#x27; setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgenticLab：一个能够看见、思考和行动的现实世界机器人代理平台</div>
<div class="mono" style="margin-top:8px">近期大型视觉-语言模型（VLMs）的进步展示了通用的开放词汇感知和推理能力，但在不规则、野外环境中的长期闭环执行能力尚不清楚。基于VLM的抓取操作管道在不同研究组的设置之间难以比较，许多评估依赖于模拟、特权状态或特别设计的设置。我们提出了AgenticLab，这是一个模型无关的机器人代理平台和开放世界抓取操作基准。AgenticLab 提供了一个闭环代理管道，用于感知、任务分解、在线验证和重新规划。使用AgenticLab，我们在不规则环境中对最先进的基于VLM的代理进行了基准测试。我们的基准测试揭示了离线视觉-语言测试（例如VQA和静态图像理解）无法捕捉到的几种失败模式，包括多步语义一致性的失效、遮挡和场景变化下的物体语义以及空间推理不足导致的可靠操作失败。我们将发布完整的硬件和软件堆栈，以支持可重复的评估并加速通用机器人代理的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AgenticLab is a platform designed to evaluate the real-world manipulation capabilities of large vision-language models in unstructured environments. It provides a closed-loop agent pipeline for perception, task decomposition, and online verification. The benchmark reveals several failure modes not captured by offline vision-language tests, such as multi-step grounding consistency issues, object grounding under occlusion, and insufficient spatial reasoning. The platform aims to support reproducible evaluation and accelerate research on general-purpose robot agents.</div>
<div class="mono" style="margin-top:8px">研究旨在评估大型视觉-语言模型（VLMs）在非结构化环境中的实际操作能力。AgenticLab 是一个模型通用的平台，提供了一个闭环代理管道，用于感知、任务分解和在线验证。实验表明，存在多步接地不一致和空间推理不足等问题，这些在线视觉-语言测试（如VQA和静态图像理解）无法捕捉到。</div>
</details>
</div>
<div class="card">
<div class="title">Temperature Scaling Attack Disrupting Model Confidence in Federated Learning</div>
<div class="meta-line">Authors: Kichang Lee, Jaeho Jin, JaeYeon Park, Songkuk Kim, JeongGil Ko</div>
<div class="meta-line">First: 2026-02-06T12:01:54+00:00 · Latest: 2026-02-09T01:55:58+00:00</div>
<div class="meta-line">Comments: 20 pages, 20 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06638v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.06638v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predictive confidence serves as a foundational control signal in mission-critical systems, directly governing risk-aware logic such as escalation, abstention, and conservative fallback. While prior federated learning attacks predominantly target accuracy or implant backdoors, we identify confidence calibration as a distinct attack objective. We present the Temperature Scaling Attack (TSA), a training-time attack that degrades calibration while preserving accuracy. By injecting temperature scaling with learning rate-temperature coupling during local training, malicious updates maintain benign-like optimization behavior, evading accuracy-based monitoring and similarity-based detection. We provide a convergence analysis under non-IID settings, showing that this coupling preserves standard convergence bounds while systematically distorting confidence. Across three benchmarks, TSA substantially shifts calibration (e.g., 145% error increase on CIFAR-100) with &lt;2 accuracy change, and remains effective under robust aggregation and post-hoc calibration defenses. Case studies further show that confidence manipulation can cause up to 7.2x increases in missed critical cases (healthcare) or false alarms (autonomous driving), even when accuracy is unchanged. Overall, our results establish calibration integrity as a critical attack surface in federated learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>温度缩放攻击破坏联邦学习模型的信心</div>
<div class="mono" style="margin-top:8px">预测置信度是关键系统中基础的控制信号，直接管理诸如升级、弃权和保守回退等风险意识逻辑。尽管先前的联邦学习攻击主要针对准确度或植入后门，我们发现置信度校准是一个独特的攻击目标。我们提出了温度缩放攻击（TSA），这是一种训练时攻击，它降低校准效果同时保持准确度。通过在本地训练期间注入温度缩放并结合学习率-温度耦合，恶意更新保持类似良性优化行为，从而逃避基于准确度的监控和基于相似性的检测。我们提供了在非IID设置下的收敛性分析，表明这种耦合保持了标准的收敛性边界，同时系统地扭曲了置信度。在三个基准测试中，TSA显著改变了校准（例如，CIFAR-100上的误差增加145%），且准确度变化小于2%，即使在鲁棒聚合和事后校准防御下仍然有效。案例研究进一步表明，置信度操纵可能导致高达7.2倍的误判关键案例（医疗保健）或误报（自动驾驶）增加，即使准确度未发生变化。总体而言，我们的结果确立了校准完整性在联邦学习中的关键攻击面。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces the Temperature Scaling Attack (TSA), a training-time attack that degrades model confidence calibration without affecting accuracy. By coupling temperature scaling with learning rate during local training, malicious updates can evade accuracy-based monitoring and similarity-based detection. Experiments on three benchmarks show that TSA can increase calibration error by up to 145% with minimal accuracy loss (&lt;2% change), and can significantly impact risk-aware logic in mission-critical systems, such as healthcare and autonomous driving, even when accuracy remains unchanged.</div>
<div class="mono" style="margin-top:8px">论文提出了温度缩放攻击（TSA），该攻击在训练时降低模型的置信度校准而不影响准确率。通过在本地训练中将温度缩放与学习率耦合，恶意更新可以规避基于准确率的监控和基于相似性的检测。在三个基准上的实验表明，TSA可以使校准误差增加高达145%，同时准确率变化小于2%，并且即使准确率不变，也会影响关键任务系统中的风险感知逻辑，如医疗和自动驾驶中的误报和漏报情况。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms</div>
<div class="meta-line">Authors: Minsung Yoon, Sung-Eui Yoon</div>
<div class="meta-line">Venue: ICRA 2025</div>
<div class="meta-line">First: 2026-02-03T11:17:42+00:00 · Latest: 2026-02-09T00:32:57+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2025. Project page: https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03397v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03397v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA25/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadruped robots face limitations in long-range navigation efficiency due to their reliance on legs. To ameliorate the limitations, we introduce a Reinforcement Learning-based Active Transporter Riding method (\textit{RL-ATR}), inspired by humans&#x27; utilization of personal transporters, including Segways. The \textit{RL-ATR} features a transporter riding policy and two state estimators. The policy devises adequate maneuvering strategies according to transporter-specific control dynamics, while the estimators resolve sensor ambiguities in non-inertial frames by inferring unobservable robot and transporter states. Comprehensive evaluations in simulation validate proficient command tracking abilities across various transporter-robot models and reduced energy consumption compared to legged locomotion. Moreover, we conduct ablation studies to quantify individual component contributions within the \textit{RL-ATR}. This riding ability could broaden the locomotion modalities of quadruped robots, potentially expanding the operational range and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过利用个人运输平台提高四足机器人导航效率</div>
<div class="mono" style="margin-top:8px">四足机器人由于依赖腿部，在长距离导航效率方面存在局限性。为克服这些局限，我们提出了一种基于强化学习的主动运输器骑行方法（RL-ATR），灵感来源于人类利用个人运输器，如平衡车。RL-ATR 包含一个运输器骑行策略和两个状态估计器。策略根据运输器特定的控制动力学制定合适的机动策略，而估计器通过推断不可观测的机器人和运输器状态来解决非惯性参考系中的传感器模糊性。在模拟中的全面评估验证了其在各种运输器-机器人模型中高效命令跟踪能力和相比腿部运动的能耗降低。此外，我们进行了消融研究以量化 RL-ATR 中各个组件的贡献。这种骑行能力可以拓宽四足机器人的运动模式，可能扩展其操作范围和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the navigation efficiency of quadruped robots by leveraging personal transportation platforms. It introduces a Reinforcement Learning-based Active Transporter Riding method (RL-ATR) that includes a transporter riding policy and two state estimators. The method improves command tracking and reduces energy consumption compared to legged locomotion. Ablation studies quantify the contributions of individual components within the RL-ATR system, demonstrating its effectiveness in various scenarios.</div>
<div class="mono" style="margin-top:8px">论文提出了一种基于强化学习的主动运输器骑行方法（RL-ATR），借鉴了人类使用个人运输器的方式，以提高四足机器人的远距离导航效率。该方法包括骑行策略和两个状态估计器来处理传感器在非惯性参考系中的模糊性。实验结果表明，与腿足运动相比，该方法在指令跟踪和能量消耗方面表现出改进。进一步的消融研究量化了每个组件在RL-ATR系统中的贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-based Initialization of Trajectory Optimization for Path-following Problems of Redundant Manipulators</div>
<div class="meta-line">Authors: Minsung Yoon, Mincheul Kang, Daehyung Park, Sung-Eui Yoon</div>
<div class="meta-line">Venue: ICRA 2023</div>
<div class="meta-line">First: 2026-02-03T11:44:20+00:00 · Latest: 2026-02-09T00:29:00+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2023. Project page: https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA23_RLITG/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03418v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03418v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA23_RLITG/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Trajectory optimization (TO) is an efficient tool to generate a redundant manipulator&#x27;s joint trajectory following a 6-dimensional Cartesian path. The optimization performance largely depends on the quality of initial trajectories. However, the selection of a high-quality initial trajectory is non-trivial and requires a considerable time budget due to the extremely large space of the solution trajectories and the lack of prior knowledge about task constraints in configuration space. To alleviate the issue, we present a learning-based initial trajectory generation method that generates high-quality initial trajectories in a short time budget by adopting example-guided reinforcement learning. In addition, we suggest a null-space projected imitation reward to consider null-space constraints by efficiently learning kinematically feasible motion captured in expert demonstrations. Our statistical evaluation in simulation shows the improved optimality, efficiency, and applicability of TO when we plug in our method&#x27;s output, compared with three other baselines. We also show the performance improvement and feasibility via real-world experiments with a seven-degree-of-freedom manipulator.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的轨迹优化初始化方法在冗余 manipulator 路径跟踪问题中的应用</div>
<div class="mono" style="margin-top:8px">轨迹优化（TO）是一种高效工具，用于生成冗余 manipulator 的关节轨迹以遵循 6 维笛卡尔路径。优化性能很大程度上取决于初始轨迹的质量。然而，选择高质量的初始轨迹并不容易，并且由于解轨迹空间极其庞大以及在配置空间中缺乏任务约束的先验知识，需要大量的时间预算。为了解决这个问题，我们提出了一种基于学习的初始轨迹生成方法，该方法通过采用示例引导的强化学习在较短的时间预算内生成高质量的初始轨迹。此外，我们建议使用投影到零空间的模仿奖励来考虑零空间约束，通过高效学习在专家演示中捕获的运动的运动学可行性。我们的统计评估表明，当我们将方法的输出插入 TO 中时，与三个其他基线相比，TO 的优化性、效率和适用性得到了提高。我们还通过七自由度 manipulator 的实际实验展示了性能改进和可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of generating high-quality initial trajectories for trajectory optimization in redundant manipulators, which is crucial for efficient path-following. It proposes a learning-based method using example-guided reinforcement learning to generate these trajectories quickly. The method incorporates a null-space projected imitation reward to handle null-space constraints. Experimental results in simulation and real-world demonstrate that this approach improves optimality, efficiency, and applicability compared to other methods.</div>
<div class="mono" style="margin-top:8px">论文针对冗余 manipulator 轨迹优化中高质量初始轨迹生成的挑战，提出了一种使用示例引导的强化学习方法来快速生成这些轨迹。该方法结合了 null 空间投影模仿奖励来处理 null 空间约束。仿真和七自由度 manipulator 的实际实验结果表明，与三种基线方法相比，该方法在最优性、效率和适用性方面有所改进。</div>
</details>
</div>
<div class="card">
<div class="title">Evasion of IoT Malware Detection via Dummy Code Injection</div>
<div class="meta-line">Authors: Sahar Zargarzadeh, Mohammad Islam</div>
<div class="meta-line">First: 2026-02-09T00:18:23+00:00 · Latest: 2026-02-09T00:18:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08170v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.08170v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Internet of Things (IoT) has revolutionized connectivity by linking billions of devices worldwide. However, this rapid expansion has also introduced severe security vulnerabilities, making IoT devices attractive targets for malware such as the Mirai botnet. Power side-channel analysis has recently emerged as a promising technique for detecting malware activity based on device power consumption patterns. However, the resilience of such detection systems under adversarial manipulation remains underexplored.
  This work presents a novel adversarial strategy against power side-channel-based malware detection. By injecting structured dummy code into the scanning phase of the Mirai botnet, we dynamically perturb power signatures to evade AI/ML-based anomaly detection without disrupting core functionality. Our approach systematically analyzes the trade-offs between stealthiness, execution overhead, and evasion effectiveness across multiple state-of-the-art models for side-channel analysis, using a custom dataset collected from smartphones of diverse manufacturers. Experimental results show that our adversarial modifications achieve an average attack success rate of 75.2\%, revealing practical vulnerabilities in power-based intrusion detection frameworks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过注入假代码规避物联网恶意软件检测</div>
<div class="mono" style="margin-top:8px">物联网（IoT）通过将数十亿设备连接到全球，彻底改变了连接性。然而，这种快速扩展也引入了严重的安全漏洞，使物联网设备成为诸如Mirai僵尸网络等恶意软件的诱人目标。最近，基于设备功耗模式的侧信道分析已逐渐成为检测恶意软件活动的一种有前途的技术。然而，此类检测系统在对抗性操纵下的鲁棒性尚未得到充分探索。
本研究提出了一种针对基于功耗模式的恶意软件检测的新颖对抗策略。通过在Mirai僵尸网络的扫描阶段注入结构化的假代码，我们动态地扰动功耗特征，以规避基于AI/ML的异常检测，而不破坏核心功能。我们的方法系统地分析了在多种先进的侧信道分析模型中，隐蔽性、执行开销和规避效果之间的权衡，使用从不同制造商的智能手机中收集的自定义数据集。实验结果表明，我们的对抗性修改实现了75.2%的平均攻击成功率，揭示了基于功耗的入侵检测框架中的实用漏洞。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of evading power side-channel-based malware detection in IoT devices by injecting dummy code into the Mirai botnet. The study demonstrates that by perturbing power signatures, the botnet can evade AI/ML-based anomaly detection while maintaining functionality. The experimental results indicate an average attack success rate of 75.2%, highlighting the practical vulnerabilities in current power-based intrusion detection systems.</div>
<div class="mono" style="margin-top:8px">这项研究通过向Mirai僵尸网络的扫描阶段注入假代码，以扰乱基于功率侧信道的恶意软件检测，从而应对物联网设备中的这一挑战。实验结果表明，这种对抗性修改的平均攻击成功率达到了75.2%，揭示了当前基于功率的入侵检测框架中的实际漏洞。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
