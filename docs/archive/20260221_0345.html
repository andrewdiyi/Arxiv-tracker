<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-21 03:45</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260221_0345</div>
    <div class="row"><div class="card">
<div class="title">When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</div>
<div class="meta-line">Authors: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-19T18:59:20+00:00 · Latest: 2026-02-19T18:59:20+00:00</div>
<div class="meta-line">Comments: Website: https://vla-va.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vla-va.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉优先于语言：评估和缓解VLAs中的反事实失败</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动模型（VLAs）承诺将语言指令应用于机器人控制，但在实践中往往未能忠实执行语言指令。当面对缺乏强烈场景特定监督的指令时，VLAs会遭受反事实失败：它们基于由数据集偏差引起的视觉捷径行动，反复执行已学得的行为，并选择在训练期间频繁出现的对象，而不考虑语言意图。为了系统地研究这一问题，我们引入了LIBERO-CF，这是第一个用于VLAs的反事实基准，通过在视觉上合理的LIBERO布局下分配替代指令来评估语言跟随能力。我们的评估表明，反事实失败在最先进的VLAs中普遍存在但尚未得到充分探索。我们提出了反事实行动指导（CAG），这是一种简单而有效的双分支推理方案，明确地在VLAs中正则化语言条件。CAG结合了一个标准的VLA策略和一个未受语言条件的视觉-行动（VA）模块，在行动选择期间进行反事实比较。这种设计减少了对视觉捷径的依赖，提高了对未观察任务的鲁棒性，并且不需要额外的演示或对现有架构或预训练模型进行修改。广泛的实验表明，它可以在各种VLAs中实现即插即用集成，并且具有一致的改进。例如，在LIBERO-CF中，CAG在语言跟随准确性上提高了9.7%，在未观察任务上的任务成功率提高了3.6%，使用无训练策略，配以VA模型时，进一步提高了15.5%和8.5%。在实际应用中，CAG将反事实失败减少了9.4%，并将任务成功率平均提高了17.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of counterfactual failures in Vision-Language-Action models (VLAs), where models rely on visual shortcuts rather than language instructions. To evaluate this, the authors introduce LIBERO-CF, a benchmark for counterfactual failures. They propose Counterfactual Action Guidance (CAG), a dual-branch inference scheme that improves language following accuracy and task success, especially on under-observed tasks, by reducing reliance on visual shortcuts. Experiments show that CAG enhances robustness and performance across various VLAs without requiring additional training or modifications to existing models.</div>
<div class="mono" style="margin-top:8px">研究关注Vision-Language-Action模型（VLAs）中的反事实失败问题，即模型依赖于视觉捷径而非语言指令。为此，研究人员引入了LIBERO-CF基准来评估这一问题。他们发现这些失败在最先进的VLAs中普遍存在。为解决这一问题，他们提出了反事实动作指导（CAG），这是一种双分支推理方案，能够提高语言跟随准确性和任务成功率，特别是在未观察到的任务上。在LIBERO-CF上，CAG将语言跟随准确率提高了9.7%，任务成功率提高了3.6%，与Vision-Action模型结合使用时，进一步提高了15.5%和8.5%。在实际评估中，CAG将反事实失败减少了9.4%，并将任务成功率提高了17.2%。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Step Duration for Accurate Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds</div>
<div class="meta-line">Authors: Zhaoyang Xiang, Victor Paredes, Guillermo A. Castillo, Ayonga Hereid</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2024-03-25T19:18:25+00:00 · Latest: 2026-02-19T18:19:15+00:00</div>
<div class="meta-line">Comments: 7 pages, 7 figures. Accepted to IEEE/RSJ IROS 2025. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.17136v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.17136v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional one-step preview planning algorithms for bipedal locomotion struggle to generate viable gaits when walking across terrains with restricted footholds, such as stepping stones. To overcome such limitations, this paper introduces a novel multi-step preview foot placement planning algorithm based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of walking robots. Our proposed approach adaptively changes the step duration and the swing foot trajectory for optimal foot placement under constraints, thereby enhancing the long-term stability of the robot and significantly improving its ability to navigate environments with tight constraints on viable footholds. We demonstrate its effectiveness through various simulation scenarios with complex stepping-stone configurations and external perturbations. These tests underscore its improved performance for navigating foothold-restricted terrains, even with external disturbances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应性步长调整以实现准确的足部放置：在受限 foothold 地形上实现稳健的双足运动</div>
<div class="mono" style="margin-top:8px">传统的双足运动一步预览规划算法在跨越受限 foothold 地形（如踏石）时难以生成可行的步态。为克服这些限制，本文提出了一种基于行走机器人 Divergent Component of Motion (DCM) 的步对步离散演变的多步预览足部放置规划算法。我们提出的方法适应性地调整步长和摆动腿轨迹，以在约束条件下实现最佳足部放置，从而增强机器人的长期稳定性和显著提高其在受限 foothold 地形环境中导航的能力。通过各种包含复杂踏石配置和外部干扰的仿真场景，我们展示了其有效性。这些测试强调了其在外部干扰下导航受限 foothold 地形的改进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of bipedal locomotion on terrains with restricted footholds by proposing a multi-step preview foot placement planning algorithm. The algorithm adapts step duration and swing foot trajectory based on the Divergent Component of Motion (DCM) to enhance stability and navigation capabilities. Experimental results from various simulation scenarios show improved performance in navigating complex stepping-stone terrains with external perturbations.</div>
<div class="mono" style="margin-top:8px">本文提出了一种多步预览足部放置规划算法，以解决在受限 foothold 地形上的双足行走问题。该算法根据行走机器人的发散运动分量（DCM）自适应调整步长和摆动腿轨迹，以增强稳定性和导航能力。各种复杂踏石地形的仿真测试结果表明，即使在外部干扰下，该算法也能有效导航受限 foothold 地形。</div>
</details>
</div>
<div class="card">
<div class="title">IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control</div>
<div class="meta-line">Authors: Qilong Cheng, Matthew Mackay, Ali Bereyhi</div>
<div class="meta-line">First: 2026-02-19T16:50:31+00:00 · Latest: 2026-02-19T16:50:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17537v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IRIS：基于学习的任务特定电影机器人手臂用于视动运动控制</div>
<div class="mono" style="margin-top:8px">机器人摄像系统能够实现超越人类能力的动态、可重复运动，但其采用受限于工业级平台的高成本和操作复杂性。我们介绍了智能机器人成像系统（IRIS），这是一种专为自主、基于学习的电影运动控制设计的6-DOF操作臂。IRIS 结合了轻量级的全3D打印硬件设计和基于动作分块与变换器（ACT）的目标条件视动模仿学习框架。该系统直接从人类示范中学习对象感知和感知平滑的摄像机轨迹，无需显式的几何编程。整个平台成本低于1000美元，支持1.5公斤负载，并实现约1毫米的重复性。实际实验表明，该系统能够准确跟踪轨迹、可靠自主执行，并在多种电影运动中泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a cost-effective and user-friendly robotic camera system for cinematic motion control. IRIS, a 6-DOF manipulator, uses a lightweight 3D-printed design and a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT) to learn camera trajectories from human demonstrations. Key findings include accurate trajectory tracking, reliable autonomous execution, and generalization across various cinematic motions, with a platform cost under $1,000 USD and a 1.5 kg payload capacity achieving approximately 1 mm repeatability.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种低成本且用户友好的机器人摄像系统，用于电影运动控制。IRIS 是一个6-DOF机械臂，采用轻量级3D打印设计，并使用基于Action Chunking with Transformers (ACT)的目标条件视觉-运动模仿学习框架，从人类演示中学习摄像机轨迹。关键发现包括准确的轨迹跟踪、可靠的自主执行以及在各种电影运动中的泛化能力，平台成本低于1000美元，负载能力为1.5公斤，重复精度约为1毫米。</div>
</details>
</div>
<div class="card">
<div class="title">Proximal powered knee placement: a case study</div>
<div class="meta-line">Authors: Kyle R. Embry, Lorenzo Vianello, Jim Lipsey, Frank Ursetta, Michael Stephens, Zhi Wang, Ann M. Simon, Andrea J. Ikeda, Suzanne B. Finucane, Shawana Anarwala, Levi J. Hargrove</div>
<div class="meta-line">First: 2026-02-19T16:16:20+00:00 · Latest: 2026-02-19T16:16:20+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE RAS/EMBS 11th International Conference on Biomedical Robotics and Biomechatronics (BioRob 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17502v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17502v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>近端动力膝关节安装：案例研究</div>
<div class="mono" style="margin-top:8px">下肢截肢影响全球数百万人，导致行动能力下降、行走速度减慢和日常及社交活动参与度降低。动力假肢膝关节可以通过主动辅助膝关节扭矩部分恢复行动能力，改善步态对称性、坐起和站立转换以及行走速度。然而，动力组件增加的重量可能会削弱这些益处，影响步态力学并增加代谢成本。因此，优化质量分布，而不是简单地减少总质量，可能提供更有效和实用的解决方案。在本探索性研究中，我们评估了在一小群人中将动力假肢膝关节上方安装的可行性。与下方安装相比，上方配置显示出行走速度（一名参与者提高9.2%）和步频（提高3.6%）的改善，步态对称性则表现出混合效果。运动学测量表明，两种配置下的膝关节活动范围和峰值速度相似。在斜坡和楼梯上的额外测试证实了控制策略在多种运动任务中的稳健性。初步结果显示，上方安装功能可行，精心的质量分布可以保持动力辅助的益处，同时减轻增加重量的不利影响。需要进一步研究来确认这些趋势并指导设计和临床建议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores the feasibility of placing a powered prosthetic knee above the knee joint in a small cohort, aiming to optimize mass distribution for improved mobility. Compared to below-knee placement, the above-knee configuration showed enhanced walking speed and cadence, with mixed effects on gait symmetry. The study confirmed the robustness of the control strategy across various tasks, suggesting that above-knee placement can preserve the benefits of powered assistance while mitigating the negative effects of added weight, though further research is needed to confirm these findings.</div>
<div class="mono" style="margin-top:8px">本研究探索了在小样本群体中将假肢膝关节动力装置置于膝关节上方的可行性，旨在优化质量分布以提高移动性。与膝关节下方放置相比，膝关节上方配置显示出更快的行走速度和步频，但对步态对称性的影响则参差不齐。研究证实了控制策略在多种运动任务中的稳健性，表明膝关节上方放置可以保留动力辅助的好处，同时减轻附加重量的负面影响，但还需进一步研究来确认这些发现。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection</div>
<div class="meta-line">Authors: Yichen Lu, Siwei Nie, Minlong Lu, Xudong Yang, Xiaobo Zhang, Peng Zhang</div>
<div class="meta-line">First: 2026-02-19T15:54:55+00:00 · Latest: 2026-02-19T15:54:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17484v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17484v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace&#x27;s verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追踪复制像素和正则化块亲和性在复制检测中的应用</div>
<div class="mono" style="margin-top:8px">图像复制检测（ICD）旨在通过稳健的特征表示学习来识别图像对之间的篡改内容。虽然自监督学习（SSL）已经提升了ICD系统的性能，但现有的视图级对比方法由于缺乏细粒度的对应学习，难以处理复杂的编辑。我们通过两种关键创新解决了这一限制。首先，我们提出PixTrace——一个像素坐标跟踪模块，能够在编辑变换中保持明确的空间映射。其次，我们引入了CopyNCE，这是一种几何引导的对比损失，使用PixTrace验证的映射得出的重叠比来正则化块亲和性。我们的方法将像素级的可追踪性与块级的相似性学习相结合，抑制了SSL训练中的监督噪声。广泛的实验不仅展示了最先进的性能（匹配器88.7% uAP / 83.9% RP90，描述符72.6% uAP / 68.4% RP90，DISC21数据集），还展示了比现有方法更好的可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of detecting copied pixels in image pairs by proposing PixTrace, a pixel coordinate tracking module, and CopyNCE, a geometrically-guided contrastive loss. These innovations enhance the robustness of feature representation learning, particularly for sophisticated edits. The method improves upon existing view-level contrastive methods by maintaining explicit spatial mappings and regularizing patch affinity. Experiments show that the proposed method achieves state-of-the-art performance on the DISC21 dataset and offers better interpretability compared to previous approaches.</div>
<div class="mono" style="margin-top:8px">论文通过提出像素坐标跟踪模块PixTrace和几何导向的对比损失CopyNCE，解决了图像中复制内容检测的挑战。这些创新增强了细粒度对应关系的学习，提高了图像复制检测的效果。该方法在DISC21数据集上取得了最先进的结果，匹配器的uAP为88.7%，RP90为83.9%，描述符的uAP为72.6%，RP90为68.4%，同时提供了比现有方法更好的可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models</div>
<div class="meta-line">Authors: Clemence Grislain, Hamed Rahimi, Olivier Sigaud, Mohamed Chetouani</div>
<div class="meta-line">First: 2025-09-19T15:19:38+00:00 · Latest: 2026-02-19T15:45:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16072v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.16072v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://clemgris.github.io/I-FailSense/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-conditioned robotic manipulation in open-world settings requires not only accurate task execution but also the ability to detect failures for robust deployment in real-world environments. Although recent advances in vision-language models (VLMs) have significantly improved the spatial reasoning and task-planning capabilities of robots, they remain limited in their ability to recognize their own failures. In particular, a critical yet underexplored challenge lies in detecting semantic misalignment errors, where the robot executes a task that is semantically meaningful but inconsistent with the given instruction. To address this, we propose a method for building datasets targeting Semantic Misalignment Failures detection, from existing language-conditioned manipulation datasets. We also present I-FailSense, an open-source VLM framework with grounded arbitration designed specifically for failure detection. Our approach relies on post-training a base VLM, followed by training lightweight classification heads, called FS blocks, attached to different internal layers of the VLM and whose predictions are aggregated using an ensembling mechanism. Experiments show that I-FailSense outperforms state-of-the-art VLMs, both comparable in size and larger, in detecting semantic misalignment errors. Notably, despite being trained only on semantic misalignment detection, I-FailSense generalizes to broader robotic failure categories and effectively transfers to other simulation environments and real-world with zero-shot or minimal post-training. The datasets and models are publicly released on HuggingFace (Webpage: https://clemgris.github.io/I-FailSense/).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>I-FailSense：基于视觉语言模型的通用机器人故障检测</div>
<div class="mono" style="margin-top:8px">开放世界中的语言条件化机器人操作不仅需要准确的任务执行，还需要具备检测故障的能力，以在真实环境中实现稳健部署。尽管近期视觉语言模型（VLMs）在空间推理和任务规划方面取得了显著进步，但在识别自身故障方面仍有限制。特别是，一个关键但尚未充分探索的挑战在于检测语义对齐错误，即机器人执行的任务在语义上是有意义的，但与给定指令不一致。为解决这一问题，我们提出了一种方法，从现有的语言条件化操作数据集中构建针对语义对齐错误检测的数据集。我们还介绍了I-FailSense，一个专为故障检测设计的开源VLM框架，具有基于事实的仲裁。我们的方法依赖于在基础VLM上进行后训练，然后训练轻量级分类头，称为FS块，将其附加到VLM的不同内部层，并使用集成机制聚合其预测。实验表明，I-FailSense在检测语义对齐错误方面优于现有VLM，无论是大小相当还是更大的模型。值得注意的是，尽管仅在语义对齐检测上进行训练，I-FailSense仍能泛化到更广泛的机器人故障类别，并有效转移到其他模拟环境和真实世界中，无需或只需少量后训练。数据集和模型已在HuggingFace上公开发布（网址：https://clemgris.github.io/I-FailSense/）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the need for robots to detect their own failures in open-world settings, particularly semantic misalignment errors, by proposing I-FailSense, an open-source VLM framework. The method involves post-training a base VLM and adding lightweight classification heads, called FS blocks, which are aggregated using an ensembling mechanism. Experiments show that I-FailSense outperforms state-of-the-art VLMs in detecting semantic misalignment errors and generalizes well to other failure categories and environments with minimal post-training.</div>
<div class="mono" style="margin-top:8px">论文旨在解决机器人在开放环境中进行操作时需要检测自身错误，特别是语义不匹配错误的问题。提出了一种开源VLM框架I-FailSense，该框架在基础VLM上进行后训练，并添加了轻量级分类头来检测这些错误。实验表明，I-FailSense在检测语义不匹配错误方面优于现有VLM，并且在其他错误类别和环境中具有良好的泛化能力，只需少量训练即可实现迁移。</div>
</details>
</div>
<div class="card">
<div class="title">2Mamba2Furious: Linear in Complexity, Competitive in Accuracy</div>
<div class="meta-line">Authors: Gabriel Mongaras, Eric C. Larson</div>
<div class="meta-line">First: 2026-02-19T13:45:23+00:00 · Latest: 2026-02-19T13:45:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>2Mamba2Furious: 线性在复杂度上，竞争在准确度上</div>
<div class="mono" style="margin-top:8px">线性注意力变换器由于其高效性已成为softmax注意力的强有力替代品。然而，线性注意力在表达能力上较弱，导致准确度低于softmax注意力。为了弥合softmax注意力和线性注意力之间的准确度差距，我们对Mamba-2这一非常强大的线性注意力变体进行了操作。我们首先将Mamba-2简化为其最基本和最重要的组成部分，评估哪些具体选择使其最准确。从简化后的Mamba变体（Mamba-2S）中，我们改进了A-掩码并增加了隐藏状态的阶数，从而提出了一种名为2Mamba的方法，该方法在准确度上几乎与softmax注意力相当，但在长上下文长度下具有更高的内存效率。我们还研究了有助于超越softmax注意力准确度的Mamba-2元素。所有实验的代码均已提供</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the accuracy of linear attention transformers, which are more efficient than softmax attention but less expressive. By simplifying Mamba-2 to its core components and enhancing the A-mask and hidden state order, the authors developed 2Mamba, which achieves nearly the same accuracy as softmax attention while being much more memory-efficient for long context lengths. Key improvements include the A-mask and increased hidden state order, leading to competitive accuracy with reduced resource consumption.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提高线性注意力变压器的表达能力来增强其准确性，这些变压器与softmax注意力相比表达能力较低。研究简化了Mamba-2这一线性注意力变体的核心组件，并通过改进A-mask和增加隐藏状态的阶数，提出了2Mamba方法。该方法在长上下文长度下几乎与softmax注意力具有相同的准确性，但内存使用量大大减少。</div>
</details>
</div>
<div class="card">
<div class="title">Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises</div>
<div class="meta-line">Authors: Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbin Li, Yiming Li</div>
<div class="meta-line">First: 2025-04-30T15:21:25+00:00 · Latest: 2026-02-19T12:16:56+00:00</div>
<div class="meta-line">Comments: To appear in TIFS 2026. 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21730v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.21730v2">PDF</a> · <a href="https://github.com/NcepuQiaoTing/Cert-SSB">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample&#x27;s certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cert-SSBD: 认证样本特定平滑噪声的后门防御认证</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）容易受到后门攻击的影响，攻击者通过操纵一小部分训练数据植入隐藏的后门。受攻击的模型在干净样本上表现正常，但在后门样本上将其错误分类为攻击者指定的目标类别，对实际应用中的DNN构成了重大威胁。目前，已经提出了几种经验防御方法来缓解后门攻击，但这些方法往往被更先进的后门技术绕过。相比之下，基于随机平滑的认证防御显示出前景，通过向训练和测试样本添加随机噪声来对抗后门攻击。在本文中，我们揭示了现有的随机平滑防御隐含地假设所有样本与决策边界等距，但在实践中这可能不成立，导致认证性能不佳。为解决这一问题，我们提出了一种样本特定的认证后门防御方法，称为Cert-SSB。Cert-SSB首先使用随机梯度上升优化每个样本的噪声幅度，确保样本特定的噪声水平，然后应用于多个受污染的训练集以重新训练多个平滑模型。之后，Cert-SSB聚合多个平滑模型的预测生成最终的鲁棒预测。特别是，在这种情况下，现有的认证方法变得不适用，因为优化的噪声在不同样本之间变化。为了克服这一挑战，我们引入了一种基于存储更新的认证方法，该方法动态调整每个样本的认证区域以提高认证性能。我们在多个基准数据集上进行了广泛的实验，证明了我们提出方法的有效性。我们的代码可在https://github.com/NcepuQiaoTing/Cert-SSB/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of deep neural networks to backdoor attacks by proposing Cert-SSB, a sample-specific certified backdoor defense method. It optimizes noise magnitude for each sample using stochastic gradient ascent and applies this noise to multiple poisoned training sets to retrain smoothed models. The method then aggregates predictions from these models to generate a robust prediction. The authors introduce a storage-update-based certification method to handle varying noise across samples, improving certification performance. Experiments on benchmark datasets show the effectiveness of Cert-SSB in defending against backdoor attacks.</div>
<div class="mono" style="margin-top:8px">本文提出了一种样本特定的认证后门防御方法Cert-SSB，通过为每个样本优化噪声幅度并应用到多个受污染的训练集上重新训练平滑模型，然后聚合它们的预测。作者引入了一种存储更新基的认证方法来处理样本间噪声变化的问题，从而提高认证性能。在基准数据集上的实验表明了Cert-SSB的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment</div>
<div class="meta-line">Authors: Han Zhao, Jingbo Wang, Wenxuan Song, Shuai Chen, Yang Liu, Yan Wang, Haoang Li, Donglin Wang</div>
<div class="meta-line">First: 2026-02-19T11:00:46+00:00 · Latest: 2026-02-19T11:00:46+00:00</div>
<div class="meta-line">Comments: Project Website: https://h-zhao1997.github.io/frappe</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://h-zhao1997.github.io/frappe">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FRAPPE：通过多未来表示对齐将世界建模融入通用政策</div>
<div class="mono" style="margin-top:8px">使VLA模型能够预测环境动态，即世界建模，已被认为是提高机器人推理和泛化能力的关键。然而，当前的方法面临两个主要问题：1. 训练目标迫使模型过度强调像素级重建，限制了语义学习和泛化；2. 推理过程中依赖预测的未来观察会导致误差累积。为了解决这些挑战，我们提出了未来表示对齐通过并行渐进扩展（FRAPPE）。我们的方法采用两阶段微调策略：在中期训练阶段，模型学习预测未来观察的潜在表示；在后期训练阶段，我们并行扩展计算负载并同时与多个不同的视觉基础模型对齐表示。通过显著提高微调效率并减少对标注动作数据的依赖，FRAPPE提供了一种可扩展且数据高效的途径，以增强通用机器人政策的世界意识。在RoboTwin基准测试和实际任务上的实验表明，FRAPPE优于现有最佳方法，并在长时序和未见过的场景中表现出强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FRAPPE addresses the limitations of current world modeling approaches by introducing a two-stage fine-tuning strategy. In the mid-training phase, the model learns to predict the latent representations of future observations, and in the post-training phase, it aligns these representations with multiple visual foundation models. This method improves fine-tuning efficiency and reduces the need for action-annotated data, leading to better generalization in long-horizon and unseen scenarios compared to state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">FRAPPE通过引入两阶段微调策略来解决当前VLA模型的限制。中期训练阶段，模型学习预测未来潜在表示；后期阶段，扩展计算工作量并同时与多个视觉基础模型对齐表示。这种方法提高了微调效率，减少了对标注动作数据的依赖，从而增强了机器人政策的世界意识。实验表明，FRAPPE在长时序和未见过的任务中表现出色，优于现有方法并具有较强的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place</div>
<div class="meta-line">Authors: Antonio Rapuano, Yaolei Shen, Federico Califano, Chiara Gabellieri, Antonio Franchi</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-19T09:38:32+00:00 · Latest: 2026-02-19T09:38:32+00:00</div>
<div class="meta-line">Comments: Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17199v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>悬索的连续和混合动力学的非线性预测控制及其在空中抓取和放置中的应用</div>
<div class="mono" style="margin-top:8px">本文提出了一种框架，用于基于偏微分方程（PDEs）的高保真模型与适合实时控制的降阶表示相结合的空中操作可伸缩悬索的方法。PDEs使用有限差分法离散化，并采用适当的正交分解提取降阶模型（ROM），以保留主导变形模式并显著降低计算复杂性。基于此ROM，提出了一种非线性模型预测控制方案，能够稳定悬索振荡并处理混合过渡，如载荷的附着和脱离。仿真结果证实了ROM的稳定性和效率，以及控制器在各种操作条件下调节悬索动力学的有效性。附加仿真展示了ROM在受限环境中的轨迹规划应用，证明了所提方法的灵活性。总体而言，该框架使装有悬垂柔性悬索的无人驾驶航空器（UAV）能够实现实时、动力学感知的控制。</div>
</details>
</div>
<div class="card">
<div class="title">The Bots of Persuasion: Examining How Conversational Agents&#x27; Linguistic Expressions of Personality Affect User Perceptions and Decisions</div>
<div class="meta-line">Authors: Uğur Genç, Heng Gu, Chadha Degachi, Evangelos Niforatos, Senthil Chandrasegaran, Himanshu Verma</div>
<div class="meta-line">First: 2026-02-19T09:10:41+00:00 · Latest: 2026-02-19T09:10:41+00:00</div>
<div class="meta-line">Comments: Accepted to be presented at CHI&#x27;26 in Barcelona</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA&#x27;s composite personality did not affect participants&#x27; decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>说服性的机器人：探讨对话代理的语言个性表达如何影响用户感知和决策</div>
<div class="mono" style="margin-top:8px">由大型语言模型驱动的对话代理（CAs）越来越能够通过语言表现出复杂的人格特质，但这些表现如何影响用户尚不清楚。因此，我们研究了语言上表达的CA人格如何影响用户在慈善捐赠情境下的决策和感知。在一项众包研究中，360名参与者与八个CAs之一互动，每个CA表现出由三种语言方面组成的人格特质：态度（乐观/悲观）、权威（权威/顺从）和推理（情感/理性）。虽然CA的整体人格特质并未影响参与者的决策，但它确实影响了他们的感知和情绪反应。特别是，与悲观CA互动的参与者感到情绪状态较低，对活动的认同感较低，认为CA不够可信和不那么有能力，但倾向于向慈善机构捐款更多。信任感、能力和情境同理心的感知显著预测了捐款决策。我们的研究强调了CA作为操纵工具的风险，它们会微妙地影响用户的感知和决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how conversational agents&#x27; linguistic expressions of personality impact user perceptions and decisions in the context of charitable giving. Participants interacted with eight CAs, each projecting a unique personality through three linguistic aspects: attitude, authority, and reasoning. While the CA&#x27;s personality did not affect donation decisions, it significantly influenced users&#x27; perceptions and emotional responses. Users felt less affinity towards the cause and perceived the CA as less trustworthy and competent when interacting with pessimistic CAs, yet these CAs still received more donations. The study highlights the potential risks of CAs manipulating user perceptions and decisions through subtle linguistic cues.</div>
<div class="mono" style="margin-top:8px">本研究探讨了对话代理通过语言表达个性如何影响用户在慈善捐赠中的感知和决策。参与者与八个具有不同个性的对话代理互动，这些个性通过态度、权威和推理三个方面体现。虽然对话代理的个性没有影响捐赠决策，但它显著影响了用户的感知和情感反应。与悲观对话代理互动的用户感到情感上更不投入，认为对话代理不够可信和有能力，但更倾向于捐款。信任、能力和情境同理心是预测捐赠决策的关键因素。研究强调了对话代理作为潜在操纵工具，可以微妙地影响用户感知和决策的风险。</div>
</details>
</div>
<div class="card">
<div class="title">Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy</div>
<div class="meta-line">Authors: Huishi Huang, Jack Klusmann, Haozhe Wang, Shuchen Ji, Fengkang Ying, Yiyuan Zhang, John Nassour, Gordon Cheng, Daniela Rus, Jun Liu, Marcelo H Ang, Cecilia Laschi</div>
<div class="meta-line">First: 2026-02-19T06:56:47+00:00 · Latest: 2026-02-19T06:56:47+00:00</div>
<div class="meta-line">Comments: Camera-ready version for RoboSoft 2026. 8 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system&#x27;s response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理人机交互在增强现实中的抓取应用：刚柔机器人协同</div>
<div class="mono" style="margin-top:8px">混合刚柔机器人结合了刚性操作臂的精确性和柔性臂的顺应性和适应性，为不规则环境中的多功能抓取提供了有前景的方法。然而，协调混合机器人仍然具有挑战性，由于建模、感知和跨域运动学的困难。在本文中，我们提出了一种新颖的基于增强现实(AR)的物理人机交互框架，该框架使用户能够直接远程操作混合刚柔机器人执行简单的接近和抓取任务。通过AR头显，用户可以与集成在通用物理引擎中的机器人系统模拟模型进行交互，该物理引擎叠加在真实系统上，允许在实际部署之前进行模拟执行。为了确保虚拟和物理机器人之间的一致行为，我们引入了一种基于软机器人固有几何特性的实测参数识别管道，使我们能够准确地建模其静态和动态行为以及控制系统响应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents an augmented reality-based framework for physical human-robot interaction to facilitate the teleoperation of a hybrid rigid-soft robot for grasping tasks. The system uses an AR headset to allow users to interact with a simulated model of the robot, which is integrated into a physics engine and superimposed on the real system. A real-to-simulation parameter identification pipeline is introduced to ensure consistent behavior between the virtual and physical robots, enabling accurate modeling of the soft robot&#x27;s static and dynamic behavior and the control system&#x27;s response. Key findings include successful demonstration of the framework for simple reaching and grasping tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于增强现实的物理人机交互框架，以协助操作混合刚柔机器人执行抓取任务。系统使用AR头显让用户与集成到物理引擎中的机器人模拟模型进行交互，并将其叠加在真实系统上。引入了一个从现实到模拟的参数识别管道，以确保虚拟和物理机器人的行为一致，从而准确建模软机器人的静态和动态行为以及控制系统响应。主要发现包括成功演示了该框架用于简单的抓取任务。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success</div>
<div class="meta-line">Authors: Varun Burde, Pavel Burget, Torsten Sattler</div>
<div class="meta-line">First: 2026-02-19T05:55:01+00:00 · Latest: 2026-02-19T05:55:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17101v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17101v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物体姿态估计与重建对机器人抓取成功率影响的基准测试</div>
<div class="mono" style="margin-top:8px">3D重建是许多机器人感知任务的基础层，包括6D物体姿态估计和抓取姿态生成。现代物体的3D重建方法可以从多视角图像中生成视觉和几何上令人印象深刻的网格，但标准的几何评估并不能反映重建质量如何影响下游任务，如机器人操作性能。本文通过引入一个大规模的基于物理的基准测试来填补这一空白，该基准测试根据其在抓取中的功能有效性评估6D姿态估计器和3D网格模型。我们通过在各种重建的3D网格上生成抓取并执行它们在真实模型上的操作，分析模型保真度的影响，模拟使用不完美的模型生成的抓取姿态如何影响与真实物体的交互。这评估了从3D重建中产生的姿态误差、抓取鲁棒性和几何不准确性对抓取性能的综合影响。我们的结果表明，重建伪影显著减少了抓取姿态候选的数量，但在姿态准确估计的情况下，对抓取性能的影响微乎其微。我们的结果还表明，抓取成功与姿态误差之间的关系主要由空间误差主导，即使是简单的平移误差也能揭示对称物体抓取姿态的成功。本文为如何通过机器人进行物体操作的感知系统提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper benchmarks the effects of object pose estimation and reconstruction on robotic grasping success by introducing a large-scale, physics-based evaluation. It analyzes how reconstruction quality impacts grasping performance through the generation and execution of grasps on various 3D meshes, revealing that while reconstruction artifacts reduce the number of grasp candidates, accurate pose estimation minimizes their impact on grasping success. The study also finds that spatial errors are more critical than rotational errors for grasping symmetric objects.</div>
<div class="mono" style="margin-top:8px">该论文通过引入大规模的物理基准评估物体姿态估计和重建对机器人抓取成功率的影响。它分析了模型保真度对抓取生成和执行的影响，结果显示虽然重建中的缺陷会减少抓取候选数，但准确的姿态估计可以最大限度地减少其对抓取性能的影响。研究发现，空间误差比姿态误差对抓取成功率的影响更大，尤其是对对称物体而言。</div>
</details>
</div>
<div class="card">
<div class="title">RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation</div>
<div class="meta-line">Authors: Yixue Zhang, Kun Wu, Zhi Gao, Zhen Zhao, Pei Ren, Zhiyuan Xu, Fei Liao, Xinhua Wang, Shichao Fan, Di Wu, Qiuxuan Feng, Meng Li, Zhengping Che, Chang Liu, Jian Tang</div>
<div class="meta-line">First: 2026-02-18T13:29:43+00:00 · Latest: 2026-02-19T04:26:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16444v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robogene-boost-vla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboGene：通过多样性驱动的代理框架增强VLA预训练</div>
<div class="mono" style="margin-top:8px">通用机器人操作的追求受到多样性和真实世界交互数据稀缺性的阻碍。与视觉或语言中的网页数据收集不同，机器人数据收集是一个涉及高昂物理成本的主动过程。因此，自动化任务策展以最大化数据价值仍然是一个关键但尚未充分探索的挑战。现有的手动方法不可扩展且偏向于常见任务，而现成的基础模型往往会产生物理上不可行的指令。为了解决这个问题，我们引入了RoboGene，这是一种代理框架，旨在自动化生成单臂、双臂和移动机器人广泛物理可行的操作任务。RoboGene 结合了三个核心组件：多样性驱动的采样以实现广泛的任务覆盖、自我反思机制以强制执行物理约束以及人工在环改进以持续改进。我们进行了广泛的定量分析和大规模真实世界实验，收集了18000个轨迹的数据集，并引入了新的指标来评估任务的质量、可行性和多样性。结果表明，RoboGene 显著优于最先进的基础模型（例如GPT-4o、Gemini 2.5 Pro）。此外，真实世界实验表明，使用RoboGene预训练的VLA模型在成功率和泛化能力上表现更优，突显了高质量任务生成的重要性。我们的项目可在https://robogene-boost-vla.github.io/找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboGene is an agentic framework that automates the generation of diverse, physically plausible manipulation tasks for robotic manipulation. It integrates diversity-driven sampling, self-reflection mechanisms, and human-in-the-loop refinement. Extensive experiments and real-world testing show that RoboGene outperforms existing methods and leads to higher success rates and better generalization in VLA models pre-training.</div>
<div class="mono" style="margin-top:8px">RoboGene 是一个自动化框架，用于生成多样且物理上可行的机器人操作任务。它结合了多样性的采样、自我反思机制和人工在环改进。大量实验和实地测试表明，RoboGene 超越了现有方法，并在 VLA 模型预训练中实现了更高的成功率和更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation</div>
<div class="meta-line">Authors: Yejin Kim, Wilbert Pumacay, Omar Rayyan, Max Argus, Winson Han, Eli VanderBilt, Jordi Salvador, Abhay Deshpande, Rose Hendrix, Snehal Jauhri, Shuo Liu, Nur Muhammad Mahi Shafiullah, Maya Guru, Ainaz Eftekhar, Karen Farley, Donovan Clay, Jiafei Duan, Arjun Guru, Piper Wolters, Alvaro Herrasti, Ying-Chun Lee, Georgia Chalvatzaki, Yuchen Cui, Ali Farhadi, Dieter Fox, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-11T20:16:31+00:00 · Latest: 2026-02-19T00:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11337v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.11337v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolmoSpaces：大规模开放生态系统，用于机器人导航和操作</div>
<div class="mono" style="margin-top:8px">大规模部署机器人需要应对日常情况的长尾效应。现有机器人基准在场景布局、物体几何形状和任务规范的多样性方面存在不足。衡量这种泛化能力需要在物理评估无法提供的规模和多样性上建立基础设施。我们引入了MolmoSpaces，一个完全开放的生态系统，用于支持大规模机器人策略基准测试。MolmoSpaces包含超过23万个多样化的室内环境，从手工制作的家庭场景到程序生成的多房间房屋，拥有13万个丰富的注释物体资产，包括4.8万个可操作物体及其4200万个稳定抓取。这些环境对模拟器不具依赖性，支持MuJoCo、Isaac和ManiSkill等流行选项。该生态系统支持所有类型的实体任务：静态和移动操作、导航以及需要跨整个室内环境协调感知、规划和交互的多房间长期任务。我们还设计了MolmoSpaces-Bench，一个包含8个任务的基准套件，机器人与我们的多样化场景和丰富注释物体进行交互。我们的实验表明，MolmoSpaces-Bench在模拟到现实的关联性上表现出色（R=0.96，ρ=0.98），确认了更新且更强的零样本策略在我们的基准测试中优于早期版本，并识别了提示措辞、初始关节位置和相机遮挡的关键敏感性。通过MolmoSpaces及其开源资产和工具，我们为机器人学习研究提供了可扩展的数据生成、策略训练和基准创建的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MolmoSpaces is designed to support large-scale benchmarking of robot policies by providing a diverse ecosystem of over 230k indoor environments and 130k annotated object assets. This system, simulator-agnostic and covering a wide range of tasks from manipulation to navigation, includes MolmoSpaces-Bench, a benchmark suite of 8 tasks. Experiments show strong sim-to-real correlation and improved zero-shot policies, highlighting key sensitivities to prompt phrasing, initial joint positions, and camera occlusion.</div>
<div class="mono" style="margin-top:8px">MolmoSpaces旨在解决机器人在多样化真实环境中的鲁棒性问题，包含超过23万个室内环境和13万个标注物体资产，支持多种机器人任务。该生态系统对模拟器兼容，并包括一个基准套件MolmoSpaces-Bench，包含8个任务。实验显示了强仿真实验关联性和零样本策略的改进，并指出了关键的敏感性，如提示措辞和初始关节位置。</div>
</details>
</div>
<div class="card">
<div class="title">Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement</div>
<div class="meta-line">Authors: Minku Kim, Kuan-Chia Chen, Aayam Shrestha, Li Fuxin, Stefan Lee, Alan Fern</div>
<div class="meta-line">First: 2026-02-14T19:11:02+00:00 · Latest: 2026-02-18T23:55:04+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13850v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人哈诺伊：探究基于技能的整体身体控制技能组合方法</div>
<div class="mono" style="margin-top:8px">我们研究了一种基于技能的框架，用于类人方块重组，该框架通过在任务级别按顺序使用可重用技能来实现长期执行。在我们的架构中，所有技能都通过一个共享的任务无关的整体身体控制器（WBC）执行，提供了一致的闭环接口用于技能组合，而不同于使用每个技能的单独低级控制器的非共享设计。我们发现，简单地重复使用相同的预训练WBC在长期执行中会降低鲁棒性，因为新技能及其组合会诱导状态和命令分布的变化。我们通过一个简单的数据聚合程序解决了这个问题，该程序通过在域随机化下闭环技能执行的回放来增强共享-WBC的训练。为了评估该方法，我们引入了“类人哈诺伊”长周期的塔式方块重组基准，并在模拟和Digit V3类人机器人上报告了结果，展示了完全自主的长期重组，并量化了共享-WBC方法相对于非共享基线的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates a skill-based framework for humanoid box rearrangement, using a shared whole-body controller (WBC) to enable long-horizon execution by sequencing reusable skills. The study finds that reusing the same pretrained WBC can reduce robustness over long horizons due to shifted state and command distributions from new skills and their compositions. To address this, the authors propose a data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. The approach is evaluated on a long-horizon Tower-of-Hanoi box rearrangement benchmark, showing fully autonomous rearrangement over extended horizons and demonstrating the benefits of the shared-WBC approach over non-shared baselines.</div>
<div class="mono" style="margin-top:8px">研究提出了一种基于技能的人形盒子重新排列框架，利用共享的整体身体控制器（WBC）通过按顺序使用可重用技能来实现长时间执行。通过引入数据聚合程序来解决同一预训练WBC在长时间执行中的鲁棒性降低问题。该方法使用Humanoid Hanoi基准进行评估，展示了在长时间范围内实现自主重新排列的成功，并强调了共享WBC方法相对于非共享基线的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming</div>
<div class="meta-line">Authors: Philip Sosnin, Jodie Knapp, Fraser Kennedy, Josh Collyer, Calvin Tsay</div>
<div class="meta-line">First: 2026-02-18T23:18:45+00:00 · Latest: 2026-02-18T23:18:45+00:00</div>
<div class="meta-line">Comments: Accepted to the 23rd International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16944v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16944v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces a verification framework that provides both sound and complete guarantees for data poisoning attacks during neural network training. We formulate adversarial data manipulation, model training, and test-time evaluation in a single mixed-integer quadratic programming (MIQCP) problem. Finding the global optimum of the proposed formulation provably yields worst-case poisoning attacks, while simultaneously bounding the effectiveness of all possible attacks on the given training pipeline. Our framework encodes both the gradient-based training dynamics and model evaluation at test time, enabling the first exact certification of training-time robustness. Experimental evaluation on small models confirms that our approach delivers a complete characterization of robustness against data poisoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用混合整数规划的确切认证数据投毒攻击</div>
<div class="mono" style="margin-top:8px">本研究引入了一种验证框架，为神经网络训练期间的数据投毒攻击提供既准确又完整的保证。我们将对抗性数据操纵、模型训练和测试时评估统一在一个混合整数二次规划（MIQCP）问题中。找到所提出形式的全局最优解可以证明产生最坏情况的投毒攻击，同时同时限制所有可能的攻击在给定训练管道中的有效性。我们的框架编码了基于梯度的训练动力学以及测试时的模型评估，使我们能够首次实现训练时鲁棒性的精确认证。实验评估表明，我们的方法可以完全表征数据投毒攻击下的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work presents a verification framework using mixed-integer quadratic programming to provide exact certification of data-poisoning attacks during neural network training. The framework combines adversarial data manipulation, model training, and test-time evaluation into a single optimization problem. The approach yields provably worst-case poisoning attacks and bounds the effectiveness of all possible attacks on the training pipeline. Experiments on small models show that the method offers a complete characterization of robustness against data poisoning.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种通过将数据投毒攻击问题形式化为混合整数二次规划（MIQCP）问题的验证框架。该框架提供了完备的保证，能够找到最坏情况的数据投毒攻击，并限制所有可能攻击的有效性。它同时编码了训练动态和测试时的评估，使得首次实现了训练时鲁棒性的精确认证。实验结果表明，该方法能够完全表征小模型对数据投毒的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reinforcement Learning-Based Locomotion for Resource-Constrained Quadrupeds with Exteroceptive Sensing</div>
<div class="meta-line">Authors: Davide Plozza, Patricia Apostol, Paul Joseph, Simon Schläpfer, Michele Magno</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2025-05-18T20:29:23+00:00 · Latest: 2026-02-18T22:43:05+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at the IEEE International Conference on Robotics and Automation (ICRA), Atlanta 2025. The code is available at github.com/ETH-PBL/elmap-rl-controller</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12537v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12537v2">PDF</a> · <a href="http://github.com/ETH-PBL/elmap-rl-controller">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compact quadrupedal robots are proving increasingly suitable for deployment in real-world scenarios. Their smaller size fosters easy integration into human environments. Nevertheless, real-time locomotion on uneven terrains remains challenging, particularly due to the high computational demands of terrain perception. This paper presents a robust reinforcement learning-based exteroceptive locomotion controller for resource-constrained small-scale quadrupeds in challenging terrains, which exploits real-time elevation mapping, supported by a careful depth sensor selection. We concurrently train both a policy and a state estimator, which together provide an odometry source for elevation mapping, optionally fused with visual-inertial odometry (VIO). We demonstrate the importance of positioning an additional time-of-flight sensor for maintaining robustness even without VIO, thus having the potential to free up computational resources. We experimentally demonstrate that the proposed controller can flawlessly traverse steps up to 17.5 cm in height and achieve an 80% success rate on 22.5 cm steps, both with and without VIO. The proposed controller also achieves accurate forward and yaw velocity tracking of up to 1.0 m/s and 1.5 rad/s respectively. We open-source our training code at github.com/ETH-PBL/elmap-rl-controller.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于环境感知的资源受限四足机器人鲁棒强化学习驱动行走</div>
<div class="mono" style="margin-top:8px">紧凑型四足机器人在实际应用场景中越来越适合部署。它们较小的尺寸促进了与人类环境的轻松集成。然而，在不平坦地形上的实时行走仍然具有挑战性，尤其是由于地形感知的高计算需求。本文提出了一种适用于资源受限的小型四足机器人在复杂地形上的鲁棒强化学习驱动的环境感知行走控制器，该控制器利用实时高程映射，并通过仔细选择深度传感器加以支持。我们同时训练了一个策略和一个状态估计器，它们一起提供高程映射的里程计来源，可选地与视觉惯性里程计（VIO）融合。我们证明了额外放置一个飞行时间传感器对于保持鲁棒性的重要性，即使没有VIO，也能释放计算资源。实验表明，所提出的控制器可以完美地跨越高达17.5厘米的台阶，并在有和没有VIO的情况下，22.5厘米台阶的成功率达到80%。所提出的控制器还实现了高达1.0米/秒的前向和1.5弧度/秒的偏航速度跟踪。我们在github.com/ETH-PBL/elmap-rl-controller开源了训练代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of real-time locomotion for small quadruped robots on uneven terrains by developing a robust reinforcement learning-based exteroceptive locomotion controller. The method combines real-time elevation mapping using a carefully selected depth sensor and concurrent training of a policy and state estimator. Experimental results show that the controller can successfully navigate steps up to 22.5 cm in height, with an 80% success rate on 22.5 cm steps, and achieve accurate velocity tracking of up to 1.0 m/s and 1.5 rad/s. Additionally, the controller maintains robustness even without visual-inertial odometry, freeing up computational resources.</div>
<div class="mono" style="margin-top:8px">本文通过开发一种基于强化学习的外部感知控制器，解决了小型四足机器人在不平地形上实时行走的挑战。该方法涉及训练一个策略和一个状态估计器，使用实时高程映射，并由精心选择的深度传感器支持。控制器在有和没有视觉惯性里程计的情况下，成功跨越了高达17.5厘米的台阶，并在22.5厘米的台阶上实现了80%的成功率。它还实现了最高1.0米/秒的前向和1.5弧度/秒的偏航速度跟踪。额外的时间飞行传感器在没有视觉惯性里程计的情况下增强了鲁棒性，从而释放了计算资源。</div>
</details>
</div>
<div class="card">
<div class="title">SparTa: Sparse Graphical Task Models from a Handful of Demonstrations</div>
<div class="meta-line">Authors: Adrian Röfer, Nick Heppert, Abhinav Valada</div>
<div class="meta-line">First: 2026-02-18T21:54:35+00:00 · Latest: 2026-02-18T21:54:35+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16911v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16911v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method&#x27;s demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SparTa: 稀疏图形任务模型从少量演示中学习</div>
<div class="mono" style="margin-top:8px">高效学习长期操作任务是机器人从演示学习中的一个核心挑战。与最近专注于直接在动作域中学习任务的努力不同，我们关注的是推断机器人在任务中应实现什么，而不是如何实现。为此，我们使用一系列图形对象关系来表示不断变化的场景状态。我们提出了一种演示分割和聚合方法，提取一系列操作图，并估计任务各阶段的对象状态分布。与仅捕捉部分交互或短暂时间窗口的先前基于图的方法不同，我们的方法捕捉从控制开始到操作结束的完整对象交互。为了在学习多个演示时提高鲁棒性，我们还使用预训练的视觉特征进行对象匹配。在广泛的实验中，我们评估了我们方法的演示分割准确性以及从多个演示中学习找到所需最小任务模型的实用性。最后，我们在仿真和真实机器人上部署了拟合的模型，证明了由此产生的任务表示支持跨环境的可靠执行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to efficiently learn long-horizon manipulation tasks from a few demonstrations by focusing on inferring the robot&#x27;s goals rather than the specific actions. The method involves representing scene states with graphical object relationships and using a demonstration segmentation and pooling approach to extract manipulation graphs and estimate object state distributions. Experiments show that this approach improves segmentation accuracy and the robustness of learning from multiple demonstrations, leading to reliable task execution in various environments.</div>
<div class="mono" style="margin-top:8px">研究旨在通过聚焦于推断机器人的目标而非具体动作，从少量演示中高效学习长时操作任务。方法包括用图形对象关系表示场景状态，并使用演示分割和聚合方法提取操作图和估计对象状态分布。实验表明，这种方法提高了分割准确性，并增强了从多个演示中学习的鲁棒性，从而在不同环境中实现可靠的执行任务。</div>
</details>
</div>
<div class="card">
<div class="title">SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation</div>
<div class="meta-line">Authors: Kushal Kedia, Tyler Ga Wei Lum, Jeannette Bohg, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-18T20:42:39+00:00 · Latest: 2026-02-18T20:42:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16863v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimToolReal：一种面向对象的零样本灵巧工具操作策略</div>
<div class="mono" style="margin-top:8px">操作工具的能力显著扩展了机器人可以执行的任务集。然而，工具操作代表了一类具有挑战性的灵巧性，需要抓取细长物体、手持物体旋转以及进行有力的交互。由于收集这些行为的遥操作数据具有挑战性，因此模拟到现实的强化学习（RL）是一种有前途的替代方案。然而，先前的方法通常需要大量的工程努力来建模物体并调整每个任务的奖励函数。在本文中，我们提出了SimToolReal，朝着为工具操作生成通用的模拟到现实的RL策略迈出了一步。我们不是专注于单一物体和任务，而是通过模拟程序生成大量工具样物体素，并训练一个具有通用目标的单个RL策略，即操纵每个物体到随机目标姿态。这种方法使SimToolReal能够在测试时无需任何物体或任务特定的训练即可执行通用灵巧工具操作。我们证明SimToolReal在性能上比先前的重新瞄准和固定抓取方法高出37%，并且与在特定目标物体和任务上训练的专家RL策略的性能相当。最后，我们展示了SimToolReal在一系列日常工具上具有泛化能力，在120个现实世界滚轮中实现了超过24个任务、12个物体实例和6个工具类别的零样本强性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SimToolReal, a method for sim-to-real reinforcement learning in tool manipulation tasks. It generates a variety of tool-like objects in simulation and trains a single policy to manipulate these objects to random goal poses. This approach allows the policy to perform dexterous tool manipulation without task-specific training. Experiments show that SimToolReal outperforms previous methods by 37% and achieves strong zero-shot performance across 120 real-world rollouts involving 24 tasks, 12 object instances, and 6 tool categories.</div>
<div class="mono" style="margin-top:8px">论文提出了SimToolReal，一种用于工具操作任务的模拟到现实的强化学习方法。该方法在模拟中生成多种工具样物体，并训练一个单一策略将任何物体移动到随机目标位置。这种方法允许策略在无需特定任务训练的情况下执行通用的灵巧工具操作。SimToolReal 的性能比之前的方法高出 37%，并在 120 个涉及 24 任务和 6 种工具类别的真实世界演示中实现了强大的零样本性能。</div>
</details>
</div>
<div class="card">
<div class="title">One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation</div>
<div class="meta-line">Authors: Zhenyu Wei, Yunchao Yao, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-18T18:59:57+00:00 · Latest: 2026-02-18T18:59:57+00:00</div>
<div class="meta-line">Comments: Project Page: https://zhenyuwei2003.github.io/OHRA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16712v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhenyuwei2003.github.io/OHRA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一掌统治一切：统一灵巧操作的规范表示</div>
<div class="mono" style="margin-top:8px">当前的灵巧操作策略大多假设固定的手部设计，严重限制了它们对具有不同运动学和结构布局的新实体的泛化能力。为克服这一限制，我们引入了一个参数化的规范表示，统一了广泛的灵巧手架构。它包括一个统一的参数空间和一个规范的URDF格式，提供三个关键优势。1) 参数空间捕捉了有效学习算法训练所需的形态和运动学变化。2) 可以在我们的空间中学习一个结构化的潜在流形，其中不同实体之间的插值会产生平滑且物理上合理的形态过渡。3) 规范的URDF标准化了动作空间，同时保留了原始URDF的动力学和功能特性，使跨实体学习策略变得高效可靠。我们通过广泛的分析和实验验证了这些优势，包括抓取策略回放、VAE潜在编码和跨实体零样本转移。具体而言，我们在统一表示上训练了一个VAE，以获得一个紧凑且语义丰富的潜在嵌入，并开发了一个基于规范表示的抓取策略，该策略在灵巧手之间具有泛化能力。我们通过模拟和在未见过的形态上的实际任务（例如，3指LEAP手的零样本成功率高达81.9%）证明，我们的框架统一了结构多样手的表示空间和动作空间，为跨手学习提供了可扩展的基础，以实现通用灵巧操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitation of fixed hand designs in dexterous manipulation policies by introducing a parameterized canonical representation that unifies various hand architectures. The method includes a unified parameter space and a canonical URDF format, which enhances learning efficiency and enables smooth transitions between different hand morphologies. Key experimental findings show that the proposed framework achieves 81.9% zero-shot success rate on 3-finger LEAP Hand tasks, demonstrating its capability to generalize across diverse hand designs.</div>
<div class="mono" style="margin-top:8px">本文针对当前灵巧操作策略主要针对固定手型设计的局限性，提出了一个参数化的统一表示方法，以统一各种手型架构。该方法提供了一个统一的参数空间和一个标准的URDF格式，关键优势包括捕捉形态和运动学变化、学习一个结构化的潜在流形以实现手型形态之间的平滑过渡、以及标准化动作空间同时保留动态特性。实验通过抓取策略回放、VAE潜在编码和跨手型零样本转移验证了这些优势，实现了在未见过的手型（如3指LEAP手）上达到很高的成功率，例如81.9%的零样本成功率。</div>
</details>
</div>
<div class="card">
<div class="title">EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data</div>
<div class="meta-line">Authors: Ruijie Zheng, Dantong Niu, Yuqi Xie, Jing Wang, Mengda Xu, Yunfan Jiang, Fernando Castañeda, Fengyuan Hu, You Liang Tan, Letian Fu, Trevor Darrell, Furong Huang, Yuke Zhu, Danfei Xu, Linxi Fan</div>
<div class="meta-line">First: 2026-02-18T18:59:05+00:00 · Latest: 2026-02-18T18:59:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoScale：通过多样化第一人称人类数据扩展灵巧操作</div>
<div class="mono" style="margin-top:8px">人类行为是学习物理智能最具扩展性的数据来源之一，但如何有效利用它来实现灵巧操作仍不清楚。尽管先前的工作在受限环境中展示了从人类到机器人的转移，但大规模人类数据是否能支持精细的、高自由度的灵巧操作尚不清楚。我们提出了EgoScale，这是一种基于大规模第一人称人类数据的从人类到灵巧操作的转移框架。我们在一个超过20,854小时的动作标注第一人称人类视频上训练了一个视觉语言动作（VLA）模型，数据量超过先前努力的20倍，并发现人类数据规模与验证损失之间存在对数线性关系。这种验证损失与下游真实机器人性能高度相关，确立了大规模人类数据作为可预测监督源的地位。除了规模，我们引入了一个简单的两阶段转移配方：大规模人类预训练后，进行轻量级的人机对齐中期训练。这使得在最少的机器人监督下实现强大的长时灵巧操作和一次性的任务适应成为可能。我们的最终策略在使用22个自由度的灵巧机器人手时，平均成功率提高了54%，并且能够有效地转移到自由度较低的机器人手上，表明大规模人类运动提供了可重复使用、与具体身体无关的运动先验知识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EgoScale is a framework that leverages large-scale egocentric human data to enable dexterous manipulation by robots. It trains a Vision Language Action model on over 20,854 hours of human action data, showing a log-linear relationship between data scale and validation loss, which correlates with robot performance. The method involves two stages: large-scale human pretraining and lightweight human-robot alignment, leading to improved success rates and effective transfer to robots with different degrees of freedom.</div>
<div class="mono" style="margin-top:8px">EgoScale 是一个框架，利用大规模的主观视角人类数据将精细操作技能转移到机器人上。通过在超过20,854小时的人类动作数据上训练Vision Language Action模型，研究人员发现数据规模与验证损失之间存在对数线性关系，这与机器人性能相关。该框架采用两阶段转移方法，结合大规模的人类预训练和轻量级的机器人对齐，实现了显著的精细操作改进和任务适应，同时减少了对机器人监督的需求。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</div>
<div class="meta-line">Authors: Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</div>
<div class="meta-line">First: 2026-02-18T18:55:02+00:00 · Latest: 2026-02-18T18:55:02+00:00</div>
<div class="meta-line">Comments: Project page: https://hero-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16705v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hero-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类人机器人开放词汇视觉移动物体末端执行器控制学习</div>
<div class="mono" style="margin-top:8px">使用类人机器人在野外对任意物体进行视觉移动物体操作需要精确的末端执行器（EE）控制和通过视觉输入（例如RGB-D图像）对场景的通用理解。现有方法基于现实世界的模仿学习，由于难以收集大规模训练数据集，因此表现出有限的泛化能力。本文提出了一种新的范式HERO，用于类人机器人物体移动物体操作，结合了大型视觉模型的强大泛化能力和开放词汇理解与模拟训练中的强大控制性能。我们通过设计一种准确的残差感知末端执行器跟踪策略来实现这一点。该末端执行器跟踪策略结合了经典机器人学与机器学习。它使用a) 逆运动学将残差末端执行器目标转换为参考轨迹，b) 用于准确前运动学的已学习神经前向模型，c) 目标调整，以及d) 重新规划。这些创新共同帮助我们将末端执行器跟踪误差降低了3.2倍。我们使用这种准确的末端执行器跟踪器构建了一个模块化移动物体系统，其中使用开放词汇大型视觉模型实现强大的视觉泛化。我们的系统能够在从办公室到咖啡馆等多样化的现实环境中操作，机器人能够可靠地操作各种日常物体（例如茶杯、苹果、玩具），这些物体位于43cm至92cm高度的表面上。在模拟和现实世界中的系统模块化和端到端测试表明我们提出的设计的有效性。我们认为本文中的进展可以为训练类人机器人与日常物体交互开辟新的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces HERO, a new paradigm for object manipulation by humanoid robots that combines strong generalization from large vision models with accurate end-effector control from simulated training. The method includes an accurate residual-aware end-effector tracking policy that integrates classical robotics with machine learning, reducing tracking error by 3.2x. The system successfully performs loco-manipulation in various real-world environments, manipulating objects like mugs and apples on surfaces ranging from 43cm to 92cm high, demonstrating effective visual generalization and control performance.</div>
<div class="mono" style="margin-top:8px">本文提出了HERO，一种新的框架，使类人机器人能够在多种环境中执行物体操作。该系统结合了大型视觉模型的强大泛化能力和通过残差感知末端执行器跟踪策略实现的精确控制。该策略结合了经典机器人技术和机器学习技术，包括逆运动学、神经前向模型、目标调整和重新规划。结果显著降低了末端执行器跟踪误差3.2倍，使机器人能够在办公室和咖啡馆等真实世界环境中可靠地操作各种物体。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to unfold cloth: Scaling up world models to deformable object manipulation</div>
<div class="meta-line">Authors: Jack Rome, Stephen James, Subramanian Ramamoorthy</div>
<div class="meta-line">First: 2026-02-18T18:14:41+00:00 · Latest: 2026-02-18T18:14:41+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习展开布料：将世界模型扩展到可变形物体操作</div>
<div class="mono" style="margin-top:8px">学习操作布料既是机器人研究中的一个典型问题，也是从辅助护理到服务业等多种应用中的一个直接相关问题。可变形物体的复杂物理特性使得布料操作问题变得非平凡。为了创建一个能够应对各种形状、大小、折叠和皱纹模式的通用操作策略，除了通常的外观变化问题，仔细考虑模型结构及其对泛化性能的影响变得至关重要。在本文中，我们提出了一种使用最近提出的强化学习架构DreamerV2变体的空中布料操作方法。我们的实现修改了该架构以利用表面法线输入，并修改了回放缓冲区和数据增强程序。这些修改共同代表了机器人所使用的世界模型的改进，解决了机器人操作的物体的物理复杂性。我们在模拟中进行了评估，并在物理机器人设置中进行了零样本部署，展示了对不同布料类型的空中展开，证明了我们提出架构的泛化优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of robotic cloth manipulation, which is crucial for various applications. The authors use a modified DreamerV2 reinforcement learning architecture to handle the complex physics of deformable objects. By incorporating surface normals and adjusting the replay buffer and data augmentation, they enhance the world model&#x27;s ability to generalize across different cloth types. The approach is evaluated in both simulation and a physical robot setup, successfully demonstrating the ability to unfold various cloths in-air, highlighting the generalization benefits of their method.</div>
<div class="mono" style="margin-top:8px">该论文解决了机器人布料操作这一既是机器人研究基本问题又在多种应用中具有重要意义的挑战。作者使用修改后的DreamerV2强化学习架构来处理可变形物体的复杂物理特性。通过引入表面法线并调整回放缓冲区和数据增强程序，他们增强了世界模型的泛化能力，使其能够处理不同类型的布料。该方法在模拟和物理机器人设置中进行了评估，成功展示了在未对特定形状或模式进行训练的情况下，能够展开各种布料类型的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Elements of Robot Morphology: Supporting Designers in Robot Form Exploration</div>
<div class="meta-line">Authors: Amy Koike, Serena Ge Guo, Xinning He, Callie Y. Kim, Dakota Sullivan, Bilge Mutlu</div>
<div class="meta-line">First: 2026-02-09T21:13:20+00:00 · Latest: 2026-02-18T17:41:00+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, Proceedings of the 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09203v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09203v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人形态学要素：支持设计师进行机器人形态探索</div>
<div class="mono" style="margin-top:8px">机器人形态，即机器人的形状、结构，是人机交互（HRI）中的关键设计空间，影响着机器人的功能、表达方式及其与人的互动。尽管其重要性不言而喻，但关于设计框架如何指导系统性形态探索的研究却很少。为填补这一空白，我们提出了机器人形态学要素框架，该框架识别出五个基本要素：感知、关节、末端执行器、运动方式和结构。该框架源自对现有机器人的分析，支持对多样化机器人形态的结构化探索。为了实现该框架，我们开发了形态探索模块（MEB），这是一种可触控的模块，能够促进对机器人形态的手动、协作性实验。我们通过案例研究和设计研讨会对该框架和工具包进行了评估，展示了它们如何支持分析、创意生成、反思以及协作机器人设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the lack of systematic guidance in exploring robot morphology, which is crucial for human-robot interaction. It introduces a framework called Elements of Robot Morphology, identifying five fundamental elements: perception, articulation, end effectors, locomotion, and structure. To operationalize this framework, the authors developed Morphology Exploration Blocks (MEB), tangible blocks for hands-on experimentation. The evaluation through case studies and design workshops demonstrated that the framework and toolkit support analysis, ideation, reflection, and collaborative robot design.</div>
<div class="mono" style="margin-top:8px">论文针对机器人形态探索缺乏系统指导的问题，提出了一个名为机器人形态元素的框架，识别了感知、运动、末端执行器、移动和结构五个基本元素。为了实现这一框架，作者开发了形态探索块（MEB），用于实物实验。通过案例研究和设计研讨会的评估表明，该框架和工具包支持分析、创意生成、反思和协作机器人设计。</div>
</details>
</div>
<div class="card">
<div class="title">FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency</div>
<div class="meta-line">Authors: Yifei Su, Ning Liu, Dong Chen, Zhen Zhao, Kun Wu, Meng Li, Zhiyuan Xu, Zhengping Che, Jian Tang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T14:12:53+00:00 · Latest: 2026-02-18T13:54:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08822v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08822v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative modeling-based visuomotor policies have been widely adopted in robotic manipulation, attributed to their ability to model multimodal action distributions. However, the high inference cost of multi-step sampling limits its applicability in real-time robotic systems. Existing approaches accelerate sampling in generative modeling-based visuomotor policies by adapting techniques originally developed to speed up image generation. However, a major distinction exists: image generation typically produces independent samples without temporal dependencies, while robotic manipulation requires generating action trajectories with continuity and temporal coherence. To this end, we propose FreqPolicy, a novel approach that first imposes frequency consistency constraints on flow-based visuomotor policies. Our work enables the action model to capture temporal structure effectively while supporting efficient, high-quality one-step action generation. Concretely, we introduce a frequency consistency constraint objective that enforces alignment of frequency-domain action features across different timesteps along the flow, thereby promoting convergence of one-step action generation toward the target distribution. In addition, we design an adaptive consistency loss to capture structural temporal variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53 tasks across 3 simulation benchmarks, proving its superiority over existing one-step action generators. We further integrate FreqPolicy into the vision-language-action (VLA) model and achieve acceleration without performance degradation on 40 tasks of LIBERO. Besides, we show efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreqPolicy: 基于频率一致性的高效流基视动策略</div>
<div class="mono" style="margin-top:8px">基于生成建模的视动策略在机器人操作中得到了广泛应用，这得益于它们能够建模多模态动作分布的能力。然而，多步采样的高推理成本限制了其在实时机器人系统中的应用。现有的方法通过适应原本用于加速图像生成的技术来加速基于生成建模的视动策略的采样。然而，一个主要的区别在于：图像生成通常产生独立的样本，没有时间依赖性，而机器人操作需要生成具有连续性和时间一致性的动作轨迹。为此，我们提出了一种名为FreqPolicy的新方法，首先在流基视动策略上施加频率一致性约束。我们的工作使动作模型能够有效地捕捉时间结构，同时支持高效、高质量的一步动作生成。具体而言，我们引入了一个频率一致性约束目标，该目标强制频率域动作特征在流的不同时间步之间对齐，从而促进一步动作生成向目标分布收敛。此外，我们设计了一种自适应一致性损失来捕捉机器人操作任务中固有的结构时间变化。我们在3个仿真基准上的53个任务上评估了FreqPolicy，证明了它在现有的一步动作生成器中的优越性。我们进一步将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在LIBERO的40个任务上实现了加速，且未降低性能。此外，我们在真实世界的机器人场景中展示了其效率和有效性，推理频率为93.5 Hz。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FreqPolicy is a novel approach that imposes frequency consistency constraints on flow-based visuomotor policies to enable efficient and high-quality one-step action generation for robotic manipulation. The method introduces a frequency consistency constraint objective and an adaptive consistency loss to promote temporal coherence in action trajectories. Experiments on 53 tasks across three simulation benchmarks demonstrate FreqPolicy&#x27;s superiority over existing one-step action generators, and it also accelerates the vision-language-action model without degrading performance on 40 tasks of LIBERO. Additionally, FreqPolicy shows efficiency and effectiveness in real-world robotic scenarios with an inference frequency of 93.5 Hz.</div>
<div class="mono" style="margin-top:8px">FreqPolicy 是一种通过频率一致性约束流基运动视觉策略来实现高效且高质量的一步动作生成的新方法，以适应机器人操作的需求。它引入了频率一致性约束目标和自适应一致性损失，以促进动作轨迹的时间连贯性。FreqPolicy 在三个仿真基准上的 53 个任务中优于现有的一步动作生成器，并且与视觉-语言-动作模型集成后，在 LIBERO 的 40 个任务上实现了加速且不降低性能。此外，它在真实世界机器人场景中表现出高效性和有效性，具有 93.5 Hz 的推理频率。</div>
</details>
</div>
<div class="card">
<div class="title">Reactive Motion Generation With Particle-Based Perception in Dynamic Environments</div>
<div class="meta-line">Authors: Xiyuan Zhao, Huijun Li, Lifeng Zhu, Zhikai Wei, Xianyi Zhu, Aiguo Song</div>
<div class="meta-line">First: 2026-02-18T13:48:54+00:00 · Latest: 2026-02-18T13:48:54+00:00</div>
<div class="meta-line">Comments: This paper has 20 pages, 15 figures, and 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16462v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reactive motion generation in dynamic and unstructured scenarios is typically subject to essentially static perception and system dynamics. Reliably modeling dynamic obstacles and optimizing collision-free trajectories under perceptive and control uncertainty are challenging. This article focuses on revealing tight connection between reactive planning and dynamic mapping for manipulators from a model-based perspective. To enable efficient particle-based perception with expressively dynamic property, we present a tensorized particle weight update scheme that explicitly maintains obstacle velocities and covariance meanwhile. Building upon this dynamic representation, we propose an obstacle-aware MPPI-based planning formulation that jointly propagates robot-obstacle dynamics, allowing future system motion to be predicted and evaluated under uncertainty. The model predictive method is shown to significantly improve safety and reactivity with dynamic surroundings. By applying our complete framework in simulated and noisy real-world environments, we demonstrate that explicit modeling of robot-obstacle dynamics consistently enhances performance over state-of-the-art MPPI-based perception-planning baselines avoiding multiple static and dynamic obstacles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于粒子感知的动力学环境下的反应式运动生成</div>
<div class="mono" style="margin-top:8px">在动态和非结构化的场景中，反应式运动生成通常依赖于基本静态的感知和系统动力学。准确建模动态障碍物并在感知和控制不确定性下优化无碰撞轨迹是具有挑战性的。本文从模型的角度重点揭示了反应式规划与动态制图之间的紧密联系。为了实现高效的粒子感知并具有动态特性，我们提出了一种张量化的粒子权重更新方案，该方案明确地维护了障碍物的速度和协方差。基于这种动态表示，我们提出了一种障碍物感知的MPPI基规划公式，该公式联合传播了机器人-障碍物动力学，使得未来系统的运动可以在不确定性下被预测和评估。模型预测方法被证明可以显著提高在动态环境中的安全性和反应性。通过在模拟和嘈杂的真实环境中应用我们完整的框架，我们证明了对机器人-障碍物动力学的显式建模在避免多个静态和动态障碍物方面始终优于最先进的MPPI基感知-规划基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of reactive motion generation in dynamic environments by integrating dynamic perception and planning. It introduces a tensorized particle weight update scheme to maintain obstacle velocities and covariance, and proposes an obstacle-aware Model Predictive Path Integral (MPPI) planning formulation that jointly propagates robot-obstacle dynamics. The results show that this approach significantly improves safety and reactivity compared to state-of-the-art methods in both simulated and real-world environments with dynamic obstacles.</div>
<div class="mono" style="margin-top:8px">本文解决了动态环境中反应式运动生成的挑战，通过结合动态感知和规划。它引入了一种张量化的粒子权重更新方案来维持障碍物的速度和协方差，并提出了一种考虑障碍物的Model Predictive Path Integral (MPPI) 规划公式，该公式联合传播了机器人-障碍物的动力学。结果显示，这种方法在包含多个静态和动态障碍物的模拟和真实环境中显著提高了安全性和反应性，优于最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</div>
<div class="meta-line">Authors: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi</div>
<div class="meta-line">First: 2026-02-12T17:46:52+00:00 · Latest: 2026-02-18T11:55:37+00:00</div>
<div class="meta-line">Comments: VIRENA is under active development and currently in use at the University of Zurich. This preprint will be updated as new features are released. For the latest version and to inquire about demos or pilot collaborations, contact the authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12207v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12207v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA&#x27;s no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRENA：虚拟竞技场，用于研究、教育和民主创新</div>
<div class="mono" style="margin-top:8px">数字平台塑造人们的沟通、讨论和意见形成方式。由于数据访问受限、现实世界实验的伦理限制以及现有研究工具的局限性，研究这些动态变得越来越困难。VIRENA（虚拟竞技场）是一个平台，它能够在现实社交媒体环境中进行受控实验。多个参与者可以同时在基于信息流的平台（Instagram、Facebook、Reddit）和即时通讯应用（WhatsApp、Messenger）的现实复制品中互动。由大型语言模型驱动的AI代理可以与人类一起参与，具有可配置的人格和现实行为。研究人员可以通过无需编程技能的可视化界面操控内容审核方法、预排定刺激内容，并在不同条件下运行实验。VIRENA 使以前不切实际的研究设计成为可能：研究人类与AI的互动、实验性地比较干预措施以及观察群体讨论的展开。基于开源技术，确保数据在机构控制之下并符合数据保护要求，VIRENA 目前在苏黎世大学使用，并可供试点合作。VIRENA 的无代码界面使其跨学科和跨行业领域中受控社交媒体模拟变得可行。本文档记录了其设计、架构和功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VIRENA is a platform designed to enable controlled experimentation in realistic social media environments, addressing the challenges of data access and ethical constraints. It allows multiple participants to interact in replicas of social media platforms and messaging apps, with AI agents participating alongside humans. Researchers can manipulate content moderation and run experiments through a no-code interface. Key findings include the ability to study human-AI interactions, compare moderation interventions, and observe group deliberation in realistic settings, which were previously impractical with existing tools.</div>
<div class="mono" style="margin-top:8px">VIRENA 是一个平台，旨在通过现实社交媒体环境中的受控实验来解决数据访问受限和伦理约束的问题。它允许参与者在 Instagram、Facebook、Reddit、WhatsApp 和 Messenger 的复制品中互动，同时 AI 代理与人类一起参与。研究人员可以通过无代码的可视化界面操纵内容审核并运行实验。主要发现包括能够研究人与 AI 的互动、实验性地比较干预措施以及在现实环境中观察群体讨论，从而使以前不可能的研究设计成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">AMBER: A tether-deployable gripping crawler with compliant microspines for canopy manipulation</div>
<div class="meta-line">Authors: P. A. Wigner, L. Romanello, A. Hammad, P. H. Nguyen, T. Lan, S. F. Armanini, B. B. Kocer, M. Kovac</div>
<div class="meta-line">First: 2025-12-08T16:17:56+00:00 · Latest: 2026-02-18T11:42:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07680v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.07680v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90$^\circ$ body roll and inclination, while effective climbing on branches inclined up to 67.5$^\circ$, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10$^\circ$, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. The crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing. The aerial deployment is demonstrated at a conceptual and feasibility level, while full drone-crawler integration is left as future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMBER：一种用于树冠操作的可展式抓取爬行器及其顺应式微钩爪</div>
<div class="mono" style="margin-top:8px">本文介绍了一种用于树木冠层内适应性移动和操作的可空中部署爬行器。该系统结合了基于顺应式微钩爪的履带、双履带旋转夹持器和弹性尾巴，使其能够在不同曲率和倾斜角度的树枝上实现安全附着和稳定行进。实验表明，该爬行器能够在高达90°的滚转和倾斜角度下可靠抓握，有效攀爬倾斜角度高达67.5°的树枝，最大速度为每秒0.55个身体长度。顺应式履带允许最大10°的偏航转向，提高其在不规则表面的机动性。功率测量显示，该爬行器的无量纲运输成本比典型悬停功率消耗低一个数量级，具有高效运行的特点。该爬行器为环境采样和冠层内传感提供了一个坚固且低功耗的平台。空中部署在概念和可行性层面进行了演示，而完整的无人机-爬行器集成则留作未来工作。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped</div>
<div class="meta-line">Authors: Saumya Karan, Neerav Maram, Suraj Borate, Madhu Vadali</div>
<div class="meta-line">First: 2026-02-18T11:14:22+00:00 · Latest: 2026-02-18T11:14:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16371v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16371v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">SLOT (Soft Legged Omnidirectional Tetrapod), a tendon-driven soft quadruped robot with 3D-printed TPU legs, is presented to study physics-informed modeling and control of compliant legged locomotion using only four actuators. Each leg is modeled as a deformable continuum using discrete Cosserat rod theory, enabling the capture of large bending deformations, distributed elasticity, tendon actuation, and ground contact interactions. A modular whole-body modeling framework is introduced, in which compliant leg dynamics are represented through physically consistent reaction forces applied to a rigid torso, providing a scalable interface between continuum soft limbs and rigid-body locomotion dynamics. This formulation allows efficient whole-body simulation and real-time control without sacrificing physical fidelity. The proposed model is embedded into a convex model predictive control framework that optimizes ground reaction forces over a 0.495 s prediction horizon and maps them to tendon actuation through a physics-informed force-angle relationship. The resulting controller achieves asymptotic stability under diverse perturbations. The framework is experimentally validated on a physical prototype during crawling and walking gaits, achieving high accuracy with less than 5 mm RMSE in center of mass trajectories. These results demonstrate a generalizable approach for integrating continuum soft legs into model-based locomotion control, advancing scalable and reusable modeling and control methods for soft quadruped robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>腱驱动软四足动物的动态建模与MPC行走研究</div>
<div class="mono" style="margin-top:8px">SLOT（软腿全方位四足动物），一种使用3D打印TPU腿的腱驱动软四足动物机器人，用于研究仅使用四个执行器的顺应性腿足运动的物理启发式建模和控制。每条腿被建模为可变形连续体，使用离散柯西尔杆理论，能够捕捉到大弯曲变形、分布弹性、腱驱动和地面接触相互作用。引入了一种模块化的全身建模框架，在该框架中，通过在刚性躯干上施加物理一致的反作用力来表示顺应腿的动力学，提供了一种连续软肢和刚体运动动力学之间的可扩展接口。该公式允许高效的整体身体仿真和实时控制，而不牺牲物理精度。所提出的模型嵌入到凸模型预测控制框架中，该框架在0.495秒的预测窗口内优化地面反作用力，并通过物理启发的力-角关系将其映射到腱驱动。所得到的控制器在多种扰动下实现了渐近稳定性。该框架在爬行和行走步态的物理原型上进行了实验验证，实现了高精度，中心质量轨迹的RMSE小于5毫米。这些结果展示了将连续软腿整合到基于模型的运动控制中的通用方法，推进了软四足动物机器人可扩展和可重用建模与控制方法的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study presents SLOT, a tendon-driven soft quadruped robot, to investigate compliant legged locomotion using discrete Cosserat rod theory for modeling each leg as a deformable continuum. The modular whole-body modeling framework represents compliant leg dynamics through physically consistent reaction forces applied to a rigid torso, enabling efficient simulation and real-time control. The proposed model is embedded in a convex model predictive control framework, achieving asymptotic stability under various perturbations and validating the controller with less than 5 mm RMSE in center of mass trajectories during crawling and walking gaits.</div>
<div class="mono" style="margin-top:8px">该研究介绍了使用离散柯西尔杆理论建模每个腿的腱驱动软四足机器人SLOT，以研究柔顺腿足运动。引入了一种模块化的整体建模框架，实现了高效的仿真和实时控制同时保持物理精度。提出的模型被集成到一个凸模型预测控制框架中，在各种扰动下实现了渐近稳定性。在爬行和行走姿态的物理原型上进行的实验验证显示，中心质量轨迹的准确性很高，RMSE小于5毫米。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
