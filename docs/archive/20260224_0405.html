<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-24 04:05</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260224_0405</div>
    <div class="row"><div class="card">
<div class="title">Snapping Actuators with Asymmetric and Sequenced Motion</div>
<div class="meta-line">Authors: Xin Li, Ye Jin, Mohsen Jafarpour, Hugo de Souza Oliveira, Edoardo Milana</div>
<div class="meta-line">First: 2026-02-20T18:45:17+00:00 · Latest: 2026-02-20T18:45:17+00:00</div>
<div class="meta-line">Comments: 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18421v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18421v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Snapping instabilities in soft structures offer a powerful pathway to achieve rapid and energy-efficient actuation. In this study, an eccentric dome-shaped snapping actuator is developed to generate controllable asymmetric motion through geometry-induced instability. Finite element simulations and experiments reveal consistent asymmetric deformation and the corresponding pressure characteristics. By coupling four snapping actuators in a pneumatic network, a compact quadrupedal robot achieves coordinated wavelike locomotion using only a single pressure input. The robot exhibits frequency-dependent performance with a maximum speed of 72.78~mm/s at 7.5~Hz. These findings demonstrate the potential of asymmetric snapping mechanisms for physically controlled actuation and lay the groundwork for fully untethered and efficient soft robotic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有非对称和序贯运动的弹射执行器</div>
<div class="mono" style="margin-top:8px">软结构中的弹射不稳定性为实现快速且能量高效的执行提供了强大的途径。在本研究中，开发了一种偏心圆顶形弹射执行器，通过几何诱导的不稳定性产生可控的非对称运动。有限元仿真和实验揭示了一致的非对称变形及其相应的压力特性。通过将四个弹射执行器耦合在气动网络中，一个紧凑的四足机器人仅使用单个压力输入即可实现协调的波状运动。该机器人表现出频率依赖性性能，在7.5 Hz时的最大速度为72.78 mm/s。这些发现展示了非对称弹射机制在物理控制执行中的潜力，并为完全无缆且高效的软体机器人系统奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study develops an eccentric dome-shaped snapping actuator to produce controllable asymmetric motion via geometry-induced instability. Finite element simulations and experiments confirm the asymmetric deformation and pressure characteristics. By connecting four such actuators in a pneumatic network, a quadrupedal robot achieves coordinated wavelike locomotion with a maximum speed of 72.78 mm/s at 7.5 Hz, showcasing the potential of asymmetric snapping mechanisms for efficient actuation in soft robots.</div>
<div class="mono" style="margin-top:8px">本研究开发了一种通过几何诱导不稳定性实现可控不对称运动的偏心穹形弹射执行器。有限元模拟和实验验证了不对称变形和压力特性。通过将四个这样的执行器耦合在一个气动网络中，一个紧凑的四足机器人展示了协调的波浪式运动，最大速度为72.78 mm/s（频率7.5 Hz），突显了不对称弹射机制在高效软机器人系统中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf</div>
<div class="meta-line">Authors: Wenqi Jiang, Jason Clemons, Karu Sankaralingam, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-20T18:02:28+00:00 · Latest: 2026-02-20T18:02:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18397v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18397v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我能让我的VLA跑多快？VLA-Perf解析VLA推理性能</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型在各种具身AI任务中展现了令人印象深刻的性能。然而，由于VLA模型架构和推理系统的庞大组合空间，部署VLA模型到真实世界机器人时面临的严格实时推理约束使得VLA的推理性能景观尚未被充分理解。在本文中，我们提出一个基本的研究问题：我们应该如何设计未来的VLA模型和系统以支持实时推理？为了解决这个问题，我们首先引入了VLA-Perf，一种分析VLA模型和推理系统任意组合的推理性能的分析性能模型。利用VLA-Perf，我们进行了首次系统性的VLA推理性能景观研究。从模型设计的角度来看，我们探讨了模型缩放、模型架构选择、长上下文视频输入、异步推理和双系统模型管道对推理性能的影响。从部署角度来看，我们分析了VLA推理应在设备上、边缘服务器上还是云端执行，并探讨了硬件能力和网络性能如何共同决定端到端延迟。通过从我们全面评估中提炼出的15个关键见解，我们希望这项工作能为未来VLA模型和推理系统的设计提供实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of real-time inference for Vision-Language-Action (VLA) models by introducing VLA-Perf, an analytical performance model. It systematically studies the impact of model scaling, architecture, long-context inputs, and deployment locations on VLA inference performance. Key findings include the importance of model architecture and the trade-offs between on-device, edge, and cloud execution for minimizing latency.</div>
<div class="mono" style="margin-top:8px">本文通过引入VLA-Perf分析性能模型，系统地评估了模型缩放、架构、长视频上下文输入、异步推理和双系统管道对推理性能的影响。研究还探讨了VLA推理的最佳部署位置，考虑了硬件和网络性能。关键发现包括15条实用指南，以支持未来VLA模型和推理系统的实时性能设计。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot Interactive Perception</div>
<div class="meta-line">Authors: Venkatesh Sripada, Frank Guerin, Amir Ghalamzan</div>
<div class="meta-line">First: 2026-02-20T17:30:25+00:00 · Latest: 2026-02-20T17:30:25+00:00</div>
<div class="meta-line">Comments: Original manuscript submitted on April 24, 2025. Timestamped and publicly available on OpenReview: https://openreview.net/forum?id=7MhpFcr5Nx</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18374v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18374v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM&#x27;s visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>零样本交互感知</div>
<div class="mono" style="margin-top:8px">交互感知（IP）使机器人能够在其工作空间中提取隐藏信息并通过物理交互物体和改变环境状态来执行操作计划——这对于解决复杂、部分可观测场景中的遮挡和模糊至关重要。我们提出了零样本IP（ZS-IP），这是一种新颖的框架，将多策略操作（推和抓取）与记忆驱动的视觉语言模型（VLM）结合，以指导机器人交互并解决语义查询。ZS-IP 结合了三个关键组件：（1）增强观察（EO）模块，该模块通过常规关键点和我们提出的推线——一种针对推操作定制的新型2D视觉增强，增强了VLM的视觉感知，（2）记忆引导的操作模块，通过上下文查找强化语义推理，以及（3）基于VLM输出执行推、拉或抓取的机器人控制器。与针对拾取和放置优化的基于网格的增强不同，推线捕捉接触丰富的操作的利用机会，显著提高了推操作的性能。我们在具有不同遮挡和任务复杂度的7-DOF Franka Panda 手臂上评估了ZS-IP。我们的实验表明，ZS-IP 在推操作任务中优于被动和视角基于的感知技术，如基于标记的视觉提示（MOKA），同时保持非目标元素的完整性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a framework for robots to resolve occlusions and ambiguities in complex, partially observable scenarios through interactive perception. ZS-IP integrates an Enhanced Observation module with pushlines for visual perception, a memory-guided action module for semantic reasoning, and a robotic controller for executing manipulation tasks. Experiments show that ZS-IP outperforms passive and viewpoint-based techniques, especially in pushing tasks, while maintaining the integrity of non-target elements.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种框架，使机器人能够在复杂环境中通过交互感知和操作物体，解决遮挡和歧义问题。Zero-Shot Interactive Perception (ZS-IP)框架结合了多策略操作与记忆驱动的视觉语言模型。关键组件包括使用推线增强观察模块，用于推动作；记忆引导的操作模块进行语义推理；以及基于视觉语言模型输出的机器人控制器。实验表明，ZS-IP在各种具有遮挡和任务复杂性的场景中，特别是在推动作任务中，优于现有技术，同时保持非目标元素的完整性。</div>
</details>
</div>
<div class="card">
<div class="title">CAIMAN: Causal Action Influence Detection for Sample-efficient Loco-manipulation</div>
<div class="meta-line">Authors: Yuanchen Yuan, Jin Cheng, Núria Armengol Urpí, Stelian Coros</div>
<div class="meta-line">First: 2025-02-02T16:16:53+00:00 · Latest: 2026-02-20T15:50:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.00835v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.00835v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling legged robots to perform non-prehensile loco-manipulation is crucial for enhancing their versatility. Learning behaviors such as whole-body object pushing often requires sophisticated planning strategies or extensive task-specific reward shaping, especially in unstructured environments. In this work, we present CAIMAN, a practical reinforcement learning framework that encourages the agent to gain control over other entities in the environment. CAIMAN leverages causal action influence as an intrinsic motivation objective, allowing legged robots to efficiently acquire object pushing skills even under sparse task rewards. We employ a hierarchical control strategy, combining a low-level locomotion module with a high-level policy that generates task-relevant velocity commands and is trained to maximize the intrinsic reward. To estimate causal action influence, we learn the dynamics of the environment by integrating a kinematic prior with data collected during training. We empirically demonstrate CAIMAN&#x27;s superior sample efficiency and adaptability to diverse scenarios in simulation, as well as its successful transfer to real-world systems without further fine-tuning. A video demo is available at https://www.youtube.com/watch?v=dNyvT04Cqaw.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAIMAN：基于因果动作影响的样本高效腿足操作检测</div>
<div class="mono" style="margin-top:8px">使腿足机器人能够执行非抓握操作对于提高其灵活性至关重要。学习全身物体推举等行为通常需要复杂的规划策略或大量的任务特定奖励塑造，特别是在非结构化环境中。在本文中，我们提出了CAIMAN，一种实用的强化学习框架，鼓励代理获得对环境其他实体的控制。CAIMAN利用因果动作影响作为内在动机目标，使腿足机器人即使在稀疏的任务奖励下也能高效地获得物体推举技能。我们采用分层控制策略，结合低级运动模块和生成任务相关速度命令的高级策略，并训练以最大化内在奖励。为了估计因果动作影响，我们通过结合运动学先验和训练期间收集的数据来学习环境动力学。我们在模拟中实证展示了CAIMAN的优越样本效率和对多种场景的适应性，并成功将其转移到实际系统中而无需进一步微调。视频演示可在https://www.youtube.com/watch?v=dNyvT04Cqaw获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CAIMAN is a reinforcement learning framework designed to enable legged robots to perform non-prehensile loco-manipulation tasks, such as whole-body object pushing, by leveraging causal action influence as an intrinsic motivation. The framework uses a hierarchical control strategy, combining a low-level locomotion module with a high-level policy that generates task-relevant velocity commands. CAIMAN demonstrates superior sample efficiency and adaptability in simulation and successfully transfers to real-world systems without further fine-tuning.</div>
<div class="mono" style="margin-top:8px">CAIMAN 是一种强化学习框架，旨在通过利用因果动作影响作为内在动机，使腿足机器人能够执行非抓取式移动操作任务，如全身物体推动。该框架采用分层控制策略，结合低级运动模块和生成任务相关速度命令的高级策略。CAIMAN 在模拟中展示了出色的样本效率和适应性，并成功地转移到了现实世界系统中，无需进一步微调。</div>
</details>
</div>
<div class="card">
<div class="title">SimVLA: A Simple VLA Baseline for Robotic Manipulation</div>
<div class="meta-line">Authors: Yuankai Luo, Woping Chen, Tong Liang, Baiqiao Wang, Zhenguo Li</div>
<div class="meta-line">First: 2026-02-20T14:04:27+00:00 · Latest: 2026-02-20T14:04:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18224v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://frontierrobo.github.io/SimVLA">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SimVLA：一种简单的VLA基线用于机器人操作</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型已成为通用机器人操作的有前途的范式，通过大规模预训练实现出色表现。该领域迅速发展，引入了额外的空间先验和多样化的架构创新。然而，这些进步往往伴随着不同的训练配方和实现细节，这使得难以区分经验收益的确切来源。在本文中，我们介绍了SimVLA，这是一种简化的基本模型，旨在为VLA研究建立一个透明的参考点。通过严格分离感知与控制，使用标准的视觉-语言骨干和轻量级的动作头，并标准化关键的训练动态，我们证明了最小的设计可以达到最先进的性能。尽管只有0.5B参数，SimVLA在标准的模拟基准测试中优于多十亿参数的模型，且无需机器人预训练。SimVLA在真实机器人上的性能也与pi0.5相当。我们的结果确立了SimVLA作为稳健、可重复的基本模型的地位，使未来架构创新的经验收益归因更加清晰。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research introduces SimVLA, a simplified baseline for Vision-Language-Action models in robotic manipulation, aiming to provide a transparent reference point. By decoupling perception from control and standardizing training dynamics, SimVLA demonstrates that a minimal design with only 0.5B parameters can outperform multi-billion-parameter models on simulation benchmarks and achieve on-par real-robot performance. This work establishes SimVLA as a robust baseline for future architectural innovations in VLA research.</div>
<div class="mono" style="margin-top:8px">研究旨在通过分离感知与控制并标准化训练动态，为机器人操作中的视觉-语言-行动（VLA）模型提供一个透明的基准。SimVLA仅包含0.5B参数，在标准模拟基准测试中优于多亿参数的模型，并且在真实机器人上的性能与pi0.5相当，证明了简单的设计也能达到最先进的性能，无需大量预训练。</div>
</details>
</div>
<div class="card">
<div class="title">Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi</div>
<div class="meta-line">Authors: Roman Akinshin, Elizaveta Lopatina, Kirill Bogatikov, Nikolai Kiz, Anna V. Makarova, Mikhail Lebedev, Miguel Altamirano Cabrera, Dzmitry Tsetserukou, Valerii Kangler</div>
<div class="meta-line">First: 2026-01-25T20:44:03+00:00 · Latest: 2026-02-20T13:21:33+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17991v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17991v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user&#x27;s focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于EMG和眼动追踪的由神经形态处理器AltAi驱动的手部假肢操作系统</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于上肢假肢的新型神经形态控制架构，结合了表面肌电图（sEMG）与眼动引导的计算机视觉。该系统利用部署在神经形态处理器AltAi上的脉冲神经网络实时分类肌电图模式，同时眼动追踪头戴设备和场景摄像头识别用户关注的对象。在我们的原型中，原本为传统GPU开发的同一肌电图识别模型被部署为脉冲网络在AltAi上运行，实现了与传统GPU相当的准确性，同时在亚瓦特的功耗下运行，这使得轻便可穿戴的实现成为可能。对于来自上肢截肢者记录的六种不同功能手势，该系统实现了与最先进的肌电接口相当的稳健识别性能。当视觉管道将决策空间限制为当前视物对象的三个上下文相关手势时，识别准确率提高到约95%，同时排除了不安全、对象不合适的抓握。这些结果表明，所提出的神经形态、上下文感知控制器可以提供高效的假肢控制，并有可能提高上肢截肢者在日常活动中的安全性和易用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a neuromorphic control system for upper-limb prostheses that integrates sEMG and eye-tracking data processed by the AltAi neuromorphic processor. The system achieves real-time EMG pattern classification and object recognition, enabling robust gesture control with high accuracy and low power consumption. The prototype demonstrates comparable recognition performance to state-of-the-art myoelectric interfaces and significantly improves accuracy when restricted to context-appropriate gestures, enhancing safety and usability for amputees.</div>
<div class="mono" style="margin-top:8px">该论文介绍了一种结合表面肌电图（sEMG）和注视引导计算机视觉的上肢假肢神经形态控制系统。该系统部署在AltAi神经形态处理器上，实时分类EMG模式并识别用户关注的对象。该系统在实现与最先进的肌电接口相当的识别性能的同时，当限制在当前视物对象的适当手势时，准确率可提高到约95%，从而提高假肢控制的安全性和易用性。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets</div>
<div class="meta-line">Authors: Haruki Abe, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-20T06:39:17+00:00 · Latest: 2026-02-20T06:39:17+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18025v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>跨体态离线强化学习在异构机器人数据集中的应用</div>
<div class="mono" style="margin-top:8px">可扩展的机器人策略预训练因收集每个平台高质量演示的成本高昂而受阻。本研究通过将离线强化学习（离线RL）与跨体态学习相结合来解决这一问题。离线RL利用专家和大量次优数据，而跨体态学习则整合了不同形态的机器人轨迹，以获取通用的控制先验。我们系统分析了这种离线RL和跨体态范式，提供了对其优势和局限性的原理性理解。为了评估这种离线RL和跨体态范式，我们构建了跨越16种不同机器人平台的运动数据集。我们的实验表明，这种结合方法在使用富含次优轨迹的数据集进行预训练时表现出色，优于纯行为克隆。然而，随着次优数据的比例和机器人类型的数量增加，我们观察到不同形态之间的冲突梯度开始阻碍学习。为解决这一问题，我们引入了一种基于体态的分组策略，将机器人按形态相似性分组，并使用组梯度更新模型。这种简单的静态分组显著减少了机器人之间的冲突，并优于现有的冲突解决方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of scalable robot policy pre-training by integrating offline reinforcement learning with cross-embodiment learning. The approach leverages both expert and suboptimal data, and aggregates trajectories from diverse robot platforms to acquire universal control priors. Experiments on 16 distinct robot platforms show that this combined method outperforms pure behavior cloning when pre-training with suboptimal data. However, increasing the diversity of robot types leads to conflicting gradients, which are mitigated by a simple, static grouping strategy based on morphological similarity.</div>
<div class="mono" style="margin-top:8px">本研究通过将离线强化学习与跨体态学习相结合，解决了机器人策略预训练的可扩展性问题。该方法利用专家和次优数据，并从不同形态的机器人中聚合轨迹以开发通用的控制先验。实验表明，该结合方法在16种不同机器人平台上优于纯行为克隆，尤其是在次优数据较多时。然而，次优数据和机器人类型增多会导致形态间的冲突梯度，通过基于形态的分组策略将机器人按形态相似性分组，并使用组梯度更新模型，可以显著减少这种冲突并提高性能。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers</div>
<div class="meta-line">Authors: Guandong Li, Mengxia Ye</div>
<div class="meta-line">First: 2026-02-20T06:24:20+00:00 · Latest: 2026-02-20T06:24:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18022v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT&#x27;s multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器中的双通道注意力引导无训练图像编辑控制</div>
<div class="mono" style="margin-top:8px">基于扩散变换器（DiT）架构的扩散基础图像编辑模型对无训练控制编辑强度有关键要求。现有的注意力操作方法仅专注于Key空间来调节注意力路由，而完全忽略了Value空间——它控制特征聚合。在本文中，我们首先揭示DiT多模态注意力层中的Key和Value投影表现出明显的偏差-增量结构，其中令牌嵌入紧密围绕特定层的偏差向量聚类。基于这一观察，我们提出了双通道注意力引导（DCAG），这是一种无训练框架，可以同时操作Key通道（控制注意力的焦点）和Value通道（控制聚合的内容）。我们提供了理论分析，表明Key通道通过非线性softmax函数操作，作为粗略的控制旋钮，而Value通道通过线性加权求和操作，作为精细的补充。两者结合的二维参数空间$(δ_k, δ_v)$能够比任何单通道方法提供更精确的编辑保真度权衡。在PIE-Bench基准（700张图像，10个编辑类别）上的广泛实验表明，DCAG在所有保真度指标上都优于仅Key引导，特别是在对象删除（LPIPS减少4.9%）和对象添加（LPIPS减少3.2%）等局部编辑任务中表现最为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for training-free control over editing intensity in diffusion-based image editing models using the Diffusion Transformer (DiT) architecture. It proposes Dual-Channel Attention Guidance (DCAG), which manipulates both the Key and Value channels to control attention routing and feature aggregation, respectively. Experiments show that DCAG outperforms Key-only guidance in all fidelity metrics, with notable improvements in localized editing tasks such as object deletion and addition, reducing LPIPS by 4.9% and 3.2%, respectively.</div>
<div class="mono" style="margin-top:8px">本文针对基于扩散变换器（DiT）架构的图像编辑模型中对编辑强度的无训练控制需求，提出了一种双通道注意力引导（DCAG）框架，同时操控Key通道和Value通道以控制注意力路由和特征聚合。实验表明，DCAG在所有保真度指标上均优于仅操控Key通道的方法，在对象删除和添加等局部编辑任务中分别减少了LPIPS 4.9%和3.2%。</div>
</details>
</div>
<div class="card">
<div class="title">UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Jiabing Yang, Yixiang Chen, Yuan Xu, Peiyan Li, Xiangnan Wu, Zichen Wen, Bowen Fang, Tao Yu, Zhengbo Zhang, Yingda Li, Kai Wang, Jing Liu, Nianfeng Liu, Yan Huang, Liang Wang</div>
<div class="meta-line">First: 2026-02-20T06:22:21+00:00 · Latest: 2026-02-20T06:22:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18020v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as &quot;key-value memory&quot;, we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer&#x27;s Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UAOR：面向视觉-语言-动作模型的不确定性感知观测重注入</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型利用预训练的视觉-语言模型（VLM）作为骨干网络，将图像和指令映射到动作，展示了广泛可移植的机器人操作潜力。为了提高性能，现有方法通常会引入额外的观测线索（例如，深度图、点云）或辅助模块（例如，物体检测器、编码器），以实现更精确和可靠的任务执行，但这些方法通常需要昂贵的数据收集和额外的训练。受语言模型中的前馈网络（FFN）可以作为“键值记忆”的启发，我们提出了一种有效的、无需训练且即插即用的模块——不确定性感知观测重注入（UAOR），用于VLA模型。具体而言，当当前的语言模型层表现出高不确定性，通过动作熵进行测量时，它会通过注意力检索将关键观测信息重新注入到下一层的前馈网络（FFN）中。这种机制有助于VLA在推理过程中更好地关注观测信息，从而生成更自信和忠实的动作。全面的实验表明，我们的方法在模拟和真实世界任务中能够一致地改进各种VLA模型，且具有最小的开销。值得注意的是，UAOR消除了对额外观测线索或模块的需求，使其成为现有VLA管道的多功能且实用的插件。项目页面位于https://uaor.jiabingyang.cn/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the performance of Vision-Language-Action (VLA) models by proposing UAOR, an uncertainty-aware observation reinjection module. This module, which operates without additional training, injects key observation information into the next layer&#x27;s Feed-Forward Network when the current layer shows high uncertainty. Experimental results demonstrate that UAOR improves various VLA models in both simulated and real-world tasks with minimal overhead, making it a versatile and practical addition to existing VLA pipelines without requiring extra observation cues or modules.</div>
<div class="mono" style="margin-top:8px">论文提出了一种不确定性感知的观察重注入方法UAOR，该方法在无需额外训练或数据收集的情况下提升了VLA模型的性能。通过在当前层不确定性较高时将关键观察信息重新注入下一层的FFN中，UAOR增强了模型在推理过程中对观察信息的关注能力，从而生成更自信和忠实的动作。实验表明，UAOR能够一致地改进各种VLA模型，并且可以轻松集成到现有管道中，无需额外的观察信息或模块。</div>
</details>
</div>
<div class="card">
<div class="title">SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Manipulation</div>
<div class="meta-line">Authors: Ruopeng Huang, Boyu Yang, Wenlong Gui, Jeremy Morgan, Erdem Biyik, Jiachen Li</div>
<div class="meta-line">First: 2026-01-14T23:03:43+00:00 · Latest: 2026-02-20T06:17:03+00:00</div>
<div class="meta-line">Comments: Project Website: https://sync-twin.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09920v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09920v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sync-twin.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate and safe robotic manipulation under dynamic and visually occluded conditions remains a core challenge in real-world deployment. We introduce SyncTwin, a novel digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware robotic manipulation in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The synchronized twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves manipulation performance and motion safety, demonstrating the effectiveness of digital twin synchronization for real-world robotic execution. The video demos and code can be found on the project website: https://sync-twin.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SyncTwin：快速构建和同步数字孪生以实现安全的机器人操作</div>
<div class="mono" style="margin-top:8px">在动态且视觉遮挡的条件下实现精确和安全的机器人操作仍然是现实世界部署中的核心挑战。我们提出了SyncTwin，这是一种新颖的数字孪生框架，统一了快速的3D场景重建和实时到模拟的同步，以在这些环境中实现稳健和安全意识的机器人操作。在离线阶段，我们使用VGGT从RGB图像快速重建对象级别的3D资产，形成可重复使用的几何库。在执行过程中，SyncTwin通过点云分割更新和彩色ICP注册连续同步数字孪生，同步的孪生使运动规划器能够在模拟中计算出碰撞自由且动态可行的轨迹，这些轨迹通过闭环的实时到模拟再到实时回路安全地在真实机器人上执行。在动态和遮挡的场景中的实验表明，SyncTwin提高了操作性能和运动安全性，证明了数字孪生同步在真实世界机器人执行中的有效性。项目演示视频和代码可以在项目网站上找到：https://sync-twin.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SyncTwin is a digital twin framework designed to enhance robotic manipulation in dynamic and visually occluded environments. It uses VGGT for fast 3D scene reconstruction and colored-ICP registration for real-to-sim synchronization. During execution, the synchronized digital twin allows motion planners to compute safe and feasible trajectories, which are then executed on the real robot. Experiments show improved manipulation performance and motion safety compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">SyncTwin 是一种数字孪生框架，旨在增强动态和视觉遮挡环境下机器人的安全操作。它利用 VGGT 进行快速 3D 重建，并使用彩色-ICP 实现实时同步，使运动规划器能够计算出安全的轨迹。实验结果显示，与传统方法相比，其在操作性能和运动安全性方面有所提升。</div>
</details>
</div>
<div class="card">
<div class="title">Quasi-Periodic Gaussian Process Predictive Iterative Learning Control</div>
<div class="meta-line">Authors: Unnati Nigam, Radhendushka Srivastava, Faezeh Marzbanrad, Michael Burke</div>
<div class="meta-line">First: 2026-02-20T06:10:10+00:00 · Latest: 2026-02-20T06:10:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\mathcal{O}(p^3)$ instead of $\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>准周期高斯过程预测迭代学习控制</div>
<div class="mono" style="margin-top:8px">重复运动任务在机器人技术中很常见，但由于环境变化和机器人磨损，性能可能会随时间下降。迭代学习控制（ILC）通过利用先前迭代的信息来补偿未来迭代中的预期误差，从而提高性能。本文将准周期高斯过程（QPGP）引入预测ILC框架，以建模和预测跨迭代的干扰和漂移。利用QPGP的最近结构方程表述，所提出的方法能够以复杂度为$\mathcal{O}(p^3)$的方式进行高效推理，而不是$\mathcal{O}(i^2p^3)$，其中$p$表示迭代内的点数，$i$表示总迭代次数，特别适用于较大的$i$。该表述还允许在不损失信息的情况下进行参数估计，使连续的GP学习在控制回路中成为可能。通过预测下一迭代的误差轮廓而不是仅依赖于过去的误差，控制器能够更快地收敛并在时间变化的干扰下保持稳定。我们在自主车辆轨迹跟踪、三连杆机器人 manipulator 和现实世界的 Stretch 机器人实验中将该方法与标准ILC和基于常规高斯过程（GP）的预测ILC进行了比较。在所有情况下，所提出的方法收敛速度更快，能够保持对注入和自然干扰的鲁棒性，同时降低计算成本。这突显了其在各种重复动力系统中的实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the performance degradation in repetitive motion tasks by proposing a Quasi-Periodic Gaussian Process Predictive Iterative Learning Control method. It uses a structural equation formulation of QPGPs to model and forecast disturbances and drifts, reducing computational complexity and enabling efficient parameter estimation. The method outperforms standard ILC and conventional GP-based ILC in terms of faster convergence and robustness under disturbances, as demonstrated through autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决环境变化和机器人磨损导致的性能下降问题，提高机器人重复运动任务的表现。该研究引入了一种基于拟周期高斯过程预测迭代学习控制的方法，利用QPGP的结构方程模型来预测干扰和漂移。该方法降低了计算复杂度，实现了参数估计的有效性，从而在各种干扰下比标准ILC和基于GP的传统ILC方法更快收敛并保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly</div>
<div class="meta-line">Authors: Zehao Jin, Yaoye Zhu, Chen Zhang, Yanan Sui</div>
<div class="meta-line">First: 2026-02-20T05:09:28+00:00 · Latest: 2026-02-20T05:09:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17997v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17997v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly&#x27;s brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全脑联接组图形模型实现果蝇全身运动控制</div>
<div class="mono" style="margin-top:8px">全脑生物神经网络自然支持全身运动的学习和控制。然而，将脑联接组用作具身强化学习中的神经网络控制器尚未被探索。我们研究了使用成年果蝇大脑的精确神经架构来控制其身体运动。我们开发了果蝇联接组图形模型（FlyGM），其静态结构与成年果蝇的完整联接组完全相同，用于全身运动控制。为了实现动态控制，FlyGM 将静态联接组表示为有向消息传递图，以施加从感觉输入到运动输出的生物基础信息流。结合生物力学果蝇模型，我们的方法在各种运动任务中实现了稳定的控制，无需针对特定任务进行架构调整。为了验证基于联接组模型的结构优势，我们将其与度保持重连图、随机图和多层感知机进行比较，结果显示 FlyGM 具有更高的样本效率和更优的性能。这项工作表明，静态脑联接组可以被转换为实现运动控制的具身学习有效神经策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to explore the use of brain connectomes as neural network controllers for embodied reinforcement learning. The study develops Fly-connectomic Graph Model (FlyGM), which uses the exact neural architecture of an adult fruit fly&#x27;s brain to control whole-body locomotion. The model shows higher sample efficiency and superior performance compared to degree-preserving rewired graphs, random graphs, and multilayer perceptrons, achieving stable control across various locomotion tasks without task-specific architectural tuning.</div>
<div class="mono" style="margin-top:8px">研究旨在探索将脑连接组用作体现强化学习中神经网络控制器的可能性。研究开发了Fly-connectomic Graph Model (FlyGM)，该模型使用成年果蝇大脑的精确神经架构来控制全身运动。该模型在各种运动任务中表现出更高的样本效率和优越性能，无需针对特定任务进行架构调整，即可实现稳定的控制，优于度保持重连图、随机图和多层感知机。</div>
</details>
</div>
<div class="card">
<div class="title">HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms</div>
<div class="meta-line">Authors: Bingjie Chen, Zihan Wang, Zhe Han, Guoping Pan, Yi Cheng, Houde Liu</div>
<div class="meta-line">First: 2025-09-24T15:51:17+00:00 · Latest: 2026-02-20T03:22:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20263v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.20263v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional IK methods for redundant humanoid manipulators emphasize end-effector (EE) tracking, frequently producing configurations that are valid mechanically but not human-like. We present Human-Like Inverse Kinematics (HL-IK), a lightweight IK framework that preserves EE tracking while shaping whole-arm configurations to appear human-like, without full-body sensing at runtime. The key idea is a learned elbow prior: using large-scale human motion data retargeted to the robot, we train a FiLM-modulated spatio-temporal attention network (FiSTA) to predict the next-step elbow pose from the EE target and a short history of EE-elbow states.This prediction is incorporated as a small residual alongside EE and smoothness terms in a standard Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the most challenging trajectories. Hardware teleoperation on a robot distinct from simulation further confirms the gains in anthropomorphism. HL-IK is simple to integrate, adaptable across platforms via our pipeline, and adds minimal computation, enabling human-like motions for humanoid robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HL-IK：类人形冗余人形机械臂逆运动学的轻量级实现</div>
<div class="mono" style="margin-top:8px">传统冗余人形机械臂的IK方法侧重末端执行器（EE）跟踪，经常产生机械上有效但不类人的配置。我们提出了类人形逆运动学（HL-IK），这是一种轻量级的IK框架，它在保持末端执行器跟踪的同时，通过调整整个臂的配置使其看起来更类人，而无需在运行时进行全身感知。关键思想是一种学习到的肘部先验：利用大规模的人体运动数据重新定向到机器人，我们训练了一个FiLM调制的空间-时间注意力网络（FiSTA），以根据末端执行器目标和末端执行器-肘部状态的短期历史预测下一步的肘部姿态。该预测作为末端执行器和平滑性项的小残差与标准的Levenberg-Marquardt优化器结合，使HL-IK成为数值IK堆栈中的即插即用添加。在超过183,000个仿真步骤中，HL-IK在平均情况下将手臂相似性位置和方向误差分别减少了30.6%和35.4%，在最具有挑战性的轨迹上分别减少了42.2%和47.4%。在与仿真不同的机器人上的硬件遥操作进一步证实了类人化的提升。HL-IK易于集成，通过我们的流水线在不同平台上具有良好的适应性，并且增加了最少的计算量，从而能够为类人形机器人生成类人形动作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the issue of traditional inverse kinematics (IK) methods producing mechanical but unnatural arm configurations in humanoid robots. The authors propose HL-IK, a lightweight framework that incorporates a learned elbow prior to shape arm configurations more human-like. This is achieved by training a FiLM-modulated spatio-temporal attention network (FiSTA) to predict elbow poses based on end-effector targets and history. Experimental results show that HL-IK reduces arm-similarity position and direction errors by 30.6% and 35.4% on average, and by 42.2% and 47.4% on challenging trajectories, enhancing the anthropomorphism of the robot&#x27;s movements.</div>
<div class="mono" style="margin-top:8px">研究针对传统人形机器人逆运动学（IK）方法导致的机械但不自然的手臂姿态问题，提出了HL-IK框架，该框架利用学习到的肘关节先验来预测类似人类的肘关节姿态，并将其集成到标准的IK优化器中。实验结果显示，HL-IK在平均情况下将手臂相似的位置和方向误差分别减少了30.6%和35.4%，在最具挑战性的轨迹上则分别减少了42.2%和47.4%，提升了机器人动作的人类化程度。</div>
</details>
</div>
<div class="card">
<div class="title">ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models</div>
<div class="meta-line">Authors: Guoheng Sun, Tingting Du, Kaixi Feng, Chenxiang Luo, Xingguo Ding, Zheyu Shen, Ziyao Wang, Yexiao He, Ang Li</div>
<div class="meta-line">First: 2026-02-20T03:06:22+00:00 · Latest: 2026-02-20T03:06:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17951v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17951v1">PDF</a> · <a href="https://github.com/CASE-Lab-UMD/ROCKET-VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ROCKET：面向空间感知的残差导向多层对齐的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型能够实现指令跟随的机器人操作，但它们通常是在2D数据上进行预训练，缺乏3D空间理解。一种有效的方法是表示对齐，其中使用一个强大的视觉基础模型来引导2D VLA模型。然而，现有方法通常只在单一层进行监督，未能充分利用分布在深度中的丰富信息；同时，简单的多层对齐会导致梯度干扰。我们提出了ROCKET，一种面向残差的多层表示对齐框架，将多层对齐形式化为对齐一个残差流到另一个残差流。具体来说，ROCKET 使用一个共享的投影器，通过层不变映射将VLA主干的多层与一个强大的3D视觉基础模型的多层进行对齐，从而减少梯度冲突。我们提供了理论依据和实证分析，表明共享的投影器是足够的，并优于先前的设计，进一步提出了马特罗什卡风格的稀疏激活方案来平衡多个对齐损失。我们的实验表明，结合无训练层选择策略，ROCKET 只需要大约4%的计算预算，而在LIBERO上实现了98.5%的最新技术水平成功率。我们还展示了ROCKET在LIBERO-Plus和RoboTwin以及多个VLA模型上的优越性能。代码和模型权重可以在https://github.com/CASE-Lab-UMD/ROCKET-VLA/找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ROCKET is a residual-oriented multi-layer alignment framework designed to enhance the 3D spatial understanding of Vision-Language-Action models. It uses a shared projector to align multiple layers of the VLA backbone with a 3D vision foundation model, reducing gradient conflicts. Experiments show that ROCKET achieves a 98.5% success rate on LIBERO with only 4% of the compute budget, outperforming previous methods and demonstrating superior performance across various VLA models and datasets.</div>
<div class="mono" style="margin-top:8px">ROCKET 是一种残差导向的多层对齐框架，旨在增强 Vision-Language-Action 模型的 3D 空间理解能力。它使用共享投影器将 VLA 主干的多层与 3D 视觉基础模型对齐，减少梯度冲突。实验表明，ROCKET 在 LIBERO 上实现了 98.5% 的成功率，仅需 4% 的计算预算，并且在多种 VLA 模型和数据集上表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation</div>
<div class="meta-line">Authors: Kei Ikemura, Yifei Dong, Florian T. Pokorny</div>
<div class="meta-line">First: 2026-02-20T00:33:20+00:00 · Latest: 2026-02-20T00:33:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17921v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17921v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在于形变和易碎物体操作中的末端执行器协同设计</div>
<div class="mono" style="margin-top:8px">操作形变和易碎物体仍然是机器人技术中的一个基本挑战，因为复杂的接触动力学和对物体完整性的严格要求。现有方法通常孤立地优化末端执行器设计或控制策略，限制了可实现的性能。在本文中，我们提出了第一个同时优化形变和易碎物体操作中末端执行器形态和操作控制的协同设计框架。我们引入了（1）一种潜在于形变的形状参数化方法，使末端执行器几何优化既具有表现力又易于处理，（2）一种应力感知的双层协同设计管道，将形态和控制优化耦合在一起，以及（3）一种针对零样本现实世界部署的特权到点云策略蒸馏方案。我们在包括抓取和推动果冻以及舀取鱼片的具有挑战性的食物操作任务上评估了我们的方法。仿真和现实世界实验表明了所提出方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of manipulating deformable and fragile objects by presenting a co-design framework that jointly optimizes end-effector morphology and control strategies. The method uses a latent diffeomorphic shape parameterization for end-effector geometry optimization and a stress-aware bi-level co-design pipeline to couple morphology and control optimization. It also includes a privileged-to-pointcloud policy distillation scheme for real-world deployment. The approach was evaluated on tasks such as grasping and pushing jelly and scooping fillets, showing its effectiveness in both simulation and real-world experiments.</div>
<div class="mono" style="margin-top:8px">该研究通过提出一种联合优化末端执行器形态和操作控制的共设计框架，解决了机器人操作变形和易碎物体的挑战。方法使用了潜流形形状参数化进行末端执行器几何优化，应力感知的双层共设计管道耦合形态和控制优化，并使用特权到点云策略蒸馏方案实现零样本的现实世界部署。实验结果表明，该方法在处理果冻和鱼片等食品操作任务中表现出色，能够实现稳健的操作。</div>
</details>
</div>
<div class="card">
<div class="title">WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection</div>
<div class="meta-line">Authors: Mingzhang Zhu, Alvin Zhu, Jose Victor S. H. Ramos, Beom Jun Kim, Yike Shi, Yufeng Wu, Ruochen Hou, Quanyou Wang, Eric Song, Tony Fan, Yuchen Cui, Dennis W. Hong</div>
<div class="meta-line">First: 2026-02-20T00:12:45+00:00 · Latest: 2026-02-20T00:12:45+00:00</div>
<div class="meta-line">Comments: 7 pages, 9 figures, submitted to IEEE UR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17908v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17908v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WHED：一种用于自然高质量演示收集的可穿戴手部外骨骼</div>
<div class="mono" style="margin-top:8px">灵巧操作的可扩展学习仍受限于由于遮挡、复杂的手部运动学和丰富的接触交互，难以收集自然的、高保真的多指手的人类演示。我们提出了WHED，一种可穿戴手部外骨骼系统，旨在野外演示捕捉，遵循两个原则：可穿戴优先的操作以支持长时间使用和一个姿态宽容、自由移动的拇指连接，以保留自然的拇指行为同时保持与目标机器人拇指自由度的一致映射。WHED 结合了连杆驱动的手指接口、被动适应的贴合、改良的被动手部以及坚固的本体感受传感，并配备了一个内置的传感/电源模块。我们还提供了一个端到端的数据管道，同步关节编码器、AR 基础末端执行器姿态以及腕部安装的视觉观察，并支持后处理以实现时间对齐和回放。我们展示了其在代表性抓取和操作序列上的可行性，这些序列涵盖了精确捏取和全手握持，并展示了收集的演示与回放执行之间的定性一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of collecting natural and high-quality human demonstrations for dexterous manipulation due to occlusion and complex hand movements. WHED, a wearable hand exoskeleton, is designed for extended use and captures natural thumb behaviors while maintaining a consistent mapping to the target robot thumb. The system integrates a linkage-driven finger interface, a modified passive hand with robust sensing, and an onboard module. An end-to-end data pipeline synchronizes joint encoders, end-effector poses, and visual observations, supporting post-processing for time alignment and replay. WHED demonstrates feasibility in capturing precision pinch and full-hand enclosure grasps.</div>
<div class="mono" style="margin-top:8px">研究旨在解决由于遮挡和复杂手部运动而难以收集自然且高保真的人类演示的问题。介绍了WHED，一种可穿戴的手部外骨骼系统，以确保穿戴舒适并保持自然拇指动作。关键实验发现包括成功收集并回放了精确捏取和全手封闭抓取等复杂手部运动，展示了该系统在捕捉和再现复杂手部动作方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array</div>
<div class="meta-line">Authors: Zachary Turcotte, François Grondin</div>
<div class="meta-line">First: 2026-02-19T20:35:43+00:00 · Latest: 2026-02-19T20:35:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17818v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17818v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>借我一只耳朵：使用具有麦克风阵列的机械臂进行语音增强</div>
<div class="mono" style="margin-top:8px">在嘈杂环境中，语音增强性能显著下降，限制了语音控制技术在工业环境中的应用，如制造工厂。现有的语音增强解决方案主要依赖于先进的数字信号处理技术、深度学习方法或复杂的软件优化技术。本文介绍了一种新的增强策略，通过动态修改麦克风阵列的几何形状以适应不断变化的声学条件，从而实现物理优化阶段。一个由16个麦克风组成的阵列安装在具有七个自由度的机械臂操作器上，麦克风分为四组，每组四个，包括一组靠近末端执行器。系统通过调整操作器关节角度来重新配置阵列，使末端执行器麦克风靠近目标说话者，从而提高参考信号质量。该方法结合了声源定位技术、计算机视觉、逆运动学、最小方差无失真响应波束形成器以及使用深度神经网络的时间-频率掩蔽。实验结果表明，该方法在多种输入信噪比条件下优于其他传统录音配置，实现了更高的尺度不变信噪比和更低的词错误率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of speech enhancement in noisy environments by proposing a novel method that uses a robotic arm with a sixteen-microphone array to dynamically adjust the microphone positions. The system employs sound source localization, computer vision, and inverse kinematics to optimize the microphone array geometry, and uses a deep neural network for time-frequency masking. Experiments show that this approach outperforms traditional configurations, with higher scale-invariant signal-to-distortion ratio and lower word error rate across various signal-to-noise ratios.</div>
<div class="mono" style="margin-top:8px">研究旨在通过改善嘈杂环境中的语音增强，使语音控制技术能够在工业环境中更好地部署。提出了一种新颖的方法，使用具有16个麦克风阵列的机械臂，根据声源定位和逆运动学调整麦克风位置，以提高信号质量。该系统在各种信噪比条件下优于传统配置，显示出更高的尺度不变信噪比和更低的词错误率。</div>
</details>
</div>
<div class="card">
<div class="title">When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</div>
<div class="meta-line">Authors: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-19T18:59:20+00:00 · Latest: 2026-02-19T18:59:20+00:00</div>
<div class="meta-line">Comments: Website: https://vla-va.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vla-va.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉优先于语言：评估和缓解VLAs中的反事实失败</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动模型（VLAs）承诺将语言指令应用于机器人控制，但在实践中往往未能忠实执行语言指令。当面对缺乏强烈场景特定监督的指令时，VLAs会遭受反事实失败：它们基于由数据集偏差引起的视觉捷径行动，反复执行已学得的行为，并选择在训练期间频繁出现的对象，而不管语言意图如何。为了系统地研究这一问题，我们引入了LIBERO-CF，这是第一个用于VLAs的反事实基准，通过在视觉上合理的LIBERO布局下分配替代指令来评估语言跟随能力。我们的评估表明，反事实失败在最先进的VLAs中普遍存在但尚未得到充分探索。我们提出了反事实行动指导（CAG），这是一种简单而有效的双分支推理方案，明确地在VLAs中正则化语言条件。CAG结合了一个标准的VLA策略和一个未受语言条件的视觉-行动（VA）模块，在行动选择期间进行反事实比较。这种设计减少了对视觉捷径的依赖，提高了对未观察任务的鲁棒性，并且无需额外演示或修改现有架构或预训练模型。广泛的实验表明，它可以在各种VLAs中实现即插即用集成，并且具有持续改进。例如，在LIBERO-CF中，CAG在语言跟随准确性上提高了9.7%，在未观察任务上的任务成功率提高了3.6%，使用无训练策略，配以VA模型时，进一步提高了15.5%和8.5%。在实际应用评估中，CAG将反事实失败减少了9.4%，并将任务成功率平均提高了17.2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to address counterfactual failures in Vision-Language-Action models (VLAs) by introducing LIBERO-CF, a benchmark for evaluating language-following capability. The research finds that these models often rely on visual shortcuts due to dataset biases, leading to poor performance on under-observed tasks. To mitigate this, the authors propose Counterfactual Action Guidance (CAG), a dual-branch inference scheme that integrates a standard VLA policy with a language-unconditioned Vision-Action module, significantly improving language following accuracy and task success on under-observed tasks by 9.7% and 3.6%, respectively, and reducing counterfactual failures by 9.4% in real-world evaluations.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入LIBERO-CF基准来解决Vision-Language-Action模型（VLAs）中的反事实失败问题，该基准用于评估语言跟随能力。研究发现，这些模型由于数据集偏差往往依赖视觉捷径，导致在未观察到的任务上表现不佳。为解决这一问题，作者提出了一种名为Counterfactual Action Guidance（CAG）的双分支推理方案，该方案将标准的VLA策略与一个语言无关的Vision-Action模块结合，显著提高了语言跟随准确率和未观察到任务上的任务成功率，分别提高了9.7%和3.6%，并在实际评估中减少了9.4%的反事实错误。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Step Duration for Accurate Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds</div>
<div class="meta-line">Authors: Zhaoyang Xiang, Victor Paredes, Guillermo A. Castillo, Ayonga Hereid</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2024-03-25T19:18:25+00:00 · Latest: 2026-02-19T18:19:15+00:00</div>
<div class="meta-line">Comments: 7 pages, 7 figures. Accepted to IEEE/RSJ IROS 2025. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.17136v3">Abs</a> · <a href="https://arxiv.org/pdf/2403.17136v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional one-step preview planning algorithms for bipedal locomotion struggle to generate viable gaits when walking across terrains with restricted footholds, such as stepping stones. To overcome such limitations, this paper introduces a novel multi-step preview foot placement planning algorithm based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of walking robots. Our proposed approach adaptively changes the step duration and the swing foot trajectory for optimal foot placement under constraints, thereby enhancing the long-term stability of the robot and significantly improving its ability to navigate environments with tight constraints on viable footholds. We demonstrate its effectiveness through various simulation scenarios with complex stepping-stone configurations and external perturbations. These tests underscore its improved performance for navigating foothold-restricted terrains, even with external disturbances.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>适应性步长以实现准确的足部放置：在受限立足点地形上实现稳健的双足运动</div>
<div class="mono" style="margin-top:8px">传统的双足运动一步预览规划算法在跨越受限立足点地形（如踏石）时难以生成可行的步态。为克服这些限制，本文提出了一种基于行走机器人步行Divergent Component of Motion (DCM) 的步到步离散演变的新型多步预览足部放置规划算法。我们提出的方法通过在约束条件下适当地改变步长和摆动腿轨迹来实现最佳足部放置，从而增强机器人的长期稳定性和显著提高其在受限立足点地形环境中导航的能力。我们通过各种具有复杂踏石配置和外部干扰的仿真场景展示了其有效性。这些测试强调了其在外部干扰下导航受限立足点地形的改进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of bipedal locomotion on terrains with restricted footholds by proposing a multi-step preview foot placement planning algorithm. The algorithm adaptively adjusts step duration and swing foot trajectory based on the Divergent Component of Motion (DCM) to enhance long-term stability. Experimental results from various simulation scenarios show that the proposed method significantly improves the robot&#x27;s ability to navigate such terrains, even under external disturbances.</div>
<div class="mono" style="margin-top:8px">本文提出了一种多步预览足部放置规划算法，以解决在受限 foothold 地形上的双足行走问题。该算法根据行走机器人的发散运动分量（DCM）自适应调整步长和摆动腿轨迹，以提高稳定性和导航能力。仿真测试表明，该算法在具有外部干扰的复杂踏石地形中表现出色，证明了其在受限 foothold 地形上的导航能力。</div>
</details>
</div>
<div class="card">
<div class="title">IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control</div>
<div class="meta-line">Authors: Qilong Cheng, Matthew Mackay, Ali Bereyhi</div>
<div class="meta-line">First: 2026-02-19T16:50:31+00:00 · Latest: 2026-02-19T16:50:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17537v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IRIS：基于学习的任务特定电影机器人手臂用于视动运动控制</div>
<div class="mono" style="margin-top:8px">机器人摄像系统能够实现超越人类能力的动态、可重复运动，但其采用受限于工业级平台的高成本和操作复杂性。我们介绍了智能机器人成像系统（IRIS），这是一种专为自主、基于学习的电影运动控制设计的6自由度 manipulator。IRIS 结合了轻量级的全3D打印硬件设计和基于动作分块与变换器（ACT）的目标条件视动模仿学习框架。该系统直接从人类示范中学习对象感知和感知平滑的摄像机轨迹，消除了显式几何编程的需要。整个平台成本低于1000美元，支持1.5公斤负载，并实现约1毫米的重复性。实际实验表明，该系统能够准确跟踪轨迹、可靠自主执行，并在多种电影运动中泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a cost-effective and easy-to-use robotic camera system for cinematic motion control. IRIS, a 6-DOF manipulator, integrates a lightweight 3D-printed hardware with a learning-based visuomotor imitation framework. The system can learn object-aware and smooth camera trajectories from human demonstrations, achieving accurate trajectory tracking and reliable autonomous execution. Experiments show that IRIS can generalize across various cinematic motions with high repeatability.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种低成本且易于使用的机器人摄像系统，用于电影运动控制。IRIS 是一个6-DOF机械臂，采用轻量级3D打印设计，并使用基于Action Chunking with Transformers (ACT)的目标条件视觉-运动模仿学习框架，从人类示范中学习对象感知和平滑的摄像机轨迹。关键发现包括准确的轨迹跟踪、可靠的自主执行以及在各种电影运动中的一般化，该系统成本低于1000美元，承载能力为1.5公斤，重复精度约为1毫米。</div>
</details>
</div>
<div class="card">
<div class="title">Proximal powered knee placement: a case study</div>
<div class="meta-line">Authors: Kyle R. Embry, Lorenzo Vianello, Jim Lipsey, Frank Ursetta, Michael Stephens, Zhi Wang, Ann M. Simon, Andrea J. Ikeda, Suzanne B. Finucane, Shawana Anarwala, Levi J. Hargrove</div>
<div class="meta-line">First: 2026-02-19T16:16:20+00:00 · Latest: 2026-02-19T16:16:20+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE RAS/EMBS 11th International Conference on Biomedical Robotics and Biomechatronics (BioRob 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17502v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17502v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>近端动力膝关节安置：案例研究</div>
<div class="mono" style="margin-top:8px">下肢截肢影响全球数百万人，导致行动能力下降、行走速度减慢和日常及社交活动参与度降低。动力假肢膝关节可以通过主动辅助膝关节扭矩部分恢复行动能力，改善步态对称性、坐起立转换和行走速度。然而，动力组件增加的重量可能会削弱这些益处，负面影响步态力学并增加代谢成本。因此，优化质量分布，而不是简单地减少总质量，可能提供更有效和实用的解决方案。在本探索性研究中，我们评估了在小样本组中将膝关节动力传动装置安置在上方的可行性。与下方安置相比，上方配置显示出行走速度（一名参与者提高9.2%）和步频（提高3.6%）的改善，步态对称性则表现出混合效果。运动学测量表明，两种配置下的膝关节活动范围和峰值速度相似。在斜坡和楼梯上的额外测试证实了控制策略在多种运动任务中的稳健性。初步结果显示，上方安置功能上是可行的，精心的质量分布可以保持动力辅助的益处，同时减轻增加重量的不利影响。需要进一步的研究来确认这些趋势并指导设计和临床建议。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the feasibility of placing a powered prosthetic knee above the knee compared to below the knee. The research aimed to optimize mass distribution to enhance mobility and reduce the negative effects of added weight. Key findings include improved walking speed and cadence for one participant, with mixed effects on gait symmetry. The above-knee configuration maintained similar knee range of motion and peak velocity, and the control strategy was robust across various tasks. These results suggest that above-knee placement can be functionally feasible and beneficial for powered prosthetics.</div>
<div class="mono" style="margin-top:8px">本研究评估了将动力假肢膝关节动力装置置于上方而非下方的可行性。上方配置提高了行走速度和步频，同时保持了相似的膝关节活动范围和峰值速度。控制策略在多种任务中表现稳健，表明上方配置可以保留动力辅助的好处，同时减轻额外重量的负面影响。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection</div>
<div class="meta-line">Authors: Yichen Lu, Siwei Nie, Minlong Lu, Xudong Yang, Xiaobo Zhang, Peng Zhang</div>
<div class="meta-line">First: 2026-02-19T15:54:55+00:00 · Latest: 2026-02-19T15:54:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17484v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17484v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace&#x27;s verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追踪复制像素和正则化块亲和性在复制检测中的应用</div>
<div class="mono" style="margin-top:8px">图像复制检测（ICD）旨在通过稳健的特征表示学习来识别图像对之间的篡改内容。虽然自监督学习（SSL）已经提升了ICD系统，但现有的视图级对比方法由于缺乏细粒度对应学习，难以应对复杂的编辑。我们通过两种关键创新解决了这一限制。首先，我们提出了PixTrace——一个像素坐标追踪模块，它在编辑变换中保持了明确的空间映射。其次，我们引入了CopyNCE，这是一种几何引导的对比损失，它使用PixTrace验证映射得出的重叠比来正则化块亲和性。我们的方法将像素级的可追踪性与块级相似性学习相结合，抑制了SSL训练中的监督噪声。广泛的实验不仅展示了最先进的性能（匹配器88.7% uAP / 83.9% RP90，描述符72.6% uAP / 68.4% RP90，DISC21数据集），还展示了比现有方法更好的可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve image copy detection by addressing the limitations of existing view-level contrastive methods in handling sophisticated edits. The authors introduce PixTrace, a pixel coordinate tracking module, and CopyNCE, a geometrically-guided contrastive loss, which together enhance fine-grained correspondence learning. Experiments show that their method achieves state-of-the-art performance with 88.7% uAP and 83.9% RP90 for the matcher, and 72.6% uAP and 68.4% RP90 for the descriptor on the DISC21 dataset, while also offering better interpretability compared to existing methods.</div>
<div class="mono" style="margin-top:8px">该论文通过提出像素坐标跟踪模块PixTrace和几何导向的对比损失CopyNCE，解决了图像复制检测中的挑战。这些创新通过保持显式的空间映射和正则化块之间的相似性，增强了检测复杂编辑的能力。实验表明，所提出的方法在DISC21数据集上优于现有方法，匹配器的uAP达到88.7%，RP90达到83.9%，描述符的uAP达到72.6%，RP90达到68.4%。</div>
</details>
</div>
<div class="card">
<div class="title">I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models</div>
<div class="meta-line">Authors: Clemence Grislain, Hamed Rahimi, Olivier Sigaud, Mohamed Chetouani</div>
<div class="meta-line">First: 2025-09-19T15:19:38+00:00 · Latest: 2026-02-19T15:45:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16072v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.16072v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://clemgris.github.io/I-FailSense/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-conditioned robotic manipulation in open-world settings requires not only accurate task execution but also the ability to detect failures for robust deployment in real-world environments. Although recent advances in vision-language models (VLMs) have significantly improved the spatial reasoning and task-planning capabilities of robots, they remain limited in their ability to recognize their own failures. In particular, a critical yet underexplored challenge lies in detecting semantic misalignment errors, where the robot executes a task that is semantically meaningful but inconsistent with the given instruction. To address this, we propose a method for building datasets targeting Semantic Misalignment Failures detection, from existing language-conditioned manipulation datasets. We also present I-FailSense, an open-source VLM framework with grounded arbitration designed specifically for failure detection. Our approach relies on post-training a base VLM, followed by training lightweight classification heads, called FS blocks, attached to different internal layers of the VLM and whose predictions are aggregated using an ensembling mechanism. Experiments show that I-FailSense outperforms state-of-the-art VLMs, both comparable in size and larger, in detecting semantic misalignment errors. Notably, despite being trained only on semantic misalignment detection, I-FailSense generalizes to broader robotic failure categories and effectively transfers to other simulation environments and real-world with zero-shot or minimal post-training. The datasets and models are publicly released on HuggingFace (Webpage: https://clemgris.github.io/I-FailSense/).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>I-FailSense：基于视觉-语言模型的通用机器人故障检测</div>
<div class="mono" style="margin-top:8px">开放世界环境中的语言条件化机器人操作不仅需要准确的任务执行，还需要检测故障的能力，以在实际环境中实现稳健部署。尽管近期视觉-语言模型（VLMs）在空间推理和任务规划方面取得了显著进步，但在识别自身故障方面仍有限制。特别是，一个关键但尚未充分探索的挑战在于检测语义对齐错误，即机器人执行的任务在语义上是有意义的，但与给定指令不一致。为解决这一问题，我们提出了一种方法，从现有的语言条件化操作数据集中构建针对语义对齐错误检测的数据集。我们还介绍了I-FailSense，一个具有基于地面仲裁的开源VLM框架，专门用于故障检测。我们的方法依赖于在基础VLM上进行后训练，然后训练轻量级分类头，称为FS块，将其附加到VLM的不同内部层，并使用集成机制聚合其预测。实验表明，I-FailSense在检测语义对齐错误方面优于现有的VLM，无论是大小相当还是更大的模型。值得注意的是，尽管仅在语义对齐检测上进行训练，I-FailSense仍能泛化到更广泛的机器人故障类别，并有效转移到其他模拟环境和现实世界中，无需或只需少量后训练。数据集和模型已在HuggingFace上公开发布（网址：https://clemgris.github.io/I-FailSense/）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to improve robotic failure detection in open-world settings by addressing semantic misalignment errors. It proposes I-FailSense, a VLM framework that post-trains a base VLM and adds lightweight classification heads to detect these errors. Experiments show that I-FailSense outperforms existing VLMs in detecting semantic misalignment errors and generalizes well to other failure categories and environments with minimal training.</div>
<div class="mono" style="margin-top:8px">论文旨在通过解决语义对齐错误来提高机器人在开放环境中的故障检测能力。它提出了I-FailSense框架，该框架对基础VLM进行后训练，并添加了轻量级分类头来检测这些错误。实验表明，I-FailSense在检测语义对齐错误方面优于现有VLM，并且在其他故障类别和环境中具有良好的泛化能力，只需少量训练即可。</div>
</details>
</div>
<div class="card">
<div class="title">2Mamba2Furious: Linear in Complexity, Competitive in Accuracy</div>
<div class="meta-line">Authors: Gabriel Mongaras, Eric C. Larson</div>
<div class="meta-line">First: 2026-02-19T13:45:23+00:00 · Latest: 2026-02-19T13:45:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>2Mamba2狂热: 线性在复杂性上，竞争在准确性上</div>
<div class="mono" style="margin-top:8px">线性注意力变换器由于其效率已成为softmax注意力的强有力替代品。然而，线性注意力在表达能力上较弱，导致准确性低于softmax注意力。为了弥合softmax注意力和线性注意力之间的准确性差距，我们操控了Mamba-2，这是一种非常强大的线性注意力变体。我们首先将Mamba-2简化为其最基本和最重要的组成部分，评估哪些具体选择使其最准确。从简化后的Mamba变体（Mamba-2S）中，我们改进了A-掩码并增加了隐藏状态的阶数，从而提出了一种名为2Mamba的方法，该方法在准确性上几乎与softmax注意力相当，但在长上下文长度下具有更高的内存效率。我们还研究了有助于超越softmax注意力准确性的Mamba-2元素。所有实验的代码均已提供</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the accuracy of linear attention transformers, which are more efficient than softmax attention but less expressive. The authors simplify Mamba-2, a linear attention variant, to its core components and then improve the A-mask and increase the order of the hidden state, leading to a method called 2Mamba that matches softmax attention accuracy while being more memory efficient for long context lengths. Key findings show that 2Mamba achieves nearly the same accuracy as softmax attention but with significant memory savings. Code for the experiments is available.</div>
<div class="mono" style="margin-top:8px">研究旨在提高线性注意力变压器的准确性，这些变压器虽然比softmax注意力更高效，但表达能力较弱。作者简化了Mamba-2，将其核心组件提炼出来，并改进了A-mask，增加了隐藏状态的阶数，从而提出了2Mamba方法，该方法在长上下文长度下具有与softmax注意力相近的准确性，但内存效率更高。关键发现表明，2Mamba在准确性和内存效率之间取得了良好的平衡。实验代码已提供。</div>
</details>
</div>
<div class="card">
<div class="title">Cert-SSBD: Certified Backdoor Defense with Sample-Specific Smoothing Noises</div>
<div class="meta-line">Authors: Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbin Li, Yiming Li</div>
<div class="meta-line">First: 2025-04-30T15:21:25+00:00 · Latest: 2026-02-19T12:16:56+00:00</div>
<div class="meta-line">Comments: To appear in TIFS 2026. 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21730v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.21730v2">PDF</a> · <a href="https://github.com/NcepuQiaoTing/Cert-SSB">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample&#x27;s certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cert-SSBD: 认证样本特定平滑噪声的后门防御认证</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）容易受到后门攻击的影响，攻击者通过操纵一小部分训练数据植入隐藏的后门。受攻击的模型在干净样本上表现正常，但在后门样本上将其错误分类为攻击者指定的目标类别，对实际应用中的DNN构成了重大威胁。目前，已经提出了几种经验防御方法来缓解后门攻击，但这些方法往往被更先进的后门技术绕过。相比之下，基于随机平滑的认证防御显示出潜力，通过向训练和测试样本添加随机噪声来对抗后门攻击。在本文中，我们揭示了现有的随机平滑防御隐含地假设所有样本与决策边界等距，但在实践中这可能不成立，导致认证性能不佳。为解决这一问题，我们提出了一种样本特定的认证后门防御方法，称为Cert-SSB。Cert-SSB首先使用随机梯度上升优化每个样本的噪声幅度，确保样本特定的噪声水平，然后应用于多个受污染的训练集以重新训练多个平滑模型。之后，Cert-SSB聚合多个平滑模型的预测生成最终的鲁棒预测。特别是，在这种情况下，现有的认证方法变得不适用，因为优化的噪声在不同样本之间变化。为了克服这一挑战，我们引入了一种基于存储更新的认证方法，该方法动态调整每个样本的认证区域以提高认证性能。我们在多个基准数据集上进行了广泛的实验，证明了我们提出方法的有效性。我们的代码可在https://github.com/NcepuQiaoTing/Cert-SSB/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of deep neural networks to backdoor attacks by proposing a certified defense method called Cert-SSB. It optimizes noise magnitude for each sample using stochastic gradient ascent and applies it to multiple poisoned training sets to retrain several smoothed models. The method then aggregates predictions from these models to generate a robust prediction. The authors introduce a storage-update-based certification method to improve performance. Experiments on benchmark datasets show the effectiveness of Cert-SSB in defending against backdoor attacks.</div>
<div class="mono" style="margin-top:8px">本文提出了一种样本特定的认证后门防御方法Cert-SSB，以应对深度神经网络对后门攻击的脆弱性。该方法针对每个样本优化噪声幅度，并将其应用于多个受污染的训练集以重新训练平滑模型，然后聚合这些模型的预测。作者引入了一种存储更新基的认证方法，以处理样本间噪声变化的问题，从而提高认证性能。在基准数据集上的实验表明了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment</div>
<div class="meta-line">Authors: Han Zhao, Jingbo Wang, Wenxuan Song, Shuai Chen, Yang Liu, Yan Wang, Haoang Li, Donglin Wang</div>
<div class="meta-line">First: 2026-02-19T11:00:46+00:00 · Latest: 2026-02-19T11:00:46+00:00</div>
<div class="meta-line">Comments: Project Website: https://h-zhao1997.github.io/frappe</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://h-zhao1997.github.io/frappe">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FRAPPE：通过多未来表示对齐将世界建模融入通用政策</div>
<div class="mono" style="margin-top:8px">使VLA模型能够预测环境动态，即世界建模，已被认为是提高机器人推理和泛化的关键。然而，当前的方法面临两个主要问题：1. 训练目标迫使模型过度强调像素级重建，限制了语义学习和泛化；2. 推理过程中依赖预测的未来观察通常会导致误差累积。为了解决这些挑战，我们提出了未来表示对齐通过并行渐进扩展（FRAPPE）。我们的方法采用两阶段微调策略：在中期训练阶段，模型学习预测未来观察的潜在表示；在后期训练阶段，我们并行扩展计算负载并同时与多个不同的视觉基础模型对齐表示。通过显著提高微调效率并减少对标注动作数据的依赖，FRAPPE提供了一种可扩展且数据高效的途径，以增强通用机器人政策的世界意识。在RoboTwin基准测试和实际任务上的实验表明，FRAPPE优于现有最佳方法，并在长时序和未见过的场景中表现出强大的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FRAPPE addresses the limitations of current approaches in world modeling by introducing a two-stage fine-tuning strategy. In the mid-training phase, the model learns to predict the latent representations of future observations, and in the post-training phase, it aligns these representations with multiple visual foundation models. This method improves fine-tuning efficiency and reduces the need for action-annotated data, leading to better generalization in long-horizon and unseen scenarios compared to state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">FRAPPE通过引入两阶段微调策略来解决当前世界建模方法的局限性。在中期训练阶段，模型学习预测未来的潜在表示；在后期训练阶段，它同时与多个视觉基础模型对齐这些表示。这种方法提高了微调效率，减少了对标注动作数据的依赖，使其在长时序和未见过的场景中表现出更强的泛化能力，优于现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place</div>
<div class="meta-line">Authors: Antonio Rapuano, Yaolei Shen, Federico Califano, Chiara Gabellieri, Antonio Franchi</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-19T09:38:32+00:00 · Latest: 2026-02-19T09:38:32+00:00</div>
<div class="meta-line">Comments: Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17199v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>悬索的连续和混合动力学的非线性预测控制研究及其在空中抓取和放置中的应用</div>
<div class="mono" style="margin-top:8px">本文提出了一种框架，用于基于偏微分方程（PDEs）的高保真模型与适合实时控制的降阶表示相结合的空中操作可伸缩悬索的方法。PDEs使用有限差分法离散化，并采用适当的正交分解提取降阶模型（ROM），以保留主要变形模式并显著降低计算复杂性。基于此ROM，提出了一种非线性模型预测控制方案，能够稳定悬索振荡并处理混合过渡，如载荷的附着和脱离。仿真结果证实了ROM的稳定性和效率，以及控制器在各种操作条件下调节悬索动力学的有效性。额外的仿真展示了ROM在受限环境中的轨迹规划应用，证明了所提方法的灵活性。总体而言，该框架使无人驾驶航空器（UAV）携带悬索的实时、动力学感知控制成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a framework for controlling the dynamics of a suspended deformable cable in aerial manipulation tasks. It uses a high-fidelity model based on partial differential equations (PDEs) and a reduced-order model (ROM) derived via proper orthogonal decomposition to manage computational complexity. A nonlinear model predictive control scheme is then applied to stabilize cable oscillations and handle hybrid transitions. Simulation results show the stability, efficiency, and robustness of the ROM and the controller&#x27;s effectiveness in regulating cable dynamics under various conditions, including trajectory planning in constrained environments.</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于空中操作悬垂柔性缆线的动力学控制框架。该框架基于偏微分方程（PDEs）构建高保真模型，并通过适当的正交分解提取低阶模型（ROM），以管理计算复杂性。然后应用非线性模型预测控制方案来稳定缆线振动并处理混合过渡。仿真结果表明ROM的稳定性和效率，以及控制器在各种条件下的有效性，包括在受限环境中进行轨迹规划。</div>
</details>
</div>
<div class="card">
<div class="title">The Bots of Persuasion: Examining How Conversational Agents&#x27; Linguistic Expressions of Personality Affect User Perceptions and Decisions</div>
<div class="meta-line">Authors: Uğur Genç, Heng Gu, Chadha Degachi, Evangelos Niforatos, Senthil Chandrasegaran, Himanshu Verma</div>
<div class="meta-line">First: 2026-02-19T09:10:41+00:00 · Latest: 2026-02-19T09:10:41+00:00</div>
<div class="meta-line">Comments: Accepted to be presented at CHI&#x27;26 in Barcelona</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA&#x27;s composite personality did not affect participants&#x27; decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>说服性的机器人：探究对话代理的语言个性表达如何影响用户感知和决策</div>
<div class="mono" style="margin-top:8px">由大型语言模型驱动的对话代理（CAs）越来越能够通过语言表现出复杂的人格特质，但这些表现如何影响用户尚不清楚。因此，我们研究了在慈善捐赠的背景下，CAs通过语言表达的人格特质如何影响用户的决策和感知。在一项众包研究中，360名参与者与八个CAs之一互动，每个CA表现出由三个语言方面组成的人格特质：态度（乐观/悲观）、权威（权威/顺从）和推理（情感/理性）。虽然CA的整体人格特质并未影响参与者的决策，但它确实影响了他们的感知和情绪反应。特别是，与悲观CAs互动的参与者感到情绪状态较低，对活动的认同感较低，认为CA不够可信和不那么有能力，但倾向于向慈善机构捐款更多。信任感、能力和情境同理心的感知显著预测了捐款决策。我们的研究结果强调了CAs作为操纵工具的风险，它们会微妙地影响用户的感知和决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how conversational agents (CAs) with different linguistic personalities affect users&#x27; perceptions and decisions in charitable giving. Participants interacted with CAs projecting various combinations of optimism/pessimism, authority, and emotional/rational reasoning. While the CA&#x27;s personality did not influence donation decisions, it significantly affected users&#x27; emotional states and perceptions of trust and competence. Pessimistic CAs led to lower emotional states and affinity towards the cause, but paradoxically, participants tended to donate more. Trust, competence, and empathy were key predictors of donation decisions. The study highlights the potential for CAs to manipulate user perceptions subtly.</div>
<div class="mono" style="margin-top:8px">本研究探讨了不同语言个性的对话代理（CAs）如何影响用户在慈善捐赠中的感知和决策。参与者与表现出不同乐观/悲观、权威性和情感/理性推理个性的CAs互动。虽然CAs的个性没有影响捐赠决策，但它显著影响了用户的感情状态和对信任和能力的感知。悲观的CAs导致较低的情感状态和对事业的认同感，但参与者却更倾向于捐款。信任、能力和情境同理心是预测捐赠决策的关键因素。研究强调了CAs作为潜在操纵工具的潜在风险，可以微妙地影响用户的感知和决策。</div>
</details>
</div>
<div class="card">
<div class="title">Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy</div>
<div class="meta-line">Authors: Huishi Huang, Jack Klusmann, Haozhe Wang, Shuchen Ji, Fengkang Ying, Yiyuan Zhang, John Nassour, Gordon Cheng, Daniela Rus, Jun Liu, Marcelo H Ang, Cecilia Laschi</div>
<div class="meta-line">First: 2026-02-19T06:56:47+00:00 · Latest: 2026-02-19T06:56:47+00:00</div>
<div class="meta-line">Comments: Camera-ready version for RoboSoft 2026. 8 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system&#x27;s response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理人机交互在增强现实中的抓取应用：刚柔机器人协同</div>
<div class="mono" style="margin-top:8px">混合刚柔机器人结合了刚性操作臂的精确性和柔性臂的顺应性和适应性，为不规则环境中的多功能抓取提供了有前景的方法。然而，协调混合机器人仍然具有挑战性，由于建模、感知和跨域运动学的困难。在本文中，我们提出了一种新颖的基于增强现实(AR)的物理人机交互框架，该框架使用户能够直接远程操作混合刚柔机器人执行简单的接近和抓取任务。通过AR头显，用户可以与集成到通用物理引擎中的机器人系统模拟模型进行交互，该物理引擎叠加在真实系统上，允许在实际部署之前进行模拟执行。为了确保虚拟和物理机器人之间的一致行为，我们引入了一种基于软机器人固有几何特性的实测参数识别管道，从而能够准确建模其静态和动态行为以及控制系统响应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of coordinating hybrid rigid-soft robots for grasping tasks in unstructured environments by developing an augmented reality (AR) framework for direct teleoperation. The framework allows users to interact with a simulated model of the robotic system, which is then mapped to the real robot through a parameter identification pipeline. Key findings include the successful execution of simple reaching and grasping tasks, and the accurate modeling of the soft robot&#x27;s behavior in both virtual and physical domains.</div>
<div class="mono" style="margin-top:8px">该研究通过开发一种增强现实(AR)框架来解决协调混合刚柔机器人进行抓取任务的问题，允许用户直接操作模拟的机器人模型，并通过参数识别管道将其映射到实际机器人上。主要发现包括成功执行了简单的抓取任务，并且能够在虚拟和物理领域准确模拟软机器人的行为。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success</div>
<div class="meta-line">Authors: Varun Burde, Pavel Burget, Torsten Sattler</div>
<div class="meta-line">First: 2026-02-19T05:55:01+00:00 · Latest: 2026-02-19T05:55:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17101v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17101v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物体姿态估计和重建效果对机器人抓取成功率的基准测试</div>
<div class="mono" style="margin-top:8px">3D重建是许多机器人感知任务的基础层，包括6D物体姿态估计和抓取姿态生成。现代物体的3D重建方法可以从多视角图像中生成视觉和几何上令人印象深刻的网格，但标准的几何评估并不能反映重建质量如何影响下游任务，如机器人操作性能。本文通过引入一个大规模的基于物理的基准测试来填补这一空白，该基准测试根据其在抓取中的功能有效性评估6D姿态估计器和3D网格模型。我们通过在各种重建的3D网格上生成抓取并执行它们在真实模型上的操作，分析模型保真度的影响，模拟使用不完美的模型生成的抓取姿态如何影响与真实物体的交互。这评估了从3D重建中产生的姿态误差、抓取鲁棒性和几何不准确性对抓取性能的综合影响。我们的结果显示，重建伪影显著减少了抓取姿态候选的数量，但在姿态准确估计的情况下，对抓取性能的影响微乎其微。我们的结果还表明，抓取成功与姿态误差之间的关系主要由空间误差主导，即使是简单的平移误差也能揭示对称物体抓取姿态的成功率。本文为如何通过机器人进行物体操作的感知系统提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper benchmarks the effects of object pose estimation and reconstruction on robotic grasping success by introducing a large-scale, physics-based benchmark. The study evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. The results indicate that reconstruction artifacts reduce the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. The study also finds that the relationship between grasp success and pose error is primarily influenced by spatial error, and even simple translation errors can provide insights into the grasping success of symmetric objects.</div>
<div class="mono" style="margin-top:8px">该论文通过基准测试评估了物体姿态估计和重建对机器人抓取成功率的影响。引入了一个大规模的物理基准来评估6D姿态估计器和3D网格模型在抓取功能上的有效性。研究在各种重建的3D网格上生成抓取，并模拟其与真实物体的交互，结果显示重建中的误差减少了抓取姿态候选的数量，但在姿态准确估计的情况下对抓取性能影响甚微。研究结果表明，抓取成功率与姿态误差之间的关系主要由空间误差决定，即使是简单的平移误差也能提供对对称物体抓取姿态成功率的见解。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260223_0337.html">20260223_0337</a>
<a href="archive/20260222_0338.html">20260222_0338</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
