<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-27 03:53</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260227_0353</div>
    <div class="row"><div class="card">
<div class="title">Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</div>
<div class="meta-line">Authors: Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal, Murtuza Jadliwala, Bimal Viswanath</div>
<div class="meta-line">First: 2026-02-25T18:46:30+00:00 · Latest: 2026-02-25T18:46:30+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore. To IEEE SaTML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22197v1">PDF</a> · <a href="https://github.com/mlsecviswanath/img2imgdenoiser">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#x27;s utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现成的图像到图像模型足以击败图像保护方案</div>
<div class="mono" style="margin-top:8px">生成式人工智能（GenAI）的进步导致了各种保护策略的开发，以防止未经授权使用图像。这些方法依赖于在图像上添加不可感知的保护扰动，以阻止诸如风格模仿或深度伪造等滥用行为。尽管之前对这些保护的攻击需要专门的、定制的方法，但我们证明这已不再必要。我们展示了一种现成的图像到图像GenAI模型可以通过简单的文本提示重新利用为通用的“去噪器”，有效地移除各种保护扰动。在涵盖6种不同保护方案的8个案例研究中，我们的通用攻击不仅绕过了这些防御，还在保持图像对攻击者有用性的同时，优于现有的专门攻击。我们的研究结果揭示了当前图像保护领域中一个关键且普遍存在的漏洞，表明许多方案提供了虚假的安全感。我们强调迫切需要开发稳健的防御措施，并表明任何未来的保护机制都必须以现成的GenAI模型攻击为基准。代码可在以下仓库中获得：https://github.com/mlsecviswanath/img2imgdenoiser</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study demonstrates that off-the-shelf image-to-image Generative AI models can be repurposed as generic &#x27;denoisers&#x27; using simple text prompts to remove protective perturbations added to images. The method effectively circumvents various protection schemes and outperforms existing specialized attacks while preserving the image&#x27;s utility. The research highlights a critical vulnerability in current image protection methods, suggesting that many schemes provide a false sense of security and that future defenses must be robust against off-the-shelf GenAI models.</div>
<div class="mono" style="margin-top:8px">研究展示了可以利用现成的图像到图像生成AI模型，通过简单的文本提示作为“去噪器”来移除添加到图像中的保护性干扰。该方法能够有效绕过多种保护方案，并且在保持图像实用性的同时优于现有的专门攻击方法。研究揭示了当前图像保护方法中的一个关键漏洞，表明许多方案提供了虚假的安全感，并强调未来防御机制必须能够抵御现成的生成AI模型攻击。</div>
</details>
</div>
<div class="card">
<div class="title">Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems</div>
<div class="meta-line">Authors: Georgios Kamaras, Craig Innes, Subramanian Ramamoorthy</div>
<div class="meta-line">First: 2025-10-30T16:23:46+00:00 · Latest: 2026-02-25T17:52:16+00:00</div>
<div class="meta-line">Comments: 20 pages, 18 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26656v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26656v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在指定错误领域支持的启发式适应在随机动力学系统无likelihood推断中的应用</div>
<div class="mono" style="margin-top:8px">在机器人学中，无likelihood推断(LFI)可以提供适应学习代理的领域分布，该分布基于参数化的部署条件集。LFI假设一个任意的支持用于采样，该支持在整个初始通用先验逐步细化为更具描述性的后验过程中保持不变。然而，潜在指定错误的支持可能导致次优但错误确定的后验。为了解决这一问题，我们提出了三种启发式LFI变体：EDGE、MODE和CENTRE。每种变体都以自己的方式解释后验模式在推断步骤中的变化，并在LFI步骤中将支持与后验推断一起进行适应。我们首先揭示了支持指定错误的问题，并使用随机动力学基准评估我们的启发式方法。然后，我们评估启发式支持适应对动态可变形线性对象(DLO)操作任务中参数推断和策略学习的影响。对于参数化的DLO集合，推断结果提供了更精细的长度和刚度分类。当使用这些后验作为基于仿真的策略学习的领域分布时，它们会导致更稳健的对象中心代理性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of potentially misspecified support in likelihood-free inference (LFI) for robotics, which can result in suboptimal and falsely certain posteriors. To tackle this, three heuristic LFI variants—EDGE, MODE, and CENTRE—are proposed, each interpreting posterior mode shifts differently. These heuristics adapt the support alongside posterior inference. The study evaluates these heuristics on stochastic dynamical benchmarks and demonstrates improved parameter inference and policy learning for a dynamic deformable linear object manipulation task, leading to more robust object-centric agent performance.</div>
<div class="mono" style="margin-top:8px">本文解决了机器人领域中潜在的采样支持不准确问题，这可能导致次优且虚假确定的后验分布。为此，提出了三种基于后验模式偏移的启发式LFI方法——EDGE、MODE和CENTRE，这些方法能够适应采样支持。这些方法在随机动力学基准测试和动态可变形线性对象操作任务中进行了评估，显示出改进的参数推断和更稳健的对象中心代理性能。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Belief Vision Language Action Models</div>
<div class="meta-line">Authors: Vaidehi Bagaria, Bijo Sebastian, Nirav Kumar Patel</div>
<div class="meta-line">First: 2026-02-24T08:02:16+00:00 · Latest: 2026-02-25T17:38:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20659v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20659v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. While semantic grounding is important, long-horizon manipulation fundamentally requires persistent, action-conditioned state representations. Current VLAs lack such representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once per task, the VLM provides high-level intent, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5 percent and 37.5 percent higher success rates on multi-stage pick-and-place and stacking tasks, respectively, compared to pi_0. It also reduces inference latency by up to five times relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show the belief module is the primary driver of performance, increasing success rates from 32.5 percent without belief to 77.5 percent with belief.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递归信念视语言行动模型</div>
<div class="mono" style="margin-top:8px">视语言行动模型必须使代理能够在部分可观测性下执行长期任务。然而，大多数现有方法仍依赖于短上下文窗口或反复查询视语言模型（VLM），这导致任务进展丢失、感知同义词下的动作重复以及高推理延迟。虽然语义定位很重要，但长期操作本质上需要持久的、基于动作的状态表示。当前的VLAs缺乏这样的表示，且在时间和物理推理方面表现出有限的能力，使其不适合多阶段控制。本文引入了RB-VLA，这是一种以信念为中心的架构，通过自我监督的世界模型目标进行训练，保持一个紧凑的潜在状态编码任务相关的历史、动力学和对象交互。VLM在每次任务时查询一次，提供高层次的意图，而信念追踪任务进展，并在部分可观测性下实现有阶段意识的、因果性的控制，无需存储原始观察或随时间扩展内存。信念和意图共同条件一个扩散策略，以实现稳健的闭环执行。RB-VLA 在长期任务基准测试中优于先前的VLAs，分别在多阶段拾取和放置任务以及堆叠任务中实现了52.5%和37.5%更高的成功率，相比pi_0。它还将推理延迟降低了最多五倍，并消除了现有VLAs在时间步长上观察到的内存增长。消融实验表明，信念模块是性能的主要驱动因素，信念模块的存在将成功率从没有信念时的32.5%提高到有信念时的77.5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing vision-language-action models, which are observation-driven and suffer from loss of task progress, action repetition, and high inference latency. RB-VLA, a belief-centric architecture, is introduced to maintain a compact latent state for task-relevant history and dynamics. This model outperforms prior approaches on long-horizon tasks, achieving higher success rates and reducing inference latency. Ablation studies confirm the belief module&#x27;s critical role in performance improvement.</div>
<div class="mono" style="margin-top:8px">研究针对现有基于视觉-语言-动作模型的局限性，如观察驱动、任务进展丢失、动作重复和高推理延迟。引入了RB-VLA，一种以信念为中心的架构，用于维护任务相关的历史和动态的紧凑潜状态。该模型在长时任务上表现出色，实现了更高的成功率和降低了推理延迟。消融实验确认了信念模块在性能提升中的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Jingxuan Zhang, Yunta Hsieh, Zhongwei Wan, Haokun Lin, Xin Wang, Ziqi Wang, Yingtie Lei, Mi Zhang</div>
<div class="meta-line">First: 2026-02-23T19:55:54+00:00 · Latest: 2026-02-25T17:11:08+00:00</div>
<div class="meta-line">Comments: CVPR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20309v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20309v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QuantVLA：面向视觉-语言-行动模型的规模校准后训练量化</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型将感知、语言和控制统一起来，为具身智能体服务，但由于计算和内存需求迅速增加，尤其是在模型扩展到更长的时间范围和更大的骨干网络时，它们在实际部署中面临重大挑战。为了解决这些瓶颈，我们提出了QuantVLA，这是一种无需训练的后训练量化（PTQ）框架，据我们所知，这是首个针对VLA系统的PTQ方法，并且首次成功量化了扩散变压器（DiT）动作头。QuantVLA 包含三个规模校准组件：（1）一种选择性量化布局，将所有线性层在语言骨干和DiT中转换为整数表示，同时保持注意力投影在浮点数中，以保留原始操作调度；（2）注意力温度匹配，这是一种轻量级的每头缩放机制，用于稳定注意力概率，并在推理时将其折叠到去量化比例中；（3）输出头平衡，这是一种每层残差接口校准，用于缓解后投影能量漂移。该框架不需要额外的训练，仅使用少量未标记的校准缓冲区，并支持低位宽权重和激活的整数内核，同时保持架构不变。在LIBERO上的代表性VLA模型上，QuantVLA 超过了全精度基线的任务成功率，实现了约70%的量化组件相对内存节省，并提供了1.22倍的端到端推理延迟加速，为在严格的计算、内存和功率限制下实现可扩展的低位宽具身智能提供了实际途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">QuantVLA is a training-free post-training quantization framework for vision-language-action models, addressing the compute and memory challenges of scaling these models. It introduces three scale-calibrated components: selective quantization layout, attention temperature matching, and output head balancing. QuantVLA achieves task success rates exceeding full-precision baselines, 70% relative memory savings, and a 1.22x speedup in inference latency on LIBERO models.</div>
<div class="mono" style="margin-top:8px">QuantVLA 是一种无需训练的后训练量化框架，用于解决视觉-语言-行动模型在扩展时面临的计算和内存挑战。它包含选择性量化、注意力温度匹配和输出头平衡等组件，以稳定和优化模型。QuantVLA 提高了任务成功率，将量化组件的内存使用减少了约 70%，并将端到端推理延迟加速了 1.22 倍，使其在严格的计算、内存和功率限制下更加实用。</div>
</details>
</div>
<div class="card">
<div class="title">A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation</div>
<div class="meta-line">Authors: Georgios Kamaras, Subramanian Ramamoorthy</div>
<div class="meta-line">Venue: In IEEE Robotics and Automation Letters, Volume 10, Issue 8, August 2025, Pages 8075-8082</div>
<div class="meta-line">First: 2025-02-25T20:01:06+00:00 · Latest: 2026-02-25T17:09:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18615v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.18615v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种基于分布的实2仿2实物体中心代理适应在视觉驱动的可变形线性物体操作中的处理方法</div>
<div class="mono" style="margin-top:8px">我们提出了一种集成（或端到端）框架，用于基于视觉感知操纵可变形线性物体（DLOs）的实2仿2实问题。使用参数化的DLO集合，我们使用无似然推断（LFI）来计算物理参数的后验分布，从而可以近似模拟每个特定DLO的行为。在训练过程中，我们使用这些后验分布进行领域随机化，在仿真中使用无模型强化学习为DLO抓取任务训练特定于物体的视知觉运动策略（即，假设只有视觉和本体感觉感知）。我们通过零样本的方式部署仿训练的DLO操作策略在现实世界中，即无需任何进一步微调。在此背景下，我们评估了一种流行的LFI方法在仅使用动态操作轨迹中获得的视觉和本体感觉数据对参数化DLO集合进行精细分类的能力。然后我们研究了基于仿真的策略学习中得到的领域分布对现实世界性能的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents an integrated framework for manipulating deformable linear objects (DLOs) in the real world using visual perception. It uses likelihood-free inference to compute the posterior distributions of physical parameters for DLOs, enabling simulation-based training of object-specific visuomotor policies. The trained policies are deployed in the real world without further fine-tuning, demonstrating the framework&#x27;s effectiveness in zero-shot adaptation. The study evaluates the capability of a prominent LFI method for fine classification of DLOs using visual and proprioceptive data, and investigates the implications of these distributions on policy learning and real-world performance.</div>
<div class="mono" style="margin-top:8px">论文提出了一种集成框架，用于基于视觉感知操纵变形线性物体（DLOs）的Real2Sim2Real问题。它使用似然无参数推断来计算物理参数的后验分布，从而模拟DLO的行为。这些后验分布用于训练中的领域随机化，允许使用无模型强化学习学习特定于物体的视觉-运动策略。该方法展示了模拟训练的DLO操纵策略在现实世界的零样本部署能力，评估该方法仅使用视觉和本体感受数据在动态操纵轨迹中进行细分类的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Hongjie Fang, Shirun Tang, Mingyu Mei, Haoxiang Qin, Zihao He, Jingjing Chen, Ying Feng, Chenxi Wang, Wanxi Liu, Zaixing He, Cewu Lu, Shiquan Wang</div>
<div class="meta-line">First: 2026-02-25T16:35:24+00:00 · Latest: 2026-02-25T16:35:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22088v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://force-policy.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: https://force-policy.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>力策略：在交互框架下学习混合力-位置控制策略以应对丰富的接触操作</div>
<div class="mono" style="margin-top:8px">丰富的接触操作要求人类般的感知和力反馈整合：视觉应指导任务进展，而高频率的交互控制则需在不确定性下稳定接触。现有的基于学习的策略往往在单一网络中纠缠这些角色，权衡全局泛化与稳定的局部细化之间的关系，而以控制为中心的方法通常假设已知的任务结构或仅学习控制器参数而非结构本身。在本文中，我们形式化了一个物理上合理的交互框架，即瞬时局部基，将力调节与运动执行解耦，并提出了一种从演示中恢复它的方法。基于此，我们通过提出力策略来解决这两个问题，这是一种全局-局部视觉-力策略，其中全局策略使用视觉指导自由空间动作，接触后，高频率的局部策略结合力反馈估计交互框架并执行混合力-位置控制以实现稳定的交互。在多种丰富的接触任务中的实际实验表明，与强大的基线相比，该策略在接触建立的鲁棒性、力调节的准确性以及对具有不同几何形状和物理特性的新物体的可靠泛化方面均有所提升，最终提高了接触稳定性和执行质量。项目页面：https://force-policy.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of contact-rich manipulation by proposing a Force Policy that integrates vision and force feedback. It formulates an interaction frame to decouple force regulation from motion execution and learns this policy from demonstrations. The Force Policy consists of a global vision-guided policy for free-space actions and a local force-feedback-based policy for stable contact interaction. Experiments demonstrate improved contact stability, accurate force regulation, and reliable generalization to new objects, outperforming strong baselines across various tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了Force Policy方法，通过使用交互框架将力调节与运动执行解耦来应对接触丰富的操作挑战。该方法包括一个由视觉引导的全局策略用于自由空间动作，以及一个在接触时估计交互框架并执行混合力-位置控制的局部策略。实验表明，Force Policy在建立接触、调节力以及处理新对象方面优于强基线，提高了接触稳定性和执行质量。</div>
</details>
</div>
<div class="card">
<div class="title">FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation</div>
<div class="meta-line">Authors: Edgar Welte, Yitian Shi, Rosa Wolf, Maximillian Gilles, Rania Rayyes</div>
<div class="meta-line">First: 2026-02-25T16:06:49+00:00 · Latest: 2026-02-25T16:06:49+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22056v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowCorrect：高效交互式修正生成流策略以实现机器人操作</div>
<div class="mono" style="margin-top:8px">生成性操作策略在部署时分布偏移下可能会灾难性地失败，但许多失败是近失：机器人几乎达到了正确的姿态，并且只需一个小的纠正动作就能成功。我们提出了FlowCorrect，这是一种部署时修正框架，通过稀疏的人工微调将近失失败转化为成功，而无需重新训练整个策略。在执行过程中，人类通过轻量级的VR接口提供简短的纠正姿态微调。FlowCorrect利用这些稀疏的修正来局部适应策略，改进动作而不重新训练骨干模型，同时保持对之前学习场景的性能。我们在一个真实世界的机器人上对三个桌面任务进行了评估：拾取和放置、倒水和杯子直立。即使在低修正预算下，FlowCorrect也将难以解决的情况的成功率提高了85%，同时保持了对之前解决场景的性能。结果表明，FlowCorrect仅通过少量示范就能学习，并在实际机器人操作中的生成视知觉策略部署时实现快速、样本高效的增量、人机交互修正。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FlowCorrect is a deployment-time correction framework that uses sparse human nudges to adapt generative manipulation policies, converting near-miss failures into successes without full policy retraining. During execution, a human provides brief corrective pose nudges via a VR interface, which FlowCorrect uses to locally adapt the policy. The framework improves success on hard cases by 85% while preserving performance on previously learned scenarios, evaluated on three tabletop tasks: pick-and-place, pouring, and cup uprighting.</div>
<div class="mono" style="margin-top:8px">FlowCorrect 是一种部署时矫正框架，通过稀疏的人工调整来适应生成性操作策略，提高接近失败情况的成功率，无需重新训练整个策略。它在拾放、倒液和杯子直立三项任务上进行了评估，实现了在困难情况下的 85% 改进，同时保持了之前解决场景的性能，仅需少量的人工干预。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers</div>
<div class="meta-line">Authors: Guandong Li</div>
<div class="meta-line">First: 2026-02-20T06:24:20+00:00 · Latest: 2026-02-25T15:33:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18022v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT&#x27;s multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器中的双通道注意力引导无训练图像编辑控制</div>
<div class="mono" style="margin-top:8px">基于扩散变换器（DiT）架构的扩散基础图像编辑模型需要无训练控制编辑强度。现有的注意力操作方法仅专注于键空间来调节注意力路由，而完全忽略了值空间——它控制特征聚合。本文首先揭示了DiT多模态注意力层中的键投影和值投影表现出明显的偏差-增量结构，其中令牌嵌入紧密围绕特定层的偏差向量聚类。基于这一观察，我们提出了一种无训练框架——双通道注意力引导（DCAG），同时操控键通道（控制注意力的焦点）和值通道（控制聚合的内容）。我们提供了理论分析，表明键通道通过非线性softmax函数操作，作为粗略的控制旋钮，而值通道通过线性加权求和操作，作为精细的补充。两者共同的二维参数空间$(δ_k, δ_v)$能够比任何单通道方法提供更精确的编辑保真度权衡。在PIE-Bench基准（700张图像，10个编辑类别）上的广泛实验表明，DCAG在所有保真度指标上都优于仅键引导，特别是在局部编辑任务（如对象删除（4.9% LPIPS减少）和对象添加（3.2% LPIPS减少））方面表现最为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for training-free control over editing intensity in diffusion-based image editing models using the Diffusion Transformer (DiT) architecture. It introduces Dual-Channel Attention Guidance (DCAG), which manipulates both the Key and Value channels to control attention routing and feature aggregation, respectively. Experiments show that DCAG outperforms Key-only guidance in localized editing tasks, reducing LPIPS by 4.9% for object deletion and 3.2% for object addition compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本文针对使用Diffusion Transformer (DiT)架构的基于扩散的图像编辑模型中训练-free 控制编辑强度的需求，提出了双通道注意力引导（DCAG），该方法同时操纵 Key 和 Value 通道以提供更精确的编辑-保真度权衡。实验表明，DCAG 在 PIE-Bench 基准上的所有保真度指标中均优于仅操纵 Key 的方法，特别是在对象删除和添加等局部编辑任务中表现出显著改进。</div>
</details>
</div>
<div class="card">
<div class="title">World Guidance: World Modeling in Condition Space for Action Generation</div>
<div class="meta-line">Authors: Yue Su, Sijin Chen, Haixin Shi, Mingyu Liu, Zhengshen Zhang, Ningyuan Huang, Weiheng Zhong, Zhengbang Zhu, Yuxiao Liu, Xihui Liu</div>
<div class="meta-line">First: 2026-02-25T15:27:09+00:00 · Latest: 2026-02-25T15:27:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://selen-suyue.github.io/WoGNet/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22010v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://selen-suyue.github.io/WoGNet/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界指导：在条件空间中进行世界建模以促进动作生成</div>
<div class="mono" style="margin-top:8px">利用未来观察建模来促进动作生成为增强视觉-语言-动作（VLA）模型的能力提供了有希望的途径。然而，现有的方法难以在保持高效、可预测的未来表示和保留足够的细粒度信息以指导精确的动作生成之间取得平衡。为了解决这一局限性，我们提出了WoG（世界指导）框架，该框架通过将未来观察注入动作推理管道中，将未来观察映射到紧凑的条件中。然后，VLA被训练同时预测这些压缩条件和未来动作，从而在条件空间中实现有效的世界建模以进行动作推理。我们证明，建模和预测此条件空间不仅有助于细粒度的动作生成，还表现出更强的泛化能力。此外，它能够有效地从大量的人类操作视频中学习。广泛的实验在模拟和真实环境中的验证表明，我们的方法显著优于基于未来预测的现有方法。项目页面：https://selen-suyue.github.io/WoGNet/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the capabilities of Vision-Language-Action models by leveraging future observation modeling to enhance action generation. The proposed WoG framework maps future observations into compact conditions, which are then used to guide action inference. Experiments show that this approach not only facilitates precise action generation but also demonstrates superior generalization and learning from human manipulation videos, outperforming existing methods in both simulation and real-world environments.</div>
<div class="mono" style="margin-top:8px">论文提出了一种名为WoG（World Guidance）的框架，将未来观察映射到紧凑的条件中以增强Vision-Language-Action模型中的动作生成。通过训练模型同时预测这些条件和未来动作，WoG在条件空间内实现了有效的世界建模。实验表明，这种方法不仅提高了精细动作生成的效果，还展示了比基于未来预测的现有方法更好的泛化能力和性能。</div>
</details>
</div>
<div class="card">
<div class="title">Rod models in continuum and soft robot control: a review</div>
<div class="meta-line">Authors: Carlo Alessi, Camilla Agabiti, Daniele Caradonna, Cecilia Laschi, Federico Renda, Egidio Falotico</div>
<div class="meta-line">First: 2024-07-08T12:46:19+00:00 · Latest: 2026-02-25T15:26:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.05886v2">Abs</a> · <a href="https://arxiv.org/pdf/2407.05886v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuum and soft robots can transform diverse sectors, including healthcare, agriculture, marine, and space, thanks to their potential to adaptively interact with unstructured environments. These robots exhibit complex mechanics that pose diverse challenges in modeling and control. Among various models, continuum mechanical models based on rod theories can effectively capture the deformations of slender bodies in contact-rich scenarios. This structured review paper focuses on the role of rod models in continuum and soft robot control with a vertical approach. We provide a comprehensive summary of the mathematical background underlying the four main rod theories applied in soft robotics and their variants. Then, we review the literature on rod models applied to continuum and soft robots, providing a novel categorization in deformation classes. Finally, we survey recent model-based and learning-based control strategies leveraging rod models, highlighting their potential in real-world manipulation. We critically discuss the trends, advantages, limitations, research gaps, and possible future developments of rod models. This paper aims to guide researchers who intend to simulate and control new soft robots while providing feedback to the design and manufacturing community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续体和软体机器人控制中的杆模型：综述</div>
<div class="mono" style="margin-top:8px">连续体和软体机器人可以通过其适应性与非结构化环境交互的潜力，改变医疗、农业、海洋和太空等多个领域。这些机器人表现出复杂的机械特性，给建模和控制带来了多种挑战。在各种模型中，基于杆理论的连续体力学模型能够有效地捕捉在接触丰富的场景中细长体的变形。本文从垂直角度对杆模型在连续体和软体机器人控制中的作用进行了结构化的综述。我们提供了四种主要应用于软体机器人的杆理论及其变体的数学背景的全面总结。然后，我们回顾了应用于连续体和软体机器人的杆模型文献，提供了变形类别的新颖分类。最后，我们调查了利用杆模型的基于模型和基于学习的控制策略，突显了它们在实际操作中的潜力。我们批判性地讨论了杆模型的趋势、优势、局限性、研究空白和可能的未来发展方向。本文旨在指导希望模拟和控制新软体机器人的研究人员，并为设计和制造社区提供反馈。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper reviews the role of rod models in the control of continuum and soft robots, emphasizing their ability to model the complex deformations of slender bodies in various applications. The authors provide a comprehensive summary of the mathematical foundations of four main rod theories and their variants, and categorize the literature based on deformation classes. They also discuss recent control strategies that leverage rod models, highlighting their potential in real-world manipulation. The study identifies trends, advantages, limitations, and research gaps, aiming to guide both researchers and the design community.</div>
<div class="mono" style="margin-top:8px">本文回顾了杆模型在连续和软体机器人控制中的作用，强调了它们在复杂环境中捕捉细长体变形的能力。作者提供了四种主要杆理论及其变体的数学背景的全面总结，并回顾了它们在软体机器人中的应用。他们还讨论了利用杆模型的最新控制策略，突出了其在实际操作中的潜力。研究指出了杆模型在软体机器人控制中的趋势、优势、局限性和研究空白。</div>
</details>
</div>
<div class="card">
<div class="title">Lang2Lift: A Language-Guided Autonomous Forklift System for Outdoor Industrial Pallet Handling</div>
<div class="meta-line">Authors: Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi</div>
<div class="meta-line">First: 2025-08-21T10:28:39+00:00 · Latest: 2026-02-25T14:51:19+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.15427v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.15427v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eric-nguyen1402.github.io/lang2lift.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating pallet handling in outdoor logistics and construction environments remains challenging due to unstructured scenes, variable pallet configurations, and changing environmental conditions. In this paper, we present Lang2Lift, an end-to-end language-guided autonomous forklift system designed to support practical pallet pick-up operations in real-world outdoor settings. The system enables operators to specify target pallets using natural language instructions, allowing flexible selection among multiple pallets with different loads and spatial arrangements. Lang2Lift integrates foundation-model-based perception modules with motion planning and control in a closed-loop autonomy pipeline. Language-grounded visual perception is used to identify and segment target pallets, followed by 6D pose estimation and geometric refinement to generate manipulation-feasible insertion poses. The resulting pose estimates are directly coupled with the forklift planning and control modules to execute fully autonomous pallet pick-up maneuvers. We deploy and evaluate the proposed system on the ADAPT autonomous outdoor forklift platform across diverse real-world scenarios, including cluttered scenes, variable lighting, and different payload configurations. Tolerance-based pose evaluation further indicates accuracy sufficient for successful fork insertion. Timing and failure analyses highlight key deployment trade-offs and practical limitations, providing insights into integrating language-guided perception within industrial automation systems. Video demonstrations are available at https://eric-nguyen1402.github.io/lang2lift.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Lang2Lift：一种语言引导的自主叉车系统，用于户外工业托盘处理</div>
<div class="mono" style="margin-top:8px">在户外物流和建筑环境中自动化托盘处理仍然具有挑战性，因为存在未结构化的场景、变化的托盘配置和环境条件。本文介绍了Lang2Lift，这是一种端到端的语言引导的自主叉车系统，旨在支持在实际户外环境中的托盘拾取操作。该系统使操作员能够使用自然语言指令指定目标托盘，从而在具有不同负载和空间布局的多个托盘中实现灵活选择。Lang2Lift将基于基础模型的感知模块与运动规划和控制集成在一个闭环自主管道中。基于语言的视觉感知用于识别和分割目标托盘，随后进行6D姿态估计和几何细化以生成可操作的插入姿态。生成的姿态估计直接与叉车规划和控制模块耦合，以执行完全自主的托盘拾取操作。我们在ADAPT自主户外叉车平台上部署并评估了该系统，涵盖了多种实际场景，包括杂乱的场景、变化的照明和不同的负载配置。基于容差的姿态评估进一步表明了足够的准确性，足以实现叉子插入。时间分析和失败分析突显了关键部署权衡和实际限制，提供了将语言引导的感知集成到工业自动化系统中的见解。视频演示可在https://eric-nguyen1402.github.io/lang2lift.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Lang2Lift is an end-to-end language-guided autonomous forklift system designed for outdoor pallet handling. It uses natural language instructions to specify target pallets and integrates perception, motion planning, and control modules. Key findings include successful autonomous pallet pick-up in diverse real-world scenarios, with accurate pose estimation and sufficient tolerance for successful fork insertion despite challenging conditions like cluttered scenes and variable lighting.</div>
<div class="mono" style="margin-top:8px">Lang2Lift 是一个用于户外托盘处理的端到端语言引导自主叉车系统。它使用自然语言指令指定目标托盘，并集成感知、运动规划和控制模块。关键发现包括在多种现实世界场景中成功实现自主托盘拾取，尽管存在杂乱场景和变化光照等挑战条件，仍能实现准确的姿态估计和足够的容差以实现成功的叉子插入。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation</div>
<div class="meta-line">Authors: Haolei Tong, Yuezhe Zhang, Sophie Lueth, Georgia Chalvatzaki</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-05-19T19:12:29+00:00 · Latest: 2026-02-25T14:04:56+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE International Conference on Robotics and Automation 2026(ICRA 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13667v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.13667v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinated multi-arm manipulation requires satisfying multiple simultaneous geometric constraints across high-dimensional configuration spaces, which poses a significant challenge for traditional planning and control methods. In this work, we propose Adaptive Diffusion Constrained Sampling (ADCS), a generative framework that flexibly integrates both equality (e.g., relative and absolute pose constraints) and structured inequality constraints (e.g., proximity to object surfaces) into an energy-based diffusion model. Equality constraints are modeled using dedicated energy networks trained on pose differences in Lie algebra space, while inequality constraints are represented via Signed Distance Functions (SDFs) and encoded into learned constraint embeddings, allowing the model to reason about complex spatial regions. A key innovation of our method is a Transformer-based architecture that learns to weight constraint-specific energy functions at inference time, enabling flexible and context-aware constraint integration. Moreover, we adopt a two-phase sampling strategy that improves precision and sample diversity by combining Langevin dynamics with resampling and density-aware re-weighting. Experimental results on dual-arm manipulation tasks show that ADCS significantly improves sample diversity and generalization across settings demanding precise coordination and adaptive constraint handling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双臂机器人操作的自适应扩散约束采样</div>
<div class="mono" style="margin-top:8px">协调多臂操作需要同时满足高维配置空间中的多个几何约束，这对传统的规划和控制方法构成了重大挑战。本文提出了一种自适应扩散约束采样(ADCS)生成框架，该框架灵活地将等式约束（例如，相对和绝对姿态约束）和结构化不等式约束（例如，与物体表面的接近度）整合到基于能量的扩散模型中。等式约束使用在李代数空间中姿态差异上训练的专用能量网络进行建模，而不等式约束通过符号距离函数(SDF)表示并编码到学习的约束嵌入中，使模型能够推理复杂的空间区域。我们方法的关键创新是一种基于变换器的架构，在推理时学习约束特定的能量函数的加权，从而实现灵活且上下文相关的约束整合。此外，我们采用两阶段采样策略，通过结合拉angevin动力学、重采样和密度感知重加权来提高精度和样本多样性。实验结果表明，ADCS在需要精确协调和自适应约束处理的双臂操作任务中显著提高了样本多样性和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of coordinating two arms for manipulation by integrating both equality and inequality constraints into a generative model. The proposed Adaptive Diffusion Constrained Sampling (ADCS) uses energy networks and Signed Distance Functions to model constraints and a Transformer-based architecture to dynamically weight these constraints. A two-phase sampling strategy further enhances sample diversity and precision. Experiments demonstrate that ADCS outperforms traditional methods in tasks requiring precise coordination and adaptive constraint handling.</div>
<div class="mono" style="margin-top:8px">研究旨在通过满足多个几何约束来协调双臂进行操作。提出了一种自适应扩散约束采样（ADCS）方法，将等式和不等式约束整合到基于能量的模型中，使用专用的能量网络和Signed Distance Functions。该方法采用基于Transformer的架构动态加权约束，并采用两阶段采样策略以提高样本多样性和精度。实验表明，ADCS在需要精确协调和自适应约束处理的任务中表现优于传统方法。</div>
</details>
</div>
<div class="card">
<div class="title">Learning in the Null Space: Small Singular Values for Continual Learning</div>
<div class="meta-line">Authors: Cuong Anh Pham, Praneeth Vepakomma, Samuel Horváth</div>
<div class="meta-line">First: 2026-02-25T13:55:06+00:00 · Latest: 2026-02-25T13:55:06+00:00</div>
<div class="meta-line">Comments: 17 pages, accepted as Oral presentation at the Third Conference on Parsimony and Learning (CPAL 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21919v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21919v1">PDF</a> · <a href="https://github.com/pacman-ctm/NESS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on this principle, we introduce NESS (Null-space Estimated from Small Singular values), a CL method that applies orthogonality directly in the weight space rather than through gradient manipulation. Specifically, NESS constructs an approximate null space using the smallest singular values of each layer&#x27;s input representation and parameterizes task-specific updates via a compact low-rank adaptation (LoRA-style) formulation constrained to this subspace. The subspace basis is fixed to preserve the null-space constraint, and only a single trainable matrix is learned for each task. This design ensures that the resulting updates remain approximately in the null space of previous inputs while enabling adaptation to new tasks. Our theoretical analysis and experiments on three benchmark datasets demonstrate competitive performance, low forgetting, and stable accuracy across tasks, highlighting the role of small singular values in continual learning. The code is available at https://github.com/pacman-ctm/NESS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在零空间学习：小奇异值在持续学习中的应用</div>
<div class="mono" style="margin-top:8px">在持续学习（CL）中，缓解灾难性遗忘并允许进一步学习是一个主要挑战。基于正交的训练方法因其效率和强大的理论性质而受到关注，许多现有方法通过梯度投影来强制正交性。在本文中，我们重新审视了正交性，并利用小奇异值对应于与先前任务输入空间几乎正交的方向这一事实。基于这一原则，我们引入了NESS（从小奇异值估计零空间）方法，该方法直接在权重空间中应用正交性，而不是通过梯度操作。具体而言，NESS 使用每层输入表示的最小奇异值构建一个近似零空间，并通过受限于该子空间的紧凑低秩适应（LoRA风格）形式参数化特定任务的更新。子空间基固定以保持零空间约束，并为每个任务学习一个可训练矩阵。这种设计确保了生成的更新保持在先前输入的零空间附近，同时允许对新任务的适应。我们的理论分析和在三个基准数据集上的实验表明，NESS 具有竞争力的性能、低遗忘率和跨任务的稳定准确性，突显了小奇异值在持续学习中的作用。代码可在 https://github.com/pacman-ctm/NESS/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of catastrophic forgetting in continual learning by proposing NESS, a method that leverages small singular values to enforce orthogonality in the weight space. NESS constructs an approximate null space using the smallest singular values and updates the model via a compact low-rank adaptation constrained to this subspace. Experiments on three benchmark datasets show that NESS achieves competitive performance, low forgetting, and stable accuracy across tasks, emphasizing the importance of small singular values in continual learning.</div>
<div class="mono" style="margin-top:8px">该论文通过引入NESS方法，利用小奇异值在权重空间中强制正交性来解决连续学习中的灾难性遗忘问题。NESS 使用每一层输入表示的最小奇异值构建一个近似零空间，并通过受限于该子空间的紧凑低秩适应更新模型。理论分析和在基准数据集上的实验表明，NESS 实现了竞争力的性能、低遗忘率和跨任务的稳定准确性，突显了小奇异值在连续学习中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">ArtVIP: Articulated Digital Assets of Visual Realism, Modular Interaction, and Physical Fidelity for Robot Learning</div>
<div class="meta-line">Authors: Zhao Jin, Zhengping Che, Tao Li, Zhen Zhao, Kun Wu, Yuheng Zhang, Yinuo Zhao, Zehui Liu, Qiang Zhang, Xiaozhu Ju, Jing Tian, Yousong Xue, Jian Tang</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-06-05T12:16:27+00:00 · Latest: 2026-02-25T13:23:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04941v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.04941v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://x-humanoid-artvip.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot learning increasingly relies on simulation to advance complex ability such as dexterous manipulations and precise interactions, necessitating high-quality digital assets to bridge the sim-to-real gap. However, existing open-source articulated-object datasets for simulation are limited by insufficient visual realism and low physical fidelity, which hinder their utility for training models mastering robotic tasks in real world. To address these challenges, we introduce ArtVIP, a comprehensive open-source dataset comprising high-quality digital-twin articulated objects, accompanied by indoor-scene assets. Crafted by professional 3D modelers adhering to unified standards, ArtVIP ensures visual realism through precise geometric meshes and high-resolution textures, while physical fidelity is achieved via fine-tuned dynamic parameters. Meanwhile, the dataset pioneers embedded modular interaction behaviors within assets and pixel-level affordance annotations. Feature-map visualization and optical motion capture are employed to quantitatively demonstrate ArtVIP&#x27;s visual and physical fidelity, with its applicability validated across imitation learning and reinforcement learning experiments. Provided in USD format with detailed production guidelines, ArtVIP is fully open-source, benefiting the research community and advancing robot learning research. Our project is at https://x-humanoid-artvip.github.io/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ArtVIP：视觉逼真、模块化交互和物理保真的 articulated 数字资产，用于机器人学习</div>
<div class="mono" style="margin-top:8px">机器人学习越来越多地依赖于模拟来提升复杂的操作能力，如灵巧操作和精确交互，这需要高质量的数字资产来弥合模拟与现实之间的差距。然而，现有的开源 articulated 物体数据集在视觉逼真度和物理保真度方面存在局限性，这限制了它们在训练掌握机器人任务的模型方面的实用性。为了解决这些挑战，我们引入了 ArtVIP，这是一个全面的开源数据集，包含高质量的数字孪生 articulated 物体，以及室内场景资产。这些资产由专业的 3D 模型师按照统一标准制作，通过精确的几何网格和高分辨率纹理确保视觉逼真度，通过精细调整的动力学参数实现物理保真度。同时，数据集在资产中引入了嵌入式模块化交互行为和像素级功能注释。特征图可视化和光学动作捕捉被用来定量展示 ArtVIP 的视觉和物理保真度，其适用性通过模仿学习和强化学习实验得到了验证。以 USD 格式提供，附有详细的生产指南，ArtVIP 完全开源，惠及研究社区并推动机器人学习研究。我们的项目网址为 https://x-humanoid-artvip.github.io/ 。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ArtVIP is a comprehensive open-source dataset designed to enhance robot learning by providing high-quality digital-twin articulated objects with visual realism and physical fidelity. It includes indoor-scene assets and modular interaction behaviors, along with pixel-level affordance annotations. The dataset&#x27;s visual and physical fidelity are quantitatively demonstrated through feature-map visualization and optical motion capture, and its effectiveness is validated in imitation and reinforcement learning experiments.</div>
<div class="mono" style="margin-top:8px">ArtVIP 是一个综合性的开源数据集，旨在通过高质量的具有视觉真实感和物理精度的数字资产来提升机器人学习。它包含可动物体和室内场景资产，并具有模块化交互行为和像素级的可用性注释。ArtVIP 通过特征图可视化和光学动作捕捉展示了其视觉和物理精度，并通过模仿学习和强化学习实验验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations</div>
<div class="meta-line">Authors: Qingtao Liu, Zhengnan Sun, Yu Cui, Haoming Li, Gaofeng Li, Lin Shao, Jiming Chen, Qi Ye</div>
<div class="meta-line">Venue: IEEE Transactions on Robotics, vol. 42, pp. 799-818, 2026</div>
<div class="meta-line">First: 2026-02-25T11:38:07+00:00 · Latest: 2026-02-25T11:38:07+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Transactions on Robotics (T-RO), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21811v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21811v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DexRepNet++: 使用几何和空间手-物表示学习灵巧的机器人操作</div>
<div class="mono" style="margin-top:8px">灵巧的机器人操作由于多指机器人手的高自由度和复杂的接触而是一个具有挑战性的问题。许多现有的基于深度强化学习（DRL）的方法旨在提高在高维输出动作空间中的样本效率。然而，现有的工作往往忽略了在手-物交互的复杂输入空间中实现操作策略泛化的表示的作用。在本文中，我们提出了一种新的手-物交互表示DexRep，用于捕捉物体表面特征和手与物体之间的空间关系，以学习灵巧操作技能。基于DexRep，我们学习了三种灵巧操作任务的策略，即抓取、在手旋转、双臂传递，并进行了广泛的实验以验证其有效性。在仿真中，对于抓取任务，使用40个物体学习的策略在超过5000个未见过的、不同类别的物体上实现了87.9%的成功率，显著优于使用数千个物体训练的现有工作；对于在手旋转和传递任务，策略也使现有手-物表示的成功率和其他指标提高了20%到40%。使用DexRep的抓取策略在多摄像头和单摄像头设置下部署到真实世界中，并展示了较小的仿真到现实世界的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of dexterous robotic manipulation by proposing DexRepNet++, which introduces DexRep, a new hand-object interaction representation to capture object surface features and spatial relations. The method is applied to three tasks: grasping, in-hand reorientation, and bimanual handover. Experiments show that DexRepNet++ significantly improves success rates and other metrics compared to existing methods, achieving a 87.9% success rate on unseen objects in simulation and demonstrating a small sim-to-real gap in the real world.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的手物交互表示DexRep，用于捕捉物体表面特征和手物之间的空间关系，以解决灵巧机器人操作的挑战。该方法在抓取、在手旋转和双臂传递三个任务上进行了评估，显著提高了成功率和其他指标，在模拟中对未见过的物体抓取成功率达到了87.9%，并在真实世界中展示了较小的模拟到现实的差距。</div>
</details>
</div>
<div class="card">
<div class="title">mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning</div>
<div class="meta-line">Authors: Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath, Pieter Abbeel</div>
<div class="meta-line">First: 2026-01-29T18:11:26+00:00 · Latest: 2026-02-25T11:20:15+00:00</div>
<div class="meta-line">Comments: Comments: 11 pages; Code is available at https://github.com/mujocolab/mjlab ; Expanded sensor and domain randomization sections, added references, minor edits</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22074v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22074v2">PDF</a> · <a href="https://github.com/mujocolab/mjlab">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mjlab：一种基于GPU加速的机器人学习轻量级框架</div>
<div class="mono" style="margin-top:8px">我们介绍了mjlab，这是一种轻量级、开源的机器人学习框架，结合了GPU加速的模拟、可组合的环境以及最少的设置摩擦。mjlab采用了Isaac Lab引入的基于管理器的API，用户可以组合模块化的观察、奖励和事件构建块，并与MuJoCo Warp结合使用，实现GPU加速的物理计算。结果是一个可以通过单个命令安装、依赖最少且直接访问原生MuJoCo数据结构的框架。mjlab附带了速度跟踪、动作模仿和操作任务的参考实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation is to provide a lightweight and open-source framework for robot learning that simplifies setup and leverages GPU acceleration. The main method involves using a manager-based API and MuJoCo Warp for efficient simulation and access to native MuJoCo data structures. Key experimental findings include the framework&#x27;s ease of installation and its application in reference implementations of velocity tracking, motion imitation, and manipulation tasks.</div>
<div class="mono" style="margin-top:8px">研究动机是提供一个轻量级且开源的机器人学习框架，简化设置并利用GPU加速。主要方法是使用基于管理器的API和MuJoCo Warp进行高效的模拟和访问原生MuJoCo数据结构。关键实验发现包括该框架的易于安装以及在速度跟踪、运动模仿和操作任务的参考实现中的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild</div>
<div class="meta-line">Authors: Hao Luo, Ye Wang, Wanpeng Zhang, Haoqi Yuan, Yicheng Feng, Haiweng Xu, Sipeng Zheng, Zongqing Lu</div>
<div class="meta-line">First: 2026-02-25T09:46:42+00:00 · Latest: 2026-02-25T09:46:42+00:00</div>
<div class="meta-line">Comments: CVPR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21736v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21736v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (&gt;2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>联合对齐潜在动作：面向野外的大规模VLA预训练</div>
<div class="mono" style="margin-top:8px">尽管取得了进展，视觉-语言-动作模型（VLAs）仍受限于大规模多样机器人数据的稀缺性。虽然人类操作视频提供了丰富的替代方案，但现有方法被迫在小型精确标注数据集和广泛但标签不可靠的野外视频之间做出选择。我们提出了JALA，一种预训练框架，学习联合对齐的潜在动作。JALA 跳过了完整的视觉动态重建，而是学习一个与逆动力学和真实动作都对齐的预测动作嵌入。这产生了一个具有过渡感知和行为中心的潜在空间，用于从异构人类数据中学习。我们通过UniHand-Mix（750万视频片段，超过2000小时，结合实验室和野外视频）扩大了这种方法的规模。实验表明，JALA 在受控和非受控场景中生成了更真实的手部动作，显著提高了机器人操作的下游性能，无论是模拟还是真实世界任务。这些结果表明，联合对齐的潜在动作为从人类数据中进行VLA预训练提供了一种可扩展的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the scarcity of large-scale, diverse robot data for Vision-Language-Action models by proposing JALA, a pretraining framework that learns jointly-aligned latent actions. JALA avoids full visual dynamic reconstruction and instead learns a predictive action embedding aligned with both inverse dynamics and real actions, creating a transition-aware, behavior-centric latent space. Experiments show that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, leading to significant improvements in downstream robot manipulation performance in both simulation and real-world tasks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出JALA预训练框架解决Vision-Language-Action模型中缺乏大规模多样化机器人数据的问题。JALA不重建完整的视觉动态，而是学习一个与逆动力学和真实动作都对齐的预测动作嵌入，创建一个过渡感知的行为中心化潜在空间。实验表明，JALA在仿真和真实世界任务中均能显著提高下游机器人操作性能，生成更真实的手部动作。这表明，联合对齐的潜在动作提供了一种从人类数据中进行VLA预训练的可扩展途径。</div>
</details>
</div>
<div class="card">
<div class="title">Private and Robust Contribution Evaluation in Federated Learning</div>
<div class="meta-line">Authors: Delio Jaramillo Velez, Gergely Biczok, Alexandre Graell i Amat, Johan Ostman, Balazs Pejo</div>
<div class="meta-line">First: 2026-02-25T09:27:40+00:00 · Latest: 2026-02-25T09:27:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21721v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21721v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-silo federated learning allows multiple organizations to collaboratively train machine learning models without sharing raw data, but client updates can still leak sensitive information through inference attacks. Secure aggregation protects privacy by hiding individual updates, yet it complicates contribution evaluation, which is critical for fair rewards and detecting low-quality or malicious participants. Existing marginal-contribution methods, such as the Shapley value, are incompatible with secure aggregation, and practical alternatives, such as Leave-One-Out, are crude and rely on self-evaluation.
  We introduce two marginal-difference contribution scores compatible with secure aggregation. Fair-Private satisfies standard fairness axioms, while Everybody-Else eliminates self-evaluation and provides resistance to manipulation, addressing a largely overlooked vulnerability. We provide theoretical guarantees for fairness, privacy, robustness, and computational efficiency, and evaluate our methods on multiple medical image datasets and CIFAR10 in cross-silo settings. Our scores consistently outperform existing baselines, better approximate Shapley-induced client rankings, and improve downstream model performance as well as misbehavior detection. These results demonstrate that fairness, privacy, robustness, and practical utility can be achieved jointly in federated contribution evaluation, offering a principled solution for real-world cross-silo deployments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of evaluating contributions in federated learning while maintaining privacy and fairness. It introduces two new contribution scores, Fair-Private and Everybody-Else, which are compatible with secure aggregation. The scores are evaluated on medical image datasets and CIFAR10, showing superior performance compared to existing methods, better approximations of Shapley-induced rankings, and improved model performance and misbehavior detection.</div>
<div class="mono" style="margin-top:8px">论文解决了在保持隐私和鲁棒性的前提下，在联邦学习中进行贡献评估的挑战。引入了两种与安全聚合兼容的边际差异贡献评分方法，Fair-Private 和 Everybody-Else。这些方法满足公平性公理，抵抗操纵，并提供了隐私、鲁棒性和计算效率的理论保证。在医疗图像数据集和CIFAR10上的实验结果表明，这些评分方法优于现有方法，更好地逼近Shapley诱导的排名，并提高模型性能和恶意行为检测。</div>
</details>
</div>
<div class="card">
<div class="title">PD-VLA: Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding</div>
<div class="meta-line">Authors: Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Zhijun Li, Donglin Wang, Jun Ma, Lujia Wang, Haoang Li</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2025-03-04T06:12:08+00:00 · Latest: 2026-02-25T09:26:06+00:00</div>
<div class="meta-line">Comments: Accepted by IROS 2025, updated results on LIBERO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.02310v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.02310v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models demonstrate remarkable potential for generalizable robotic manipulation. The performance of VLA models can be improved by integrating with action chunking, a critical technique for effective control. However, action chunking linearly scales up action dimensions in VLA models with increased chunking sizes. This reduces the inference efficiency. To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. Our framework reformulates autoregressive decoding as a nonlinear system solved by parallel fixed-point iterations. This approach preserves model performance with mathematical guarantees while significantly improving decoding speed. In addition, it enables training-free acceleration without architectural changes, as well as seamless synergy with existing acceleration techniques. Extensive simulations validate that our PD-VLA maintains competitive success rates while achieving 2.52 times execution frequency on manipulators (with 7 degrees of freedom) compared with the fundamental VLA model. Furthermore, we experimentally identify the most effective settings for acceleration. Finally, real-world experiments validate its high applicability across different tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PD-VLA：通过并行解码结合动作切分加速视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型在通用机器人操作方面展现出巨大的潜力。通过结合动作切分，一种有效的控制技术，可以提高VLA模型的性能。然而，动作切分会导致VLA模型的动作维度线性增加，随着切分块大小的增加而降低推理效率。为了解决这一问题，我们提出了PD-VLA，这是第一个结合动作切分的VLA模型的并行解码框架。我们的框架将自回归解码重新表述为通过并行固定点迭代求解的非线性系统。这种方法在保持模型性能的同时，显著提高了解码速度，并且无需架构更改即可实现无训练加速，同时与现有的加速技术无缝协同。广泛的仿真验证了我们的PD-VLA在保持竞争力的同时，与基本VLA模型相比，执行频率提高了2.52倍（具有7个自由度的操作器）。此外，我们实验性地确定了加速的最佳设置。最后，实际实验验证了其在不同任务中的高适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PD-VLA is a parallel decoding framework designed to enhance the performance of Vision-Language-Action models integrated with action chunking, addressing the issue of reduced inference efficiency. By reformulating autoregressive decoding as a nonlinear system solved through parallel fixed-point iterations, PD-VLA maintains model performance while significantly increasing decoding speed. Experimental results show that PD-VLA achieves 2.52 times the execution frequency of the basic VLA model while maintaining competitive success rates. The framework also allows for training-free acceleration and seamless integration with other acceleration techniques.</div>
<div class="mono" style="margin-top:8px">论文解决了在将动作分块集成到视觉-语言-动作（VLA）模型中时，由于减少推理效率所面临的问题。提出了一种并行解码框架PD-VLA，通过重新公式化自回归解码来提高速度同时保持模型性能。PD-VLA在具有7个自由度的操纵器上实现了基本VLA模型2.52倍的执行频率，无需进行架构更改或训练。研究还确定了加速的最佳设置，并验证了PD-VLA在不同任务中的高适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Primary-Fine Decoupling for Action Generation in Robotic Imitation</div>
<div class="meta-line">Authors: Xiaohan Lei, Min Wang, Wengang Zhou, Xingyu Lu, Houqiang Li</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-02-25T08:36:45+00:00 · Latest: 2026-02-25T08:36:45+00:00</div>
<div class="meta-line">Comments: The Fourteenth International Conference on Learning Representations (ICLR), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21684v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG&#x27;s two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>初级-精细解耦在机器人模仿中的动作生成</div>
<div class="mono" style="margin-top:8px">机器人操作动作序列的多模态分布对模仿学习构成了关键挑战。为此，现有方法通常将动作空间建模为离散的标记集或连续的潜在变量分布。然而，这两种方法都存在权衡：一些方法将动作离散化为标记，从而失去精细的动作变化，而另一些方法在单阶段生成连续动作时，往往会产生不稳定的模式转换。为了解决这些限制，我们提出了初级-精细解耦的动作生成（PF-DAG），这是一种两阶段框架，将粗略的动作一致性与精细的变化解耦。首先，我们将动作片段压缩成少量的离散模式，使轻量级策略能够选择一致的粗略模式并避免模式跳跃。其次，学习一种基于模式的MeanFlow策略来生成高保真的连续动作。理论上，我们证明了PF-DAG的两阶段设计比单阶段生成策略具有更低的均方误差下界。实验上，PF-DAG在Adroit、DexArt和MetaWorld基准测试的56个任务中优于最先进的基线方法，并进一步推广到现实世界的触觉灵巧操作任务。我们的工作表明，显式的模式级解耦能够同时实现稳健的多模态建模和反应式闭环控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of modeling multi-modal action sequences in robotic manipulation by proposing Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework. PF-DAG first compresses action sequences into discrete modes to ensure consistency, and then generates fine-grained continuous actions using a mode-conditioned MeanFlow policy. Empirically, PF-DAG outperforms existing methods across various robotic manipulation tasks and demonstrates robustness in real-world tactile manipulation scenarios.</div>
<div class="mono" style="margin-top:8px">论文针对机器人操作中的多模态动作序列建模难题，提出了PF-DAG框架，该框架通过两阶段方法将粗粒度动作一致性与细粒度变化解耦。首先，PF-DAG将动作片段压缩为离散模式以确保粗粒度动作的一致性，然后使用模式条件下的MeanFlow策略生成高保真连续动作。实验结果表明，PF-DAG在各种基准测试和实际任务中均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits</div>
<div class="meta-line">Authors: Luying Feng, Yaochu Jin, Hanze Hu, Wei Chen</div>
<div class="meta-line">First: 2026-02-25T07:59:16+00:00 · Latest: 2026-02-25T07:59:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21666v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生物力学比较揭示人类与类人机器人步态的差异</div>
<div class="mono" style="margin-top:8px">由于生物结构和机械结构之间的根本差异，实现类人机器人的人类级运动仍然具有挑战性。尽管模仿学习已成为生成自然机器人运动的有前途的方法，但仅仅复制关节角度轨迹无法捕捉人类运动的基本原理。本研究提出了一种步态差异分析框架（GDAF），这是一种统一的生物力学评估框架，系统地量化了人类和双足机器人之间的运动学和动力学差异。我们应用GDAF系统地比较了人类和类人机器人在28种行走速度下的运动。为了实现可重复分析，我们收集并发布了来自先进类人控制器的连续速度类人运动数据集。我们还提供了GDAF的开源实现，包括分析、可视化和基于MuJoCo的工具，使类人机器人运动的生物力学分析定量、可解释和可重复。结果表明，尽管现代类人控制器生成的运动在视觉上看起来像人类，但在不同速度下仍然存在显著的生物力学差异。机器人在步态对称性、能量分布和关节协调方面表现出系统性偏差，表明在提高类人机器人运动的生物力学准确性和能量效率方面仍有很大的改进空间。这项工作为评估类人机器人运动提供了定量基准，并提供了数据和多功能工具以支持开发更像人类和能量效率更高的运动控制器。数据和代码将在论文被接受后公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of achieving human-like locomotion in legged robots by proposing a Gait Divergence Analysis Framework (GDAF) that quantifies kinematic and kinetic discrepancies between humans and bipedal robots. The framework is applied to systematically compare human and humanoid locomotion across 28 walking speeds, revealing significant biomechanical divergence despite visually human-like motion. The research demonstrates that robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating room for improvement in biomechanical fidelity and energetic efficiency. An open-source implementation of GDAF is provided to support further research and development.</div>
<div class="mono" style="margin-top:8px">该研究通过提出一种步态差异分析框架（GDAF），量化了人类与双足机器人之间的运动学和动力学差异，以解决在腿足机器人中实现类似人类的运动的挑战。应用GDAF在28种行走速度下，研究揭示了尽管现代人形控制器生成的运动看起来很像人类，但在步态对称性、能量分布和关节协调方面仍存在显著的生物力学差异。该研究提供了连续速度的人形运动数据集和GDAF的开源实现，以实现人形运动的定量和可重复的生物力学分析，这突显了提高人形机器人生物力学准确性和能量效率的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Correcting VLA: Online Action Refinement via Sparse World Imagination</div>
<div class="meta-line">Authors: Chenyv Liu, Wentao Tan, Lei Zhu, Fengling Li, Jingjing Li, Guoli Yang, Heng Tao Shen</div>
<div class="meta-line">First: 2026-02-25T06:58:06+00:00 · Latest: 2026-02-25T06:58:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21633v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21633v1">PDF</a> · <a href="https://github.com/Kisaragi0/SC-VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent&#x27;s internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我修正VLA：基于稀疏世界想象的在线动作细化</div>
<div class="mono" style="margin-top:8px">标准的视觉-语言-动作（VLA）模型依赖于拟合统计数据先验，限制了其对潜在物理动态的稳健理解。强化学习通过探索增强了物理关联，但通常依赖于与代理内部状态隔离的外部奖励信号。世界动作模型作为一种新兴的范式，结合了想象和控制以实现预测性规划。然而，它们依赖于隐含的上下文建模，缺乏明确的自我改进机制。为了解决这些问题，我们提出了自我修正VLA（SC-VLA），通过内在引导动作细化来实现自我改进，利用稀疏想象。我们首先通过集成辅助预测头来设计稀疏世界想象，以预测当前任务进度和未来轨迹趋势，从而约束策略编码短期物理演化。然后引入在线动作细化模块，根据预测的稀疏未来状态重塑依赖进度的密集奖励，调整轨迹方向。在模拟基准和现实世界设置中的挑战性机器人操作任务上的评估表明，SC-VLA 达到了最先进的性能，与最佳基线相比，任务吞吐量提高了16%，成功率提高了9%，并且在现实世界实验中提高了14%。代码可在 https://github.com/Kisaragi0/SC-VLA 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the robust understanding of physical dynamics in vision-language-action models by integrating imagination and control. The proposed Self-Correcting VLA (SC-VLA) uses sparse world imagination and an online action refinement module to guide action refinement and reshape dense rewards. Experiments on robot manipulation tasks show that SC-VLA outperforms existing methods, achieving higher task throughput and success rates with fewer steps, and demonstrating a 14% gain in real-world settings.</div>
<div class="mono" style="margin-top:8px">研究旨在通过结合想象和控制来提高视觉-语言-行动模型对物理动态的稳健理解。SC-VLA 通过稀疏想象和在线行动精炼实现自我改进。模型使用辅助预测头来预测任务进度和未来趋势，引导策略编码短期物理演化。在线行动精炼模块基于预测的未来状态重塑密集奖励，调整轨迹方向。实验表明，SC-VLA 在现有方法中表现最佳，实现更高的任务吞吐量和成功率，尤其是在实际环境中的表现更好。</div>
</details>
</div>
<div class="card">
<div class="title">Tacmap: Bridging the Tactile Sim-to-Real Gap via Geometry-Consistent Penetration Depth Map</div>
<div class="meta-line">Authors: Lei Su, Zhijie Peng, Renyuan Ren, Shengping Mao, Juan Du, Kaifeng Zhang, Xuezhou Zhu</div>
<div class="meta-line">First: 2026-02-25T06:40:59+00:00 · Latest: 2026-02-25T06:40:59+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21625v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Based Tactile Sensors (VBTS) are essential for achieving dexterous robotic manipulation, yet the tactile sim-to-real gap remains a fundamental bottleneck. Current tactile simulations suffer from a persistent dilemma: simplified geometric projections lack physical authenticity, while high-fidelity Finite Element Methods (FEM) are too computationally prohibitive for large-scale reinforcement learning. In this work, we present Tacmap, a high-fidelity, computationally efficient tactile simulation framework anchored in volumetric penetration depth. Our key insight is to bridge the tactile sim-to-real gap by unifying both domains through a shared deform map representation. Specifically, we compute 3D intersection volumes as depth maps in simulation, while in the real world, we employ an automated data-collection rig to learn a robust mapping from raw tactile images to ground-truth depth maps. By aligning simulation and real-world in this unified geometric space, Tacmap minimizes domain shift while maintaining physical consistency. Quantitative evaluations across diverse contact scenarios demonstrate that Tacmap&#x27;s deform maps closely mirror real-world measurements. Moreover, we validate the utility of Tacmap through an in-hand rotation task, where a policy trained exclusively in simulation achieves zero-shot transfer to a physical robot.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Tacmap：通过几何一致的穿透深度图桥接触觉模拟到现实的差距</div>
<div class="mono" style="margin-top:8px">基于视觉的触觉传感器（VBTS）对于实现灵巧的机器人操作至关重要，但触觉模拟到现实的差距仍然是一个基本瓶颈。当前的触觉模拟面临着一个持续的困境：简化的几何投影缺乏物理真实性，而高保真的有限元方法（FEM）对于大规模强化学习来说计算成本太高。在本文中，我们提出了Tacmap，这是一种基于体积穿透深度的高保真、计算高效的触觉模拟框架。我们的核心见解是通过共享变形图表示来统一这两个领域，从而桥接触觉模拟到现实的差距。具体来说，在模拟中我们计算3D交截体积作为深度图，而在现实世界中，我们使用一个自动化的数据采集装置来学习从原始触觉图像到真实深度图的稳健映射。通过在这个统一的几何空间中对齐模拟和现实世界，Tacmap 在保持物理一致性的同时最小化了领域偏移。在多种接触场景下的定量评估表明，Tacmap 的变形图与现实世界的测量结果非常接近。此外，我们通过一个在手旋转任务验证了Tacmap 的实用性，其中在模拟中训练的策略能够实现零样本转移至物理机器人。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the tactile sim-to-real gap by developing Tacmap, a high-fidelity yet computationally efficient tactile simulation framework. It uses volumetric penetration depth to unify simulation and real-world data through a deform map representation. The key findings show that Tacmap closely matches real-world measurements and enables zero-shot transfer of a policy trained in simulation to a physical robot in an in-hand rotation task.</div>
<div class="mono" style="margin-top:8px">Tacmap 通过使用体积穿透深度图来统一仿真和现实世界的触觉传感。它在仿真中计算 3D 交集体积，并通过从原始触觉图像到真实地面真实深度图的稳健映射在现实世界中进行学习。实验结果表明，Tacmap 能够接近真实世界的测量值，并使在仿真中训练的策略能够零样本转移到物理机器人上。</div>
</details>
</div>
<div class="card">
<div class="title">ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation</div>
<div class="meta-line">Authors: Enyi Wang, Wen Fan, Dandan Zhang</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-25T06:35:19+00:00 · Latest: 2026-02-25T06:35:19+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE International Conference on Robotics and Automation (ICRA 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21622v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21622v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ADM-DP：通过视觉-触觉-图融合的自适应动态模态扩散策略用于多机器人操作</div>
<div class="mono" style="margin-top:8px">多机器人操作由于协调、抓取稳定性和碰撞避免的综合需求而在共享工作空间中仍然具有挑战性。为了解决这些挑战，我们提出了自适应动态模态扩散策略（ADM-DP），该框架结合了视觉、触觉和基于图（多机器人姿态）的模态进行协调控制。ADM-DP 引入了四个关键创新。首先，增强的视觉编码器通过特征层面线性调制（FiLM）调制将 RGB 和点云特征融合，以丰富感知。其次，触觉引导的抓取策略使用力敏感电阻（FSR）反馈来检测不足的接触并触发纠正性抓取细化，提高抓取稳定性。第三，基于图的碰撞编码器利用多个代理的共享工具中心点（TCP）位置作为结构化的运动学上下文，以保持空间意识并减少代理间的干扰。第四，自适应模态注意机制（AMAM）根据任务上下文动态重新加权模态，实现灵活融合。为了实现可扩展性和模块化，采用解耦训练范式，其中代理学习独立策略同时共享空间信息。这保持了代理间的低相互依赖性，同时保留了集体意识。在七个不同的多机器人任务中，ADM-DP 在与最先进的基线相比时实现了 12-25% 的性能提升。消融研究显示，在需要多种感知模态的任务中取得了最大的改进，验证了我们自适应融合策略的有效性，并证明了其在各种操作场景中的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes ADM-DP, a framework that integrates vision, tactile, and graph-based modalities for coordinated multi-agent manipulation. Key innovations include an enhanced visual encoder, a tactile-guided grasping strategy, a graph-based collision encoder, and an adaptive modality attention mechanism. ADM-DP shows 12-25% performance gains over state-of-the-art methods across seven multi-agent tasks, particularly in scenarios requiring multiple sensory inputs.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决协调、抓取稳定性和碰撞避免问题来提升多机器人协同操作。ADM-DP 通过增强视觉编码器、触觉引导抓取策略、基于图的碰撞编码器和自适应模态注意力机制，整合了视觉、触觉和基于图的模态。该框架在七个任务中实现了12-25%的性能提升，特别是在需要多种感官输入的场景中表现尤为突出。</div>
</details>
</div>
<div class="card">
<div class="title">Jumping Control for a Quadrupedal Wheeled-Legged Robot via NMPC and DE Optimization</div>
<div class="meta-line">Authors: Xuanqi Zeng, Lingwei Zhang, Linzhu Yue, Zhitao Song, Hongbo Zhang, Tianlin Zhang, Yun-Hui Liu</div>
<div class="meta-line">First: 2026-02-25T06:13:59+00:00 · Latest: 2026-02-25T06:13:59+00:00</div>
<div class="meta-line">Comments: 8 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21612v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21612v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadrupedal wheeled-legged robots combine the advantages of legged and wheeled locomotion to achieve superior mobility, but executing dynamic jumps remains a significant challenge due to the additional degrees of freedom introduced by wheeled legs. This paper develops a mini-sized wheeled-legged robot for agile motion and presents a novel motion control framework that integrates the Nonlinear Model Predictive Control (NMPC) for locomotion and the Differential Evolution (DE) based trajectory optimization for jumping in quadrupedal wheeled-legged robots. The proposed controller utilizes wheel motion and locomotion to enhance jumping performance, achieving versatile maneuvers such as vertical jumping, forward jumping, and backflips. Extensive simulations and real-world experiments validate the effectiveness of the framework, demonstrating a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>四足轮腿机器人基于NMPC和DE优化的跳跃控制</div>
<div class="mono" style="margin-top:8px">四足轮腿机器人结合了腿式和轮式运动的优点，以实现卓越的移动性，但由于引入了轮腿的自由度，执行动态跳跃仍然是一个重大挑战。本文开发了一款小型轮腿机器人，用于敏捷运动，并提出了一种新的运动控制框架，该框架将非线性模型预测控制（NMPC）用于运动控制，以及基于差分进化（DE）的轨迹优化用于四足轮腿机器人的跳跃。所提出的控制器利用轮子运动和运动来提高跳跃性能，实现了垂直跳跃、前跳和后空翻等多种机动动作。广泛的仿真实验和实际实验验证了该框架的有效性，展示了越过0.12米障碍的前跳和达到0.5米的垂直跳跃。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of dynamic jumps in quadrupedal wheeled-legged robots by integrating Nonlinear Model Predictive Control (NMPC) for locomotion and Differential Evolution (DE) for trajectory optimization. The proposed control framework enhances jumping performance through the use of wheel motion and locomotion, enabling maneuvers like vertical jumping, forward jumping, and backflips. Experiments show successful execution of a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m.</div>
<div class="mono" style="margin-top:8px">本文通过开发一款小型轮腿式四足机器人和结合NMPC用于运动控制、DE优化用于跳跃轨迹规划的控制框架，解决了轮腿式四足机器人动态跳跃的挑战。控制器利用轮子运动和腿足运动来提升跳跃性能，实现了垂直跳跃、前跳和后空翻等动作。实验结果显示成功执行了越过0.12 m障碍的前跳和达到0.5 m高度的垂直跳跃。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection</div>
<div class="meta-line">Authors: Yichen Lu, Siwei Nie, Minlong Lu, Xudong Yang, Xiaobo Zhang, Peng Zhang</div>
<div class="meta-line">First: 2026-02-19T15:54:55+00:00 · Latest: 2026-02-25T03:43:48+00:00</div>
<div class="meta-line">Comments: Accepted by ICCV2025 Github: https://github.com/eddielyc/CopyNCE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17484v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17484v2">PDF</a> · <a href="https://github.com/eddielyc/CopyNCE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace&#x27;s verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追踪复制像素和在复制检测中正则化块亲和性</div>
<div class="mono" style="margin-top:8px">图像复制检测（ICD）旨在通过稳健的特征表示学习来识别图像对之间的篡改内容。虽然自监督学习（SSL）已经提升了ICD系统的性能，但现有的视图级对比方法由于缺乏细粒度对应学习，难以应对复杂的编辑。我们通过两种关键创新解决了这一限制。首先，我们提出了PixTrace——一个像素坐标追踪模块，能够在编辑变换中保持显式的空间映射。其次，我们引入了CopyNCE，这是一种几何引导的对比损失，使用PixTrace验证的映射得出的重叠比来正则化块亲和性。我们的方法将像素级的可追踪性与块级的相似性学习相结合，抑制了SSL训练中的监督噪声。广泛的实验不仅展示了最先进的性能（匹配器88.7% uAP / 83.9% RP90，描述符72.6% uAP / 68.4% RP90，DISC21数据集），还展示了比现有方法更好的可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of identifying manipulated content in image pairs by proposing PixTrace and CopyNCE. PixTrace tracks pixel coordinates to maintain spatial mappings, while CopyNCE uses geometric guidance to regularize patch affinity. The method improves upon existing self-supervised learning techniques by enhancing fine-grained correspondence learning. Experiments show superior performance with 88.7% uAP and 83.9% RP90 for the matcher, and 72.6% uAP and 68.4% RP90 for the descriptor on the DISC21 dataset.</div>
<div class="mono" style="margin-top:8px">论文通过提出像素坐标跟踪模块PixTrace和几何引导的对比损失CopyNCE，解决了图像对中检测复制像素的挑战。这些创新增强了细粒度对应学习并抑制了自监督学习中的监督噪声。该方法在DISC21数据集上实现了最先进的性能，匹配器的uAP为88.7%，RP90为83.9%，描述符的uAP为72.6%，RP90为68.4%。</div>
</details>
</div>
<div class="card">
<div class="title">LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies</div>
<div class="meta-line">Authors: Yue Yang, Shuo Cheng, Yu Fang, Homanga Bharadhwaj, Mingyu Ding, Gedas Bertasius, Daniel Szafir</div>
<div class="meta-line">First: 2026-02-25T03:33:39+00:00 · Latest: 2026-02-25T03:33:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21531v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21531v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yy-gx.github.io/LiLo-VLA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiLo-VLA：通过链接的对象中心策略实现组成式长时程操作</div>
<div class="mono" style="margin-top:8px">通用机器人必须掌握长时程操作，即在未结构化环境中涉及多个运动结构变化（例如，连接或分离物体）的任务。尽管视觉-语言-动作（VLA）模型有可能掌握各种原子技能，但它们在顺序组合这些技能时存在组合复杂性问题，并且由于环境敏感性容易出现级联故障。为了解决这些挑战，我们提出了LiLo-VLA（链接的局部VLA），这是一种模块化框架，能够在从未见过的任务上实现零样本泛化。我们的方法将运输与交互分离：一个抓取模块处理全局运动，而一个交互模块采用对象中心的VLA来处理感兴趣的孤立物体，确保对无关视觉特征的鲁棒性和对空间配置的不变性。关键的是，这种模块化通过动态重规划和技能重用促进了稳健的故障恢复，有效缓解了端到端方法中的级联错误。我们引入了一个包含两个具有挑战性的套件（LIBERO-Long++和Ultra-Long）的21任务模拟基准。在这些模拟中，LiLo-VLA 的平均成功率达到了69%，分别比Pi0.5高出41%和比OpenVLA-OFT高出67%。此外，对8个长时程任务的实地评估显示平均成功率达到了85%。项目页面：https://yy-gx.github.io/LiLo-VLA/</div>
</details>
</div>
<div class="card">
<div class="title">EO-1: An Open Unified Embodied Foundation Model for General Robot Control</div>
<div class="meta-line">Authors: Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Dong Wang, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Xuelong Li</div>
<div class="meta-line">First: 2025-08-28T17:26:15+00:00 · Latest: 2026-02-25T03:30:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.21112v5">Abs</a> · <a href="https://arxiv.org/pdf/2508.21112v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eo-robotics.ai/eo-1">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, we introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models. Project Page: https://eo-robotics.ai/eo-1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EO-1：一种开放统一的体化基础模型，用于通用机器人控制</div>
<div class="mono" style="margin-top:8px">人类在开放世界中无缝进行多模态推理和物理交互的能力是通用体化智能系统的核心目标。近期的视觉-语言-动作（VLA）模型，这些模型在大规模的机器人和视觉-文本数据上共同训练，已经在通用机器人控制方面取得了显著进展。然而，它们仍然无法达到人类级别的灵活性，特别是在交错推理和交互方面。在本文中，我们介绍了EO-机器人，包括EO-1模型和EO-Data1.5M数据集。EO-1是一种统一的体化基础模型，通过交错的视觉-文本-动作预训练，在多模态体化推理和机器人控制方面取得了卓越的性能。EO-1的开发基于两个关键支柱：（i）一种统一的架构，能够无差别地处理多模态输入（图像、文本、视频和动作），以及（ii）一个大规模的高质量多模态体化推理数据集EO-Data1.5M，该数据集包含超过150万个样本，重点在于交错的视觉-文本-动作理解。EO-1通过与EO-Data1.5M的自回归解码和流动匹配去噪之间的协同作用进行训练，从而实现无缝的机器人动作生成和多模态体化推理。广泛的实验表明，交错的视觉-文本-动作学习对于开放世界的理解和泛化是有效的，这通过多种长期的灵巧操作任务在多个体化系统中得到了验证。本文详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略以及训练方法，为开发先进的体化基础模型提供了宝贵的见解。项目页面：https://eo-robotics.ai/eo-1/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a unified embodied foundation model for general robot control that can perform multimodal reasoning and physical interaction. EO-1, the proposed model, is trained on a large multimodal dataset, EO-Data1.5M, through interleaved vision-text-action pre-training. The model demonstrates superior performance in generating robot actions and multimodal reasoning across various long-horizon manipulation tasks. Key findings include the effectiveness of interleaved learning for open-world understanding and generalization in embodied AI systems.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一个统一的体模基础模型EO-1，以增强机器人在多模态推理和物理交互方面的能力。EO-1基于一个统一的架构处理图像、文本、视频和动作数据，并通过大规模的多模态数据集EO-Data1.5M进行训练。该模型在需要视觉-文本-动作综合理解的任务中表现出色，通过广泛的实验展示了在开放世界理解和泛化方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</div>
<div class="meta-line">Authors: Chengshu Li, Mengdi Xu, Arpit Bahety, Hang Yin, Yunfan Jiang, Huang Huang, Josiah Wong, Sujay Garlanka, Cem Gokmen, Ruohan Zhang, Weiyu Liu, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-21T05:56:47+00:00 · Latest: 2026-02-25T03:29:42+00:00</div>
<div class="meta-line">Comments: Project website: momagen.github.io. The first four authors contribute equally. Accpeted to International Conference on Learning Representations (ICLR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18316v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.18316v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning from large-scale, diverse human demonstrations has been shown to be effective for training robots, but collecting such data is costly and time-consuming. This challenge intensifies for multi-step bimanual mobile manipulation, where humans must teleoperate both the mobile base and two high-DoF arms. Prior X-Gen works have developed automated data generation frameworks for static (bimanual) manipulation tasks, augmenting a few human demos in simulation with novel scene configurations to synthesize large-scale datasets. However, prior works fall short for bimanual mobile manipulation tasks for two major reasons: 1) a mobile base introduces the problem of how to place the robot base to enable downstream manipulation (reachability) and 2) an active camera introduces the problem of how to position the camera to generate data for a visuomotor policy (visibility). To address these challenges, MoMaGen formulates data generation as a constrained optimization problem that satisfies hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility while navigation). This formulation generalizes across most existing automated data generation approaches and offers a principled foundation for developing future methods. We evaluate on four multi-step bimanual mobile manipulation tasks and find that MoMaGen enables the generation of much more diverse datasets than previous methods. As a result of the dataset diversity, we also show that the data generated by MoMaGen can be used to train successful imitation learning policies using a single source demo. Furthermore, the trained policy can be fine-tuned with a very small amount of real-world data (40 demos) to be succesfully deployed on real robotic hardware. More details are on our project page: momagen.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoMaGen：为双臂移动操作生成符合软硬约束的演示</div>
<div class="mono" style="margin-top:8px">从大规模多样的人类演示中进行模仿学习已被证明对训练机器人有效，但收集此类数据成本高且耗时。对于多步骤双臂移动操作，人类必须同时遥控移动基座和两个高自由度手臂，这一挑战更为严峻。先前的X-Gen工作开发了用于静态双臂操作任务的自动化数据生成框架，在仿真中通过合成新的场景配置来增强少量的人类演示。然而，这些先前的工作在处理双臂移动操作任务时存在两个主要问题：1) 移动基座引入了如何放置机器人基座以实现后续操作（可达性）的问题；2) 活动摄像头引入了如何定位摄像头以生成用于视觉-运动策略的数据（可见性）的问题。为了解决这些挑战，MoMaGen将数据生成建模为一个满足硬约束（例如可达性）的同时平衡软约束（例如导航过程中的可见性）的约束优化问题。这种建模方式可以泛化到大多数现有的自动化数据生成方法，并为开发未来的方法提供了一个原则性的基础。我们在四个多步骤双臂移动操作任务上进行了评估，发现MoMaGen能够生成比之前方法更为多样的数据集。由于数据集的多样性，我们还展示了MoMaGen生成的数据可以用于使用单一来源的演示训练成功的模仿学习策略。此外，训练后的策略可以通过少量的现实世界数据（40个演示）进行微调，并成功部署在真实的机器人硬件上。更多细节请参见我们的项目页面：momagen.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MoMaGen addresses the challenges of generating demonstrations for multi-step bimanual mobile manipulation by formulating data generation as a constrained optimization problem. It ensures reachability and visibility constraints are met while generating diverse datasets. Experiments on four tasks show that MoMaGen produces more varied datasets compared to previous methods, enabling successful imitation learning policies trained from a single demo to be fine-tuned with minimal real-world data and deployed on real hardware.</div>
<div class="mono" style="margin-top:8px">MoMaGen通过将数据生成问题表述为同时满足可达性和可见性约束的优化问题，解决了多步骤双臂移动操作演示数据生成的挑战。它生成的数据集比之前的方法更加多样化，可以用于训练成功的模仿学习策略，并且只需要40个演示数据即可在真实机器人硬件上进行微调和部署。</div>
</details>
</div>
<div class="card">
<div class="title">Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups</div>
<div class="meta-line">Authors: Felipe Bartelt, Vinicius M. Gonçalves, Luciano C. A. Pimenta</div>
<div class="meta-line">First: 2026-02-25T00:06:08+00:00 · Latest: 2026-02-25T00:06:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21450v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21450v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object&#x27;s mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>矩阵李群上完全驱动系统的路径跟随构造性向量场</div>
<div class="mono" style="margin-top:8px">本文提出了一种新的向量场策略，用于控制连接的矩阵李群上的完全驱动系统，确保收敛到并沿群上定义的曲线进行遍历。我们的方法扩展了我们之前的工作（Rezende等，2022年），并在考虑欧几里得空间中的平移李群时退化为它。由于Rezende等（2022年）中的证明依赖于正交性等关键性质，例如收敛和遍历分量之间的正交性，我们通过利用李群的性质扩展了这些结果。这些性质还允许控制输入非冗余，这意味着它与李群的维数匹配，而不是嵌入空间的潜在更大维数。这在某些情况下可以导致更实用的控制输入。我们策略的一个特别值得注意的应用是在控制SE(3)上的系统中——在这种情况下，非冗余输入对应于物体的机械扭率——使其非常适合控制可以自由移动和旋转的对象，如全向无人机。在这种情况下，我们提供了一个高效的算法来计算向量场。我们使用机器人操作器进行实验验证，以证明其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a new vector field strategy for controlling fully-actuated systems on matrix Lie groups, ensuring convergence to and traversal along a specified curve. The method builds upon previous work and generalizes it to Lie groups, reducing to the previous approach when considering Euclidean translations. By leveraging Lie group properties, the control input can be non-redundant, matching the dimension of the Lie group. This is particularly useful for systems on SE(3), where the non-redundant input corresponds to the object&#x27;s mechanical twist, making it suitable for omnidirectional drones. The effectiveness of the method is demonstrated through experiments with a robotic manipulator.</div>
<div class="mono" style="margin-top:8px">本文提出了一种新的矢量场策略，用于控制矩阵李群上的完全可控系统，确保系统收敛到并沿指定曲线移动。该方法基于先前的工作，并利用李群性质实现非冗余的控制输入，这些输入与李群的维度相匹配。该方法特别适用于SE(3)上的系统，可以控制物体的机械扭转，如全向无人机。通过使用机器人 manipulator 进行实验验证，证明了该方法的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260226_0403.html">20260226_0403</a>
<a href="archive/20260225_0357.html">20260225_0357</a>
<a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0337.html">20260223_0337</a>
<a href="archive/20260222_0338.html">20260222_0338</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
