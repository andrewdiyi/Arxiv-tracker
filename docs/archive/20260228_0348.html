<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-28 03:48</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260228_0348</div>
    <div class="row"><div class="card">
<div class="title">DropVLA: An Action-Level Backdoor Attack on Vision--Language--Action Models</div>
<div class="meta-line">Authors: Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang</div>
<div class="meta-line">First: 2025-10-13T02:45:48+00:00 · Latest: 2026-02-26T18:32:27+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 tables, 3 figures. Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10932v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10932v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models map multimodal perception and language instructions to executable robot actions, making them particularly vulnerable to behavioral backdoor manipulation: a hidden trigger introduced during training can induce unintended physical actions while nominal task performance remains intact. Prior work on VLA backdoors primarily studies untargeted attacks or task-level hijacking, leaving fine-grained control over individual actions largely unexplored. In this work, we present DropVLA, an action-level backdoor attack that forces a reusable action primitive (e.g., open_gripper) to execute at attacker-chosen decision points under a realistic pipeline-black-box setting with limited data-poisoning access, using a window-consistent relabeling scheme for chunked fine-tuning. On OpenVLA-7B evaluated with LIBERO, vision-only poisoning achieves 98.67%-99.83% attack success rate (ASR) with only 0.31% poisoned episodes while preserving 98.50%-99.17% clean-task retention, and successfully triggers the targeted action within 25 control steps at 500 Hz (0.05 s). Text-only triggers are unstable at low poisoning budgets, and combining text with vision provides no consistent ASR improvement over vision-only attacks. The backdoor remains robust to moderate trigger variations and transfers across evaluation suites (96.27%, 99.09%), whereas text-only largely fails (0.72%). We further validate physical-world feasibility on a 7-DoF Franka arm with pi0-fast, demonstrating non-trivial attack efficacy under camera-relative motion that induces image-plane trigger drift. These results reveal that VLA models can be covertly steered at the granularity of safety-critical actions with minimal poisoning and without observable degradation of nominal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DropVLA：视觉-语言-行动模型中的行动级后门攻击</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型将多模态感知和语言指令映射为可执行的机器人动作，使其特别容易受到行为后门操纵：在训练期间引入的隐藏触发器可以在保持名义任务性能的同时引发意外的物理动作。先前对VLA后门的研究主要关注无目标攻击或任务级劫持，而对个体动作的精细控制尚未得到充分探索。在本研究中，我们提出了DropVLA，这是一种行动级后门攻击，能够在有限的数据污染访问和现实的管道黑盒设置下，通过窗口一致的重新标记方案进行分块微调，迫使可重用的动作原语（例如，open_gripper）在攻击者选择的决策点执行。在使用LIBERO评估的OpenVLA-7B上，仅通过视觉污染即可实现98.67%-99.83%的攻击成功率（ASR），污染的剧集比例仅为0.31%，同时保持98.50%-99.17%的任务清洁保留率，并在500 Hz（0.05秒）的25个控制步骤内成功触发目标动作。仅文本触发在低污染预算下不稳定，结合文本与视觉并未在仅视觉攻击上提供一致的ASR改进。后门对适度触发器变化具有鲁棒性，并在评估套件之间转移（96.27%，99.09%），而仅文本则大多失败（0.72%）。我们还在7自由度的Franka手臂上通过pi0-fast验证了物理世界的可行性，展示了在相机相对运动下诱导图像平面触发漂移的非平凡攻击效果。这些结果表明，VLA模型可以在最小的污染和无明显名义性能退化的情况下，以安全关键动作的粒度被隐蔽地引导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to explore fine-grained control over individual actions in Vision-Language-Action (VLA) models through an action-level backdoor attack called DropVLA. This method uses a window-consistent relabeling scheme for chunked fine-tuning under a pipeline-black-box setting with limited data-poisoning access. Key findings include a 98.67%-99.83% attack success rate with only 0.31% poisoned episodes, while preserving 98.50%-99.17% clean-task retention. The attack triggers the targeted action within 25 control steps at 500 Hz, and is robust to moderate trigger variations and transfers across evaluation suites. Physical-world validation on a 7-DoF Franka arm confirms the attack&#x27;s efficacy under image-plane trigger drift.</div>
<div class="mono" style="margin-top:8px">研究旨在通过一种名为DropVLA的动作级后门攻击，探索对Vision-Language-Action (VLA)模型中个体动作的精细控制。该方法使用窗口一致的重新标记方案进行分块微调，在有限的数据污染访问下处于管道黑盒设置中。关键发现包括98.67%-99.83%的攻击成功率，仅0.31%的污染集，同时保持98.50%-99.17%的任务清洁保留。攻击在500 Hz下25个控制步骤内触发目标动作，并且对适度的触发变化具有鲁棒性，能够在不同的评估套件中转移。物理世界验证在7自由度Franka手臂上的pi0-fast上确认了攻击的有效性，在图像平面触发漂移下具有非平凡的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Physics Informed Viscous Value Representations</div>
<div class="meta-line">Authors: Hrishikesh Viswanath, Juanwu Lu, S. Talha Bukhari, Damon Conover, Ziran Wang, Aniket Bera</div>
<div class="meta-line">First: 2026-02-26T17:53:46+00:00 · Latest: 2026-02-26T17:53:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23280v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23280v1">PDF</a> · <a href="https://github.com/HrishikeshVish/phys-fk-value-GCRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物理信息的粘性值表示</div>
<div class="mono" style="margin-top:8px">离线目标条件强化学习（GCRL）从静态预先收集的数据集中学习目标条件策略。然而，由于状态-动作空间覆盖有限，准确的价值估计仍然是一个挑战。最近，通过在偏微分方程（PDEs）上定义正则化来施加物理和几何约束的方法试图解决这一问题，例如伊孔诺方程。然而，这些形式在复杂、高维环境中往往不稳态。在本文中，我们提出了一种基于HJB方程粘性解的物理信息正则化。通过提供基于物理的归纳偏置，我们的方法将学习过程扎根于最优控制理论，在价值迭代过程中显式地正则化和限制更新。此外，我们利用费曼-卡茨定理将PDE解重新表述为期望，使目标的可计算蒙特卡洛估计避免了高阶梯度的数值不稳定性。实验表明，我们的方法提高了几何一致性，使其广泛适用于导航和高维、复杂操作任务。开源代码可在https://github.com/HrishikeshVish/phys-fk-value-GCRL/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of accurate value estimation in offline goal-conditioned reinforcement learning by proposing a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman equation. The method leverages optimal control theory and the Feynman-Kac theorem to provide a physics-based inductive bias, improving geometric consistency and making it applicable to high-dimensional manipulation tasks. Experiments show that this approach enhances geometric consistency compared to previous methods.</div>
<div class="mono" style="margin-top:8px">该研究通过提出基于Hamilton-Jacobi-Bellman方程粘性解的物理约束正则化方法，解决了离线目标导向强化学习中准确的价值估计问题。该方法利用Feynman-Kac定理实现可计算的蒙特卡洛估计，避免了数值不稳定。实验表明，该方法提高了几何一致性，适用于高维复杂导航和操作任务。</div>
</details>
</div>
<div class="card">
<div class="title">Dyslexify: A Mechanistic Defense Against Typographic Attacks in CLIP</div>
<div class="meta-line">Authors: Lorenz Hufe, Constantin Venhoff, Erblina Purelku, Maximilian Dreyer, Sebastian Lapuschkin, Wojciech Samek</div>
<div class="meta-line">First: 2025-08-28T09:08:30+00:00 · Latest: 2026-02-26T17:33:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20570v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.20570v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model&#x27;s layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce Dyslexify - a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, dyslexify improves performance by up to 22.06% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%, and demonstrate its utility in a medical foundation model for skin lesion diagnosis. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dyslexify：CLIP对抗字型攻击的机制性防御</div>
<div class="mono" style="margin-top:8px">字型攻击通过在图像中注入文本来利用多模态系统，导致目标误分类、恶意内容生成，甚至视觉语言模型的逃逸。在本研究中，我们分析了CLIP视觉编码器在字型攻击下的行为，发现模型后半部分的注意力头专门提取并传递字型信息至cls标记。基于这些见解，我们提出了Dyslexify——一种通过选择性地消除字型电路（由注意力头组成）来防御CLIP模型的对抗字型攻击的方法。无需微调，Dyslexify在字型变体的ImageNet-100数据集上性能提升高达22.06%，同时将标准ImageNet-100的准确性降低不到1%，并在皮肤病变诊断的医学基础模型中展示了其实用性。值得注意的是，我们的无需训练的方法在依赖微调的当前最先进的字型防御方法中仍具有竞争力。为此，我们发布了对抗字型攻击具有显著更强鲁棒性的Dyslexic CLIP模型系列，这些模型适用于广泛的安全关键应用，其中基于文本的操纵风险超过了文本识别的实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks.</div>
<div class="mono" style="margin-top:8px">该研究通过分析CLIP视觉编码器如何处理字型信息，并识别出专门传输这些信息的注意力头来应对字型攻击。作者引入了Dyslexify方法，通过选择性地消除这些注意力头来防御字型攻击。Dyslexify在ImageNet-100的字型变体上可提高高达22.06%的性能，且无需微调，同时在皮肤病变诊断的医学基础模型中也显示出实用性，其性能与依赖微调的当前最佳防御方法相当。</div>
</details>
</div>
<div class="card">
<div class="title">SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly</div>
<div class="meta-line">Authors: Yijie Guo, Iretiayo Akinola, Lars Johannsmeier, Hugo Hadfield, Abhishek Gupta, Yashraj Narang</div>
<div class="meta-line">First: 2026-02-26T17:26:13+00:00 · Latest: 2026-02-26T17:26:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23253v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23253v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARR：基于模拟的装配策略与非对称现实残差</div>
<div class="mono" style="margin-top:8px">机器人装配因其对精确、接触丰富的操作要求而长期面临挑战。尽管基于模拟的学习方法促进了稳健装配策略的发展，但在实际应用中其性能往往会因模拟与现实之间的差距而下降。相反，基于现实世界的强化学习（RL）方法避免了模拟与现实之间的差距，但需要大量的人工监督且缺乏对环境变化的泛化能力。在本研究中，我们提出了一种结合模拟训练的基础策略和现实世界残差策略的混合方法，以高效地适应现实世界的差异。基础策略在模拟中使用低级状态观察和密集奖励进行训练，提供初始行为的强大先验。残差策略在现实世界中使用视觉观察和稀疏奖励进行学习，以补偿动力学和传感器噪声的差异。广泛的现实世界实验表明，我们的方法SPARR在多种两部分装配任务中实现了近乎完美的成功率。与最先进的零样本模拟到现实的方法相比，SPARR将成功率提高了38.4%，同时将周期时间减少了29.7%。此外，SPARR不需要人工专业知识，而最先进的基于现实世界的RL方法则高度依赖于人工监督。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of robotic assembly by proposing SPARR, a hybrid approach that combines a simulation-trained base policy with a real-world residual policy. The base policy, trained in simulation with low-level state observations and dense rewards, provides strong initial behavior. The residual policy, learned in the real world with visual observations and sparse rewards, adapts to real-world variations. Experiments show that SPARR achieves near-perfect success rates in diverse two-part assembly tasks, outperforming state-of-the-art sim-to-real methods by 38.4% in success rates and 29.7% in cycle time, without requiring human expertise.</div>
<div class="mono" style="margin-top:8px">论文提出了一种混合方法SPARR，结合了在仿真中训练的基础策略和在现实世界中学习的残差策略，以解决机器人装配中的仿真实验到现实世界的差距问题。基础策略在仿真中训练，提供初始行为的强大先验，而残差策略在现实世界中学习，补偿动态和传感器噪声的差异。SPARR在多种两部分装配任务中实现了近乎完美的成功率，相比最先进的零样本仿真实验方法，成功率提高了38.4%，循环时间减少了29.7%，并且不需要人工专业知识。</div>
</details>
</div>
<div class="card">
<div class="title">InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation</div>
<div class="meta-line">Authors: Jiahao Liu, Cui Wenbo, Haoran Li, Dongbin Zhao</div>
<div class="meta-line">First: 2026-02-26T14:03:58+00:00 · Latest: 2026-02-26T14:03:58+00:00</div>
<div class="meta-line">Comments: 16 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23024v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23024v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InCoM：基于意图的感知与结构化协调在全身移动操作中的应用</div>
<div class="mono" style="margin-top:8px">全身移动操作是通用机器人代理的基本能力，需要协调控制移动基座和操作臂，并在动态变化的视角下保持鲁棒的感知。然而，现有方法面临两个关键挑战：基座和臂部动作之间的强耦合使全身控制优化复杂化，且在移动操作过程中视角变化时感知注意力分配往往不佳。我们提出InCoM，一种基于意图的感知与结构化协调框架，用于全身移动操作。InCoM推断潜在的运动意图，动态重新加权多尺度感知特征，实现阶段适应的感知注意力分配。为了支持鲁棒的跨模态感知，InCoM进一步引入了几何语义结构对齐机制，增强多模态对应关系。在控制方面，我们设计了一个解耦协调流匹配动作解码器，明确建模基座-臂部协调动作生成，缓解由控制耦合引起的优化难题。在没有访问特权感知信息的情况下，InCoM在三个ManiSkill-HAB场景中的成功率上分别优于最新方法28.2%、26.1%和23.6%，证明了其在全身移动操作中的强大效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenges of whole-body mobile manipulation by proposing InCoM, an intent-driven perception and structured coordination framework. InCoM dynamically reweights perceptual features based on inferred motion intent, enabling adaptive allocation of perceptual attention. It also incorporates a geometric-semantic structured alignment mechanism for robust cross-modal perception and a decoupled coordinated flow matching action decoder to alleviate control coupling. Experiments show that InCoM outperforms state-of-the-art methods by 28.2%, 26.1%, and 23.6% in success rate across three ManiSkill-HAB scenarios.</div>
<div class="mono" style="margin-top:8px">论文通过提出InCoM框架，采用意图驱动的感知和结构化协调方法来解决整体移动操作的挑战。该框架根据推断的运动意图动态重新加权感知特征，并引入几何语义对齐机制以增强跨模态感知的鲁棒性。在控制方面，它将基座和机械臂动作解耦，以缓解由于控制耦合引起的优化难题。InCoM在三个ManiSkill-HAB场景中的成功率上分别优于最先进的方法28.2%、26.1%和23.6%，展示了其在整体移动操作中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DigiArm: An Anthropomorphic 3D-Printed Prosthetic Hand with Enhanced Dexterity for Typing Tasks</div>
<div class="meta-line">Authors: Dean Zadok, Tom Naamani, Yuval Bar-Ratson, Elisha Barash, Oren Salzman, Alon Wolf, Alex M. Bronstein, Nili Krausz</div>
<div class="meta-line">First: 2026-02-26T13:55:05+00:00 · Latest: 2026-02-26T13:55:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23017v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand. Current control systems for prosthetic hands are often limited to grasping, and commercial prosthetic hands lack the precision needed for dexterous manipulation or applications that require fine finger motions. Thus, there is a critical need for accessible and replicable prosthetic designs that enable individuals to interact with electronic devices and perform precise finger pressing, such as keyboard typing or piano playing, while preserving current prosthetic capabilities. This paper presents a low-cost, lightweight, 3D-printed robotic prosthetic hand, specifically engineered for enhanced dexterity with electronic devices such as a computer keyboard or piano, as well as general object manipulation. The robotic hand features a mechanism to adjust finger abduction/adduction spacing, a 2-D wrist with the inclusion of controlled ulnar/radial deviation optimized for typing, and control of independent finger pressing. We conducted a study to demonstrate how participants can use the robotic hand to perform keyboard typing and piano playing in real time, with different levels of finger and wrist motion. This supports the notion that our proposed design can allow for the execution of key typing motions more effectively than before, aiming to enhance the functionality of prosthetic hands.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DigiArm：一种增强灵巧度的3D打印仿人假手，适用于打字任务</div>
<div class="mono" style="margin-top:8px">尽管最近取得了进展，现有的假肢仍然无法复制人类手部的灵巧度和直观控制能力。当前的假手控制系统通常仅限于抓握，而商用假手缺乏进行精细手指操作或需要精细手指动作的应用所需的精确度。因此，迫切需要可访问且可复制的假肢设计，使个体能够与电子设备互动并执行精确的手指按压，如键盘打字或钢琴演奏，同时保留现有的假肢功能。本文介绍了一种低成本、轻量级的3D打印机器人假手，专门设计用于增强与电子设备（如计算机键盘或钢琴）以及一般物体操作的灵巧度。该机器人手具有调节手指展收间距的机制，具有优化的2D手腕，包括控制尺偏/桡偏，适用于打字，并控制独立手指按压。我们进行了一项研究，以展示参与者如何实时使用机器人手进行键盘打字和钢琴演奏，不同级别的手指和手腕运动。这支持了我们的设计可以更有效地执行关键打字动作的观点，旨在增强假手的功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand.</div>
<div class="mono" style="margin-top:8px">本文通过介绍一种低成本的3D打印机械手DigiArm，解决了现有假肢手的局限性，该手专为提高打字和钢琴演奏等任务的灵巧性而设计。该手包括调整手指间距的机制和一个优化用于打字的可控手腕运动。实验结果表明，参与者能够通过改进的手指和手腕运动来执行键盘打字和钢琴演奏，证明了该设计在执行关键打字动作方面比之前的假肢手更有效。</div>
</details>
</div>
<div class="card">
<div class="title">A Perspective on Open Challenges in Deformable Object Manipulation</div>
<div class="meta-line">Authors: Ryan Paul McKennaa, John Oyekan</div>
<div class="meta-line">First: 2026-02-26T13:39:30+00:00 · Latest: 2026-02-26T13:39:30+00:00</div>
<div class="meta-line">Comments: 28 pages, 7 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22998v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22998v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deformable object manipulation (DOM) represents a critical challenge in robotics, with applications spanning healthcare, manufacturing, food processing, and beyond. Unlike rigid objects, deformable objects exhibit infinite dimensionality, dynamic shape changes, and complex interactions with their environment, posing significant hurdles for perception, modeling, and control. This paper reviews the state of the art in DOM, focusing on key challenges such as occlusion handling, task generalization, and scalable, real-time solutions. It highlights advancements in multimodal perception systems, including the integration of multi-camera setups, active vision, and tactile sensing, which collectively address occlusion and improve adaptability in unstructured environments. Cutting-edge developments in physically informed reinforcement learning (RL) and differentiable simulations are explored, showcasing their impact on efficiency, precision, and scalability. The review also emphasizes the potential of simulated expert demonstrations and generative neural networks to standardize task specifications and bridge the simulation-to-reality gap. Finally, future directions are proposed, including the adoption of graph neural networks for high-level decision-making and the creation of comprehensive datasets to enhance DOM&#x27;s real-world applicability. By addressing these challenges, DOM research can pave the way for versatile robotic systems capable of handling diverse and dynamic tasks with deformable objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于可变形物体操作中开放挑战的视角</div>
<div class="mono" style="margin-top:8px">可变形物体操作（DOM）是机器人技术中的关键挑战，其应用范围从医疗保健、制造业、食品加工到更广泛的领域。与刚性物体不同，可变形物体表现出无限的维度、动态的形状变化以及与环境的复杂交互，这给感知、建模和控制带来了重大障碍。本文回顾了DOM的最新进展，重点关注遮挡处理、任务泛化和可扩展的实时解决方案等关键挑战。文章强调了多模态感知系统的进步，包括多摄像头设置、主动视觉和触觉传感的集成，这些共同解决了遮挡问题并提高了在非结构化环境中的适应性。文章还探讨了基于物理的强化学习（RL）和可微模拟的最新进展，展示了它们在效率、精确性和可扩展性方面的影响力。回顾还强调了模拟专家演示和生成神经网络在标准化任务规范和弥合模拟与现实差距方面的潜力。最后，提出了未来方向，包括采用图神经网络进行高级决策和创建全面的数据集以增强DOM在现实世界中的适用性。通过解决这些挑战，DOM研究可以为能够处理各种动态任务的多功能机器人系统铺平道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the critical challenges in deformable object manipulation (DOM), focusing on occlusion handling, task generalization, and scalable real-time solutions. It reviews advancements in multimodal perception systems, such as multi-camera setups and tactile sensing, and explores physically informed reinforcement learning and differentiable simulations for improved efficiency and precision. The paper also highlights the potential of simulated expert demonstrations and generative neural networks to standardize task specifications and bridge the simulation-to-reality gap.</div>
<div class="mono" style="margin-top:8px">本文探讨了机器人领域中变形物体操作（DOM）的关键挑战，如遮挡处理、任务泛化和实时解决方案的可扩展性。作者回顾了多模态感知系统、物理启发的强化学习和可微模拟的进步，这些进步提高了感知、适应性和效率。他们还讨论了使用模拟专家演示和生成神经网络来标准化任务规范并改善模拟到现实的差距。未来的研究方向包括使用图神经网络进行高级决策和创建全面的数据集以增强DOM的实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy</div>
<div class="meta-line">Authors: Yuan Tang, Bruno V. Adorno, Brendan A. McGrath, Andrew Weightman</div>
<div class="meta-line">First: 2026-02-26T12:47:04+00:00 · Latest: 2026-02-26T12:47:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22952v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22952v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Percutaneous dilatational tracheostomy (PDT) is frequently performed on patients in intensive care units for prolonged mechanical ventilation. The needle puncture, as the most critical step of PDT, could lead to adverse consequences such as major bleeding and posterior tracheal wall perforation if performed inaccurately. Current practices of PDT puncture are all performed manually with no navigation assistance, which leads to large position and angular errors (5 mm and 30 degree). To improve the accuracy and reduce the difficulty of the PDT procedure, we propose a system that automates the needle insertion using a velocity-controlled robotic manipulator. Guided using pose data from two electromagnetic sensors, one at the needle tip and the other inside the trachea, the robotic system uses an adaptive constrained controller to adapt the uncertain kinematic parameters online and avoid collisions with the patient&#x27;s body and tissues near the target. Simulations were performed to validate the controller&#x27;s implementation, and then four hundred PDT punctures were performed on a mannequin to evaluate the position and angular accuracy. The absolute median puncture position error was 1.7 mm (IQR: 1.9 mm) and midline deviation was 4.13 degree (IQR: 4.55 degree), measured by the sensor inside the trachea. The small deviations from the nominal puncture in a simulated experimental setup and formal guarantees of collision-free insertions suggest the feasibility of the robotic PDT puncture.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the critical need for improved accuracy in percutaneous dilatational tracheostomy (PDT) by proposing an automated robotic system. The system uses a velocity-controlled robotic manipulator guided by electromagnetic sensors to achieve precise needle insertion. Experiments on a mannequin showed a median puncture position error of 1.7 mm and a midline deviation of 4.13 degrees, demonstrating significant improvements over manual procedures.</div>
<div class="mono" style="margin-top:8px">论文旨在通过提出一种自动化机器人系统来提高经皮气管切开术（PDT）的准确性和降低操作难度。该系统使用一个速度控制的机器人操作器，在两个电磁传感器提供的姿态数据引导下进行精确的针刺插入。研究通过模拟和在人体模型上进行400次穿刺评估了系统的性能，结果显示针刺位置的中位误差为1.7毫米，中线偏移为4.13度，显著优于手动操作方法。</div>
</details>
</div>
<div class="card">
<div class="title">DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation</div>
<div class="meta-line">Authors: Zebin Yang, Yijiahao Qi, Tong Xie, Bo Yu, Shaoshan Liu, Meng Li</div>
<div class="meta-line">First: 2026-02-26T11:34:36+00:00 · Latest: 2026-02-26T11:34:36+00:00</div>
<div class="meta-line">Comments: DAC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22896v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22896v1">PDF</a> · <a href="https://github.com/PKU-SEC-Lab/DYSL_VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model&#x27;s reasoning with a vision model&#x27;s 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action&#x27;s importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DySL-VLA：基于动态-静态层跳过的高效视觉-语言-行动模型推理框架用于机器人操作</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型通过融合语言模型的推理和视觉模型的三维理解，在机器人操作等任务中取得了显著的成功。然而，它们的高计算成本仍然是实现实时性能所需的实际应用中的主要障碍。我们观察到任务中的动作具有不同的重要性：关键步骤需要高精度，而不太重要的步骤可以容忍更大的变化。利用这一洞察，我们提出了一种名为DySL-VLA的新框架，通过根据每个动作的重要性动态跳过VLA层来解决计算成本问题。DySL-VLA将层分为两类：信息层，始终执行；增量层，可以有选择地跳过。为了在不牺牲准确性的前提下智能地跳过层，我们发明了一种先验-后跳机制来确定何时开始跳过层。我们还提出了一种跳层感知的两阶段知识蒸馏算法，以高效地将标准VLA训练成DySL-VLA。我们的实验表明，与Deer-VLA在Calvin数据集上的成功长度相比，DySL-VLA提高了2.1%，同时将可训练参数减少了85.7倍，并且相对于RoboFlamingo基线在等精度下提供了3.75倍的速度提升。我们的代码可在https://github.com/PKU-SEC-Lab/DYSL_VLA/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DySL-VLA addresses the high computational cost of Vision-Language-Action models in robotic manipulation by dynamically skipping layers based on the importance of each action. It categorizes layers into informative and incremental types, and uses a prior-post skipping guidance mechanism to determine when to skip layers. Experiments show DySL-VLA improves success length by 2.1% compared to Deer-VLA, reduces trainable parameters by 85.7 times, and provides a 3.75x speedup over RoboFlamingo at the same accuracy level.</div>
<div class="mono" style="margin-top:8px">DySL-VLA 是一种新型框架，通过根据每个动作的重要性动态跳过层来降低视觉-语言-动作模型在机器人操作中的计算成本。它将层分为信息性和增量性两类，并使用先验后跳过指导机制来决定何时跳过层。DySL-VLA 在 Calvin 数据集上的成功长度提高了 2.1%，减少了 85.7 倍的可训练参数，并且与 RoboFlamingo 基线相比，在相同准确率水平下提供了 3.75 倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion</div>
<div class="meta-line">Authors: Enda Xiang, Haoxiang Ma, Xinzhu Ma, Zicheng Liu, Di Huang</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-26T10:56:01+00:00 · Latest: 2026-02-26T10:56:01+00:00</div>
<div class="meta-line">Comments: Accepted to CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22862v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22862v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraspLDP：通过潜在扩散提升抓取策略的泛化能力</div>
<div class="mono" style="margin-top:8px">本文专注于通过模仿学习提高抓取精度和抓取策略的泛化能力。基于扩散的方法最近已成为机器人操作任务的主要方法。由于抓取是操作中的关键子任务，因此模仿学习得到的策略执行精确和泛化抓取的能力值得特别关注。现有的抓取模仿学习技术往往存在抓取执行不精确、空间泛化能力有限和对象泛化能力差的问题。为了解决这些问题，我们将在扩散策略框架中融入抓取先验知识。具体而言，我们使用潜在扩散策略来引导动作片段解码，利用抓取姿态先验确保生成的运动轨迹接近可行的抓取配置。此外，我们在扩散过程中引入自监督重构目标以嵌入抓取先验：在每次逆向扩散步骤中，我们从中间表示中回投影抓取性进行重构。仿真和真实机器人实验均表明，我们的方法显著优于基线方法，并表现出强大的动态抓取能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to improve the precision and generalization of grasping policies learned through imitation learning by addressing the limitations of existing methods. It introduces GraspLDP, which uses a latent diffusion policy to incorporate grasp prior knowledge, ensuring that generated motion trajectories align with feasible grasp configurations. The approach also includes a self-supervised reconstruction objective to embed graspness prior during the diffusion process. Experimental results show that GraspLDP outperforms baseline methods and demonstrates strong dynamic grasping capabilities in both simulation and real robot experiments.</div>
<div class="mono" style="margin-top:8px">该论文旨在通过解决现有技术的局限性，提高通过模仿学习获得的抓取策略的精确度和泛化能力。它引入了GraspLDP，使用潜扩散策略结合抓取先验知识，确保生成的运动轨迹接近可行的抓取配置。该方法还包含一个自监督重建目标，在扩散过程中嵌入抓取性先验。实验表明，GraspLDP在性能上优于基线方法，并展示了强大的动态抓取能力。</div>
</details>
</div>
<div class="card">
<div class="title">Performance and Experimental Analysis of Strain-based Models for Continuum Robots</div>
<div class="meta-line">Authors: Annika Delucchi, Vincenzo Di Paola, Andreas Müller, and Matteo Zoppi</div>
<div class="meta-line">First: 2026-02-26T10:46:13+00:00 · Latest: 2026-02-26T10:46:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22854v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22854v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续机器人基于应变的模型性能及实验分析</div>
<div class="mono" style="margin-top:8px">尽管基于应变的模型在机器人学中被广泛应用，但除了均匀弯曲测试外，没有公认的比较方法来评估其性能。此外，连续机器人原型制作的日益努力突显了评估这些模型适用性的必要性以及进行全面性能评估的必要性。为解决这一差距，本研究调查了三阶应变插值方法的形状重构能力，检查其捕捉单个和组合变形效果的能力。这些结果与几何变量应变方法进行了比较和讨论。随后，通过重塑细长杆并使用相机记录结果配置，实验验证了模拟结果。使用操作臂移动其一端来施加杆的配置，并通过反射标记提取，无需任何其他外部传感器——即沿杆放置的应变计或力矩传感器。实验表明，模型预测与观察到的形状之间有良好的一致性，平均误差为杆长的0.58%，每种配置的平均计算时间为0.32秒，优于现有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance.</div>
<div class="mono" style="margin-top:8px">该研究评估了一种第三阶应变插值方法在连续机器人中的性能，将其与几何变量应变方法进行比较。研究重点在于该模型在各种变形下重建形状的能力。实验结果表明，第三阶方法与观察到的形状有良好的一致性，平均误差为杆长的0.58%，每次配置的计算时间为0.32秒，优于现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">The AI Research Assistant: Promise, Peril, and a Proof of Concept</div>
<div class="meta-line">Authors: Tan Bui-Thanh</div>
<div class="meta-line">First: 2026-02-26T10:29:05+00:00 · Latest: 2026-02-26T10:29:05+00:00</div>
<div class="meta-line">Comments: 11 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22842v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22842v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI研究助手：潜力、风险与概念验证</div>
<div class="mono" style="margin-top:8px">人工智能能否真正为创造性数学研究做出贡献，还是仅仅自动化了常规计算并带来了错误的风险？我们通过详细的案例研究提供了实证证据：通过系统的人机合作发现了赫mite求积规则的新错误表示和界。与多个AI助手合作，我们超越了手动工作的成果，借助AI协助提出了并证明了多个定理。合作揭示了AI的卓越能力和关键局限。AI在代数操作、系统证明探索、文献综合和LaTeX准备方面表现出色。然而，每一步都需要严格的真人验证、数学直觉来制定问题以及战略方向。我们以不寻常的透明度记录了完整的研究工作流程，揭示了成功的人机数学合作模式，并指出了研究人员必须预见的失败模式。我们的经验表明，在适当的怀疑和验证协议下使用时，AI工具可以有意义地加速数学发现，但需要仔细的人类监督和深厚的专业知识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores whether AI can contribute to creative mathematical research by collaborating with humans. Through a detailed case study involving the discovery of new error representations and bounds for Hermite quadrature rules, the research demonstrates that AI excelled in algebraic manipulation and systematic proof exploration but required human verification and strategic direction. The study reveals that AI can accelerate mathematical discovery when used with appropriate skepticism and verification protocols, but it also highlights critical limitations and failure modes that must be managed.</div>
<div class="mono" style="margin-top:8px">研究探讨了AI是否能促进创造性数学研究，通过人类与AI的合作，发现了赫尔mite求积规则的新误差表示和边界。研究显示，尽管AI在代数操作和系统证明探索方面表现出色，但每一步都需要严格的验证和战略指导。合作揭示了AI在代数操作和文献综合方面的优势，但也指出了关键的局限性，强调了需要谨慎的人类监督和深厚的专业知识。</div>
</details>
</div>
<div class="card">
<div class="title">Spatially anchored Tactile Awareness for Robust Dexterous Manipulation</div>
<div class="meta-line">Authors: Jialei Huang, Yang Ye, Yuanqing Gong, Xuezhou Zhu, Yang Gao, Kaifeng Zhang</div>
<div class="meta-line">First: 2025-10-16T12:59:34+00:00 · Latest: 2026-02-26T10:21:54+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14647v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.14647v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation requires precise geometric reasoning, yet existing visuo-tactile learning methods struggle with sub-millimeter precision tasks that are routine for traditional model-based approaches. We identify a key limitation: while tactile sensors provide rich contact information, current learning frameworks fail to effectively leverage both the perceptual richness of tactile signals and their spatial relationship with hand kinematics. We believe an ideal tactile representation should explicitly ground contact measurements in a stable reference frame while preserving detailed sensory information, enabling policies to not only detect contact occurrence but also precisely infer object geometry in the hand&#x27;s coordinate system. We introduce SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an end-to-end policy framework that explicitly anchors tactile features to the hand&#x27;s kinematic frame through forward kinematics, enabling accurate geometric reasoning without requiring object models or explicit pose estimation. Our key insight is that spatially grounded tactile representations allow policies to not only detect contact occurrence but also precisely infer object geometry in the hand&#x27;s coordinate system. We validate SaTA on challenging dexterous manipulation tasks, including bimanual USB-C mating in free space, a task demanding sub-millimeter alignment precision, as well as light bulb installation requiring precise thread engagement and rotational control, and card sliding that demands delicate force modulation and angular precision. These tasks represent significant challenges for learning-based methods due to their stringent precision requirements. Across multiple benchmarks, SaTA significantly outperforms strong visuo-tactile baselines, improving success rates by up to 30 percentage while reducing task completion times by 27 percentage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于空间锚定的触觉感知以实现稳健的灵巧操作</div>
<div class="mono" style="margin-top:8px">灵巧操作需要精确的几何推理，但现有的视触觉学习方法在处理传统基于模型方法可以轻松完成的亚毫米级精度任务时存在困难。我们发现一个关键限制：虽然触觉传感器提供了丰富的接触信息，但当前的学习框架未能有效利用触觉信号的感知丰富性和其与手部运动学的空间关系。我们认为理想的触觉表示应该明确地将接触测量与稳定参考框架联系起来，同时保留详细的感官信息，使策略不仅能检测接触的发生，还能精确推断物体在手部坐标系中的几何形状。我们提出了SaTA（基于空间锚定的触觉感知），这是一种端到端的策略框架，通过前向运动学将触觉特征明确锚定到手部的运动学框架中，从而在无需物体模型或显式姿态估计的情况下实现精确的几何推理。我们的关键见解是，空间锚定的触觉表示使策略不仅能检测接触的发生，还能精确推断物体在手部坐标系中的几何形状。我们在包括自由空间中的双臂USB-C对接、需要亚毫米级对准精度的灯泡安装以及需要精细力调节和角度精度的卡片滑动等具有挑战性的灵巧操作任务上验证了SaTA。这些任务对基于学习的方法提出了严峻的精度要求。在多个基准测试中，SaTA显著优于强大的视触觉基线，成功率提高了30个百分点，任务完成时间减少了27个百分点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the precision of dexterous manipulation by addressing the limitations of existing visuo-tactile learning methods. SaTA, an end-to-end policy framework, explicitly anchors tactile features to the hand&#x27;s kinematic frame, enabling accurate geometric reasoning. The method significantly outperforms strong visuo-tactile baselines in tasks requiring sub-millimeter precision, such as bimanual USB-C mating, light bulb installation, and card sliding, with up to 30% higher success rates and 27% faster task completion times.</div>
<div class="mono" style="margin-top:8px">论文解决了实现毫米级精度的灵巧操作任务的挑战，这给现有的视觉-触觉学习方法带来了困难。它引入了SaTA，这是一种端到端的策略框架，将触觉特征锚定到手的运动坐标系中，从而实现精确的几何推理。SaTA在包括USB-C接头对接、灯泡安装和卡片滑动等任务中进行了测试，并显著优于强大的视觉-触觉基线，提高了成功率并减少了任务完成时间。</div>
</details>
</div>
<div class="card">
<div class="title">Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera</div>
<div class="meta-line">Authors: Seongyong Kim, Junhyeon Cho, Kang-Won Lee, Soo-Chul Lim</div>
<div class="meta-line">First: 2026-02-26T08:15:38+00:00 · Latest: 2026-02-26T08:15:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22733v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To catch a thrown object, a robot must be able to perceive the object&#x27;s motion and generate control actions in a timely manner. Rather than explicitly estimating the object&#x27;s 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object&#x27;s position and scale, allowing the policy to reason about the object&#x27;s motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Pixel2Catch：单色摄像头下基于多智能体模拟到现实的敏捷操作转移</div>
<div class="mono" style="margin-top:8px">为了接住一个投掷的物体，机器人必须能够感知物体的运动并及时生成控制动作。不同于显式估计物体的三维位置，这项工作关注一种新颖的方法，即利用单个RGB图像中像素级的视觉信息来识别物体的运动。这些视觉线索捕捉了物体位置和尺度的变化，使策略能够推理物体的运动。此外，为了在由配备多指手的机器人手臂组成的高自由度系统中实现稳定学习，我们设计了一种异构多智能体强化学习框架，将手臂和手定义为具有不同角色的独立智能体。每个智能体通过特定的角色观察和奖励进行协同训练，学习到的策略成功地从模拟环境转移到了现实世界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work aims to enable robots to catch thrown objects by focusing on recognizing object motion using pixel-level visual information from a single RGB camera. It introduces a novel approach that captures changes in the object&#x27;s position and scale, allowing the robot to reason about the object&#x27;s motion without explicitly estimating its 3D position. To achieve stable learning in a high-degree-of-freedom system, the authors designed a heterogeneous multi-agent reinforcement learning framework where the robot arm and multi-fingered hand are trained as independent agents with role-specific observations and rewards. The learned policies were successfully transferred from simulation to the real world, demonstrating the effectiveness of this method.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过利用单个RGB相机的像素级视觉信息来识别物体运动，从而让机器人能够接住投掷的物体。研究提出了一种新颖的方法，通过捕捉物体位置和尺度的变化来推断物体的运动，而无需明确估计其三维位置。为了在高自由度系统中实现稳定的训练，研究人员设计了一个异构多智能体强化学习框架，将机器人手臂和手视为具有不同角色的独立智能体进行训练。从模拟到现实世界的策略转移成功，证明了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline</div>
<div class="meta-line">Authors: Wenxuan Song, Jiayi Chen, Xiaoquan Sun, Huashuo Lei, Yikai Qin, Wei Zhao, Pengxiang Ding, Han Zhao, Tongxin Wang, Pengxu Hou, Zhide Zhong, Haodong Yan, Donglin Wang, Jun Ma, Haoang Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-26T06:27:37+00:00 · Latest: 2026-02-26T06:27:37+00:00</div>
<div class="meta-line">Comments: Accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs&#x27; practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考视觉-语言-行动模型的实用性：一个全面的基准和改进的基础模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型已经发展成为一种通用的机器人代理。然而，现有的VLA受到参数规模过大、预训练要求高昂以及对多样化实体应用有限的限制。为了提高VLA的实用性，我们提出了一种全面的基准和改进的基础模型。首先，我们提出了CEBench，这是一种新的基准，涵盖了模拟和现实世界中多样化的实体，并考虑了领域随机化。我们收集了14400个模拟轨迹和1600个现实世界的专家策划轨迹，以支持在CEBench上的训练。其次，使用CEBench作为我们的测试平台，我们研究了VLA实用性的三个关键方面，并提出了几个关键发现。根据这些发现，我们引入了LLaVA-VLA，这是一种轻量级但强大的VLA，旨在在消费级GPU上进行实际部署。从架构上看，它结合了紧凑的VLM骨干、多视图感知、本体感受性标记化和动作分块。为了消除对昂贵预训练的依赖，LLaVA-VLA采用了两阶段训练范式，包括后训练和微调。此外，LLaVA-VLA扩展了动作空间，以统一导航和操作。跨实体的实验展示了LLaVA-VLA的泛化能力和灵活性，而现实世界的移动操作实验则确立了它作为第一个端到端的移动操作VLA模型的地位。我们将在接受后开源所有数据集、代码和检查点，以促进可重复性和未来研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to enhance the practicality of Vision-Language-Action (VLA) models by proposing a new comprehensive benchmark (CEBench) and an improved baseline (LLaVA-VLA). CEBench covers diverse embodiments in both simulation and the real world, and LLaVA-VLA integrates a compact vision-language model backbone with multi-view perception, proprioceptive tokenization, and action chunking. The model demonstrates strong generalization and versatility across different embodiments, and it is the first end-to-end VLA for mobile manipulation, opening source codes and datasets for future research.</div>
<div class="mono" style="margin-top:8px">论文旨在通过提出一个全面的基准和改进的基础模型来提升视觉-语言-动作（VLA）模型的实用性。CEBench是一个新的基准，涵盖了模拟和真实世界中的多种实体，支持使用14.4k模拟和1.6k真实世界轨迹进行训练。研究识别了VLA的关键实用性问题，并引入了LLaVA-VLA，这是一种轻量级的VLA，集成了紧凑的VLM骨干和两阶段训练范式。LLaVA-VLA在不同实体中展示了泛化能力和灵活性，并且是第一个用于移动操作的端到端VLA模型。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Imagination for Efficient Visual World Model Planning</div>
<div class="meta-line">Authors: Junha Chun, Youngjoon Jeong, Taesup Kim</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-02T07:36:14+00:00 · Latest: 2026-02-26T06:26:43+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026; Project Page: https://nikriz1.github.io/sparse_imagination/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.01392v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.01392v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nikriz1.github.io/sparse_imagination/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices. This computational burden is particularly restrictive in robotics, where resources are severely constrained. To address this limitation, we propose a Sparse Imagination for Efficient Visual World Model Planning, which enhances computational efficiency by reducing the number of tokens processed during forward prediction. Our method leverages a sparsely trained vision-based world model based on transformers with randomized grouped attention strategy, allowing the model to flexibly adjust the number of tokens processed based on the computational resource. By enabling sparse imagination during latent rollout, our approach significantly accelerates planning while maintaining high control fidelity. Experimental results demonstrate that sparse imagination preserves task performance while dramatically improving inference efficiency. This general technique for visual planning is applicable from simple test-time trajectory optimization to complex real-world tasks with the latest VLAs, enabling the deployment of world models in real-time scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏想象以提高视觉世界模型规划效率</div>
<div class="mono" style="margin-top:8px">基于世界模型的规划显著提高了在复杂环境中的决策能力，通过使代理能够模拟未来状态并做出明智的选择。这一计算负担在资源严重受限的机器人领域尤为限制性。为解决这一限制，我们提出了一种稀疏想象以提高视觉世界模型规划效率的方法，通过减少前向预测过程中处理的令牌数量来增强计算效率。该方法利用基于变压器的稀疏训练视觉世界模型，并采用随机分组注意力策略，使模型能够根据计算资源灵活调整处理的令牌数量。通过在潜在回放期间启用稀疏想象，我们的方法显著加速了规划过程，同时保持了高控制精度。实验结果表明，稀疏想象在保持任务性能的同时，大幅提高了推理效率。这一通用的视觉规划技术适用于从简单的测试时轨迹优化到复杂的现实世界任务，利用最新的VLAs，使世界模型能够在实时场景中部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices.</div>
<div class="mono" style="margin-top:8px">研究旨在通过减少前向预测中处理的令牌数量来提高机器人领域基于世界模型的规划的计算效率。方法使用随机分组注意力的稀疏训练视觉世界模型，根据可用资源灵活调整处理的令牌数量。实验表明，稀疏想象在保持任务性能的同时显著提高了推理效率，适用于从简单的轨迹优化到复杂的现实世界任务等多种场景。</div>
</details>
</div>
<div class="card">
<div class="title">Metamorphic Testing of Vision-Language Action-Enabled Robots</div>
<div class="meta-line">Authors: Pablo Valle, Sergio Segura, Shaukat Ali, Aitor Arrieta</div>
<div class="meta-line">First: 2026-02-26T03:32:43+00:00 · Latest: 2026-02-26T03:32:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22579v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22579v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言-动作变换测试的视觉-语言-动作启用机器人</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型是多模态的机器人任务控制器，给定指令和视觉输入后，它们会产生一系列低级控制动作（或运动命令），使机器人能够在物理环境中执行所要求的任务。这些系统从多个角度面临测试判别器的问题。一方面，必须为每个指令提示定义一个测试判别器，这是一项复杂且难以泛化的做法。另一方面，当前最先进的判别器通常捕捉世界的符号表示（例如，机器人和物体状态），使任务正确性评估成为可能，但无法评估其他关键方面，如VLA启用的机器人执行任务的质量。在本文中，我们探讨了变换测试（MT）是否可以缓解此上下文中的测试判别器问题。为此，我们提出了两种变换关系模式和五个变换关系，以评估测试输入的变化是否影响VLA启用机器人的原始轨迹。涉及五个VLA模型、两个模拟机器人和四个机器人任务的实证研究表明，MT可以通过自动检测各种类型的失败（包括但不限于未完成的任务）来有效缓解测试判别器问题。更重要的是，提出的变换关系具有可泛化性，使所提出的方法适用于不同的VLA模型、机器人和任务，即使在没有测试判别器的情况下也是如此。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the test oracle problem in Vision-Language-Action (VLA) models, which are multimodal robotic task controllers. The authors propose using Metamorphic Testing (MT) to evaluate the quality of VLA-enabled robots&#x27; task execution. Two metamorphic relation patterns and five metamorphic relations are introduced to assess the impact of input changes on the robots&#x27; trajectories. An empirical study with five VLA models, two simulated robots, and four tasks demonstrates that MT can effectively detect various types of failures, particularly uncompleted tasks, and is generalizable across different models, robots, and tasks without requiring specific test oracles.</div>
<div class="mono" style="margin-top:8px">本文探讨了Vision-Language-Action (VLA)模型中的测试或acles问题，这些模型是多模态的机器人任务控制器。作者提出使用元测试（MT）来评估VLA使能机器人任务执行的质量。引入了两种元关系模式和五个元关系来评估输入变化对机器人轨迹的影响。一项涉及五个VLA模型、两个模拟机器人和四个任务的实证研究表明，MT可以有效检测各种类型的失败，特别是未完成的任务，并且该方法在不需要特定测试或acles的情况下，适用于不同的模型、机器人和任务。</div>
</details>
</div>
<div class="card">
<div class="title">A Pragmatic VLA Foundation Model</div>
<div class="meta-line">Authors: Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng</div>
<div class="meta-line">First: 2026-01-26T17:08:04+00:00 · Latest: 2026-02-26T03:30:01+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/, GM-100: https://huggingface.co/datasets/robbyant/lingbot-GM-100</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18692v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18692v2">PDF</a> · <a href="https://github.com/Robbyant/lingbot-vla/">Code1</a> · <a href="https://huggingface.co/datasets/robbyant/lingbot-GM-100">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种务实的VLA基础模型</div>
<div class="mono" style="margin-top:8px">在机器人操作方面具有巨大潜力，一个能力强大的视觉-语言-行动（VLA）基础模型有望在任务和平台之间忠实泛化，同时确保成本效益（例如，适应所需的数据显示和GPU小时数）。为此，我们开发了LingBot-VLA，使用来自9种流行双臂机器人配置的约20,000小时的真实世界数据。通过在3种机器人平台上进行系统评估，每个平台完成100个任务，每个任务有130个训练后阶段，我们的模型在性能和泛化能力方面明显优于竞争对手。我们还构建了一个高效的代码库，使用8块GPU的训练设置，每秒处理261个样本，比现有的VLA导向代码库快1.5到2.8倍（取决于所依赖的VLM基础模型）。上述特性确保了我们的模型适合实际部署。为了推进机器人学习领域的发展，我们提供了代码、基础模型和基准数据的开放访问，重点是使更多具有挑战性的任务成为可能，并促进合理的评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a cost-efficient Vision-Language-Action (VLA) foundation model for robotic manipulation, focusing on generalizability across tasks and platforms. The model, LingBot-VLA, was trained on 20,000 hours of real-world data from nine robot configurations and demonstrated superior performance across three robotic platforms, completing 100 tasks with 130 post-training episodes per task. Additionally, the model’s efficient codebase achieves a throughput of 261 samples per second, which is 1.5 to 2.8 times faster than existing VLA-oriented codebases, enhancing its practicality for real-world deployment.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种成本效益高的Vision-Language-Action (VLA) 基础模型，用于机器人操作，使其能够在任务和平台之间良好泛化。作者使用来自九种双臂机器人配置的约20,000小时的真实世界数据创建了LingBot-VLA。系统评估显示，LingBot-VLA 在三个机器人平台上表现出色，超越了竞争对手，展示了强大的性能和广泛的泛化能力。此外，该模型的高效代码库在8-GPU设置下实现了每秒261个样本的吞吐量，比现有的VLA定向代码库快1.5到2.8倍。</div>
</details>
</div>
<div class="card">
<div class="title">DreamWaQ++: Obstacle-Aware Quadrupedal Locomotion With Resilient Multi-Modal Reinforcement Learning</div>
<div class="meta-line">Authors: I Made Aswin Nahrendra, Byeongho Yu, Minho Oh, Dongkyu Lee, Seunghyun Lee, Hyeonwoo Lee, Hyungtae Lim, Hyun Myung</div>
<div class="meta-line">First: 2024-09-29T13:57:09+00:00 · Latest: 2026-02-26T02:03:32+00:00</div>
<div class="meta-line">Comments: IEEE Transactions on Robotics 2026. Project site is available at https://dreamwaqpp.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.19709v2">Abs</a> · <a href="https://arxiv.org/pdf/2409.19709v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dreamwaqpp.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadrupedal robots hold promising potential for applications in navigating cluttered environments with resilience akin to their animal counterparts. However, their floating base configuration makes them vulnerable to real-world uncertainties, yielding substantial challenges in their locomotion control. Deep reinforcement learning has become one of the plausible alternatives for realizing a robust locomotion controller. However, the approaches that rely solely on proprioception sacrifice collision-free locomotion because they require front-feet contact to detect the presence of stairs to adapt the locomotion gait. Meanwhile, incorporating exteroception necessitates a precisely modeled map observed by exteroceptive sensors over a period of time. Therefore, this work proposes a novel method to fuse proprioception and exteroception featuring a resilient multi-modal reinforcement learning. The proposed method yields a controller that showcases agile locomotion performance on a quadrupedal robot over a myriad of real-world courses, including rough terrains, steep slopes, and high-rise stairs, while retaining its robustness against out-of-distribution situations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DreamWaQ++：基于鲁棒多模态强化学习的避障四足运动控制</div>
<div class="mono" style="margin-top:8px">四足机器人在导航复杂环境方面具有巨大的应用潜力，其运动方式类似于其动物原型。然而，它们的浮动基座配置使它们容易受到现实世界不确定性的影响，给其运动控制带来了巨大挑战。深度强化学习已成为实现稳健运动控制器的一种可行替代方案。然而，依赖于本体感觉的方法牺牲了无碰撞运动，因为它们需要前足接触来检测楼梯的存在以适应运动步态。同时，结合外体感觉需要长时间由外体感觉传感器观察到精确建模的地图。因此，本工作提出了一种新的方法，将本体感觉和外体感觉融合到鲁棒多模态强化学习中。所提出的方法在四足机器人上展示了在各种现实世界路线（包括崎岖地形、陡坡和高楼梯）上的敏捷运动性能，同时保持了对离分布情况的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenges of robust locomotion for quadrupedal robots in cluttered environments by proposing a resilient multi-modal reinforcement learning method that fuses proprioception and exteroception. The method enables the robot to perform agile and collision-free locomotion on various terrains, including rough terrains, steep slopes, and high-rise stairs, while maintaining robustness against uncertainties. Key findings include the robot&#x27;s successful navigation through diverse real-world courses while retaining adaptability to out-of-distribution scenarios.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过结合本体感觉和外体感觉，利用鲁棒的多模态强化学习方法提高四足机器人的复杂环境下的行走控制能力。该方法使机器人能够在各种地形和楼梯上进行灵活且稳健的行走，而无需通过前足接触来检测楼梯。关键实验结果表明，机器人能够在崎岖地形、陡坡和高楼梯上导航，并在遇到意外情况时保持稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation</div>
<div class="meta-line">Authors: Xinyu Tan, Ningwei Bai, Harry Gardener, Zhengyang Zhong, Luoyu Zhang, Liuhaichen Yang, Zhekai Duan, Monkgogi Galeitsiwe, Zezhi Tang</div>
<div class="meta-line">First: 2026-02-26T01:16:27+00:00 · Latest: 2026-02-26T01:16:27+00:00</div>
<div class="meta-line">Comments: 7 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22514v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.
  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.
  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种无手语注释的视觉-语言-行动框架，用于实时的手语指导机器人操作，减少注释成本并避免信息损失。该系统直接将视觉手语手势映射到语义指令，确保自然且可扩展的多模态交互。实验结果表明，该系统在各种场景下能够将手语指令精确地转化为机器人动作，展示了其在无障碍、可扩展和多模态嵌入式智能方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow</div>
<div class="meta-line">Authors: Daesol Cho, Youngseok Jang, Danfei Xu, Sehoon Ha</div>
<div class="meta-line">First: 2026-02-25T22:50:51+00:00 · Latest: 2026-02-25T22:50:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22461v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22461v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EgoAVFlow is designed to address the challenge of using human egocentric videos for robot manipulation by incorporating active viewpoint control. It uses a shared 3D flow representation to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints to maintain task-critical visibility. Experiments show that EgoAVFlow outperforms previous methods in real-world scenarios, effectively maintaining visibility and performing robust manipulation without requiring robot demonstrations.</div>
<div class="mono" style="margin-top:8px">EgoAVFlow 通过利用共享的3D流表示来学习来自第一人称人类视频的数据，支持主动视角控制。它使用扩散模型来预测机器人动作和相机轨迹，并在测试时通过最大化与预测运动和场景几何相关的可见性奖励来细化视角。实验表明，EgoAVFlow 在真实世界场景中优于之前的基于人类演示的方法，能够有效保持可见性和执行稳健的操纵，而无需机器人演示。</div>
</details>
</div>
<div class="card">
<div class="title">Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</div>
<div class="meta-line">Authors: Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal, Murtuza Jadliwala, Bimal Viswanath</div>
<div class="meta-line">First: 2026-02-25T18:46:30+00:00 · Latest: 2026-02-25T18:46:30+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore. To IEEE SaTML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22197v1">PDF</a> · <a href="https://github.com/mlsecviswanath/img2imgdenoiser">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#x27;s utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现成的图像到图像模型足以击败图像保护方案</div>
<div class="mono" style="margin-top:8px">生成式人工智能（GenAI）的进步导致开发了各种保护策略，以防止未经授权使用图像。这些方法依赖于在图像上添加不可感知的保护扰动，以阻止诸如风格模仿或深度伪造等滥用行为。尽管之前对这些保护的攻击需要专门的、定制的方法，但我们证明这已不再必要。我们展示了一种现成的图像到图像GenAI模型可以通过简单的文本提示重新利用为通用的“去噪器”，有效地移除各种保护扰动。在涵盖6种不同保护方案的8个案例研究中，我们的通用攻击不仅绕过了这些防御，还在保持图像对攻击者有用性的同时，优于现有的专门攻击。我们的研究结果揭示了当前图像保护领域中一个关键且普遍存在的漏洞，表明许多方案提供了虚假的安全感。我们强调迫切需要开发稳健的防御措施，并指出任何未来的保护机制都必须以现成的GenAI模型攻击为基准。代码可在以下仓库中获得：https://github.com/mlsecviswanath/img2imgdenoiser</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study demonstrates that off-the-shelf image-to-image Generative AI models can be repurposed as generic &#x27;denoisers&#x27; using simple text prompts to remove protective perturbations added to images. Across eight case studies involving six diverse protection schemes, the general-purpose attack outperforms existing specialized attacks while maintaining the image&#x27;s utility for adversaries. This reveals a critical vulnerability in current image protection methods, suggesting they provide a false sense of security and necessitating robust defenses against off-the-shelf GenAI models.</div>
<div class="mono" style="margin-top:8px">研究展示了可以利用现成的图像到图像生成AI模型，通过简单的文本提示重新用途为“去噪器”，以移除添加到图像中的保护性干扰。在涉及六种不同保护方案的八个案例研究中，通用攻击方法不仅能够绕过这些防护措施，还优于现有的专门攻击方法，同时保持图像对攻击者的实用性。这揭示了当前图像保护方法中的一个关键漏洞，表明它们提供了虚假的安全感，并需要针对现成的生成AI模型建立 robust 的防御措施。</div>
</details>
</div>
<div class="card">
<div class="title">Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems</div>
<div class="meta-line">Authors: Georgios Kamaras, Craig Innes, Subramanian Ramamoorthy</div>
<div class="meta-line">First: 2025-10-30T16:23:46+00:00 · Latest: 2026-02-25T17:52:16+00:00</div>
<div class="meta-line">Comments: 20 pages, 18 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26656v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26656v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions.</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Belief Vision Language Action Models</div>
<div class="meta-line">Authors: Vaidehi Bagaria, Bijo Sebastian, Nirav Kumar Patel</div>
<div class="meta-line">First: 2026-02-24T08:02:16+00:00 · Latest: 2026-02-25T17:38:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20659v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20659v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language-action models must enable agents to execute long-horizon tasks under partial observability. However, most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. While semantic grounding is important, long-horizon manipulation fundamentally requires persistent, action-conditioned state representations. Current VLAs lack such representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once per task, the VLM provides high-level intent, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5 percent and 37.5 percent higher success rates on multi-stage pick-and-place and stacking tasks, respectively, compared to pi_0. It also reduces inference latency by up to five times relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show the belief module is the primary driver of performance, increasing success rates from 32.5 percent without belief to 77.5 percent with belief.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of existing vision-language-action models, which are observation-driven and suffer from high inference latency and action repetition. It introduces RB-VLA, a belief-centric architecture that maintains a compact latent state to track task progress and enable phase-aware control. RB-VLA outperforms prior models on long-horizon tasks, achieving higher success rates and reducing inference latency.</div>
<div class="mono" style="margin-top:8px">该论文针对现有视觉-语言-动作模型存在的问题，这些问题模型依赖于观察信息，导致推理延迟高和动作重复。论文提出了一种基于信念的架构RB-VLA，该架构维护一个紧凑的潜状态来跟踪任务进展，从而实现阶段感知的控制。RB-VLA在长期任务中表现出色，成功率更高，并且减少了推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Jingxuan Zhang, Yunta Hsieh, Zhongwei Wan, Haokun Lin, Xin Wang, Ziqi Wang, Yingtie Lei, Mi Zhang</div>
<div class="meta-line">First: 2026-02-23T19:55:54+00:00 · Latest: 2026-02-25T17:11:08+00:00</div>
<div class="meta-line">Comments: CVPR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20309v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20309v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QuantVLA：视觉-语言-行动模型的规模校准后训练量化</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型将感知、语言和控制统一起来，为具身智能体服务，但由于计算和内存需求迅速增加，尤其是在模型扩展到更长的时间范围和更大的骨干网络时，它们在实际部署中面临重大挑战。为了解决这些瓶颈，我们提出了QuantVLA，这是一种无需训练的后训练量化（PTQ）框架，据我们所知，这是第一个针对VLA系统的PTQ方法，并且是第一个成功量化扩散变压器（DiT）动作头的方法。QuantVLA 包含三个规模校准组件：（1）一种选择性量化布局，将所有线性层（包括语言骨干和DiT）转换为整数表示，而注意力投影保持在浮点数中，以保留原始操作计划；（2）注意力温度匹配，这是一种轻量级的每头缩放机制，用于稳定注意力概率，并在推理时折叠到去量化比例中；（3）输出头平衡，这是一种每层残差接口校准，用于减轻后投影能量漂移。该框架不需要额外的训练，仅使用少量未标记的校准缓冲区，并支持低位宽权重和激活的整数内核，同时保持架构不变。在代表性的VLA模型上，QuantVLA 超过了全精度基线的任务成功率，量化组件的相对内存节省约为70%，并提供了1.22倍的端到端推理延迟加速，为在严格的计算、内存和功率限制下实现可扩展的低位宽具身智能提供了实际途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">QuantVLA is a training-free post-training quantization framework for vision-language-action models, addressing the compute and memory challenges of scaling these models. It includes selective quantization, attention temperature matching, and output head balancing to preserve model performance. QuantVLA improves task success rates, reduces memory usage by about 70%, and accelerates inference by 1.22x, making low-bit embodied intelligence more practical under strict resource constraints.</div>
<div class="mono" style="margin-top:8px">QuantVLA 是一种无需训练的后训练量化框架，用于解决视觉-语言-行动模型在扩展时面临的计算和内存挑战。它包括选择性量化、注意力温度匹配和输出头平衡，以稳定并优化量化组件。QuantVLA 提高了任务成功率，将量化组件的内存使用减少了约 70%，并将端到端推理延迟加速了 1.22 倍，在严格的计算、内存和功耗限制下，为实现可扩展的低比特行动智能提供了实际途径。</div>
</details>
</div>
<div class="card">
<div class="title">A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation</div>
<div class="meta-line">Authors: Georgios Kamaras, Subramanian Ramamoorthy</div>
<div class="meta-line">Venue: In IEEE Robotics and Automation Letters, Volume 10, Issue 8, August 2025, Pages 8075-8082</div>
<div class="meta-line">First: 2025-02-25T20:01:06+00:00 · Latest: 2026-02-25T17:09:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18615v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.18615v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉感知的可变形线性物体操作中物体中心代理适应的实2仿2实分布处理方法</div>
<div class="mono" style="margin-top:8px">我们提出了一种集成（或端到端）框架，用于基于视觉感知操纵可变形线性物体（DLOs）的实2仿2实问题。使用参数化的DLO集合，我们使用无似然推断（LFI）来计算物理参数的后验分布，从而可以近似模拟每个特定DLO的行为。我们在训练过程中使用这些后验分布进行领域随机化，在仿真中为DLO抓取任务训练特定于物体的视知觉运动策略（即，假设只有视觉和本体感觉感知），使用无模型的强化学习。我们通过零样本的方式在现实世界中部署仿真训练的DLO操作策略，即无需任何进一步微调。在此背景下，我们评估了一种流行的LFI方法在仅使用动态操作轨迹中获得的视觉和本体感觉数据对参数化DLO集合进行精细分类的能力。然后我们研究了基于仿真的策略学习和现实世界性能中结果领域分布的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper presents an integrated framework for manipulating deformable linear objects (DLOs) in the real world using visual perception. It uses likelihood-free inference to compute the posterior distributions of physical parameters for each DLO, enabling domain randomisation in simulation for training object-specific visuomotor policies. The trained policies are deployed in the real world without further fine-tuning, demonstrating the approach&#x27;s utility and effectiveness in zero-shot transfer. The study evaluates the LFI method&#x27;s ability to perform fine classification over the parametric set of DLOs using visual and proprioceptive data during dynamic manipulation. The results show that the approach can significantly improve real-world performance by leveraging simulated data.</div>
<div class="mono" style="margin-top:8px">论文提出了一种集成框架，利用视觉感知来操纵变形线性物体（DLOs）。该框架使用似然性自由推理来计算每个DLO的物理参数后验分布，从而在模拟中进行域随机化以训练特定于物体的视觉-运动策略。训练好的策略在无需进一步微调的情况下被部署到现实世界中，展示了该方法在零样本迁移中的实用性和有效性。研究评估了LFI方法在动态操作轨迹中仅使用视觉和本体感受数据对参数化DLO集进行精细分类的能力。结果表明，通过利用模拟数据，该方法可以显著提高现实世界中的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Hongjie Fang, Shirun Tang, Mingyu Mei, Haoxiang Qin, Zihao He, Jingjing Chen, Ying Feng, Chenxi Wang, Wanxi Liu, Zaixing He, Cewu Lu, Shiquan Wang</div>
<div class="meta-line">First: 2026-02-25T16:35:24+00:00 · Latest: 2026-02-25T16:35:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22088v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://force-policy.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: https://force-policy.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>力策略：在交互框架下学习混合力-位置控制策略以应对丰富的接触操作</div>
<div class="mono" style="margin-top:8px">丰富的接触操作需要人类般感知和力反馈的整合：视觉应指导任务进展，而高频率的交互控制则需在不确定性下稳定接触。现有的基于学习的策略往往在单一网络中纠缠这些角色，权衡全局泛化与稳定局部细化之间的关系，而以控制为中心的方法通常假设已知的任务结构或仅学习控制器参数而非结构本身。在本文中，我们形式化了一个物理上合理的交互框架，即瞬时局部基，将力调节与运动执行解耦，并提出了一种从演示中恢复它的方法。基于此，我们通过提出力策略来解决这两个问题，这是一种全局-局部视觉-力策略，其中全局策略使用视觉指导自由空间动作，接触后，高频率的局部策略结合力反馈估计交互框架并执行混合力-位置控制以实现稳定交互。在多种丰富的接触任务中的实际实验表明，与强大的基线相比，该策略在接触建立的鲁棒性、力调节的准确性以及对具有不同几何形状和物理特性的新物体的可靠泛化方面均有所提升，最终提高了接触稳定性和执行质量。项目页面：https://force-policy.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of contact-rich manipulation by proposing a Force Policy that decouples force regulation from motion execution using an interaction frame. This method combines a global policy for vision-guided free-space actions with a local policy for force feedback and hybrid force-position control upon contact. Experiments demonstrate improved contact stability, accurate force regulation, and reliable generalization to new objects, outperforming strong baselines in diverse manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文提出了一种力策略，通过结合视觉和力反馈来应对接触丰富的操作挑战。该方法使用从演示中恢复的交互框架来解耦力调节和运动执行。实验表明，力策略在接触建立、力调节和对新型物体的泛化方面优于强基线，从而提高了接触稳定性和执行质量。</div>
</details>
</div>
<div class="card">
<div class="title">FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation</div>
<div class="meta-line">Authors: Edgar Welte, Yitian Shi, Rosa Wolf, Maximillian Gilles, Rania Rayyes</div>
<div class="meta-line">First: 2026-02-25T16:06:49+00:00 · Latest: 2026-02-25T16:06:49+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22056v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowCorrect：高效交互式修正生成流策略以实现机器人操作</div>
<div class="mono" style="margin-top:8px">生成性操作策略在部署时分布偏移下可能会灾难性地失败，但许多失败是近失：机器人几乎达到了正确的姿态，并且通过一个小的纠正动作就能成功。我们提出了FlowCorrect，一种在部署时的修正框架，使用稀疏的人工干预将近失失败转化为成功，而无需重新训练整个策略。在执行过程中，人类通过轻量级的VR接口提供短暂的纠正姿态干预。FlowCorrect利用这些稀疏的修正来局部适应策略，改进动作而不重新训练骨干模型，同时保持对之前学习场景的性能。我们在一个真实世界的机器人上对三个桌面任务进行了评估：拾取放置、倾倒和杯子直立。即使在低修正预算下，FlowCorrect也将难以解决的情况的成功率提高了85%，同时保持了对之前解决场景的性能。结果表明，FlowCorrect仅通过少量示范就能学习，并在实际机器人操作中的生成视知觉策略部署时实现快速、样本高效的增量、人机交互修正。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FlowCorrect is a deployment-time correction framework that uses sparse human nudges to adapt generative manipulation policies for robotic tasks, improving success rates on near-miss failures without full retraining. It evaluates on pick-and-place, pouring, and cup uprighting tasks, achieving an 85% improvement on hard cases while maintaining performance on previously solved scenarios with minimal human input.</div>
<div class="mono" style="margin-top:8px">FlowCorrect 是一种部署时纠正框架，使用稀疏的人工调整来局部适应生成性操作策略，而无需完全重新训练。在执行过程中，人类通过轻量级的 VR 接口提供简短的纠正姿态调整，FlowCorrect 使用这些调整来改进近错失败的动作。该框架在三个桌面任务：拾取和放置、倒水和杯子直立上展示了 85% 的成功率改进，同时保持对之前解决场景的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers</div>
<div class="meta-line">Authors: Guandong Li</div>
<div class="meta-line">First: 2026-02-20T06:24:20+00:00 · Latest: 2026-02-25T15:33:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18022v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT&#x27;s multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散变换器中的双通道注意力引导无训练编辑控制</div>
<div class="mono" style="margin-top:8px">基于扩散变换器（DiT）架构的扩散基础图像编辑模型对无训练编辑强度控制提出了关键要求。现有的注意力操作方法仅专注于键空间来调节注意力路由，而完全忽略了值空间——它控制特征聚合。在本文中，我们首先揭示了DiT多模态注意力层中的键投影和值投影都表现出明显的偏差-增量结构，其中令牌嵌入紧密围绕特定层的偏差向量聚类。基于这一观察，我们提出了双通道注意力引导（DCAG），这是一种无训练框架，可以同时操作键通道（控制注意力的方向）和值通道（控制聚合的内容）。我们提供了理论分析，表明键通道通过非线性softmax函数操作，作为粗略的控制旋钮，而值通道通过线性加权和操作，作为精细的补充。两者结合的二维参数空间$(δ_k, δ_v)$能够比任何单通道方法提供更精确的编辑保真度权衡。在PIE-Bench基准（700张图像，10种编辑类别）上的广泛实验表明，DCAG在所有保真度指标上都优于仅键引导，特别是在局部编辑任务如对象删除（LPIPS减少4.9%）和对象添加（LPIPS减少3.2%）方面取得了最显著的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture.</div>
</details>
</div>
<div class="card">
<div class="title">World Guidance: World Modeling in Condition Space for Action Generation</div>
<div class="meta-line">Authors: Yue Su, Sijin Chen, Haixin Shi, Mingyu Liu, Zhengshen Zhang, Ningyuan Huang, Weiheng Zhong, Zhengbang Zhu, Yuxiao Liu, Xihui Liu</div>
<div class="meta-line">First: 2026-02-25T15:27:09+00:00 · Latest: 2026-02-25T15:27:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://selen-suyue.github.io/WoGNet/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22010v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://selen-suyue.github.io/WoGNet/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes WoG (World Guidance), a framework that maps future observations into compact conditions to enhance action generation in Vision-Language-Action models. By integrating these conditions into the action inference pipeline, the model is trained to predict both compressed conditions and future actions, achieving effective world modeling. Experiments show that this approach improves fine-grained action generation and generalization capabilities, outperforming existing methods based on future prediction in both simulation and real-world environments.</div>
<div class="mono" style="margin-top:8px">该论文提出了WoG（世界引导）框架，将未来观察结果映射为紧凑的条件以增强Vision-Language-Action模型中的动作生成。通过将这些条件集成到动作推理管道中，模型被训练以同时预测压缩条件和未来动作，从而实现有效的世界建模。实验表明，该方法在细粒度动作生成和泛化能力方面优于基于未来预测的现有方法，在仿真和真实环境中的表现均优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260227_0353.html">20260227_0353</a>
<a href="archive/20260226_0403.html">20260226_0403</a>
<a href="archive/20260225_0357.html">20260225_0357</a>
<a href="archive/20260224_0405.html">20260224_0405</a>
<a href="archive/20260223_0337.html">20260223_0337</a>
<a href="archive/20260222_0338.html">20260222_0338</a>
<a href="archive/20260221_0345.html">20260221_0345</a>
<a href="archive/20260220_0347.html">20260220_0347</a>
<a href="archive/20260219_0358.html">20260219_0358</a>
<a href="archive/20260218_0358.html">20260218_0358</a>
<a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
