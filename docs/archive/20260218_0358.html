<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-18 03:58</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260218_0358</div>
    <div class="row"><div class="card">
<div class="title">BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames</div>
<div class="meta-line">Authors: Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi, Dhruv Shah, Aviral Kumar</div>
<div class="meta-line">First: 2026-02-16T18:49:56+00:00 · Latest: 2026-02-16T18:49:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15010v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BPP：通过关注关键历史帧进行长上下文机器人模仿学习</div>
<div class="mono" style="margin-top:8px">许多机器人的任务需要关注过去的观察历史。例如，在房间里找到一个物品需要记住已经搜索过的地方。然而，表现最好的机器人策略通常只依赖于当前的观察，限制了它们在这些任务中的应用。简单地依赖过去的观察往往由于虚假的相关性而失败：策略会抓住训练历史中的偶然特征，这些特征在部署到新的分布时无法泛化。我们分析了为什么策略会抓住这些虚假的相关性，并发现这个问题源于训练过程中对可能历史的覆盖有限，这种覆盖随着时间范围的增加而呈指数增长。现有的正则化技术在不同任务上提供的益处不一致，因为它们没有从根本上解决这个问题。受这些发现的启发，我们提出了大图策略（BPP），该方法依赖于由视觉-语言模型检测出的有意义的关键帧。通过将多样化的演示投影到一组与任务相关的事件上，BPP显著减少了训练和部署之间的分布偏移，而不会牺牲表达能力。我们在四个具有挑战性的现实世界操作任务和三个模拟任务上评估了BPP，所有任务都需要历史条件。BPP在现实世界评估中的成功率比最佳对比方法高70%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of robot imitation learning by focusing on key historical frames to improve long-context understanding. It identifies that existing methods often rely on current observations only, leading to poor performance on tasks requiring memory of past observations. The proposed Big Picture Policies (BPP) method uses a vision-language model to detect meaningful keyframes, reducing distribution shift and improving generalization. BPP outperforms other methods by achieving 70% higher success rates on real-world manipulation tasks compared to the best comparison method.</div>
<div class="mono" style="margin-top:8px">论文旨在通过关注关键历史帧来改进机器人模仿学习，解决需要记住过去观察结果的任务挑战。提出了一种Big Picture Policies (BPP) 方法，该方法基于视觉-语言模型检测出的有意义的关键帧进行条件判断。这种方法在训练和部署之间减少了分布差异，从而在真实世界的操作任务中取得了显著更高的成功率，相比现有方法，BPP 在真实世界评估中的成功率提高了70%。</div>
</details>
</div>
<div class="card">
<div class="title">DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI</div>
<div class="meta-line">Authors: En Yu, Haoran Lv, Jianjian Sun, Kangheng Lin, Ruitao Zhang, Yukang Shi, Yuyang Chen, Ze Chen, Ziheng Zhang, Fan Jia, Kaixin Liu, Meng Zhang, Ruitao Hao, Saike Huang, Songhan Xie, Yu Liu, Zhao Wu, Bin Xie, Pengwei Zhang, Qi Yang, Xianchi Deng, Yunfei Wei, Enwen Zhang, Hongyang Peng, Jie Zhao, Kai Liu, Wei Sun, Yajun Wei, Yi Yang, Yunqiao Zhang, Ziwei Yan, Haitao Yang, Hao Liu, Haoqiang Fan, Haowei Zhang, Junwen Huang, Yang Chen, Yunchao Ma, Yunhuan Yang, Zhengyuan Du, Ziming Liu, Jiahui Niu, Yucheng Zhao, Daxin Jiang, Wenbin Tang, Xiangyu Zhang, Zheng Ge, Erjin Zhou, Tiancai Wang</div>
<div class="meta-line">First: 2026-02-16T17:59:16+00:00 · Latest: 2026-02-16T17:59:16+00:00</div>
<div class="meta-line">Comments: Authors are listed in alphabetical order. Code is available at https://github.com/Dexmal/dexbotic</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14974v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14974v1">PDF</a> · <a href="https://github.com/Dexmal/dexbotic">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DM0：面向物理人工智能的具身原生视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">超越传统将互联网预训练模型适应物理任务的范式，我们提出了DM0，一个具身原生视觉-语言-行动（VLA）框架，专为物理人工智能设计。与将物理接地视为微调后的附带任务不同，DM0 从一开始就通过从异构数据源中学习来统一具身操作和导航。我们的方法遵循一个全面的三阶段管道：预训练、中期训练和后期训练。首先，我们使用多种语料库对视觉-语言模型（VLM）进行大规模统一预训练——无缝整合网络文本、自动驾驶场景和具身交互日志，以共同获取语义知识和物理先验。随后，我们基于VLM构建一个流匹配动作专家。为了协调高层次推理与低层次控制，DM0 采用混合训练策略：对于具身数据，动作专家的梯度不反向传播到VLM以保持泛化表示，而VLM仍然可以在非具身数据上进行训练。此外，我们引入了具身空间支架策略来构建空间链式思考（CoT）推理，有效约束了动作解空间。在RoboChallenge基准测试上的实验表明，DM0 在Table30的专家和通用设置中均实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Language Movement Primitives: Grounding Language Models in Robot Motion</div>
<div class="meta-line">Authors: Yinlong Dai, Benjamin A. Christie, Daniel J. Evans, Dylan P. Losey, Simon Stepputtis</div>
<div class="meta-line">First: 2026-02-02T21:41:08+00:00 · Latest: 2026-02-16T16:41:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02839v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02839v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling robots to perform novel manipulation tasks from natural language instructions remains a fundamental challenge in robotics, despite significant progress in generalized problem solving with foundational models. Large vision and language models (VLMs) are capable of processing high-dimensional input data for visual scene and language understanding, as well as decomposing tasks into a sequence of logical steps; however, they struggle to ground those steps in embodied robot motion. On the other hand, robotics foundation models output action commands, but require in-domain fine-tuning or experience before they are able to perform novel tasks successfully. At its core, there still remains the fundamental challenge of connecting abstract task reasoning with low-level motion control. To address this disconnect, we propose Language Movement Primitives (LMPs), a framework that grounds VLM reasoning in Dynamic Movement Primitive (DMP) parameterization. Our key insight is that DMPs provide a small number of interpretable parameters, and VLMs can set these parameters to specify diverse, continuous, and stable trajectories. Put another way: VLMs can reason over free-form natural language task descriptions, and semantically ground their desired motions into DMPs -- bridging the gap between high-level task reasoning and low-level position and velocity control. Building on this combination of VLMs and DMPs, we formulate our LMP pipeline for zero-shot robot manipulation that effectively completes tabletop manipulation problems by generating a sequence of DMP motions. Across 20 real-world manipulation tasks, we show that LMP achieves 80% task success as compared to 31% for the best-performing baseline. See videos at our website: https://collab.me.vt.edu/lmp</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言运动基元：将语言模型与机器人运动相结合</div>
<div class="mono" style="margin-top:8px">尽管基础模型在通用问题解决方面取得了显著进展，使机器人能够根据自然语言指令执行新的操作任务仍然是机器人技术中的一个基本挑战。大型视觉和语言模型（VLMs）能够处理高维输入数据以进行视觉场景和语言理解，并将任务分解为一系列逻辑步骤；然而，它们难以将这些步骤与实体化的机器人运动联系起来。另一方面，机器人基础模型输出行动指令，但在进行新型任务之前需要领域内的微调或经验。归根结底，仍然存在将抽象的任务推理与低级运动控制连接起来的基本挑战。为了解决这一断层，我们提出了语言运动基元（LMPs）框架，该框架将VLM推理与动态运动基元（DMP）参数化相结合。我们的关键见解是，DMPs提供了少量可解释的参数，而VLMs可以设置这些参数以指定多样、连续和稳定的轨迹。换句话说：VLMs可以对自由形式的自然语言任务描述进行推理，并将它们所期望的运动语义地映射到DMPs——在高层次任务推理与低级位置和速度控制之间架起桥梁。基于这种VLMs和DMPs的结合，我们制定了LMP流水线，用于零样本机器人操作，通过生成一系列DMP运动来有效完成桌面操作问题。在20项真实世界的操作任务中，我们展示了LMP实现了80%的任务成功率，而最佳基线仅为31%。请参见我们网站上的视频：https://collab.me.vt.edu/lmp</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable robots to perform novel manipulation tasks from natural language instructions by bridging the gap between high-level task reasoning and low-level motion control. The method involves using Language Movement Primitives (LMPs) that ground large vision and language models (VLMs) in Dynamic Movement Primitive (DMP) parameterization. Key findings show that LMP achieves 80% task success in 20 real-world manipulation tasks, outperforming the best baseline by 49 percentage points. The LMP framework effectively generates a sequence of DMP motions to complete tabletop manipulation problems.</div>
<div class="mono" style="margin-top:8px">论文提出了一种语言运动基元（LMP）框架，旨在解决机器人从自然语言指令执行新型操作任务的挑战。LMP将大型视觉和语言模型（VLM）的抽象任务推理与动态运动基元（DMP）参数化相结合，使VLM能够指定多样、连续和稳定的轨迹。研究结果显示，LMP在20项真实世界操作任务中实现了80%的任务成功率，远超最佳基线的31%成功率。关键在于VLM能够处理自然语言任务描述，并将所需的运动语义地映射到DMP中，从而弥合了高层次任务推理与低层次位置和速度控制之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning</div>
<div class="meta-line">Authors: Sunghwan Kim, Woojeh Chung, Zhirui Dai, Dwait Bhatt, Arth Shukla, Hao Su, Yulun Tian, Nikolay Atanasov</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-10-04T17:40:53+00:00 · Latest: 2026-02-16T16:14:48+00:00</div>
<div class="meta-line">Comments: ICRA 2026, project page: https://existentialrobotics.org/sbp_page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03885v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03885v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we demonstrate that mobile manipulation policies utilizing a 3D latent map achieve stronger spatial and temporal reasoning than policies relying solely on images. We introduce Seeing the Bigger Picture (SBP), an end-to-end policy learning approach that operates directly on a 3D map of latent features. In SBP, the map extends perception beyond the robot&#x27;s current field of view and aggregates observations over long horizons. Our mapping approach incrementally fuses multiview observations into a grid of scene-specific latent features. A pre-trained, scene-agnostic decoder reconstructs target embeddings from these features and enables online optimization of the map features during task execution. A policy, trainable with behavior cloning or reinforcement learning, treats the latent map as a state variable and uses global context from the map obtained via a 3D feature aggregator. We evaluate SBP on scene-level mobile manipulation and sequential tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons globally over the scene, (ii) leverages the map as long-horizon memory, and (iii) outperforms image-based policies in both in-distribution and novel scenes, e.g., improving the success rate by 15% for the sequential manipulation task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从全局视角出发：用于移动操作策略学习的3D潜在映射</div>
<div class="mono" style="margin-top:8px">在本文中，我们证明了利用3D潜在映射的移动操作策略在空间和时间推理方面比仅依赖图像的策略更强大。我们提出了Seeing the Bigger Picture (SBP)，这是一种端到端的策略学习方法，直接在3D潜在特征图上操作。在SBP中，地图扩展了感知范围，超越了机器人的当前视野，并在长时间范围内聚合观察结果。我们的映射方法将多视图观察结果增量融合到场景特定的潜在特征网格中。一个预先训练的、场景无关的解码器从这些特征中重建目标嵌入，并在任务执行期间使地图特征在线优化成为可能。一个可使用行为克隆或强化学习训练的策略将潜在地图视为状态变量，并通过3D特征聚合器从地图中获取全局上下文。我们在场景级别的移动操作和序列式桌面操作任务上评估了SBP。我们的实验表明，SBP (i) 在场景中全局推理，(ii) 利用地图作为长时记忆，(iii) 在分布内和新场景中均优于基于图像的策略，例如，在序列操作任务中将成功率提高了15%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces Seeing the Bigger Picture (SBP), an end-to-end policy learning approach that uses a 3D latent map for mobile manipulation, enhancing spatial and temporal reasoning compared to image-based policies. SBP fuses multiview observations into a grid of latent features, with a scene-agnostic decoder optimizing map features during task execution. The policy treats the latent map as a state variable, leveraging global context for better performance. Experiments show SBP outperforms image-based policies in both familiar and novel scenes, with a 15% improvement in success rate for sequential manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本文介绍了SBP，一种使用3D潜空间图进行移动操作的端到端策略学习方法，以增强空间和时间推理。该方法逐步融合多视图观察，并使用预训练解码器在任务执行期间优化地图特征。实验表明，SBP在全局推理和将地图作为长时记忆方面优于基于图像的策略，例如在顺序操作任务中的成功率提高了15%。</div>
</details>
</div>
<div class="card">
<div class="title">Affordance Transfer Across Object Instances via Semantically Anchored Functional Map</div>
<div class="meta-line">Authors: Xiaoxiang Dong, Weiming Zhi</div>
<div class="meta-line">First: 2026-02-16T16:04:47+00:00 · Latest: 2026-02-16T16:04:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14874v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14874v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语义锚定的功能映射在对象实例间转移功能</div>
<div class="mono" style="margin-top:8px">传统的学习从演示（LfD）通常需要繁琐的物理演示收集，这既耗时又难以扩展。最近的进展表明，机器人可以通过提取交互提示来从人类视频中学习，而无需直接参与机器人操作。然而，一个基本的挑战仍然存在：如何在不同功能相似但几何结构显著不同的对象实例之间泛化演示的交互。在本文中，我们提出了语义锚定的功能映射（SemFM），这是一种从单个视觉演示中在对象间转移功能的框架。从图像中重建的粗略网格开始，我们的方法识别出对象间的语义对应的功能区域，选择互斥的语义锚点，并使用功能映射在表面上传播这些约束，以获得密集且语义一致的对应关系。这使得演示的交互区域能够在几何结构多样的对象间以轻量且可解释的方式转移。在合成对象类别和实际机器人操作任务上的实验表明，我们的方法能够以适度的计算成本实现准确的功能转移，使其非常适合实际的机器人感知到行动管道。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of transferring affordances across different object instances by proposing Semantic Anchored Functional Maps (SemFM). Starting from a coarse mesh, the method identifies semantically corresponding functional regions and uses a functional map to propagate constraints, enabling lightweight and interpretable transfer of interaction regions across geometrically diverse objects. Experiments demonstrate accurate affordance transfer with modest computational cost, suitable for practical robotic perception-to-action pipelines.</div>
<div class="mono" style="margin-top:8px">本文解决了在不同几何形状但功能相似的对象实例之间转移功能的问题。提出了一种语义锚定的功能映射(SemFM)方法，该方法识别语义对应的功能区域，并使用功能映射传播这些约束。实验表明，该方法能够以较低的计算成本实现准确的功能转移，适用于实际的机器人应用。</div>
</details>
</div>
<div class="card">
<div class="title">EmbeWebAgent: Embedding Web Agents into Any Customized UI</div>
<div class="meta-line">Authors: Chenyang Ma, Clyde Fare, Matthew Wilson, Dave Braines</div>
<div class="meta-line">First: 2026-02-16T15:59:56+00:00 · Latest: 2026-02-16T15:59:56+00:00</div>
<div class="meta-line">Comments: Technical Report; Live Demo: https://youtu.be/Cy06Ljee1JQ</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14865v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14865v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EmbeWebAgent: 将网络代理嵌入到任何自定义UI中</div>
<div class="mono" style="margin-top:8px">大多数网络代理在人类界面级别运行，观察屏幕截图或原始DOM树，而没有应用程序级别的访问权限，这限制了其稳健性和动作表达能力。然而，在企业环境中，可以显式控制前端和后端。我们提出了EmbeWebAgent框架，该框架使用轻量级的前端钩子（经过筛选的ARIA和基于URL的观察，以及通过WebSocket暴露的每页函数注册表）直接嵌入到现有UI中，并使用可重用的后端工作流进行推理和执行动作。EmbeWebAgent对堆栈是无依赖的（例如React或Angular），支持从GUI原语到更高层次复合体的混合粒度动作，并通过MCP工具协调导航、操作和领域特定分析。我们的演示显示了最小的重制努力和基于实时UI的稳健多步骤行为。在线演示: https://youtu.be/Cy06Ljee1JQ</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EmbeWebAgent is designed to enhance the robustness and expressiveness of web agents by embedding them directly into existing UIs, leveraging lightweight frontend hooks and a backend workflow for reasoning and action execution. The framework supports various frontend technologies and allows for both GUI and higher-level actions, demonstrating robust multi-step behaviors in a live UI setting with minimal retrofitting effort.</div>
<div class="mono" style="margin-top:8px">EmbeWebAgent旨在通过直接嵌入现有UI并利用轻量级前端钩子和后端工作流来增强Web代理的稳健性和表达性。该框架支持多种前端技术，并允许执行从GUI原语到更高层次的复合动作，展示了在实时UI环境中通过最小的改造努力实现稳健的多步骤行为。</div>
</details>
</div>
<div class="card">
<div class="title">An Agentic Operationalization of DISARM for FIMI Investigation on Social Media</div>
<div class="meta-line">Authors: Kevin Tseng, Juan Carlos Toledano, Bart De Clerck, Yuliia Dukach, Phil Tinn</div>
<div class="meta-line">First: 2026-01-21T15:50:13+00:00 · Latest: 2026-02-16T14:35:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15109v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15109v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The interoperability of data and intelligence across allied partners and their respective end-user groups is considered a foundational enabler of the collective defense capability -- both conventional and hybrid -- of NATO countries. Foreign Information Manipulation and Interference (FIMI) and related hybrid activities are conducted across various societal dimensions and infospheres, posing an ever greater challenge to threat characterization, sustained situational awareness, and response coordination. Recent advances in AI have further reduced the cost of AI-augmented trolling and interference activities, such as through the generation and amplification of manipulative content. Despite the introduction of the DISARM framework as a standardized metadata and analytical framework for FIMI, operationalizing it at the scale of social media remains a challenge. We propose a framework-agnostic, agent-based operationalization of DISARM to investigate FIMI on social media. We develop an agent coordination pipeline in which specialized agentic AI components collaboratively (1) detect candidate manipulative behaviors and (2) map these behaviors onto standard DISARM taxonomies in a transparent manner. We evaluate the approach on two real-world datasets annotated by domain practitioners. Our results show that the approach is effective in scaling the predominantly manual and heavily interpretive work of FIMI analysis -- including uncovering more than 30 previously undetected Russian bot accounts during manual analysis -- and provides a direct contribution to enhancing situational awareness and data interoperability in the context of operating in media- and information-rich settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DISARM的代理化操作及其在社交媒体FIMI调查中的应用</div>
<div class="mono" style="margin-top:8px">跨盟友伙伴及其各自的最终用户群体的数据和情报互操作性被认为是北约国家集体防御能力——无论是传统还是混合——的基础性使能器。外国信息操纵和干扰（FIMI）及相关混合活动在各种社会维度和信息空间中进行，对威胁特征分析、持续态势感知和响应协调构成了越来越大的挑战。最近的人工智能进步进一步降低了人工智能增强的骚扰和干扰活动的成本，例如通过生成和放大操纵性内容。尽管已经引入了DISARM框架作为FIMI的标准元数据和分析框架，但在社交媒体上实现这一框架仍然面临挑战。我们提出了一种框架无关的基于代理的DISARM操作化方法，以调查社交媒体上的FIMI。我们开发了一个代理协调管道，其中专门的代理AI组件协作地（1）检测候选操纵行为，并（2）以透明的方式将这些行为映射到标准的DISARM分类学中。我们使用由领域专家注释的两个真实数据集评估了该方法。我们的结果表明，该方法在扩大FIMI分析的大部分手动和高度解释性工作方面是有效的——包括在手动分析中发现超过30个以前未被检测到的俄罗斯机器人账号——并且直接为在媒体和信息丰富环境中操作时增强态势感知和数据互操作性做出了贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the interoperability of data and intelligence for NATO&#x27;s collective defense against FIMI activities. It proposes an agent-based operationalization of the DISARM framework to automate the detection and mapping of manipulative behaviors on social media. The approach, evaluated on real-world datasets, demonstrates effectiveness in scaling manual FIMI analysis and uncovering previously undetected bot accounts, thereby improving situational awareness and data interoperability.</div>
<div class="mono" style="margin-top:8px">研究旨在通过增强数据和情报的互操作性，提升北约在对抗FIMI活动中的集体防御能力。它提出了一种基于代理的DISARM框架操作化方法，以自动化检测和分类社交媒体上的操纵行为。该方法使用专门的AI组件协作识别和分类这些行为，展示了在手动分析中发现超过30个之前未被检测到的俄罗斯机器人账号，并提高了态势感知和数据互操作性。</div>
</details>
</div>
<div class="card">
<div class="title">Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments</div>
<div class="meta-line">Authors: Ruofeng Wei, Kai Chen, Yui Lun Ng, Yiyao Ma, Justin Di-Lang Ho, Hon Sing Tong, Xiaomei Wang, Jing Dai, Ka-Wai Kwok, Qi Dou</div>
<div class="meta-line">First: 2026-02-16T11:46:14+00:00 · Latest: 2026-02-16T11:46:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14666v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实时单目2D和3D内腔场景感知以控制柔性内窥镜器械</div>
<div class="mono" style="margin-top:8px">内腔手术为早期胃肠道和泌尿道癌症提供了微创手术选择，但受限于手术工具和陡峭的学习曲线。机器人系统，尤其是连续体机器人，提供了灵活的器械，能够实现精确的组织切除，可能改善手术结果。本文介绍了连续体机器人系统在内腔手术中的视觉感知平台。我们的目标是利用基于单目内窥镜图像的感知算法识别柔性器械的位置和方向，并测量其与组织的距离。我们引入了基于学习的2D和3D感知算法，并开发了一个物理上逼真的模拟器，该模拟器模拟了柔性器械的动力学。该模拟器生成了逼真的内腔场景，使柔性机器人的控制和大量数据收集成为可能。使用连续体机器人原型，我们进行了模块级和系统级评估。结果表明，我们的算法提高了柔性器械的控制能力，轨迹跟随任务的操纵时间减少了超过70%，并增强了对手术场景的理解，从而实现了稳健的内腔手术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve endoluminal surgery by developing a visual perception platform for continuum robotic systems. The method involves using monocular endoscopic image-based algorithms to identify the position and orientation of flexible instruments and measure their distances from tissues. Key experimental findings show that the algorithms reduce manipulation time by over 70% for trajectory-following tasks and enhance understanding of surgical scenarios, leading to more robust endoluminal surgeries.</div>
<div class="mono" style="margin-top:8px">论文旨在通过单目感知算法提升柔性机器人器械在内镜手术中的精确控制。引入了基于学习的2D和3D感知算法以及一个物理上逼真的模拟器来模拟柔性器械的动力学。实验结果表明，在轨迹跟随任务中显著减少了操作时间，并提高了对手术场景的理解，从而提高了内镜手术的稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">BoundPlanner: A convex-set-based approach to bounded manipulator trajectory planning</div>
<div class="meta-line">Authors: Thies Oelerich, Christian Hartl-Nesic, Florian Beck, Andreas Kugi</div>
<div class="meta-line">First: 2025-02-18T21:16:11+00:00 · Latest: 2026-02-16T10:41:24+00:00</div>
<div class="meta-line">Comments: Published at RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.13286v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.13286v2">PDF</a> · <a href="http://github.com/TU-Wien-ACIN-CDS/BoundPlanner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online trajectory planning enables robot manipulators to react quickly to changing environments or tasks. Many robot trajectory planners exist for known environments but are often too slow for online computations. Current methods in online trajectory planning do not find suitable trajectories in challenging scenarios that respect the limits of the robot and account for collisions. This work proposes a trajectory planning framework consisting of the novel Cartesian path planner based on convex sets, called BoundPlanner, and the online trajectory planner BoundMPC. BoundPlanner explores and maps the collision-free space using convex sets to compute a reference path with bounds. BoundMPC is extended in this work to handle convex sets for path deviations, which allows the robot to optimally follow the path within the bounds while accounting for the robot&#x27;s kinematics. Collisions of the robot&#x27;s kinematic chain are considered by a novel convex-set-based collision avoidance formulation independent on the number of obstacles. Simulations and experiments with a 7-DoF manipulator show the performance of the proposed planner compared to state-of-the-art methods. The source code is available at github.com/TU-Wien-ACIN-CDS/BoundPlanner and videos of the experiments can be found at www.acin.tuwien.ac.at/42d4.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BoundPlanner：基于凸集的方法进行有界机械臂轨迹规划</div>
<div class="mono" style="margin-top:8px">在线轨迹规划使机器人机械臂能够快速应对变化的环境或任务。虽然存在许多针对已知环境的机器人轨迹规划器，但它们通常不适合在线计算。当前的在线轨迹规划方法在具有挑战性的场景中无法找到符合机器人限制且考虑碰撞的合适轨迹。本研究提出了一种轨迹规划框架，包括基于凸集的新笛卡尔路径规划器BoundPlanner和在线轨迹规划器BoundMPC。BoundPlanner使用凸集探索和映射碰撞自由空间，计算具有界限的参考路径。在此项工作中，BoundMPC被扩展以处理路径偏差的凸集，这使得机器人能够在考虑其运动学的情况下最优地跟随路径。通过一种新的基于凸集的碰撞避免公式，机器人运动链的碰撞被考虑，该公式独立于障碍物的数量。7-自由度机械臂的仿真和实验显示了所提规划器与现有最佳方法的性能。源代码可在github.com/TU-Wien-ACIN-CDS/BoundPlanner获取，实验视频可在www.acin.tuwien.ac.at/42d4找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">BoundPlanner is a novel trajectory planning framework for robot manipulators that uses convex sets to explore and map the collision-free space, enabling efficient online trajectory planning. The framework includes BoundPlanner for computing reference paths with bounds and BoundMPC for optimal path following while considering the robot&#x27;s kinematics. Experiments with a 7-DoF manipulator demonstrate the planner&#x27;s superior performance compared to existing methods in handling challenging scenarios and collision avoidance.</div>
<div class="mono" style="margin-top:8px">BoundPlanner 是一种基于凸集的轨迹规划框架，使用新颖的笛卡尔路径规划器来计算具有边界参考路径，并使用在线轨迹规划器 BoundMPC 来在考虑机器人运动学的同时最优地跟随这些路径并避免碰撞。7-自由度 manipulator 的仿真和实验表明，该规划器在处理具有挑战性的场景和遵守机器人限制方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">BoundMPC: Cartesian path following with error bounds based on model predictive control in the joint space</div>
<div class="meta-line">Authors: Thies Oelerich, Florian Beck, Christian Hartl-Nesic, Andreas Kugi</div>
<div class="meta-line">First: 2024-01-10T10:30:22+00:00 · Latest: 2026-02-16T10:25:37+00:00</div>
<div class="meta-line">Comments: 17 pages, 20 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.05057v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.05057v2">PDF</a> · <a href="https://github.com/thieso/boundmpc">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces the BoundMPC strategy, an innovative online model-predictive path-following approach for robot manipulators. This joint-space trajectory planner allows the following of Cartesian reference paths in the end-effector&#x27;s position and orientation, including via-points, within the desired asymmetric bounds of the orthogonal path error. These bounds encode the obstacle-free space and additional task-specific constraints in Cartesian space. Contrary to traditional path-following concepts, BoundMPC purposefully deviates from the Cartesian reference path in position and orientation to account for the robot&#x27;s kinematics, leading to more successful task executions for Cartesian reference paths. Furthermore the simple reference path formulation is computationally efficient and allows for replanning during the robot&#x27;s motion. This feature makes it possible to use this planner for dynamically changing environments and varying goals. The flexibility and performance of BoundMPC are experimentally demonstrated by five scenarios on a 7-DoF Kuka LBR iiwa 14 R820 robot. The first scenario shows the transfer of a larger object from a start to a goal pose through a confined space where the object must be tilted. The second scenario deals with grasping an object from a table where the grasping point changes during the robot&#x27;s motion, and collisions with other obstacles in the scene must be avoided. The adaptability of BoundMPC is showcased in scenarios such as the opening of a drawer, the transfer of an open container, and the wiping of a table, where it effectively handles task-specific constraints. The last scenario highlights the possibility of accounting for collisions with the entire robot&#x27;s kinematic chain. The code is readily available at https://github.com/thieso/boundmpc, inspiring you to explore its potential and adapt it to your specific robotic tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BoundMPC：基于关节空间模型预测控制的笛卡尔路径跟踪及其误差边界</div>
<div class="mono" style="margin-top:8px">本文介绍了BoundMPC策略，这是一种创新的在线模型预测路径跟踪方法，适用于机器人操作器。该关节空间轨迹规划器允许末端执行器位置和姿态跟踪笛卡尔参考路径，包括途径点，同时在期望的正交路径误差的非对称边界内进行跟踪。这些边界编码了笛卡尔空间中的无障碍空间和额外的任务特定约束。与传统的路径跟踪概念不同，BoundMPC故意在位置和姿态上偏离笛卡尔参考路径，以考虑机器人的运动学，从而更成功地执行笛卡尔参考路径任务。此外，简单的参考路径公式计算效率高，并允许机器人运动期间重新规划。这一特性使其能够用于动态变化的环境和变化的目标。BoundMPC的灵活性和性能通过在7自由度Kuka LBR iiwa 14 R820机器人上进行的五个场景实验得到了实验验证。第一个场景展示了通过狭窄空间将较大物体从起始位置转移到目标位置的过程，其中物体需要倾斜。第二个场景涉及从桌子上抓取物体，其中抓取点在机器人运动过程中发生变化，必须避免与其他障碍物的碰撞。BoundMPC的适应性在诸如抽屉的打开、开放容器的转移和桌子的擦拭等场景中得到了展示，其中它有效地处理了特定任务约束。最后一个场景展示了考虑整个机器人运动链碰撞的可能性。代码可在https://github.com/thieso/boundmpc获取，鼓励您探索其潜力并将其适应到您的特定机器人任务。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Selection as Power: Bounding Decision Authority in Autonomous Agents</div>
<div class="meta-line">Authors: Jose Manuel de la Chica Rodriguez, Juan Manuel Vera Díaz</div>
<div class="meta-line">First: 2026-02-16T10:10:47+00:00 · Latest: 2026-02-16T10:10:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14606v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14606v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous agentic systems are increasingly deployed in regulated, high-stakes domains where decisions may be irreversible and institutionally constrained. Existing safety approaches emphasize alignment, interpretability, or action-level filtering. We argue that these mechanisms are necessary but insufficient because they do not directly govern selection power: the authority to determine which options are generated, surfaced, and framed for decision. We propose a governance architecture that separates cognition, selection, and action into distinct domains and models autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained, while selection and action autonomy are bounded through mechanically enforced primitives operating outside the agent&#x27;s optimization space. The architecture integrates external candidate generation (CEFL), a governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers. We evaluate the system across multiple regulated financial scenarios under adversarial stress targeting variance manipulation, threshold gaming, framing skew, ordering effects, and entropy probing. Metrics quantify selection concentration, narrative diversity, governance activation cost, and failure visibility. Results show that mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines. This work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向权力的选择：约束自主代理的决策权</div>
<div class="mono" style="margin-top:8px">自主代理系统在监管严格、高风险的领域中越来越被部署，其中的决策可能是不可逆的，并受到机构的限制。现有的安全方法强调对齐、可解释性或行为级过滤。我们认为，这些机制是必要的，但不足够，因为它们没有直接管理选择权力：决定哪些选项被生成、呈现和框架化以供决策的权力。我们提出了一种治理架构，将认知、选择和行动分离到不同的领域，并将自主性建模为主权向量。认知自主性保持不受限制，而选择和行动自主性则通过机械执行的原语在代理的优化空间之外进行约束。该架构结合了外部候选生成（CEFL）、受治理的约简器、提交揭示熵隔离、理由验证和失败大声电路断路器。我们在多个受监管的金融场景下评估了该系统，这些场景在对抗性压力下针对方差操纵、阈值游戏、框架偏差、排序效应和熵探测。度量标准量化了选择集中度、叙述多样性、治理激活成本和失败可见性。结果表明，机械选择治理是可实现的、可审计的，并防止了确定性结果的捕获，同时保留了推理能力。尽管概率集中仍然存在，但该架构在相对于传统标量管道的自主性方面是可测量的限制。这项工作将治理重新定义为有界因果权力，而不是内部意图对齐，为在不可接受的沉默失败领域部署自主代理提供了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for governing decision-making authority in autonomous agents deployed in high-stakes domains. It proposes a governance architecture that separates cognition, selection, and action, with selection power bounded through mechanical primitives. The system is evaluated in financial scenarios, showing that it is implementable, auditable, and effective in preventing deterministic outcome capture while maintaining reasoning capacity.</div>
<div class="mono" style="margin-top:8px">论文针对高风险领域中部署的自主代理系统提出了治理需求。它提出了一种治理架构，将认知、选择和行动分离，并通过机械原语限制选择权力。该系统在金融场景中进行了评估，结果显示它是可实现、可审计的，并且能够有效防止确定性结果的捕获，同时保留推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models</div>
<div class="meta-line">Authors: Liangzhi Shi, Shuaihang Chen, Feng Gao, Yinuo Chen, Kang Chen, Tonghe Zhang, Hongzhi Zang, Weinan Zhang, Chao Yu, Yu Wang</div>
<div class="meta-line">First: 2026-02-13T05:15:50+00:00 · Latest: 2026-02-16T05:44:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12628v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12628v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越模仿：基于强化学习的仿真实际协同训练方法用于VLA模型</div>
<div class="mono" style="margin-top:8px">仿真实验提供了一种可扩展且低成本的方法来丰富视觉-语言-动作（VLA）训练，减少了对昂贵的真实机器人演示的依赖。然而，大多数仿真实际协同训练方法依赖于监督微调（SFT），将仿真视为静态的演示来源，并未充分利用大规模闭环交互。因此，实际世界中的收益和泛化能力往往受到限制。在本文中，我们提出了一种基于\underline{\textit{RL}}的仿真实际协同训练\modify{(RL-Co)}框架，该框架利用交互仿真同时保留实际世界的能力。我们的方法遵循通用的两阶段设计：首先使用SFT对真实和仿真演示的混合数据进行初始化，然后在仿真中使用强化学习进行微调，同时在实际数据上添加辅助的监督损失以锚定策略并减轻灾难性遗忘。我们在四种实际桌面上的操纵任务上使用两种代表性的VLA架构OpenVLA和$π_{0.5}$评估了该框架，观察到与仅使用实际数据微调和基于SFT的协同训练相比的一致改进，包括OpenVLA的成功率提高了24%，$π_{0.5}$提高了20%。除了更高的成功率，强化学习协同训练还提供了更强的对未见过的任务变体的泛化能力和显著提高的实际数据效率，为利用仿真增强实际机器人部署提供了一种实用且可扩展的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current sim-real co-training methods by proposing an RL-based framework that enhances real-world performance. The method combines supervised fine-tuning on a mixture of real and simulated data with reinforcement learning in simulation, while also incorporating an auxiliary supervised loss on real-world data to prevent catastrophic forgetting. Evaluations on four tabletop manipulation tasks show consistent improvements over real-only fine-tuning and SFT-based co-training, with success rates increasing by 24% and 20% on OpenVLA and $π_{0.5}$, respectively. The framework also demonstrates better generalization and improved data efficiency in real-world settings.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于RL的co-training框架，结合了交互式仿真与现实世界的能力，解决了当前sim-real co-training方法的局限性。该方法采用两阶段设计：初始使用监督微调在真实和模拟数据混合集上进行预热，随后在仿真中使用强化学习，并在真实数据上添加辅助监督损失以防止灾难性遗忘。在四个现实世界的桌面操作任务上的评估表明，与仅使用真实数据微调和基于SFT的co-training相比，该方法在OpenVLA上实现了24%的成功率提升，在$π_{0.5}$上实现了20%的提升，同时还具有更好的泛化能力和数据效率。</div>
</details>
</div>
<div class="card">
<div class="title">A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation</div>
<div class="meta-line">Authors: Steven Oh, Tomoya Takahashi, Cristian C. Beltran-Hernandez, Yuki Kuroda, Masashi Hamaya</div>
<div class="meta-line">First: 2026-02-16T03:45:04+00:00 · Latest: 2026-02-16T03:45:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14434v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14434v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://project-page-manager.github.io/CLAW/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有各向异性可选刚度的柔软手腕用于接触丰富操作中的鲁棒机器人学习</div>
<div class="mono" style="margin-top:8px">在未结构化环境中的接触丰富操作任务对机器人学习提出了显著的鲁棒性挑战，其中意外碰撞可能导致损坏并妨碍策略获取。现有软末端执行器面临根本性限制：它们要么提供有限的变形范围，要么缺乏方向刚度控制，要么需要复杂的执行系统，这会损害其实用性。本研究引入了CLAW（顺应性叶片弹簧各向异性柔软手腕），这是一种通过使用两个正交叶片弹簧和具有锁定机制的旋转关节的简单而有效的设计来解决这些限制的新型软手腕机制。CLAW 提供了6个自由度的大范围变形（40毫米横向，20毫米垂直），具有可调的各向异性刚度，可以在三种不同的模式下调节，同时保持轻巧的结构（330克）并具有较低的成本（550美元）。使用模仿学习的实验评估表明，CLAW 在基准的圆柱插入任务中的成功率达到了76%，超过了Fin Ray 夹爪（43%）和刚性夹爪替代品（36%）。CLAW 成功处理了各种接触丰富的场景，包括具有严格公差的精密装配和精细物体操作，展示了其在接触丰富领域实现鲁棒机器人学习的潜力。项目页面：https://project-page-manager.github.io/CLAW/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the robustness challenges in robot learning for contact-rich manipulation tasks by introducing CLAW, a novel soft wrist mechanism. CLAW uses two orthogonal leaf springs and rotary joints with a locking mechanism to provide large 6-degree-of-freedom deformation, anisotropic stiffness tunable across three modes, and lightweight construction. Experimental results show that CLAW achieves a 76% success rate in peg-insertion tasks, outperforming both the Fin Ray gripper and rigid gripper alternatives. It successfully handles diverse contact-rich scenarios, indicating its potential for robust robot learning.</div>
<div class="mono" style="margin-top:8px">该研究通过引入CLAW，一种新颖的软腕机制，解决了接触丰富的操作任务中机器人学习的鲁棒性挑战。CLAW 结合了两个正交的叶片弹簧和旋转关节以及锁定机制，提供了大范围的6自由度变形、可调的各向异性刚度以及轻巧的结构。实验结果显示，CLAW 在 peg 插入任务中的成功率达到了76%，优于 Fin Ray 夹爪和刚性夹爪的替代方案。它能够处理多种接触丰富的场景，表明其在接触丰富的领域中具有实现鲁棒机器人学习的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">EigenSafe: A Spectral Framework for Learning-Based Probabilistic Safety Assessment</div>
<div class="meta-line">Authors: Inkyu Jang, Jonghae Park, Sihyun Cho, Chams E. Mballo, Claire J. Tomlin, H. Jin Kim</div>
<div class="meta-line">First: 2025-09-22T13:12:13+00:00 · Latest: 2026-02-16T02:48:48+00:00</div>
<div class="meta-line">Comments: Inkyu Jang and Jonghae Park contributed equally to this work. Project Webpage: https://eigen-safe.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17750v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.17750v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eigen-safe.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present EigenSafe, an operator-theoretic framework for safety assessment of learning-enabled stochastic systems. In many robotic applications, the dynamics are inherently stochastic due to factors such as sensing noise and environmental disturbances, and it is challenging for conventional methods such as Hamilton-Jacobi reachability and control barrier functions to provide a well-calibrated safety critic that is tied to the actual safety probability. We derive a linear operator that governs the dynamic programming principle for safety probability, and find that its dominant eigenpair provides critical safety information for both individual state-action pairs and the overall closed-loop system. The proposed framework learns this dominant eigenpair, which can be used to either inform or constrain policy updates. We demonstrate that the learned eigenpair effectively facilitates safe reinforcement learning. Further, we validate its applicability in enhancing the safety of learned policies from imitation learning through robot manipulation experiments using a UR3 robotic arm in a food preparation task.</div></details>
</div>
<div class="card">
<div class="title">AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation</div>
<div class="meta-line">Authors: Morgan Byrd, Donghoon Baek, Kartik Garg, Hyunyoung Jung, Daesol Cho, Maks Sorokin, Robert Wright, Sehoon Ha</div>
<div class="meta-line">First: 2026-02-16T00:29:53+00:00 · Latest: 2026-02-16T00:29:53+00:00</div>
<div class="meta-line">Comments: Website: https://morganbyrd03.github.io/adaptmanip/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://morganbyrd03.github.io/adaptmanip/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaptManip：基于在线递归状态估计的自适应全身物体拾取与输送学习</div>
<div class="mono" style="margin-top:8px">本文介绍了自适应全身操作框架AdaptManip，该框架使类人机器人能够自主执行集成导航、物体拾取和输送任务。与依赖人类演示且对干扰敏感的先前模仿学习方法不同，AdaptManip旨在通过强化学习训练一个鲁棒的操作策略，无需人类演示或远程操作数据。所提出的框架由三个耦合组件组成：（1）一个递归物体状态估计器，能够在有限视野和遮挡下实时跟踪操作中的物体；（2）一个全身基础策略，用于稳健的移动，以及残差操作控制，以实现稳定的物体拾取和输送；（3）一个基于LiDAR的机器人全局位置估计器，提供抗漂移定位。所有组件均在仿真中使用强化学习训练，并以零样本方式部署在实际硬件上。实验结果表明，AdaptManip在适应性和总体成功率方面显著优于基线方法，包括基于模仿学习的方法，即使在遮挡下，准确的物体状态估计也能提高操作性能。我们进一步展示了类人机器人上的完全自主的现实世界导航、物体拾取和输送。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AdaptManip is a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. It uses reinforcement learning to train a robust policy without human demonstrations or teleoperation data. The framework includes a real-time object state estimator, a whole-body base policy for stable manipulation, and a drift-robust localization system. Experimental results show that AdaptManip outperforms imitation learning-based approaches in adaptability and success rate, and accurate object state estimation enhances manipulation performance even under occlusions.</div>
<div class="mono" style="margin-top:8px">AdaptManip 是一个全自主框架，用于人形机器人进行集成导航、物体拾取和交付。它使用强化学习来训练一个鲁棒的策略，无需人类演示或远程操作数据。该框架包括一个实时物体状态估计器、一个用于稳定操作的全身基础策略，以及一个基于 LiDAR 的机器人位置估计器。实验结果表明，AdaptManip 在适应性和成功率方面优于基线方法，并且准确的物体状态估计即使在遮挡下也能提高操作性能。</div>
</details>
</div>
<div class="card">
<div class="title">GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints</div>
<div class="meta-line">Authors: Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li</div>
<div class="meta-line">First: 2026-01-23T17:13:54+00:00 · Latest: 2026-02-15T23:19:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16905v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16905v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#x27;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#x27;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GRIP：面向混合专家架构的算法无关机器遗忘方法及其几何路由器约束</div>
<div class="mono" style="margin-top:8px">大型语言模型的机器遗忘（MU）对于AI安全至关重要，但现有方法无法适用于混合专家架构（MoE）。我们发现传统遗忘方法利用了MoE的架构弱点：它们通过操纵路由器将查询重定向到知识较少的专家，而不是直接删除知识，导致模型功能丧失和表面遗忘。我们提出了几何路由不变性保持（GRIP），这是一种面向MoE的算法无关遗忘框架。我们的核心贡献是一种几何约束，通过将路由器梯度更新投影到专家特定的零空间中实现。关键的是，这种做法将路由稳定性与参数刚性分离开来：对于保留的知识，离散的专家选择保持稳定，而连续的路由器参数在零空间内保持可塑性，允许模型进行必要的内部重构以满足遗忘目标。这迫使遗忘优化直接从专家参数中删除知识，而不是利用路由器操纵的表面捷径。GRIP作为适配器，约束路由器参数更新而不修改底层遗忘算法。大规模MoE模型的广泛实验表明，我们的适配器在所有测试的遗忘方法中实现了超过95%的路由稳定性，同时保持其功能。通过阻止现有算法利用MoE模型的路由器弱点，GRIP将现有的遗忘研究从密集架构扩展到MoE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of machine unlearning (MU) for Mixture-of-Experts (MoE) architectures, where existing methods fail to generalize effectively. The proposed method, Geometric Routing Invariance Preservation (GRIP), introduces a geometric constraint that projects router gradient updates into an expert-specific null-space, decoupling routing stability from parameter rigidity. This allows the model to reconfigure internally to satisfy unlearning objectives, directly erasing knowledge from expert parameters rather than manipulating routers. Experiments show that GRIP maintains over 95% routing stability while preserving model utility across various unlearning methods.</div>
<div class="mono" style="margin-top:8px">研究解决了Mixture-of-Experts（MoE）架构中机器遗忘的挑战，传统的遗忘方法通过操纵路由器来重新定向查询，导致表面性的遗忘和模型性能的损失。提出的Geometric Routing Invariance Preservation（GRIP）框架引入了一个几何约束，将路由器梯度更新投影到专家特定的零空间，解耦了路由稳定性和参数刚性。这使得模型能够内部重新配置以满足遗忘目标，直接从专家参数中删除知识。实验表明，GRIP在各种遗忘方法下保持了超过95%的路由稳定性，同时保留了模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing</div>
<div class="meta-line">Authors: Alex Clinton, Thomas Zeng, Yiding Chen, Xiaojin Zhu, Kirthevasan Kandasamy</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-08T20:14:48+00:00 · Latest: 2026-02-15T22:05:10+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07272v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07272v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern data marketplaces and data sharing consortia increasingly rely on incentive mechanisms to encourage agents to contribute data. However, schemes that reward agents based on the quantity of submitted data are vulnerable to manipulation, as agents may submit fabricated or low-quality data to inflate their rewards. Prior work has proposed comparing each agent&#x27;s data against others&#x27; to promote honesty: when others contribute genuine data, the best way to minimize discrepancy is to do the same. Yet prior implementations of this idea rely on very strong assumptions about the data distribution (e.g. Gaussian), limiting their applicability. In this work, we develop reward mechanisms based on a novel, two-sample test inspired by the Cramér-von Mises statistic. Our methods strictly incentivize agents to submit more genuine data, while disincentivizing data fabrication and other types of untruthful reporting. We establish that truthful reporting constitutes a (possibly approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We theoretically instantiate our method in three canonical data sharing problems and show that it relaxes key assumptions made by prior work. Empirically, we demonstrate that our mechanism incentivizes truthful data sharing via simulations and on real-world language and image data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cramér-von Mises方法激励真实数据共享</div>
<div class="mono" style="margin-top:8px">现代数据市场和数据共享联盟越来越多地依赖激励机制来鼓励各方贡献数据。然而，基于提交数据量的奖励方案容易受到操控，因为各方可能会提交伪造或低质量的数据以增加奖励。先前的工作提出了将各方的数据与其他人的数据进行比较以促进诚实：当其他人贡献真实数据时，最小化差异的最佳方式是也提交真实数据。然而，先前实现这一想法的方法依赖于非常强的数据分布假设（例如，高斯分布），限制了其适用性。在本研究中，我们基于Cramér-von Mises统计量启发的一种新颖的两样本检验开发了奖励机制。我们的方法严格激励各方提交更多真实数据，同时抑制数据伪造和其他类型的不实报告。我们证明了在贝叶斯和先验无假设的环境中，真实报告构成一个（可能是近似的）纳什均衡。我们理论地在三个典型的数据共享问题中实例化了我们的方法，并表明它放松了先前工作的关键假设。通过模拟和现实世界语言和图像数据，我们实验证明了我们的机制激励真实数据共享。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of data manipulation in data marketplaces by developing a reward mechanism based on the Cramér-von Mises statistic. The method incentivizes agents to submit genuine data and discourages fabrication. Theoretical analysis shows that truthful reporting is a Nash equilibrium, and empirical results from simulations and real-world data support the effectiveness of the proposed mechanism in promoting truthful data sharing.</div>
<div class="mono" style="margin-top:8px">本文通过开发基于Cramér-von Mises统计量启发的新型两样本检验的奖励机制，解决了数据市场中数据真实共享的问题。该方法严格激励数据提供者提交真实数据，并抑制数据伪造。理论分析表明，在贝叶斯和先验无假设的设置下，真实报告构成纳什均衡。从模拟和真实世界数据的实验证明，该机制有效地促进了真实数据的共享。</div>
</details>
</div>
<div class="card">
<div class="title">Muscle Coactivation in the Sky: Geometry and Pareto Optimality of Energy vs. Promptness in Multirotors</div>
<div class="meta-line">Authors: Antonio Franchi</div>
<div class="meta-line">First: 2026-02-15T16:35:46+00:00 · Latest: 2026-02-15T16:35:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14222v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14222v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics and human biomechanics, the tension between energy economy and kinematic readiness is well recognized; this work brings that fundamental principle to aerial multirotors. We show that the limited torque of the motors and the nonlinear aerodynamic map from rotor speed to thrust naturally give rise to the novel concept of promptness-a metric akin to dynamic aerodynamic manipulability. By treating energy consumption as a competing objective and introducing a geometric fiber-bundle formulation, we turn redundancy resolution into a principled multi-objective program on affine fibers. The use of the diffeomorphic transformation linearizing the signed-quadratic propulsion model allows us to lay the foundations for a rigorous study of the interplay between these costs. Through an illustrative case study on 4-DoF allocation on the hexarotor, we reveal that this interplay is fiber-dependent and physically shaped by hardware inequalities. For unidirectional thrusters, the feasible fibers are compact, yielding interior allocations and a short Pareto arc, while torque demands break symmetry and separate the optima. Conversely, with reversible propellers, the null space enables antagonistic rotor co-contraction that drives promptness to hardware limits, making optimal endurance and agility fundamentally incompatible in those regimes. Ultimately, rather than relying on heuristic tuning or black box algorithms to empirically improve task execution, this framework provides a foundational understanding of why and how to achieve agility through geometry-aware control allocation, offering possible guidance for vehicle design, certification metrics, and threat-aware flight operation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>天空中的肌肉共激活：多旋翼器能量与及时性之间的几何与帕累托最优性</div>
<div class="mono" style="margin-top:8px">在机器人学和人体生物力学中，能量经济与运动准备之间的张力已被广泛认识；本研究将这一基本原理引入空中多旋翼器。我们展示了电机的有限扭矩和从旋翼速度到升力的非线性气动映射自然地产生了新的及时性概念——类似于动态气动可操作性的度量标准。通过将能量消耗视为竞争目标并引入几何纤维束形式，我们将冗余度解决转化为在仿射纤维上的原则性多目标程序。使用使符号二次推进模型线性化的微分同胚变换，我们为这些成本之间的相互作用奠定了严谨研究的基础。通过六旋翼机4-自由度分配的示例研究，我们揭示了这种相互作用是纤维依赖的，并且由硬件不平等性物理塑造。对于单向喷气器，可行纤维是紧凑的，产生内部分配和短的帕累托弧，而扭矩需求打破对称性并分离最优值。相反，对于可逆螺旋桨，零空间使对抗性旋翼共收缩驱动及时性达到硬件极限，使得在这些状态下最优耐久性和敏捷性根本不可兼得。最终，本框架不仅依赖于启发式调优或黑盒算法来经验性地提高任务执行，还提供了通过几何感知控制分配实现敏捷性的基础理解，为飞行器设计、认证指标和威胁感知飞行操作提供了可能的指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work explores the trade-off between energy efficiency and kinematic readiness in multirotor drones, introducing a new concept called promptness. By formulating redundancy resolution as a multi-objective optimization problem and using geometric and diffeomorphic methods, the study reveals that the interplay between energy consumption and promptness is fiber-dependent and influenced by hardware constraints. The research finds that for unidirectional thrusters, optimal allocations are interior and promptness is limited, while reversible propellers enable antagonistic co-contraction, pushing promptness to hardware limits and making agility and endurance incompatible.</div>
<div class="mono" style="margin-top:8px">该研究探讨了多旋翼在能量效率和响应速度之间的权衡，灵感来源于机器人学和人体生物力学的原则。通过将冗余分辨率公式化为多目标优化问题，并引入响应速度的概念，作者揭示了这些成本之间的相互作用取决于纤维，并受到硬件不等性的影响。对于单向推进器，可行的纤维是紧凑的，导致内部分配和短的帕累托弧，而扭矩需求则分离了最优值。对于可逆螺旋桨，空空间使对抗性旋翼的协同收缩成为可能，将响应速度推至硬件极限，使得在这些情况下最优的耐久性和敏捷性是不兼容的。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Neural Network-Assisted Design Optimization of Soft Fin-Ray Fingers for Enhanced Grasping Performance</div>
<div class="meta-line">Authors: Ali Ghanizadeh, Ali Ahmadi, Arash Bahrami</div>
<div class="meta-line">First: 2025-05-31T10:16:58+00:00 · Latest: 2026-02-15T15:56:03+00:00</div>
<div class="meta-line">Comments: Major revision correcting errors that affected the reported results. Corresponding results were regenerated, and the manuscript was updated accordingly</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00494v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.00494v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The internal structure of the Fin-Ray fingers plays a significant role in their adaptability and grasping performance. However, modeling the grasp force and deformation behavior for design purposes is challenging. When the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two gives rise to a multi-objective optimization problem. We employ the finite element method to estimate the deflections and contact forces of the Fin-Ray fingers grasping cylindrical objects, generating a dataset of 120 simulations. This dataset includes three input variables: the thickness of the front and support beams, the thickness of the crossbeams, and the equal spacing between the crossbeams, which are the design variables in the optimization. This dataset is then used to construct a multilayer perceptron (MLP) with four output neurons predicting the contact force and tip displacement in two directions. The magnitudes of maximum contact force and maximum tip displacement are two optimization objectives, showing the trade-off between force and delicate manipulation. The set of solutions is found using the non-dominated sorting genetic algorithm (NSGA-II). The results of the simulations demonstrate that the proposed methodology can be used to improve the design and grasping performance of soft grippers, aiding to choose a design not only for delicate grasping but also for high-force applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多目标神经网络辅助设计优化软鳍刺手指以增强抓取性能</div>
<div class="mono" style="margin-top:8px">鳍刺手指的内部结构对其适应性和抓取性能起着重要作用。然而，为了设计目的建模抓取力和变形行为具有挑战性。当鳍刺手指变得更刚硬并能够施加更大的力时，它在处理物体时变得不够精细。这种对比导致了一个多目标优化问题。我们使用有限元方法来估算鳍刺手指抓取圆柱形物体时的挠度和接触力，生成了120个模拟的数据集。该数据集包括三个输入变量：前梁和支撑梁的厚度、横梁的厚度以及横梁之间的等间距，这些是优化中的设计变量。然后使用这些数据集构建了一个具有四个输出神经元的多层感知器（MLP），预测两个方向上的接触力和尖端位移。最大接触力的大小和最大尖端位移的大小是两个优化目标，显示了力和精细操作之间的权衡。使用非支配排序遗传算法（NSGA-II）找到一组解。模拟结果表明，所提出的方法可以用于改进软夹爪的设计和抓取性能，有助于选择不仅适用于精细抓取，也适用于高力应用的设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of optimizing the design of Fin-Ray fingers for enhanced grasping performance by modeling their grasp force and deformation behavior. Using the finite element method, a dataset of 120 simulations was generated, which included three design variables: the thickness of the front and support beams, the thickness of the crossbeams, and the spacing between the crossbeams. A multilayer perceptron was trained to predict the contact force and tip displacement, and the non-dominated sorting genetic algorithm (NSGA-II) was employed to find a set of solutions that balance force and delicate manipulation. The results show that the proposed methodology can improve the design of soft grippers for both delicate and high-force applications.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过平衡力和精细操作来优化Fin-Ray手指的设计以提升抓取性能。研究人员使用有限元分析模拟了120种场景，捕捉了梁厚和间距的影响。然后应用多层感知器来预测接触力和位移，并使用NSGA-II优化最大接触力和尖端位移。结果表明，所提出的方法可以有效改进软夹爪的设计，适用于高力和精细抓取任务。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation</div>
<div class="meta-line">Authors: Yue Chen, Muqing Jiang, Kaifeng Zheng, Jiaqi Liang, Chenrui Tie, Haoran Lu, Ruihai Wu, Hao Dong</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-15T15:39:05+00:00 · Latest: 2026-02-15T15:39:05+00:00</div>
<div class="meta-line">Comments: Accept to ICLR 2026, Project page: https://pa3ff.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14193v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14193v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pa3ff.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Articulated object manipulation is essential for various real-world robotic tasks, yet generalizing across diverse objects remains a major challenge. A key to generalization lies in understanding functional parts (e.g., door handles and knobs), which indicate where and how to manipulate across diverse object categories and shapes. Previous works attempted to achieve generalization by introducing foundation features, while these features are mostly 2D-based and do not specifically consider functional parts. When lifting these 2D features to geometry-profound 3D space, challenges arise, such as long runtimes, multi-view inconsistencies, and low spatial resolution with insufficient geometric information. To address these issues, we propose Part-Aware 3D Feature Field (PA3FF), a novel dense 3D feature with part awareness for generalizable articulated object manipulation. PA3FF is trained by 3D part proposals from a large-scale labeled dataset, via a contrastive learning formulation. Given point clouds as input, PA3FF predicts a continuous 3D feature field in a feedforward manner, where the distance between point features reflects the proximity of functional parts: points with similar features are more likely to belong to the same part. Building on this feature, we introduce the Part-Aware Diffusion Policy (PADP), an imitation learning framework aimed at enhancing sample efficiency and generalization for robotic manipulation. We evaluate PADP on several simulated and real-world tasks, demonstrating that PA3FF consistently outperforms a range of 2D and 3D representations in manipulation scenarios, including CLIP, DINOv2, and Grounded-SAM. Beyond imitation learning, PA3FF enables diverse downstream methods, including correspondence learning and segmentation tasks, making it a versatile foundation for robotic manipulation. Project page: https://pa3ff.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习部分感知的密集3D特征场以实现通用的 articulated 物体操作</div>
<div class="mono" style="margin-top:8px">articulated 物体操作对于各种实际机器人任务至关重要，但在不同物体之间实现通用性仍然是一个主要挑战。通用性的关键在于理解功能部分（例如门把手和旋钮），这表明在不同物体类别和形状中如何进行操作。以往的工作试图通过引入基础特征来实现通用性，但这些特征主要是2D基础的，并未特别考虑功能部分。当将这些2D特征提升到几何丰富的3D空间时，会出现诸如长时间运行、多视图不一致性和低空间分辨率等问题，缺乏足够的几何信息。为了解决这些问题，我们提出了部分感知3D特征场（PA3FF），这是一种具有部分感知能力的新型密集3D特征，用于实现通用的articulated物体操作。PA3FF通过大规模标注数据集中的3D部分提案进行训练，采用对比学习形式。给定点云作为输入，PA3FF以前馈方式预测一个连续的3D特征场，其中点特征之间的距离反映了功能部分的接近程度：具有相似特征的点更有可能属于同一部分。在此基础上，我们引入了部分感知扩散策略（PADP），这是一种旨在提高样本效率和机器人操作通用性的模仿学习框架。我们在多个模拟和实际任务中评估了PADP，结果显示PA3FF在操作场景中始终优于CLIP、DINOv2和Grounded-SAM等多种2D和3D表示。除了模仿学习，PA3FF还使多种下游方法成为可能，包括对应学习和分割任务，使其成为机器人操作的多功能基础。项目页面：https://pa3ff.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the generalization of articulated object manipulation in robotics by focusing on functional parts. The method involves developing Part-Aware 3D Feature Field (PA3FF), which is trained using 3D part proposals from a large dataset and predicts a dense 3D feature field. PA3FF enhances sample efficiency and generalization through the Part-Aware Diffusion Policy (PADP). Experiments show that PA3FF outperforms 2D and 3D representations in various manipulation tasks, including correspondence learning and segmentation. This work provides a versatile foundation for robotic manipulation.</div>
<div class="mono" style="margin-top:8px">研究旨在通过关注功能部件来提高机器人对 articulated 物体操作的泛化能力。方法是开发 Part-Aware 3D Feature Field (PA3FF)，该方法通过大型数据集中的 3D 部分提案进行训练，并预测一个密集的 3D 特征场。PA3FF 通过 Part-Aware Diffusion Policy (PADP) 提高了样本效率和泛化能力。实验表明，PA3FF 在各种操作任务中，包括对应学习和分割任务，优于 2D 和 3D 表示。这项工作为机器人操作提供了一个多功能的基础。</div>
</details>
</div>
<div class="card">
<div class="title">LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer</div>
<div class="meta-line">Authors: Lihan Zha, Asher J. Hancock, Mingtong Zhang, Tenny Yin, Yixuan Huang, Dhruv Shah, Allen Z. Ren, Anirudha Majumdar</div>
<div class="meta-line">First: 2026-02-11T06:09:11+00:00 · Latest: 2026-02-15T15:32:16+00:00</div>
<div class="meta-line">Comments: Project website: https://lap-vla.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10556v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10556v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lap-vla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model&#x27;s input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAP：语言-动作预训练使零样本跨体态迁移成为可能</div>
<div class="mono" style="margin-top:8px">机器人领域的一个长期目标是能够零样本部署在新机器人体态上的通用策略，无需针对每个体态进行适应。尽管进行了大规模的多体态预训练，现有的视觉-语言-动作模型（VLAs）仍然紧密耦合于其训练体态，并且通常需要昂贵的微调。我们引入了语言-动作预训练（LAP），这是一种简单的配方，直接将低级机器人动作表示为自然语言，使动作监督与预训练的视觉-语言模型的输入-输出分布相一致。LAP 不需要学习分词器，不需要昂贵的标注，也不需要针对特定体态的架构设计。基于 LAP，我们提出了 LAP-3B，据我们所知，这是第一个在无需任何体态特定微调的情况下实现显著零样本迁移至未见过的机器人体态的视觉-语言-动作模型。在多个新型机器人和操作任务上，LAP-3B 达到了超过 50% 的平均零样本成功率，相比之前最强的视觉-语言-动作模型，性能提升了约 2 倍。我们还展示了 LAP 能够实现高效的适应和有利的扩展，并且在共享的语言-动作格式中统一了动作预测和视觉问答，从而通过协同训练获得额外的增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a generalist policy for robotics that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. The method introduces Language-Action Pre-training (LAP), which represents low-level robot actions in natural language, aligning action supervision with the pre-trained vision-language model&#x27;s input-output distribution. LAP-3B, based on LAP, achieves substantial zero-shot transfer to previously unseen robot embodiments, with an average zero-shot success rate of over 50%, demonstrating a 2x improvement over previous Vision-Language-Action models.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种通用的机器人策略，能够在无需适应的情况下部署到新的机器人身体上。研究引入了语言-动作预训练（LAP），将机器人动作表示为自然语言，从而实现零样本迁移，无需微调。基于LAP的LAP-3B在多种机器人和任务上实现了超过50%的平均零样本成功率，性能比之前的模型提高了约两倍。该方法简化了动作表示，并统一了动作预测和VQA，通过协同训练带来了额外的收益。</div>
</details>
</div>
<div class="card">
<div class="title">Direction Matters: Learning Force Direction Enables Sim-to-Real Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Yifei Yang, Anzhe Chen, Zhenjie Zhu, Kechun Xu, Yunxuan Mao, Yufei Wei, Lu Chen, Rong Xiong, Yue Wang</div>
<div class="meta-line">First: 2026-02-15T14:57:13+00:00 · Latest: 2026-02-15T14:57:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yifei-y.github.io/project-pages/DirectionMatters/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer. Inspired by the success of privileged supervision in kinematic tasks, we employ a human-designed finite state machine based position/force controller in simulation to provide privileged guidance. The resulting policy is trained to predict the end-effector pose, contact state, and crucially the desired contact force direction. Unlike force magnitudes, which are highly sensitive to simulation inaccuracies, force directions encode high-level task geometry and remain robust across the sim-to-real gap. At deployment, these predictions configure a force-aware admittance controller. By combining the policy&#x27;s directional intent with a constant, low-cost manually tuned force magnitude, the system generates adaptive, task-aligned compliance. This tuning is lightweight, typically requiring only a single scalar per contact state. We provide theoretical analysis for stability and robustness to disturbances. Experiments on four real-world tasks, i.e., microwave opening, peg-in-hole, whiteboard wiping, and door opening, demonstrate that our approach significantly outperforms strong baselines in both success rate and robustness. Videos are available at: https://yifei-y.github.io/project-pages/DirectionMatters/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>方向很重要：学习力方向使模拟到现实的接触丰富操作成为可能</div>
<div class="mono" style="margin-top:8px">由于接触动力学固有的差异，接触丰富的操作的模拟到现实的转移仍然具有挑战性。尽管现有方法往往依赖于昂贵的现实世界数据或使用盲从性通过固定控制器，我们提出了一种框架，该框架利用了专家设计的控制器逻辑进行转移。受特权监督在运动学任务中成功的启发，我们在模拟中使用了基于位置/力的人工设计的有限状态机控制器来提供特权指导。由此产生的策略被训练以预测末端执行器姿态、接触状态以及关键的期望接触力方向。与高度敏感于模拟不准确性的力大小不同，力方向编码了高级任务几何结构，并且在模拟到现实的差距中保持稳健。在部署时，这些预测配置了一个力感知的阻抗控制器。通过结合策略的方向意图与一个恒定的、低成本的手动调优力大小，该系统生成了适应性强、任务对齐的从性。这种调优是轻量级的，通常只需要每个接触状态一个标量。我们提供了稳定性和对干扰的鲁棒性的理论分析。在四个真实世界任务上的实验，即微波炉开门、孔中插针、黑板擦洗和开门，表明我们的方法在成功率和鲁棒性方面显著优于强基线。有关视频请参见：https://yifei-y.github.io/project-pages/DirectionMatters/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of sim-to-real transfer for contact-rich manipulation by proposing a framework that uses a human-designed finite state machine-based position/force controller in simulation to provide privileged guidance. The policy is trained to predict end-effector pose, contact state, and the desired contact force direction, which remains robust across the sim-to-real gap. Experiments on four real-world tasks show that the approach significantly outperforms strong baselines in success rate and robustness.</div>
<div class="mono" style="margin-top:8px">该论文通过在模拟中使用基于人类设计的有限状态机的位置/力控制器来提供特权指导，以解决接触丰富的操纵从模拟到现实的转移挑战。该策略被训练以预测末端执行器姿态、接触状态以及所需的接触力方向，这些力方向在模拟到现实的差距中保持稳健。在四个真实世界的任务上的实验表明，该方法在成功率和鲁棒性方面显著优于强基线。</div>
</details>
</div>
<div class="card">
<div class="title">Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects</div>
<div class="meta-line">Authors: Xinan Rong, Changhuang Wan, Aochen He, Xiaolong Li, Gangshan Jing</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-02-15T11:40:39+00:00 · Latest: 2026-02-15T11:40:39+00:00</div>
<div class="meta-line">Comments: This paper has been accepted by IEEE Robotics and Automation Letters. The experimental video is avaialable at: https://www.youtube.com/watch?v=kcf9dVW0Dpo</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14104v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14104v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Precise in-hand manipulation of force-sensitive objects typically requires judicious coordinated force planning as well as accurate contact force feedback and control. Unlike multi-arm platforms with gripper end effectors, multi-fingered hands rely solely on fingertip point contacts and are not able to apply pull forces, therefore poses a more challenging problem. Furthermore, calibrated torque sensors are lacking in most commercial dexterous hands, adding to the difficulty. To address these challenges, we propose a dual-layer framework for multi-finger coordination, enabling high-precision manipulation of force-sensitive objects through joint control without tactile feedback. This approach solves coordinated contact force planning by incorporating graph rigidity and force closure constraints. By employing a force-to-position mapping, the planned force trajectory is converted to a joint trajectory. We validate the framework on a custom dexterous hand, demonstrating the capability to manipulate fragile objects-including a soft yarn, a plastic cup, and a raw egg-with high precision and safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于刚性约束的多指协调控制以实现力敏感物体的精确手中操作</div>
<div class="mono" style="margin-top:8px">精确的手中操作通常需要精细协调的力规划以及准确的接触力反馈和控制。与具有夹具末端执行器的多臂平台不同，多指手仅依赖指尖点接触，无法施加拉力，因此提出了更具挑战性的问题。此外，大多数商用灵巧手缺乏校准的扭矩传感器，增加了难度。为了解决这些挑战，我们提出了一种双层框架，通过关节控制而不依赖触觉反馈，实现力敏感物体的高精度操作。该方法通过结合图刚性和力闭合约束来解决协调接触力规划问题。通过采用力到位置的映射，计划的力轨迹被转换为关节轨迹。我们在一个定制的灵巧手中验证了该框架，展示了其能够以高精度和安全性操作易碎物体——包括柔软的纱线、塑料杯和生鸡蛋的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenges of precise in-hand manipulation of force-sensitive objects using multi-fingered hands. The method involves a dual-layer framework that incorporates graph rigidity and force closure constraints to plan coordinated contact forces. The planned force trajectory is then converted to a joint trajectory using a force-to-position mapping. Experiments on a custom dexterous hand show high-precision manipulation of fragile objects such as a soft yarn, a plastic cup, and a raw egg, proving the effectiveness of the approach.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决协调力规划和缺乏触觉反馈的问题，利用多指手精确操作力敏感物体。提出的双层框架结合了图刚性和力闭合约束来规划接触力并将其转换为关节轨迹。实验在一款定制的灵巧手中验证了该方法的有效性，展示了对柔软线、塑料杯和生鸡蛋等脆弱物体的高精度操作能力。</div>
</details>
</div>
<div class="card">
<div class="title">SemanticFeels: Semantic Labeling during In-Hand Manipulation</div>
<div class="meta-line">Authors: Anas Al Shikh Khalil, Haozhi Qi, Roberto Calandra</div>
<div class="meta-line">First: 2026-02-15T11:22:05+00:00 · Latest: 2026-02-15T11:22:05+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14099v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14099v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As robots become increasingly integrated into everyday tasks, their ability to perceive both the shape and properties of objects during in-hand manipulation becomes critical for adaptive and intelligent behavior. We present SemanticFeels, an extension of the NeuralFeels framework that integrates semantic labeling with neural implicit shape representation, from vision and touch. To illustrate its application, we focus on material classification: high-resolution Digit tactile readings are processed by a fine-tuned EfficientNet-B0 convolutional neural network (CNN) to generate local material predictions, which are then embedded into an augmented signed distance field (SDF) network that jointly predicts geometry and continuous material regions. Experimental results show that the system achieves a high correspondence between predicted and actual materials on both single- and multi-material objects, with an average matching accuracy of 79.87% across multiple manipulation trials on a multi-material object.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SemanticFeels：手持操作期间的语义标注</div>
<div class="mono" style="margin-top:8px">随着机器人越来越多地融入日常任务，它们在手持操作期间感知物体的形状和属性的能力变得至关重要，这对于适应性和智能行为至关重要。我们提出了SemanticFeels，这是一种扩展的NeuralFeels框架，将语义标注与神经隐式形状表示结合在一起，来自视觉和触觉。为了说明其应用，我们专注于材料分类：高分辨率的Digit触觉读数由微调的EfficientNet-B0卷积神经网络（CNN）处理，生成局部材料预测，然后嵌入到增强的符号距离场（SDF）网络中，该网络联合预测几何形状和连续材料区域。实验结果表明，该系统在单材料和多材料物体上预测的材料与实际材料之间具有高度的一致性，多材料物体在多次操作试验中的平均匹配准确率为79.87%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance a robot&#x27;s ability to perceive object properties during in-hand manipulation for adaptive behavior. SemanticFeels extends the NeuralFeels framework by integrating semantic labeling with neural implicit shape representation from vision and touch. The system uses a fine-tuned EfficientNet-B0 CNN to predict local material properties from high-resolution tactile data, which are then embedded into an augmented signed distance field network. The results show that the system achieves an average matching accuracy of 79.87% in predicting material properties across multiple manipulation trials on multi-material objects.</div>
<div class="mono" style="margin-top:8px">研究旨在提升机器人在手持操作过程中感知物体属性的能力，这对于实现适应性行为至关重要。SemanticFeels将语义标签与基于视觉和触觉数据的神经隐式形状表示相结合。它使用细调后的EfficientNet-B0 CNN从高分辨率的触觉读数中预测局部材料属性，然后将这些属性嵌入到增强的体素距离场网络中。该系统在对多材料物体进行多次操作试验时，材料分类的平均匹配准确率为79.87%。</div>
</details>
</div>
<div class="card">
<div class="title">DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation</div>
<div class="meta-line">Authors: Zixuan Chen, Junhui Yin, Yangtao Chen, Jing Huo, Pinzhuo Tian, Jieqi Shi, Yiwen Hou, Yinchuan Li, Yang Gao</div>
<div class="meta-line">First: 2025-05-01T13:52:19+00:00 · Latest: 2026-02-15T08:11:31+00:00</div>
<div class="meta-line">Comments: RAL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.00527v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.00527v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://deco226.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalizing language-conditioned multi-task imitation learning (IL) models to novel long-horizon 3D manipulation tasks is challenging. To address this, we propose DeCo (Task Decomposition and Skill Composition), a model-agnostic framework that enhances zero-shot generalization to compositional long-horizon manipulation tasks. DeCo decomposes IL demonstrations into modular atomic tasks based on gripper-object interactions, creating a dataset that enables models to learn reusable skills. At inference, DeCo uses a vision-language model (VLM) to parse high-level instructions, retrieve relevant skills, and dynamically schedule their execution. A spatially-aware skill-chaining module ensures smooth, collision-free transitions between skills. We introduce DeCoBench, a benchmark designed to evaluate compositional generalization in long-horizon manipulation tasks. DeCo improves the success rate of three IL models, RVT-2, 3DDA, and ARP, by 66.67%, 21.53%, and 57.92%, respectively, on 12 novel tasks. In real-world experiments, the DeCo-enhanced model, trained on only 6 atomic tasks, completes 9 novel tasks in zero-shot, with a 53.33% improvement over the baseline model. Project website: https://deco226.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeCo：长时3D操作任务分解与技能组合以实现零样本泛化</div>
<div class="mono" style="margin-top:8px">将基于语言的多任务模仿学习（IL）模型泛化到新的长时3D操作任务中具有挑战性。为了解决这个问题，我们提出了DeCo（任务分解与技能组合）模型，这是一种通用框架，能够增强对组合长时操作任务的零样本泛化能力。DeCo根据夹持器-物体交互将IL演示分解为模块化的原子任务，创建一个数据集，使模型能够学习可重用的技能。在推理时，DeCo使用视觉-语言模型（VLM）解析高级指令，检索相关技能并动态调度其执行。空间感知技能链模块确保技能之间的平滑、无碰撞过渡。我们引入了DeCoBench，一个用于评估长时操作任务中组合泛化能力的基准。DeCo分别将RVT-2、3DDA和ARP三种IL模型在12个新任务上的成功率提高了66.67%、21.53%和57.92%。在真实世界实验中，使用DeCo增强的模型仅在6个原子任务上进行训练，就能在零样本情况下完成9个新任务，比基线模型提高了53.33%。项目网站：https://deco226.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DeCo is a model-agnostic framework that decomposes long-horizon 3D manipulation tasks into modular atomic tasks to enhance zero-shot generalization. It uses a vision-language model to parse instructions and dynamically schedule skills, with a spatially-aware skill-chaining module ensuring smooth transitions. DeCo significantly improves the success rates of three IL models on 12 novel tasks, with up to 66.67% improvement, and achieves a 53.33% improvement in real-world zero-shot experiments with a model trained on only 6 atomic tasks.</div>
<div class="mono" style="margin-top:8px">DeCo 是一个框架，将长时3D操作任务分解为模块化的原子任务，以增强零样本泛化能力。它使用视觉语言模型解析指令并动态调度技能，具有空间感知的技能链模块确保平滑过渡。DeCo 在12个新型任务上显著提高了三种IL模型的成功率，最高可达66.67%的提升，并在真实世界实验中展示了使用仅6个原子任务训练的模型，实现了53.33%的改进。</div>
</details>
</div>
<div class="card">
<div class="title">RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation</div>
<div class="meta-line">Authors: Xinhua Wang, Kun Wu, Zhen Zhao, Hu Cao, Yinuo Zhao, Zhiyuan Xu, Meng Li, Shichao Fan, Di Wu, Yixue Zhang, Ning Liu, Zhengping Che, Jian Tang</div>
<div class="meta-line">First: 2026-02-15T07:40:00+00:00 · Latest: 2026-02-15T07:40:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14032v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14032v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://x-roboaug.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enhancing the generalization capability of robotic learning to enable robots to operate effectively in diverse, unseen scenes is a fundamental and challenging problem. Existing approaches often depend on pretraining with large-scale data collection, which is labor-intensive and time-consuming, or on semantic data augmentation techniques that necessitate an impractical assumption of flawless upstream object detection in real-world scenarios. In this work, we propose RoboAug, a novel generative data augmentation framework that significantly minimizes the reliance on large-scale pretraining and the perfect visual recognition assumption by requiring only the bounding box annotation of a single image during training. Leveraging this minimal information, RoboAug employs pre-trained generative models for precise semantic data augmentation and integrates a plug-and-play region-contrastive loss to help models focus on task-relevant regions, thereby improving generalization and boosting task success rates. We conduct extensive real-world experiments on three robots, namely UR-5e, AgileX, and Tien Kung 2.0, spanning over 35k rollouts. Empirical results demonstrate that RoboAug significantly outperforms state-of-the-art data augmentation baselines. Specifically, when evaluating generalization capabilities in unseen scenes featuring diverse combinations of backgrounds, distractors, and lighting conditions, our method achieves substantial gains over the baseline without augmentation. The success rates increase from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0. These results highlight the superior generalization and effectiveness of RoboAug in real-world manipulation tasks. Our project is available at https://x-roboaug.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboAug：通过区域对比数据增强实现从单个注释到数百个场景的机器人操作</div>
<div class="mono" style="margin-top:8px">增强机器人学习的一般化能力，使其能够在多种未见过的场景中有效操作是一个基本且具有挑战性的问题。现有方法通常依赖于大规模数据收集的预训练，这既耗时又费力，或者依赖于语义数据增强技术，这需要在实际场景中假设上游对象检测的完美性，这是不切实际的。在本工作中，我们提出了一种名为RoboAug的新型生成数据增强框架，该框架显著减少了对大规模预训练和完美视觉识别假设的依赖，仅在训练期间需要单张图像的边界框注释。利用这些最少的信息，RoboAug利用预训练生成模型进行精确的语义数据增强，并集成了一个即插即用的区域对比损失，帮助模型专注于任务相关区域，从而提高一般化能力和任务成功率。我们在三个机器人UR-5e、AgileX和Tien Kung 2.0上进行了广泛的现实世界实验，涵盖了超过35k次滚动。实验证明，RoboAug显著优于现有的数据增强基准。具体而言，在评估在不同背景、干扰物和光照条件下的未见过场景的一般化能力时，我们的方法在基准数据增强方法上取得了显著的提升。UR-5e的成功率从0.09提高到0.47，AgileX从0.16提高到0.60，Tien Kung 2.0从0.19提高到0.67。这些结果突显了RoboAug在现实世界操作任务中的一般化能力和有效性。我们的项目可在https://x-roboaug.github.io/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboAug is a generative data augmentation framework that requires only a single image&#x27;s bounding box annotation to significantly enhance the generalization capability of robotic learning. By integrating a region-contrastive loss, RoboAug improves task success rates in diverse, unseen scenes. Experiments on three robots—UR-5e, AgileX, and Tien Kung 2.0—show that RoboAug outperforms existing data augmentation methods, with success rates increasing from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0.</div>
<div class="mono" style="margin-top:8px">RoboAug 是一种生成式数据增强框架，仅需单张图像的边界框标注即可显著提升机器学习的泛化能力。通过集成区域对比损失，RoboAug 在多种未见过的场景中提高了任务成功率。实验结果显示，RoboAug 在 UR-5e、AgileX 和 Tien Kung 2.0 三款机器人上表现优于现有方法，成功率分别从 0.09 提升至 0.47，从 0.16 提升至 0.60，从 0.19 提升至 0.67。</div>
</details>
</div>
<div class="card">
<div class="title">GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System</div>
<div class="meta-line">Authors: Hung-Jui Huang, Mohammad Amin Mirzaee, Michael Kaess, Wenzhen Yuan</div>
<div class="meta-line">First: 2025-08-21T22:20:43+00:00 · Latest: 2026-02-15T05:06:05+00:00</div>
<div class="meta-line">Comments: 20 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.15990v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.15990v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://joehjhuang.github.io/gelslam">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately perceiving an object&#x27;s pose and shape is essential for precise grasping and manipulation. Compared to common vision-based methods, tactile sensing offers advantages in precision and immunity to occlusion when tracking and reconstructing objects in contact. This makes it particularly valuable for in-hand and other high-precision manipulation tasks. In this work, we present GelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to estimate object pose over long periods and reconstruct object shapes with high fidelity. Unlike traditional point cloud-based approaches, GelSLAM uses tactile-derived surface normals and curvatures for robust tracking and loop closure. It can track object motion in real time with low error and minimal drift, and reconstruct shapes with submillimeter accuracy, even for low-texture objects such as wooden tools. GelSLAM extends tactile sensing beyond local contact to enable global, long-horizon spatial perception, and we believe it will serve as a foundation for many precise manipulation tasks involving interaction with objects in hand. The video demo, code, and dataset are available at https://joehjhuang.github.io/gelslam.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GelSLAM：一种实时、高保真和鲁棒的3D触觉SLAM系统</div>
<div class="mono" style="margin-top:8px">准确感知物体的姿态和形状对于精确抓取和操作至关重要。与常见的基于视觉的方法相比，触觉传感在跟踪和重建接触中的物体时具有更高的精度和对遮挡的免疫性。这使其特别适用于手内和其他高精度操作任务。在本文中，我们提出了GelSLAM，这是一种依赖于触觉传感的实时3D SLAM系统，用于长时间估计物体姿态并以高保真度重建物体形状。与传统的基于点云的方法不同，GelSLAM 使用触觉衍生的表面法线和曲率进行鲁棒跟踪和闭环。它可以实时跟踪物体运动，误差低且漂移小，并且即使对于低纹理物体（如木制工具）也能以亚毫米级的精度重建形状。GelSLAM 将触觉传感扩展到局部接触之外，以实现全局、长视程的空间感知，我们认为它将成为许多涉及手部与物体交互的精确操作任务的基础。视频演示、代码和数据集可在 https://joehjhuang.github.io/gelslam/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GelSLAM is a real-time 3D SLAM system that uses tactile sensing to estimate object pose and reconstruct shapes with high fidelity, offering precision and immunity to occlusion. It employs tactile-derived surface normals and curvatures for robust tracking and loop closure, achieving submillimeter accuracy even for low-texture objects. GelSLAM can track object motion in real time with minimal drift and is designed for high-precision manipulation tasks. The system extends tactile sensing beyond local contact to enable global, long-horizon spatial perception, making it suitable for in-hand and other precise manipulation tasks.</div>
<div class="mono" style="margin-top:8px">GelSLAM 是一种实时 3D SLAM 系统，利用触觉传感精确跟踪物体姿态并重建物体形状，提供高精度和对遮挡的免疫。不同于传统方法，GelSLAM 使用触觉衍生的表面法线和曲率进行鲁棒跟踪和回环闭合，即使对于低纹理物体如木制工具也能实现亚毫米级精度。该系统可以实时跟踪物体运动，几乎没有漂移，适用于高精度操作任务。主要发现包括实时跟踪低误差和高保真度形状重建。</div>
</details>
</div>
<div class="card">
<div class="title">WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL</div>
<div class="meta-line">Authors: Zhennan Jiang, Shangqing Zhou, Yutong Jiang, Zefang Huang, Mingjie Wei, Yuhui Chen, Tianxing Zhou, Zhen Guo, Hao Lin, Quanlu Zhang, Yu Wang, Haoran Li, Chao Yu, Dongbin Zhao</div>
<div class="meta-line">First: 2026-02-15T03:48:20+00:00 · Latest: 2026-02-15T03:48:20+00:00</div>
<div class="meta-line">Comments: 21pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13977v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WoVR：世界模型作为可靠的后训练VLA策略模拟器与RL</div>
<div class="mono" style="margin-top:8px">强化学习（RL）为视觉-语言-行动（VLA）模型解锁了超越模仿学习的能力，但其对大规模现实世界交互的需求阻碍了直接部署在物理机器人上的应用。近期工作尝试使用学习到的世界模型作为策略优化的模拟器，然而闭环想象回放不可避免地遭受幻觉和长时间误差累积。这些误差不仅降低了视觉保真度，还破坏了优化信号，促使策略利用模型不准确之处而非真正的任务进展。我们提出了WoVR，一种基于世界模型的后训练RL框架，用于VLA策略。WoVR 不假设忠实的世界模型，而是明确调节RL与不完美想象动力学的交互方式。通过可控的动作条件化视频世界模型提高回放稳定性，通过关键帧初始化回放重塑想象交互以减少有效误差深度，并通过世界模型-策略共同进化保持策略-模拟器对齐。在LIBERO基准和真实世界机器人操作上的广泛实验表明，WoVR 使稳定长时间想象回放和有效的策略优化成为可能，将平均LIBERO成功率从39.95%提高到69.2%（+29.3分），真实机器人成功率从61.7%提高到91.7%（+30.0分）。这些结果表明，在幻觉被明确控制时，学习到的世界模型可以作为强化学习的实际模拟器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes WoVR, a framework for using world models in reinforcement learning for Vision-Language-Action policies, addressing the issue of hallucination in imagined rollouts. It introduces a controllable video world model, keyframe-initialized rollouts, and world model-policy co-evolution to stabilize rollouts and maintain policy-simulator alignment. Experiments show that WoVR improves success rates on LIBERO benchmarks and real-world robotic manipulation tasks, significantly enhancing policy optimization and task performance.</div>
<div class="mono" style="margin-top:8px">论文提出了WoVR框架，利用世界模型在视觉-语言-动作策略的强化学习中，解决想象回放中的幻觉问题。该框架采用可控的视频世界模型、关键帧初始化回放和世界模型-策略协同进化来稳定回放并改进策略优化。实验结果显示，WoVR在LIBERO基准测试和真实机器人操作任务中提高了成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance</div>
<div class="meta-line">Authors: Tzu-Hsien Lee, Fidan Mahmudova, Karthik Desingh</div>
<div class="meta-line">First: 2025-12-11T23:35:05+00:00 · Latest: 2026-02-15T00:23:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11173v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11173v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rpm-lab-umn.github.io/category-level-last-meter-nav/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving precise positioning of the mobile manipulator&#x27;s base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从单个实例的RGB演示中学习类别级别的最后一米导航</div>
<div class="mono" style="margin-top:8px">精确定位移动操作器基座对于后续成功的操作动作至关重要。大多数基于RGB的导航系统只能保证粗略的米级精度，使其不适合移动操作中的精确定位阶段。这一差距导致操作策略无法在其训练演示的分布内运行，导致频繁的执行失败。我们通过引入以对象为中心的模仿学习框架来解决这一差距，使四足移动操作器机器人仅使用其机载摄像头的RGB观察结果就能实现操作准备就绪的定位。我们的方法将导航策略条件化为三个输入：目标图像、机载摄像头的多视图RGB观察结果以及指定目标对象的文本提示。然后，一个语言驱动的分割模块和一个空间分数矩阵解码器提供显式的对象定位和相对姿态推理。使用类别内单个对象实例的真实世界数据，该系统在具有挑战性光照和背景条件的多种环境中泛化到未见过的对象实例。为了全面评估这一点，我们引入了两个指标：边缘对齐指标，使用真实方向；对象对齐指标，评估机器人如何面对目标。在这些指标下，当相对于未见过的目标对象定位时，我们的策略在边缘对齐方面取得了73.47%的成功率，在对象对齐方面取得了96.94%的成功率。这些结果表明，可以在类别级别实现最后一米的精确导航，无需深度、LiDAR或地图先验，从而为统一的移动操作提供可扩展的途径。项目页面：https://rpm-lab-umn.github.io/category-level-last-meter-nav/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the precision of mobile manipulator navigation to achieve manipulation-ready positioning, which is crucial for successful manipulation actions. The method introduces an object-centric imitation learning framework that uses RGB observations and a text prompt to navigate a quadruped robot to the target object. The system achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects in diverse environments. This demonstrates that precise last-meter navigation can be achieved without depth sensors, LiDAR, or map priors, facilitating scalable mobile manipulation.</div>
<div class="mono" style="margin-top:8px">研究旨在提高移动操作器的导航精度，以实现准备进行操作的定位，这对于成功执行操作至关重要。方法引入了一种基于对象的模仿学习框架，该框架使用RGB观察和文本提示来导航四足机器人到达目标物体。该系统在对未见过的目标物体进行定位时，在多种环境中的边缘对齐和物体对齐方面分别取得了73.47%和96.94%的成功率。这表明，无需深度传感器、LiDAR或地图先验，即可实现精确的最后一米导航，从而促进可扩展的移动操作。</div>
</details>
</div>
<div class="card">
<div class="title">VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model</div>
<div class="meta-line">Authors: Yanjiang Guo, Tony Lee, Lucy Xiaoyang Shi, Jianyu Chen, Percy Liang, Chelsea Finn</div>
<div class="meta-line">First: 2026-02-12T15:21:47+00:00 · Latest: 2026-02-15T00:04:15+00:00</div>
<div class="meta-line">Comments: Project Page: https://sites.google.com/view/vlaw-arxiv</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12063v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12063v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/vlaw-arxiv">Project1</a> · <a href="https://sites.google.com/view/vla-w">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLAW：视觉-语言-动作策略和世界模型的迭代联合改进</div>
<div class="mono" style="margin-top:8px">本文的目标是通过迭代在线交互来提高视觉-语言-动作（VLA）模型的性能和可靠性。由于在现实世界中收集策略回放数据成本高昂，我们研究是否可以使用一个学习到的模拟器——特别是动作条件下的视频生成模型——来生成额外的回放数据。不幸的是，现有的世界模型缺乏用于策略改进所需的物理精度：它们主要是在缺乏多种不同物理交互（特别是失败案例）覆盖的数据集上进行训练，难以准确模拟接触丰富的物体操作中的细微但关键的物理细节。我们提出了一种简单的迭代改进算法，使用现实世界的回放数据来提高世界模型的精度，然后可以使用该模型生成补充的合成数据以改进VLA模型。在我们对真实机器人进行的实验中，我们使用这种方法提高了最先进的VLA模型在多个下游任务上的性能。与基线策略相比，绝对成功率提高了39.2%，使用生成的合成回放数据训练提高了11.6%。有关视频，请访问此匿名网站：https://sites.google.com/view/vla-w</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to enhance the performance and reliability of vision-language-action models through iterative online interaction. To address the high cost of collecting real-world policy rollouts, the authors propose using a learned simulator to generate additional data. However, existing world models lack the necessary physical fidelity. The authors introduce an iterative improvement algorithm that uses real-world rollouts to enhance the world model&#x27;s fidelity, which in turn generates synthetic data to improve the VLA model. Experiments on a real robot show a 39.2% absolute success rate improvement over the base policy and a 11.6% improvement from training with synthetic rollouts.</div>
<div class="mono" style="margin-top:8px">本文旨在通过迭代在线交互来提升视觉-语言-动作模型的性能和可靠性。为了解决收集真实世界策略回放数据成本高的问题，作者提出使用一个学习到的模拟器来生成额外的数据。然而，现有的世界模型缺乏必要的物理精度。作者引入了一个迭代改进算法，使用真实世界的回放数据来提升世界模型的精度，进而生成合成数据以改进VLA模型。在真实机器人上的实验表明，与基线策略相比，绝对成功率提高了39.2%，而通过训练合成回放数据则提高了11.6%。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260217_0342.html">20260217_0342</a>
<a href="archive/20260216_0338.html">20260216_0338</a>
<a href="archive/20260215_0336.html">20260215_0336</a>
<a href="archive/20260213_0401.html">20260213_0401</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
