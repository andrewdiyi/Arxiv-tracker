<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-13 04:01</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260213_0401</div>
    <div class="row"><div class="card">
<div class="title">YOR: Your Own Mobile Manipulator for Generalizable Robotics</div>
<div class="meta-line">Authors: Manan H Anjaria, Mehmet Enes Erciyes, Vedant Ghatnekar, Neha Navarkar, Haritheja Etukuru, Xiaole Jiang, Kanad Patel, Dhawal Kabra, Nicholas Wojno, Radhika Ajay Prayage, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui</div>
<div class="meta-line">First: 2026-02-11T18:59:00+00:00 · Latest: 2026-02-11T18:59:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11150v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://www.yourownrobot.ai/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR&#x27;s capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>YOR：您的专属移动 manipulator 以实现通用机器人技术</div>
<div class="mono" style="margin-top:8px">近期机器人学习的进步激发了对能够接近人类水平能力的平台的兴趣。这种兴趣与执行器商品化的结合，推动了低成本机器人平台的增长。然而，预算内的最优移动操作形式仍然是一个开放的问题。我们介绍了 YOR，一个开源、低成本的移动 manipulator，集成了全向底座、伸缩垂直提升装置和两臂带夹爪，以实现全身移动和操作。我们的设计强调模块化、使用现成组件的组装简便性和经济性，材料成本低于10,000美元。我们通过完成需要协调全身控制、双臂操作和自主导航的任务来展示 YOR 的能力。总体而言，YOR 以远低于现有平台的成本提供了移动操作研究的竞争性功能。项目网站：https://www.yourownrobot.ai/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces YOR, a low-cost mobile manipulator designed for generalizable robotics research. YOR integrates an omnidirectional base, a telescopic lift, and two arms with grippers, enabling whole-body mobility and manipulation. The design focuses on modularity, ease of assembly, and affordability, with a bill-of-materials cost under 10,000 USD. Experiments show YOR can perform coordinated whole-body control, bimanual manipulation, and autonomous navigation, offering competitive functionality at a lower cost compared to existing platforms.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种低成本的移动 manipulator，以实现机器人的人类水平能力。YOR 是一个开源平台，集成了全向底座、伸缩垂直提升装置和两个带有夹爪的手臂。实验结果表明，YOR 可以执行需要全身协调控制、双臂操作和自主导航的任务，其功能竞争力且成本远低于现有平台。</div>
</details>
</div>
<div class="card">
<div class="title">APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots</div>
<div class="meta-line">Authors: Yikai Wang, Tingxuan Leng, Changyi Lin, Shiqi Liu, Shir Simon, Bingqing Chen, Jonathan Francis, Ding Zhao</div>
<div class="meta-line">First: 2026-02-11T18:55:11+00:00 · Latest: 2026-02-11T18:55:11+00:00</div>
<div class="meta-line">Comments: Project Website: https://apex-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11143v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://apex-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid locomotion has advanced rapidly with deep reinforcement learning (DRL), enabling robust feet-based traversal over uneven terrain. Yet platforms beyond leg length remain largely out of reach because current RL training paradigms often converge to jumping-like solutions that are high-impact, torque-limited, and unsafe for real-world deployment. To address this gap, we propose APEX, a system for perceptive, climbing-based high-platform traversal that composes terrain-conditioned behaviors: climb-up and climb-down at vertical edges, walking or crawling on the platform, and stand-up and lie-down for posture reconfiguration. Central to our approach is a generalized ratchet progress reward for learning contact-rich, goal-reaching maneuvers. It tracks the best-so-far task progress and penalizes non-improving steps, providing dense yet velocity-free supervision that enables efficient exploration under strong safety regularization. Based on this formulation, we train LiDAR-based full-body maneuver policies and reduce the sim-to-real perception gap through a dual strategy: modeling mapping artifacts during training and applying filtering and inpainting to elevation maps during deployment. Finally, we distill all six skills into a single policy that autonomously selects behaviors and transitions based on local geometry and commands. Experiments on a 29-DoF Unitree G1 humanoid demonstrate zero-shot sim-to-real traversal of 0.8 meter platforms (approximately 114% of leg length), with robust adaptation to platform height and initial pose, as well as smooth and stable multi-skill transitions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APEX：学习适应性高平台穿越的人形机器人</div>
<div class="mono" style="margin-top:8px">借助深度强化学习（DRL），人形机器人的行进方式取得了快速进步，使其能够在不平坦的地面上稳健地行走。然而，由于当前的强化学习训练方法往往收敛于跳跃式的解决方案，这些解决方案高冲击、扭矩限制且在实际部署中不安全，因此平台高度超出腿部长度的部分仍然难以触及。为解决这一问题，我们提出了APEX系统，该系统基于感知的攀爬式高平台穿越，结合地形条件下的行为：垂直边缘的攀爬上下、平台上的行走或爬行、站立和躺下以重新配置姿态。我们方法的核心在于一种通用的棘轮进步奖励，用于学习接触丰富的、目标导向的机动。该奖励追踪迄今为止的最佳任务进展，并惩罚非改进的步骤，从而提供密集但无速度的监督，使在强安全正则化下高效探索成为可能。基于此框架，我们训练了基于LiDAR的全身机动策略，并通过双重策略减少模拟到现实的感知差距：在训练过程中建模映射伪影，并在部署时对高程图进行滤波和修补。最后，我们将所有六种技能提炼成一个单一策略，该策略根据局部几何结构和命令自主选择行为并进行转换。在29-自由度的Unitree G1人形机器人上的实验表明，该策略能够实现零样本模拟到现实的0.8米平台穿越（约114%的腿部长度），并且能够稳健地适应平台高度和初始姿态，同时实现平滑稳定的多技能转换。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">APEX is designed to enable humanoid robots to traverse high platforms safely and efficiently using deep reinforcement learning. The system learns adaptive behaviors such as climbing and standing, and uses a ratchet progress reward to promote contact-rich, goal-directed movements. Experiments show that APEX can traverse platforms up to 0.8 meters high, which is about 114% of the robot&#x27;s leg length, with robust performance across different initial poses and heights.</div>
<div class="mono" style="margin-top:8px">APEX 是一种使类人机器人能够使用攀爬行为跨越高平台的系统。它利用了一种通用的棘轮进度奖励来学习接触丰富的动作，从而实现高效的探索和安全的现实世界部署。实验表明，APEX 可以跨越高达 0.8 米的平台，相当于腿长的 114%，并且能够适应不同的初始姿态和高度，实现平滑稳定的多技能转换。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows</div>
<div class="meta-line">Authors: Shaswat Garg, Matin Moezzi, Brandon Da Silva</div>
<div class="meta-line">First: 2026-02-11T18:54:48+00:00 · Latest: 2026-02-11T18:54:48+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures, IEEE International Conference on Robotics and Automation 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11142v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive normalizing flow policies at both the high- and low-levels of the hierarchy is introduced. This design enables tractable log-likelihood computation, efficient sampling, and the ability to model rich multimodal behaviors. New theoretical guarantees are derived, including explicit KL-divergence bounds for Real-valued non-volume preserving (RealNVP) policies and PAC-style sample efficiency results, showing that NF-HIQL preserves stability while improving generalization. Empirically, NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling, and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior goal-conditioned and hierarchical baselines, demonstrating superior robustness under limited data and highlighting the potential of flow-based architectures for scalable, data-efficient hierarchical reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于归一化流的数据高效分层目标导向强化学习</div>
<div class="mono" style="margin-top:8px">分层目标导向强化学习（H-GCRL）提供了一种强大的框架，用于通过将复杂、长期的任务分解为结构化的子目标来应对这些任务。然而，其实际应用受到数据效率差和策略表达能力有限的阻碍，尤其是在离线或数据稀缺的环境中。本文提出了一种新的框架——基于归一化流的分层隐式Q学习（NF-HIQL），该框架在分层的高低层都用表达性强的归一化流策略取代了单模高斯策略。这种设计使得可以进行可处理的对数似然计算、高效采样，并能够建模丰富的多模态行为。推导了新的理论保证，包括实值非体积保持（RealNVP）策略的显式KL散度界和PAC风格的样本效率结果，表明NF-HIQL在保持稳定性的同时提高了泛化能力。实验上，NF-HIQL在OGBench中的行走、带球和多步操作等多样化的长期任务中进行了评估。NF-HIQL在所有任务中都优于先前的目标导向和分层基线，展示了在数据有限条件下更强的鲁棒性，并突显了基于流的架构在可扩展、数据高效分层强化学习中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of hierarchical goal-conditioned reinforcement learning (H-GCRL) by introducing NF-HIQL, which uses normalizing flow policies to enhance data efficiency and policy expressivity. Theoretical guarantees are provided, and empirical results show that NF-HIQL outperforms previous methods in various long-horizon tasks, particularly under data-scarce conditions.</div>
<div class="mono" style="margin-top:8px">该研究通过引入使用流形策略的NF-HIQL框架来解决H-GCRL的数据效率和策略表达性问题。理论保证和实验结果表明，NF-HIQL在各种长时任务中表现优于先前方法，特别是在数据稀缺条件下表现出更好的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">LCIP: Loss-Controlled Inverse Projection of High-Dimensional Image Data</div>
<div class="meta-line">Authors: Yu Wang, Frederik L. Dennig, Michael Behrisch, Alexandru Telea</div>
<div class="meta-line">First: 2026-02-11T18:52:46+00:00 · Latest: 2026-02-11T18:52:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11141v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Projections (or dimensionality reduction) methods $P$ aim to map high-dimensional data to typically 2D scatterplots for visual exploration. Inverse projection methods $P^{-1}$ aim to map this 2D space to the data space to support tasks such as data augmentation, classifier analysis, and data imputation. Current $P^{-1}$ methods suffer from a fundamental limitation -- they can only generate a fixed surface-like structure in data space, which poorly covers the richness of this space. We address this by a new method that can `sweep&#x27; the data space under user control. Our method works generically for any $P$ technique and dataset, is controlled by two intuitive user-set parameters, and is simple to implement. We demonstrate it by an extensive application involving image manipulation for style transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LCIP：可控逆投影的高维图像数据</div>
<div class="mono" style="margin-top:8px">投影（或降维）方法 $P$ 的目标是将高维数据映射到通常为 2D 散点图的空间，以便进行可视化探索。逆投影方法 $P^{-1}$ 的目标是将这个 2D 空间映射回数据空间，以支持数据增强、分类器分析和数据插补等任务。当前的 $P^{-1}$ 方法存在一个根本性的局限性——它们只能生成数据空间中的固定表面结构，这不能很好地覆盖这个空间的丰富性。我们通过一种新的方法来解决这个问题，该方法可以在用户控制下“扫掠”数据空间。我们的方法适用于任何 $P$ 技术和数据集，由两个直观的用户设置参数控制，实现简单。我们通过一个涉及图像操作以实现风格转移的广泛应用来演示这一点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces LCIP, a new inverse projection method that allows users to control the generation of high-dimensional data structures, addressing the limitation of existing methods that can only produce fixed surface-like structures. LCIP is generic, easy to implement, and controlled by two intuitive parameters. The method is demonstrated through an extensive application involving image manipulation for style transfer.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种名为LCIP的新逆投影方法，允许用户控制高维数据结构的生成，解决了现有方法只能生成固定表面结构的问题。LCIP通用、易于实现，并由两个直观的参数控制。该方法通过图像风格迁移的广泛应用进行了演示。</div>
</details>
</div>
<div class="card">
<div class="title">A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Computers</div>
<div class="meta-line">Authors: Jeffrey Joan Sam, Janhavi Sathe, Nikhil Chigali, Naman Gupta, Radhey Ruparel, Yicheng Jiang, Janmajay Singh, James W. Berck, Arko Barman</div>
<div class="meta-line">First: 2025-07-14T20:02:40+00:00 · Latest: 2026-02-11T18:32:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10775v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.10775v2">PDF</a> · <a href="https://github.com/RiceD2KLab/SWiM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring substantial operational costs. Recent developments in image segmentation could enable the development of reliable and cost-effective autonomous inspection systems. While these models often require large amounts of training data to achieve satisfactory results, publicly available annotated spacecraft segmentation data are very scarce. Here, we present a new dataset of nearly 64k annotated spacecraft images that was created using real spacecraft models, superimposed on a mixture of real and synthetic backgrounds generated using NASA&#x27;s TTALOS pipeline. To mimic camera distortions and noise in real-world image acquisition, we also added different types of noise and distortion to the images. Our dataset includes images with several real-world challenges, including noise, camera distortions, glare, varying lighting conditions, varying field of view, partial spacecraft visibility, brightly-lit city backgrounds, densely patterned and confounding backgrounds, aurora borealis, and a wide variety of spacecraft geometries. Finally, we finetuned YOLOv8 and YOLOv11 models for spacecraft segmentation to generate performance benchmarks for the dataset under well-defined hardware and inference time constraints to mimic real-world image segmentation challenges for real-time onboard applications in space on NASA&#x27;s inspector spacecraft. The resulting models, when tested under these constraints, achieved a Dice score of 0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second. The dataset and models for performance benchmark are available at https://github.com/RiceD2KLab/SWiM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种新的数据集及实时航天器分割性能基准</div>
<div class="mono" style="margin-top:8px">部署在外太空的航天器由于暴露在危险环境中，经常遭受各种形式的损害。此外，通过宇航员出舱活动或机器人操作进行在轨维修存在重大风险，导致高昂的操作成本。最近的图像分割技术的发展可能使开发可靠的自主检查系统成为可能。然而，这些模型通常需要大量训练数据才能达到满意的效果，而公开的标注航天器分割数据非常稀缺。在此，我们使用真实的航天器模型，并结合NASA的TTALOS管道生成的真实和合成背景的混合体，创建了一个近64,000张标注的航天器图像的新数据集。为了模拟真实世界图像获取中的相机畸变和噪声，我们还向图像中添加了不同类型的噪声和畸变。该数据集包括多种真实世界挑战，如噪声、相机畸变、反光、光照条件变化、视场变化、部分航天器可见性、明亮的城市背景、密集的图案和混淆背景、极光以及各种航天器几何形状。最后，我们针对航天器分割对YOLOv8和YOLOv11模型进行了微调，以在明确的硬件和推理时间约束下生成数据集的性能基准，以模拟NASA的检查员航天器中实时机载应用的图像分割挑战。在这些约束条件下测试后，生成的模型实现了Dice分数0.92、Hausdorff距离0.69和推理时间约0.5秒。数据集和用于性能基准的模型可在https://github.com/RiceD2KLab/SWiM/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need for reliable autonomous inspection systems for spacecraft by developing a new dataset of nearly 64,000 annotated images. The dataset includes various real-world challenges such as noise, camera distortions, and varying lighting conditions. The authors fine-tuned YOLOv8 and YOLOv11 models on this dataset and achieved a Dice score of 0.92, a Hausdorff distance of 0.69, and an inference time of about 0.5 seconds under defined hardware constraints.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过开发包含近64,000张标注图像的新数据集，解决对太空探测器进行可靠自主检查的需求。该数据集涵盖了各种现实世界中的挑战，如噪声、相机失真和光照变化。作者对YOLOv8和YOLOv11模型进行了微调，实现了Dice分数为0.92、Hausdorff距离为0.69以及在定义的硬件约束下推理时间为约0.5秒。</div>
</details>
</div>
<div class="card">
<div class="title">GameDevBench: Evaluating Agentic Capabilities Through Game Development</div>
<div class="meta-line">Authors: Wayne Chi, Yixiong Fang, Arnav Yayavaram, Siddharth Yayavaram, Seth Karten, Qiuhong Anna Wei, Runkun Chen, Alexander Wang, Valerie Chen, Ameet Talwalkar, Chris Donahue</div>
<div class="meta-line">First: 2026-02-11T18:15:11+00:00 · Latest: 2026-02-11T18:15:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11103v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11103v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5&#x27;s performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GameDevBench：通过游戏开发评估代理能力</div>
<div class="mono" style="margin-top:8px">尽管在编码代理方面取得了快速进展，但其多模态对应物的进步却落后了。一个关键挑战是缺乏能够结合软件开发复杂性和对深入多模态理解需求的评估测试平台。游戏开发提供了一个这样的测试平台，因为代理必须在处理着色器、精灵和动画等内在多模态资产的同时，导航大型密集的代码库，这些资产位于视觉游戏场景中。我们提出了GameDevBench，这是第一个用于评估代理在游戏开发任务上的基准。GameDevBench 包含了从网络和视频教程中提取的132项任务。这些任务需要大量的多模态理解并且非常复杂——平均解决方案的代码行数和文件更改量是之前软件开发基准的三倍多。代理仍然难以应对游戏开发，最好的代理仅解决了54.5%的任务。我们发现感知任务难度与多模态复杂性之间存在强烈的相关性，成功率为游戏导向任务的46.9%下降到2D图形任务的31.6%。为了提高多模态能力，我们引入了两种简单的基于图像和视频的反馈机制。尽管它们很简单，但这些方法始终能够提高性能，最大的变化是Claude Sonnet 4.5的性能从33.3%提高到47.7%。我们公开发布了GameDevBench，以支持进一步研究代理游戏开发。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GameDevBench evaluates agents&#x27; agentic capabilities through game development tasks, addressing the lack of comprehensive testbeds for multimodal agents. It consists of 132 tasks from web and video tutorials, requiring significant multimodal understanding. The best agent solves only 54.5% of tasks, with success rates dropping from 46.9% on gameplay tasks to 31.6% on 2D graphics tasks. Two simple feedback mechanisms improve performance, with Claude Sonnet&#x27;s success rate increasing from 33.3% to 47.7%.</div>
<div class="mono" style="margin-top:8px">GameDevBench 通过游戏开发任务评估代理的能动能力，解决了结合软件开发复杂性和深度多模态理解的评价测试床稀缺问题。基准包括来自网络和视频教程的132项任务，需要大量的多模态理解和复杂解决方案。最好的代理仅解决了54.5%的任务，成功率从46.9%的游戏玩法任务下降到31.6%的2D图形任务。引入简单的图像和视频反馈机制提高了性能，Claude Sonnet 4.5的性能从33.3%提高到47.7%。GameDevBench 已公开发布，以支持进一步研究代理游戏开发。</div>
</details>
</div>
<div class="card">
<div class="title">RISE: Self-Improving Robot Policy with Compositional World Model</div>
<div class="meta-line">Authors: Jiazhi Yang, Kunyang Lin, Jinwei Li, Wencong Zhang, Tianwei Lin, Longyan Wu, Zhizhong Su, Hao Zhao, Ya-Qin Zhang, Li Chen, Ping Luo, Xiangyu Yue, Hongyang Li</div>
<div class="meta-line">First: 2026-02-11T17:43:36+00:00 · Latest: 2026-02-11T17:43:36+00:00</div>
<div class="meta-line">Comments: Project page: https://opendrivelab.com/kai0-rl/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11075v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RISE：基于组合世界模型的自改进机器人策略</div>
<div class="mono" style="margin-top:8px">尽管在模型容量和数据获取方面持续扩大，视觉-语言-动作（VLA）模型在接触丰富和动态操作任务中仍然脆弱，其中细微的操作偏差可能会累积成失败。虽然强化学习（RL）提供了一条稳健性的原则路径，但在物理世界中的在线RL受到安全风险、硬件成本和环境重置的限制。为了弥合这一差距，我们提出了RISE，一种通过想象实现机器人强化学习的可扩展框架。其核心是一个组合世界模型，该模型（i）通过可控动力学模型预测多视角的未来，（ii）使用进步价值模型评估想象的结果，产生对策略改进具有信息性的优势。这种组合设计允许状态和价值由最适合但不同的架构和目标进行定制。这些组件被集成到一个闭环的自改进管道中，该管道不断生成想象的回放、估计优势并在想象空间中更新策略，而无需昂贵的物理交互。在三个具有挑战性的实际任务中，RISE在动态砖块排序、背包打包和盒子关闭方面分别取得了显著改进，绝对性能分别提高了35%以上、45%以上和35%以上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RISE is a scalable framework for robotic reinforcement learning through imagination, addressing the brittleness of Vision-Language-Action models in dynamic manipulation tasks. It uses a Compositional World Model to predict future states and evaluate outcomes, providing informative advantages for policy improvement. RISE significantly outperforms previous methods, achieving more than a 35% absolute performance increase in dynamic brick sorting, backpack packing, and box closing tasks.</div>
<div class="mono" style="margin-top:8px">RISE 是一种通过想象实现机器人强化学习的可扩展框架，旨在解决 Vision-Language-Action 模型在动态操作任务中的脆弱性。它使用组合世界模型来预测未来状态并评估结果，为策略改进提供有信息量的优势。RISE 在动态砖块排序 (+35%)、背包打包 (+45%) 和盒子关闭 (+35%) 等三项挑战性实际任务中，显著优于先前的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Chatting with Images for Introspective Visual Thinking</div>
<div class="meta-line">Authors: Junfei Wu, Jian Guan, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tienie Tan</div>
<div class="meta-line">First: 2026-02-11T17:42:37+00:00 · Latest: 2026-02-11T17:42:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11073v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#x27;&#x27;thinking with images&#x27;&#x27; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose &#x27;&#x27;chatting with images&#x27;&#x27;, a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance visual reasoning in large vision-language models by addressing the loss of fine-grained visual information and the difficulty in cross-modal alignment. The proposed method, &#x27;chatting with images,&#x27; involves a new framework where language prompts guide the model to dynamically re-encode multiple image regions, improving the coupling between linguistic reasoning and visual state updates. Experiments show that ViLaVT, a novel LVLM equipped with a dynamic vision encoder, achieves strong and consistent improvements, especially in complex multi-image and video-based spatial reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本文针对当前大型视觉-语言模型（LVLM）依赖于文本推理和单次视觉编码，可能导致细粒度视觉信息丢失的问题，提出了‘与图像对话’的新框架，将视觉操作重新定义为语言引导的特征调制。该模型名为ViLaVT，通过结合监督微调和强化学习的两阶段课程进行训练，以增强其推理能力。实验结果显示，ViLaVT在八个基准测试中表现出色，特别是在复杂的多图像和基于视频的空间推理任务上取得了显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">SQ-CBF: Signed Distance Functions for Numerically Stable Superquadric-Based Safety Filtering</div>
<div class="meta-line">Authors: Haocheng Zhao, Lukas Brunke, Oliver Lagerquist, Siqi Zhou, Angela P. Schoellig</div>
<div class="meta-line">First: 2026-02-11T17:19:43+00:00 · Latest: 2026-02-11T17:19:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11049v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11049v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring safe robot operation in cluttered and dynamic environments remains a fundamental challenge. While control barrier functions provide an effective framework for real-time safety filtering, their performance critically depends on the underlying geometric representation, which is often simplified, leading to either overly conservative behavior or insufficient collision coverage. Superquadrics offer an expressive way to model complex shapes using a few primitives and are increasingly used for robot safety. To integrate this representation into collision avoidance, most existing approaches directly use their implicit functions as barrier candidates. However, we identify a critical but overlooked issue in this practice: the gradients of the implicit SQ function can become severely ill-conditioned, potentially rendering the optimization infeasible and undermining reliable real-time safety filtering. To address this issue, we formulate an SQ-based safety filtering framework that uses signed distance functions as barrier candidates. Since analytical SDFs are unavailable for general SQs, we compute distances using the efficient Gilbert-Johnson-Keerthi algorithm and obtain gradients via randomized smoothing. Extensive simulation and real-world experiments demonstrate consistent collision-free manipulation in cluttered and unstructured scenes, showing robustness to challenging geometries, sensing noise, and dynamic disturbances, while improving task efficiency in teleoperation tasks. These results highlight a pathway toward safety filters that remain precise and reliable under the geometric complexity of real-world environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SQ-CBF：基于超球体的符号距离函数数值稳定的安全过滤</div>
<div class="mono" style="margin-top:8px">在拥挤和动态环境中确保机器人操作的安全仍然是一个基本挑战。虽然控制屏障函数为实时安全过滤提供了一个有效的框架，但其性能严重依赖于底层的几何表示，这通常是简化的，导致行为要么过于保守，要么碰撞覆盖不足。超球体提供了一种使用少数几种基本形状来表示复杂形状的表达方式，并且越来越多地用于机器人安全。为了将这种表示法集成到碰撞避免中，大多数现有方法直接使用它们的隐式函数作为屏障候选。然而，我们识别出这种做法中的一个关键但被忽视的问题：隐式SQ函数的梯度可能会变得严重病态，这可能导致优化不可行并削弱可靠的实时安全过滤。为了解决这个问题，我们提出了一个基于超球体的安全过滤框架，使用符号距离函数作为屏障候选。由于一般超球体的分析SDF不可用，我们使用高效的Gilbert-Johnson-Keerthi算法计算距离，并通过随机平滑获得梯度。广泛的仿真实验和真实世界实验表明，在拥挤和未结构化的场景中，该方法能够一致地实现无碰撞操作，显示出对复杂几何形状、传感噪声和动态干扰的鲁棒性，同时在远程操作任务中提高任务效率。这些结果突显了一条途径，即在现实世界环境的几何复杂性下，保持安全过滤的精确性和可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of ensuring safe robot operation in complex environments by proposing a new safety filtering framework called SQ-CBF. It uses signed distance functions derived from superquadrics to overcome the limitations of existing implicit function-based approaches, which can suffer from ill-conditioned gradients. The authors employ the Gilbert-Johnson-Keerthi algorithm for distance computation and randomized smoothing for gradient estimation. The results show that this method consistently maintains collision-free operation in cluttered scenes, demonstrating robustness to various challenges and improving task efficiency in teleoperation tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了一种新的安全过滤框架SQ-CBF，通过使用超曲面的符号距离函数来解决现有隐函数方法中存在的梯度病态问题。该方法使用Gilbert-Johnson-Keerthi算法进行距离计算，并通过随机平滑获取梯度。实验结果表明，SQ-CBF能够在复杂场景中保持一致的无碰撞操作，处理复杂的几何形状，并在远程操作任务中提高任务效率。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling World Model for Hierarchical Manipulation Policies</div>
<div class="meta-line">Authors: Qian Long, Yueze Wang, Jiaxi Song, Junbo Zhang, Peiyan Li, Wenxuan Wang, Yuqi Wang, Haoyang Li, Shaoxuan Xie, Guocai Yao, Hanbo Zhang, Xinlong Wang, Zhongyuan Wang, Xuguang Lan, Huaping Liu, Xinghang Li</div>
<div class="meta-line">First: 2026-02-11T16:12:33+00:00 · Latest: 2026-02-11T16:12:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10983v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vista-wm.github.io/}{https://vista-wm.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \href{https://vista-wm.github.io/}{https://vista-wm.github.io}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展世界模型以实现分层操作策略</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在通用机器人操作方面具有潜力，但在分布外（OOD）设置中仍然脆弱，尤其是在有限的机器人数据情况下。为了解决泛化瓶颈，我们引入了一种分层视觉-语言-动作框架\our{}，该框架利用大规模预训练世界模型的泛化能力，以实现稳健且可泛化的VISTA视觉子目标任务分解。我们的分层框架\our{}由世界模型作为高层规划者和VLA作为低层执行者组成。高层世界模型首先将操作任务分解为带有目标图像的子任务序列，低层策略遵循文本和视觉指导生成动作序列。与原始文本目标规范相比，这些合成的目标图像为低层策略提供了视觉和物理上的细节，使其能够在未见过的对象和新场景中进行泛化。我们在大量分布外场景中验证了视觉目标合成和我们的分层VLA策略，并且在世界模型生成的指导下，相同结构的VLA在新场景中的性能可以从14%提升到69%。结果表明，我们的方法在分布外场景中明显优于之前的基线方法。项目页面：\href{https://vista-wm.github.io/}{https://vista-wm.github.io}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the brittleness of Vision-Language-Action (VLA) models in out-of-distribution settings by proposing a hierarchical framework that leverages a pre-trained world model for robust subgoal decomposition. The method involves a high-level world model that divides manipulation tasks into subtasks with goal images, guiding a low-level VLA policy to execute actions. Experimental results show a significant improvement in performance, from 14% to 69%, in novel scenarios when using the guidance from the world model, outperforming previous baselines in out-of-distribution settings.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出一种基于预训练世界模型的层次框架来解决Vision-Language-Action (VLA)模型在分布外环境中的脆弱性问题。该方法包括一个高层世界模型，它将操作任务分解为带有目标图像的子任务，指导低层VLA策略执行动作。实验结果表明，在使用世界模型生成的指导时，性能从14%提升到69%，特别是在分布外场景中，该方法优于之前的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation</div>
<div class="meta-line">Authors: Yuhao Chen, Zhihao Zhan, Xiaoxin Lin, Zijian Song, Hao Liu, Qinhan Lyu, Yubo Zu, Xiao Chen, Zhiyuan Liu, Tao Pu, Tianshui Chen, Keze Wang, Liang Lin, Guangrun Wang</div>
<div class="meta-line">First: 2026-02-11T16:08:30+00:00 · Latest: 2026-02-11T16:08:30+00:00</div>
<div class="meta-line">Comments: 12 pages, 11 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10980v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10980v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RADAR：通过现实世界动力学、空间物理智能和自主评估衡量视觉-语言-行动泛化</div>
<div class="mono" style="margin-top:8px">VLA模型在体态智能方面取得了显著进展，然而其评估主要局限于模拟环境或高度受限的现实世界设置中。这种不匹配造成了显著的现实差距，即良好的基准性能往往掩盖了在不同物理环境中的泛化能力不足。我们指出现有基准测试实践中的三个系统性缺陷，阻碍了公平可靠的模型比较。1. 现有基准未能模拟现实世界动力学，忽视了动态物体配置、机器人初始状态、光照变化和传感器噪声等关键因素。2. 当前评估协议忽视了空间物理智能，将评估简化为机械操作任务，未能测试几何推理。3. 该领域缺乏可扩展的完全自主评估，依赖于简单的二维指标或成本高、有偏见且不可扩展的人工监督系统。为解决这些局限性，我们引入了RADAR（现实世界自主动力学与推理基准），旨在在现实条件下系统评估VLA泛化能力。RADAR整合了三个核心组件：1. 一套原理性的物理动力学；2. 专门测试空间推理和物理理解的任务；3. 基于三维指标的完全自主评估流程，消除了人工监督的需要。我们应用RADAR审核了多个最先进的VLA模型，并发现其表面的熟练程度背后存在严重的脆弱性。在轻微物理动力学下，性能急剧下降，预期的三维IoU从0.261降至0.068。此外，模型的空间推理能力有限。这些发现将RADAR定位为可靠和可泛化的现实世界评估VLA模型的必要基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the reality gap in evaluating vision-language-action (VLA) models by introducing RADAR, a benchmark that evaluates models under realistic conditions. RADAR addresses three main shortcomings: it models real-world dynamics, tests spatial reasoning, and uses a fully autonomous evaluation pipeline. The study finds that state-of-the-art VLA models perform poorly under realistic conditions, with performance dropping significantly under physical dynamics and sensor noise, and limited spatial reasoning capabilities. This highlights the need for more robust benchmarks in VLA evaluation.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入RADAR基准解决视觉-语言-行动（VLA）模型的现实差距问题，该基准在现实条件下评估模型。RADAR解决了当前基准的三个不足：模拟现实世界动力学、测试空间推理和物理理解，并使用基于3D度量的完全自主评估管道。关键发现表明，最先进的VLA模型在轻微物理动力学下表现不佳，3D IoU显著下降，并且表现出有限的空间推理能力。这突显了在VLA模型中需要更严格的评估方法。</div>
</details>
</div>
<div class="card">
<div class="title">HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba</div>
<div class="meta-line">Authors: Yinuo Wang, Yuanyang Qi, Jinzhao Zhou, Pengxiang Meng, Xiaowen Tao</div>
<div class="meta-line">First: 2025-09-22T17:19:55+00:00 · Latest: 2026-02-11T16:00:35+00:00</div>
<div class="meta-line">Comments: 12 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18046v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end reinforcement learning (RL) for humanoid locomotion is appealing for its compact perception-action mapping, yet practical policies often suffer from training instability, inefficient feature fusion, and high actuation cost. We present HuMam, a state-centric end-to-end RL framework that employs a single-layer Mamba encoder to fuse robot-centric states with oriented footstep targets and a continuous phase clock. The policy outputs joint position targets tracked by a low-level PD loop and is optimized with PPO. A concise six-term reward balances contact quality, swing smoothness, foot placement, posture, and body stability while implicitly promoting energy saving. On the JVRC-1 humanoid in mc-mujoco, HuMam consistently improves learning efficiency, training stability, and overall task performance over a strong feedforward baseline, while reducing power consumption and torque peaks. To our knowledge, this is the first end-to-end humanoid RL controller that adopts Mamba as the fusion backbone, demonstrating tangible gains in efficiency, stability, and control economy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HuMam：通过Mamba端到端深度强化学习实现类人运动控制</div>
<div class="mono" style="margin-top:8px">端到端的强化学习（RL）在类人运动控制中具有吸引力，因为它可以实现紧凑的感知-行动映射，但实际策略往往遭受训练不稳定、特征融合效率低下和高执行成本的问题。我们提出了HuMam，这是一种状态为中心的端到端RL框架，使用单层Mamba编码器融合以机器人为中心的状态与定向足步目标和连续相位时钟。策略输出由低级PD环跟踪的关节位置目标，并使用PPO进行优化。简洁的六项奖励平衡了接触质量、摆动平滑度、脚部放置、姿态和身体稳定性，同时隐式促进节能。在mc-mujoco中的JVRC-1类人机器人上，HuMam在强大的前馈基线之上，持续提高学习效率、训练稳定性和整体任务性能，同时降低能耗和扭矩峰值。据我们所知，这是第一个采用Mamba作为融合骨干的端到端类人RL控制器，展示了在效率、稳定性和控制经济性方面的实际收益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">HuMam is an end-to-end deep reinforcement learning framework for humanoid locomotion that uses a single-layer Mamba encoder to integrate robot-centric states with footstep targets and a continuous phase clock. The policy outputs joint position targets and is optimized with PPO. HuMam improves learning efficiency, training stability, and task performance compared to a feedforward baseline, while reducing power consumption and torque peaks. To the authors&#x27; knowledge, this is the first end-to-end humanoid RL controller using Mamba for feature fusion, showing tangible gains in efficiency, stability, and control economy.</div>
<div class="mono" style="margin-top:8px">HuMam 是一种用于人形机器人运动控制的端到端深度强化学习框架，使用单层 Mamba 编码器整合了机器人中心状态、脚步目标和相位时钟。策略输出关节位置目标并使用 PPO 进行优化。HuMam 在提高学习效率、训练稳定性和任务性能方面优于前馈基线，同时减少了功率消耗和扭矩峰值。据作者所知，这是首个采用 Mamba 作为融合骨干的端到端人形机器人 RL 控制器，展示了在效率、稳定性和控制经济性方面的实际收益。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Learning a Generalizable 3D Scene Representation from 2D Observations</div>
<div class="meta-line">Authors: Martin Gromniak, Jan-Gerrit Habekost, Sebastian Kamp, Sven Magg, Stefan Wermter</div>
<div class="meta-line">First: 2026-02-11T15:22:41+00:00 · Latest: 2026-02-11T15:22:41+00:00</div>
<div class="meta-line">Comments: Paper accepted at ESANN 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10943v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10943v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a method for predicting 3D scene occupancy from 2D robot observations, focusing on a global workspace frame rather than camera-centric coordinates. The model, a Generalizable Neural Radiance Field, integrates flexible source views and generalizes to unseen object arrangements without requiring scene-specific fine-tuning. Evaluated on a humanoid robot, the model achieves a 26mm reconstruction error, demonstrating its ability to infer complete 3D occupancy beyond traditional stereo vision methods, trained on 40 real scenes.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种方法，从2D机器人观察中预测3D场景表示，重点是使用全局工作空间框架而非相机中心坐标系。该模型，即通用神经辐射场，结合了灵活的源视图，并在不需要特定场景微调的情况下泛化到未见过的对象排列。在人形机器人上的实验表明，该模型的重建误差为26mm，验证了其超越传统立体视觉方法的能力，能够推断出完整的3D占用情况。</div>
</details>
</div>
<div class="card">
<div class="title">Neural-Augmented Kelvinlet for Real-Time Soft Tissue Deformation Modeling</div>
<div class="meta-line">Authors: Ashkan Shahbazi, Kyvia Pereira, Jon S. Heiselman, Elaheh Akbari, Annie C. Benson, Sepehr Seifi, Xinyuan Liu, Garrison L. Johnston, Jie Ying Wu, Nabil Simaan, Michael I. Miga, Soheil Kolouri</div>
<div class="meta-line">First: 2025-06-06T19:22:49+00:00 · Latest: 2026-02-11T14:01:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08043v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.08043v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate and efficient modeling of soft-tissue interactions is fundamental for advancing surgical simulation, surgical robotics, and model-based surgical automation. To achieve real-time latency, classical Finite Element Method (FEM) solvers are often replaced with neural approximations; however, naively training such models in a fully data-driven manner without incorporating physical priors frequently leads to poor generalization and physically implausible predictions. We present a novel physics-informed neural simulation framework that enables real-time prediction of soft-tissue deformations under complex single- and multi-grasper interactions. Our approach integrates Kelvinlet-based analytical priors with large-scale FEM data, capturing both linear and nonlinear tissue responses. This hybrid design improves predictive accuracy and physical plausibility across diverse neural architectures while maintaining the low-latency performance required for interactive applications. We validate our method on challenging surgical manipulation tasks involving standard laparoscopic grasping tools, demonstrating substantial improvements in deformation fidelity and temporal stability over existing baselines. These results establish Kelvinlet-augmented learning as a principled and computationally efficient paradigm for real-time, physics-aware soft-tissue simulation in surgical AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经增强Kelvinlet用于实时软组织变形建模</div>
<div class="mono" style="margin-top:8px">准确而高效的软组织相互作用建模是推动手术模拟、手术机器人和基于模型的手术自动化发展的基础。为了实现实时延迟，经典的有限元方法（FEM）求解器通常被神经近似所取代；然而，仅通过完全数据驱动的方式训练此类模型而不结合物理先验，通常会导致泛化能力差和物理上不合理的预测。我们提出了一种新的物理知情神经模拟框架，能够实现实时预测复杂单握和多握工具作用下的软组织变形。该方法将基于Kelvinlet的分析先验与大规模FEM数据相结合，捕捉线性和非线性组织响应。这种混合设计在多种神经架构中提高了预测准确性和物理合理性，同时保持了交互应用所需的低延迟性能。我们在涉及标准腹腔镜抓取工具的复杂手术操作任务上验证了该方法，展示了与现有基线相比在变形保真度和时间稳定性方面的显著改进。这些结果确立了Kelvinlet增强学习作为实时、物理感知软组织模拟的原理性且计算高效的范式的地位，在手术AI中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper presents a physics-informed neural simulation framework for real-time modeling of soft-tissue deformations. It combines Kelvinlet-based analytical priors with FEM data to improve predictive accuracy and physical plausibility. The method demonstrates substantial improvements in deformation fidelity and temporal stability over existing baselines in surgical manipulation tasks involving laparoscopic grasping tools.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过结合物理先验与神经网络，提高手术应用中软组织变形的实时建模。方法采用混合方式，结合Kelvinlet分析先验与FEM数据，捕捉线性和非线性组织响应。实验表明，这种方法在手术操作任务中提高了变形精度和时间稳定性，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch</div>
<div class="meta-line">Authors: Xingyi Zhang, Yulei Ye, Kaifeng Huang, Wenhao Li, Xiangfeng Wang</div>
<div class="meta-line">First: 2026-02-11T12:54:53+00:00 · Latest: 2026-02-11T12:54:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10814v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>观察、规划、拍摄：在Scratch中评估多模态GUI代理</div>
<div class="mono" style="margin-top:8px">基于块的编程环境如Scratch在低代码教育中发挥着核心作用，然而，通过图形用户界面（GUI）构建程序的AI代理能力评估仍处于探索阶段。我们引入了ScratchWorld，这是一个用于评估多模态GUI代理在Scratch中构建程序任务能力的基准。ScratchWorld基于“使用-修改-创造”教学框架，包含83个精心策划的任务，涵盖四个不同的问题类别：创造、调试、扩展和计算。为了严格诊断代理失败的原因，基准采用了两种互补的交互模式：原始模式要求精细的拖放操作以直接评估视觉-运动控制能力，而复合模式使用高级语义API来分离程序推理与GUI执行。为了确保可靠的评估，我们提出了一种基于执行的评估协议，通过浏览器环境中的运行时测试验证构建的Scratch程序的功能正确性。广泛的实验表明，最先进的多模态语言模型和GUI代理之间存在显著的推理-执行差距，突显了尽管具有强大的规划能力，但在精细的GUI操作方面仍存在持续的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces ScratchWorld, a benchmark for evaluating multimodal GUI agents in Scratch, focusing on program-by-construction tasks. It includes 83 curated tasks divided into four categories: Create, Debug, Extend, and Compute. The benchmark uses two interaction modes: primitive mode for assessing visuomotor control and composite mode for separating program reasoning from GUI execution. The study finds a significant gap between planning capabilities and fine-grained GUI manipulation, indicating ongoing challenges for AI agents in low-code environments like Scratch.</div>
<div class="mono" style="margin-top:8px">论文介绍了ScratchWorld，这是一个用于评估Scratch中多模态GUI代理的基准，专注于程序构建任务。它包含83个分类任务，分为四个类别：Create、Debug、Extend和Compute。基准使用了两种交互模式：原始模式用于评估视觉运动控制，复合模式用于分离程序推理和GUI执行。研究发现，规划能力与精细的GUI操作之间存在显著差距，表明AI代理在低代码环境如Scratch中仍然面临持续的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks</div>
<div class="meta-line">Authors: Enrico Ahlers, Daniel Passon, Yannic Noller, Lars Grunske</div>
<div class="meta-line">First: 2026-02-11T12:13:25+00:00 · Latest: 2026-02-11T12:13:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10780v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious training process. Backdoors can be used to induce unwanted behavior by including a certain trigger in the input. Existing mitigations filter training data, modify the model, or perform expensive input modifications on samples. If a vulnerable model has already been deployed, however, those strategies are either ineffective or inefficient. To address this gap, we propose our inference-time backdoor mitigation approach called FIRE (Feature-space Inference-time REpair). We hypothesize that a trigger induces structured and repeatable changes in the model&#x27;s internal representation. We view the trigger as directions in the latent spaces between layers that can be applied in reverse to correct the inference mechanism. Therefore, we turn the backdoored model against itself by manipulating its latent representations and moving a poisoned sample&#x27;s features along the backdoor directions to neutralize the trigger. Our evaluation shows that FIRE has low computational overhead and outperforms current runtime mitigations on image benchmarks across various attacks, datasets, and network architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用FIRE解决它：利用潜在空间方向进行深度神经网络运行时后门缓解</div>
<div class="mono" style="margin-top:8px">机器学习模型越来越多地出现在我们的日常生活中；因此，它们成为恶意攻击者试图操控的系统目标。一个已知的漏洞是在神经网络中通过受污染的训练数据或恶意训练过程植入后门。后门可以通过在输入中包含特定触发器来诱导不希望的行为。现有的缓解措施过滤训练数据、修改模型或对样本进行昂贵的输入修改。然而，如果一个易受攻击的模型已经部署，这些策略要么无效要么效率低下。为了解决这一差距，我们提出了一种名为FIRE（特征空间推理修复）的推理时后门缓解方法。我们假设触发器会在模型的内部表示中引起结构化和可重复的变化。我们将触发器视为层间潜在空间中的方向，可以通过反向应用这些方向来纠正推理机制。因此，我们通过操纵后门模型的潜在表示，将受污染样本的特征沿着后门方向移动，以抵消触发器。我们的评估表明，FIRE具有较低的计算开销，并在各种攻击、数据集和网络架构的图像基准测试中优于当前的运行时缓解措施。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the vulnerability of deep neural networks to backdoors, which are introduced by poisoned training data or malicious training processes. It proposes a runtime mitigation technique called FIRE (Feature-space Inference-time REpair) that manipulates latent representations to neutralize the trigger. FIRE is evaluated on image benchmarks and demonstrates low computational overhead and superior performance compared to existing runtime mitigations across different attacks, datasets, and network architectures.</div>
<div class="mono" style="margin-top:8px">论文针对由中毒训练数据引入的深度神经网络后门漏洞，提出了FIRE（特征空间推理修复）方法，通过操纵潜在表示来抵消触发器的影响。FIRE在图像基准测试中被评估，并且显示其在低计算开销的情况下优于现有运行时缓解措施。</div>
</details>
</div>
<div class="card">
<div class="title">Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking</div>
<div class="meta-line">Authors: Vaisakh Shaj, Cameron Barker, Aidan Scannell, Andras Szecsenyi, Elliot J. Crowley, Amos Storkey</div>
<div class="meta-line">First: 2026-02-11T11:11:45+00:00 · Latest: 2026-02-11T11:11:45+00:00</div>
<div class="meta-line">Comments: Preprint. A version of this work was accepted and presented at the 1st Workshop on Epistemic Intelligence in Machine Learning (EIML) at EurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10743v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10743v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by reframing sequence modelling through a probabilistic lens, using Bayesian filters as a core primitive. While classical filters such as Kalman filters provide principled state estimation and uncertainty tracking, they are typically viewed as inherently sequential. We show that reparameterising the Kalman filter in information form enables its updates to be computed via an associative scan, allowing efficient parallel training. Building on this insight, we introduce the Kalman Linear Attention (KLA) layer, a neural sequence-modelling primitive that performs time-parallel probabilistic inference while maintaining explicit belief-state uncertainty. KLA offers strictly more expressive nonlinear updates and gating than GLA variants while retaining their computational advantages. On language modelling tasks, KLA matches or outperforms modern SSMs and GLAs across representative discrete token-manipulation and state-tracking benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>卡尔曼线性注意力：并行贝叶斯滤波高效语言建模和状态跟踪</div>
<div class="mono" style="margin-top:8px">状态空间语言模型如Mamba和门控线性注意力(GLA)由于其线性复杂度和并行训练而成为transformer的高效替代品，但往往缺乏复杂推理所需的表达能力和稳健的状态跟踪。我们通过将序列建模重新构想为概率视角，使用贝叶斯滤波器作为核心原语来解决这些限制。虽然经典的卡尔曼滤波器等滤波器提供了原理性的状态估计和不确定性跟踪，但它们通常被视为固有的顺序。我们展示了将卡尔曼滤波器重新参数化为信息形式，使其更新可以通过关联扫描计算，从而实现高效的并行训练。基于这一见解，我们引入了卡尔曼线性注意力(KLA)层，这是一种能够在保持显式信念状态不确定性的同时进行时间并行概率推理的神经序列建模原语。KLA提供了比GLA变体更严格的非线性更新和门控，同时保留其计算优势。在语言建模任务中，KLA在代表性的离散标记操作和状态跟踪基准测试中与现代状态空间模型和GLA相当或更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance state-space language models by integrating Bayesian filters for efficient and robust state tracking, addressing the limitations of existing models like Mamba and GLA. The key method involves reparameterizing the Kalman filter in information form to enable parallel training, leading to the development of the Kalman Linear Attention (KLA) layer. Experimental results show that KLA outperforms or matches modern SSMs and GLAs on various language modeling benchmarks, offering better expressivity and explicit uncertainty tracking.</div>
<div class="mono" style="margin-top:8px">论文通过引入Kalman Linear Attention (KLA)解决了Mamba和门控线性注意力(GLA)等状态空间语言模型的局限性，KLA结合了GLA的计算效率和贝叶斯滤波器的稳健状态跟踪能力。KLA通过将卡尔曼滤波器重新参数化为信息形式来实现并行训练，并在离散标记操作和状态跟踪基准测试中优于或匹配现代状态空间模型和GLA变体。</div>
</details>
</div>
<div class="card">
<div class="title">From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving</div>
<div class="meta-line">Authors: Sining Ang, Yuguang Yang, Chenxu Dang, Canyu Chen, Cheng Chi, Haiyan Liu, Xuanyao Mao, Jason Bao, Xuliang, Bingchuan Sun, Yan Wang</div>
<div class="meta-line">First: 2026-02-11T10:25:05+00:00 · Latest: 2026-02-11T10:25:05+00:00</div>
<div class="meta-line">Comments: 22 pages (10 pages main text + 12 pages appendix), 18 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10719v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10719v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer&#x27;s confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从表征互补到双系统：结合VLM和仅视觉骨干进行端到端驾驶</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）驾驶通过语言启用的骨干增强端到端（E2E）规划，但仍然不清楚除了通常的准确度-成本权衡之外的变化。我们通过在RecogDrive中使用完整的VLM和仅视觉骨干来重新审视这个问题，并在相同的扩散Transformer规划器下进行实例化。RQ1：在骨干层面，VLM可以在仅视觉骨干之上引入额外的子空间。RQ2：这种独特的子空间导致了一些长尾场景中的不同行为：VLM倾向于更具侵略性，而ViT则更为保守，两者在约2-3%的测试场景中分别胜出；通过选择每个场景中VLM和ViT分支中更好的轨迹的先验知识，我们获得了93.58的PDMS上限。RQ3：为了充分利用这一观察结果，我们提出了HybridDriveVLA，该系统同时运行ViT和VLM分支，并使用学习得分器选择它们的终点轨迹，将PDMS提高到92.10。最后，DualDriveVLA 实现了一种实用的快慢策略：默认运行ViT，并在得分器的信心低于阈值时仅调用VLM；在15%的场景中调用VLM可实现91.00的PDMS，同时提高吞吐量3.2倍。代码将被发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores the integration of Vision-Language-Action (VLA) systems with end-to-end driving, focusing on the complementary roles of Vision-Language Models (VLM) and vision-only backbones. The research introduces HybridDriveVLA, which combines both models to improve performance, and DualDriveVLA, a practical implementation that selectively uses the VLM. Key findings show that the VLM introduces unique subspaces, leading to different behaviors in long-tail scenarios, with each model excelling in about 2-3% of cases. The HybridDriveVLA achieves a PDMS of 92.10, while DualDriveVLA, by invoking the VLM only when necessary, reaches 91.00 PDMS with a 3.2x throughput improvement.</div>
<div class="mono" style="margin-top:8px">该研究探讨了将Vision-Language-Action (VLA)系统与端到端驾驶结合的方式，重点关注视觉语言模型（VLM）和纯视觉后端的互补作用。研究引入了HybridDriveVLA，结合了两种模型以提高性能，并提出了DualDriveVLA，这是一种实用的快速-慢速策略，仅在必要时调用VLM。关键发现表明，VLM引入了独特的子空间，在某些长尾场景中表现出不同的行为，每种模型在约2-3%的情况下表现出色。HybridDriveVLA实现了92.10的PDMS，而DualDriveVLA通过仅在必要时调用VLM，达到了91.00的PDMS，并提高了3.2倍的吞吐量。</div>
</details>
</div>
<div class="card">
<div class="title">Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation</div>
<div class="meta-line">Authors: Songen Gu, Yunuo Cai, Tianyu Wang, Simo Wu, Yanwei Fu</div>
<div class="meta-line">First: 2026-02-11T10:23:52+00:00 · Latest: 2026-02-11T10:23:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10717v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes &amp; Models will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>说、梦与行：学习基于指令驱动的机器人操作视频世界模型</div>
<div class="mono" style="margin-top:8px">机器人操作需要预测环境在执行动作后的演变，但大多数现有系统缺乏这种预测能力，往往导致错误和低效。尽管视觉语言模型（VLMs）提供了高层次的指导，但它们不能明确预测未来状态，现有的世界模型要么只能预测短暂的未来，要么生成空间上不一致的帧。为了解决这些挑战，我们提出了一种快速且预测性的视频条件动作框架。该方法首先选择并调整一个稳健的视频生成模型以确保可靠的未来预测，然后应用对抗蒸馏以实现快速的、少量步骤的视频生成，最后训练一个动作模型，该模型利用生成的视频和实际观察来纠正空间错误。广泛的实验表明，我们的方法生成了时间上连贯、空间上准确的视频预测，直接支持精确的操作，相对于现有基线在体感一致性、空间引用能力和任务完成方面取得了显著改进。代码与模型将发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve robotic manipulation by developing a framework that can predict future environmental states accurately. The method involves selecting and adapting a robust video generation model, applying adversarial distillation for fast video generation, and training an action model that uses both generated and real observations to correct spatial errors. The experiments demonstrate that this approach produces temporally coherent and spatially accurate video predictions, leading to better embodiment consistency, spatial referring ability, and task completion compared to existing methods.</div>
<div class="mono" style="margin-top:8px">论文通过开发一种基于视频条件动作的框架来增强预测能力，使用了稳健的视频生成模型、对抗蒸馏进行快速视频生成，并结合生成和真实观察来训练动作模型以纠正空间误差。该方法在体感一致性、空间引用能力和任务完成方面显著优于现有系统。</div>
</details>
</div>
<div class="card">
<div class="title">Omnidirectional Dual-Arm Aerial Manipulator with Proprioceptive Contact Localization for Landing on Slanted Roofs</div>
<div class="meta-line">Authors: Martijn B. J. Brummelhuis, Nathan F. Lepora, Salua Hamaza</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-11T10:03:17+00:00 · Latest: 2026-02-11T10:03:17+00:00</div>
<div class="meta-line">Comments: Accepted into 2026 International Conference on Robotics and Automation (ICRA) in Vienna</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10703v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Operating drones in urban environments often means they need to land on rooftops, which can have different geometries and surface irregularities. Accurately detecting roof inclination using conventional sensing methods, such as vision-based or acoustic techniques, can be unreliable, as measurement quality is strongly influenced by external factors including weather conditions and surface materials. To overcome these challenges, we propose a novel unmanned aerial manipulator morphology featuring a dual-arm aerial manipulator with an omnidirectional 3D workspace and extended reach. Building on this design, we develop a proprioceptive contact detection and contact localization strategy based on a momentum-based torque observer. This enables the UAM to infer the inclination of slanted surfaces blindly - through physical interaction - prior to touchdown. We validate the approach in flight experiments, demonstrating robust landings on surfaces with inclinations of up to 30.5 degrees and achieving an average surface inclination estimation error of 2.87 degrees over 9 experiments at different incline angles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全方位双臂空中操作臂及其 proprioceptive 接触定位技术用于斜屋顶着陆</div>
<div class="mono" style="margin-top:8px">在城市环境中操作无人机通常意味着它们需要在屋顶着陆，而屋顶可能具有不同的几何形状和表面不规则性。使用传统的基于视觉或声学的传感方法准确检测屋顶倾斜角度可能会不可靠，因为测量质量会受到天气条件和表面材料等外部因素的影响。为克服这些挑战，我们提出了一种新型的无人驾驶空中操作臂形态，该形态具有全方位3D工作空间和延长的臂展的双臂空中操作臂。在此设计基础上，我们开发了一种基于动量的扭矩观察器的 proprioceptive 接触检测和接触定位策略。这使UAM能够在着陆前通过物理交互盲测斜面的倾斜角度。我们在飞行实验中验证了该方法，展示了在倾斜角度高达30.5度的表面上实现稳健着陆，并在不同倾斜角度的9次实验中实现了平均表面倾斜角度估计误差为2.87度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve drone landing accuracy on slanted rooftops by developing a dual-arm aerial manipulator with an omnidirectional workspace. The method involves a proprioceptive contact detection and localization strategy using a momentum-based torque observer. Experiments show the drone can achieve robust landings on surfaces with up to 30.5-degree inclines, with an average estimation error of 2.87 degrees across nine trials at different incline angles.</div>
<div class="mono" style="margin-top:8px">本文解决了无人机在城市屋顶着陆时准确检测屋顶倾斜角度的挑战。提出了一种具有全方位工作空间和双臂的空中 manipulator，并采用一种基于动量的扭矩观察器进行接触检测策略。通过物理交互，空中 manipulator 能够盲测表面倾斜角度，实现对最大30.5度倾斜表面的稳健着陆，并在九次不同倾斜角度的实验中平均估计误差为2.87度。</div>
</details>
</div>
<div class="card">
<div class="title">AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Zhifeng Rao, Wenlong Chen, Lei Xie, Xia Hua, Dongfu Yin, Zhen Tian, F. Richard Yu</div>
<div class="meta-line">First: 2026-02-11T09:57:32+00:00 · Latest: 2026-02-11T09:57:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AugVLA-3D：深度驱动的特征增强用于视觉-语言-行动模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-行动（VLA）模型在机器人感知和控制方面取得了显著进展，但大多数现有方法主要依赖于使用2D图像训练的VLM，这限制了它们在复杂3D环境中的空间理解和动作定位。为了解决这一限制，我们提出了一种新的框架，将深度估计集成到VLA模型中，以丰富3D特征表示。具体来说，我们采用了一个名为VGGT的深度估计基线，从标准RGB输入中提取几何感知的3D线索，从而高效利用现有的大规模2D数据集，同时隐式恢复3D结构信息。为了进一步提高这些深度衍生特征的可靠性，我们引入了一个新的模块，称为行动助手，该模块用先验动作约束学习到的3D表示，并确保其与下游控制任务的一致性。通过将增强的3D特征与传统的2D视觉标记融合，我们的方法显著提高了VLA模型的泛化能力和鲁棒性。实验结果表明，所提出的方法不仅在几何上模糊的场景中增强了感知能力，还提高了动作预测的准确性。这项工作突显了深度驱动的数据增强和辅助专家监督在机器人系统中从2D观察到3D感知决策之间的潜在作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the spatial understanding and action grounding of Vision-Language-Action (VLA) models in 3D environments by integrating depth estimation into these models. The method uses a depth estimation baseline called VGGT to extract 3D cues from RGB inputs and introduces an action assistant module to ensure the consistency of 3D representations with control tasks. Experiments show that this approach improves the generalization and robustness of VLA models, particularly in geometrically ambiguous scenarios and action prediction accuracy.</div>
<div class="mono" style="margin-top:8px">研究旨在通过整合深度估计来增强Vision-Language-Action (VLA) 模型在3D环境中的空间理解和动作定位能力。方法使用深度估计基线VGGT从RGB输入中提取3D线索，并引入动作助手模块以确保与控制任务的一致性。实验结果表明，在几何上模糊的场景中，该方法提高了泛化能力和动作预测准确性。</div>
</details>
</div>
<div class="card">
<div class="title">3D-Printed Anisotropic Soft Magnetic Coating for Directional Rolling of a Magnetically Actuated Capsule Robot</div>
<div class="meta-line">Authors: Jin Zhou, Chongxun Wang, Zikang Shen, Fangzhou Xia</div>
<div class="meta-line">First: 2026-02-11T09:43:03+00:00 · Latest: 2026-02-11T09:43:03+00:00</div>
<div class="meta-line">Comments: Submitted for review at IEEE/ASME Advanced Intelligenet Mechatronics Conference (2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10688v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Capsule robots are promising tools for minimally invasive diagnostics and therapy, with applications from gastrointestinal endoscopy to targeted drug delivery and biopsy sampling. Conventional magnetic capsule robots embed bulky permanent magnets at both ends, reducing the usable cavity by about 10-20 mm and limiting integration of functional modules. We propose a compact, 3D-printed soft capsule robot with a magnetic coating that replaces internal magnets, enabling locomotion via a thin, functional shell while preserving the entire interior cavity as a continuous volume for medical payloads. The compliant silicone-magnetic composite also improves swallowability, even with a slightly larger capsule size. Magnetostatic simulations and experiments confirm that programmed NSSN/SNNS pole distributions provide strong anisotropy and reliable torque generation, enabling stable bidirectional rolling, omnidirectional steering, climbing on 7.5 degree inclines, and traversal of 5 mm protrusions. Rolling motion is sustained when the magnetic field at the capsule reaches at least 0.3 mT, corresponding to an effective actuation depth of 30 mm in our setup. Future work will optimize material composition, coating thickness, and magnetic layouts to enhance force output and durability, while next-generation robotic-arm-based field generators with closed-loop feedback will address nonlinearities and expand maneuverability. Together, these advances aim to transition coating-based capsule robots toward reliable clinical deployment and broaden their applications in minimally invasive diagnostics and therapy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于磁驱动胶囊机器人定向滚动的3D打印各向异性软磁涂层</div>
<div class="mono" style="margin-top:8px">胶囊机器人是微创诊断和治疗的有前途的工具，适用于从消化道内窥镜检查到靶向药物输送和活检采样的多种应用。传统磁驱动胶囊机器人在两端嵌入大尺寸永久磁铁，这会减少可用腔体约10-20毫米，并限制功能模块的集成。我们提出了一种紧凑型3D打印软胶囊机器人，其磁涂层替代内部磁铁，通过薄而功能的外壳实现运动，同时保持整个内部腔体作为连续体积用于医疗载荷。顺应性硅胶-磁复合材料还提高了吞咽性，即使胶囊尺寸稍大也是如此。磁稳态模拟和实验表明，编程的NSSN/SNNS极分布提供了强烈的各向异性和可靠的扭矩生成，使机器人能够实现稳定的双向滚动、全方位转向、在7.5度斜面上攀爬以及通过5毫米的凸起。当胶囊处的磁场达到至少0.3 mT时，滚动运动得以持续，对应于我们设置中的有效驱动深度为30毫米。未来的工作将优化材料组成、涂层厚度和磁布局以增强输出力和耐用性，而下一代基于机器人臂的磁场生成器结合闭环反馈将解决非线性问题并扩展机动性。这些进步旨在将基于涂层的胶囊机器人推向可靠的临床部署，并扩大其在微创诊断和治疗中的应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to develop a compact, 3D-printed soft capsule robot with a magnetic coating to replace internal magnets, preserving the entire interior cavity for medical payloads. The robot uses a compliant silicone-magnetic composite for improved swallowability and strong anisotropic magnetic fields for bidirectional rolling, omnidirectional steering, and climbing on inclines. Magnetostatic simulations and experiments confirm the effectiveness of the design, with stable rolling motion sustained at magnetic field strengths of at least 0.3 mT, corresponding to an actuation depth of 30 mm.</div>
<div class="mono" style="margin-top:8px">该研究提出了一种3D打印的软胶囊机器人，通过磁涂层替代内部磁铁，使得内部空间更大且更易于吞咽。磁畴模拟和实验表明，该涂层可以产生强大的各向异性并可靠地生成扭矩，实现双向滚动、全方位转向和越过障碍物。为了维持滚动运动，磁场所需强度至少为0.3 mT，对应的有效驱动深度为30 mm。未来的工作将优化材料和磁布局以增强输出力和耐用性。</div>
</details>
</div>
<div class="card">
<div class="title">Localized Graph-Based Neural Dynamics Models for Terrain Manipulation</div>
<div class="meta-line">Authors: Chaoqi Liu, Yunzhu Li, Kris Hauser</div>
<div class="meta-line">First: 2025-03-30T01:24:10+00:00 · Latest: 2026-02-11T09:22:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.23270v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.23270v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predictive models can be particularly helpful for robots to effectively manipulate terrains in construction sites and extraterrestrial surfaces. However, terrain state representations become extremely high-dimensional especially to capture fine-resolution details and when depth is unknown or unbounded. This paper introduces a learning-based approach for terrain dynamics modeling and manipulation, leveraging the Graph-based Neural Dynamics (GBND) framework to represent terrain deformation as motion of a graph of particles. Based on the principle that the moving portion of a terrain is usually localized, our approach builds a large terrain graph (potentially millions of particles) but only identifies a very small active subgraph (hundreds of particles) for predicting the outcomes of robot-terrain interaction. To minimize the size of the active subgraph we introduce a learning-based approach that identifies a small region of interest (RoI) based on the robot&#x27;s control inputs and the current scene. We also introduce a novel domain boundary feature encoding that allows GBNDs to perform accurate dynamics prediction in the RoI interior while avoiding particle penetration through RoI boundaries. Our proposed method is both orders of magnitude faster than naive GBND and it achieves better overall prediction accuracy. We further evaluated our framework on excavation and shaping tasks on terrain with different granularity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于局部化图的神经动力学模型在地形操控中的应用</div>
<div class="mono" style="margin-top:8px">预测模型对于机器人在建筑工地和外星球表面有效操控地形特别有帮助。然而，地形状态表示变得极其高维，尤其是在细节分辨率高且深度未知或无界的情况下。本文介绍了一种基于学习的地形动力学建模和操控方法，利用基于图的神经动力学（GBND）框架将地形变形表示为粒子图的运动。基于地形移动部分通常局部化的原理，我们的方法构建了一个大型地形图（可能包含数百万个粒子），但仅识别一个非常小的活跃子图（数百个粒子）来预测机器人-地形交互的结果。为了最小化活跃子图的大小，我们引入了一种基于学习的方法，根据机器人的控制输入和当前场景来识别感兴趣区域（RoI）。我们还引入了一种新颖的领域边界特征编码，使GBND能够在RoI内部进行准确的动力学预测，同时避免粒子穿透RoI边界。我们提出的方法比朴素的GBND快多个数量级，并且整体预测精度更高。我们进一步在不同粒度的地形上对挖掘和塑造任务进行了框架评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a learning-based approach for modeling and manipulating terrain dynamics using a Graph-based Neural Dynamics (GBND) framework. The method focuses on a localized active subgraph to predict the outcomes of robot-terrain interaction, significantly reducing computational complexity while maintaining accuracy. The approach identifies a small region of interest based on the robot&#x27;s control inputs and scene, and introduces a novel domain boundary feature encoding to prevent particle penetration. The proposed method is orders of magnitude faster than naive GBND and achieves better overall prediction accuracy.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种基于Graph-based Neural Dynamics (GBND)框架的地形动力学建模和操纵方法。该方法专注于局部活动子图来预测机器人与地形的交互结果，显著降低了计算复杂性同时保持了准确性。该方法根据机器人的控制输入和当前场景来识别一个小区域，并引入了一种新的域边界特征编码，以防止粒子穿透该区域边界。所提出的方法比朴素的GBND快了多个数量级，并且具有更好的整体预测精度。</div>
</details>
</div>
<div class="card">
<div class="title">Pitch Angle Control of a Magnetically Actuated Capsule Robot with Nonlinear FEA-based MPC and EKF Multisensory Fusion</div>
<div class="meta-line">Authors: Chongxun Wang, Zikang Shen, Apoorav Rathore, Akanimoh Udombeh, Harrison Teng, Fangzhou Xia</div>
<div class="meta-line">First: 2026-02-11T07:59:52+00:00 · Latest: 2026-02-11T07:59:52+00:00</div>
<div class="meta-line">Comments: This version is submitted for review at IEEE/ASME Transactions on Mechatronics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10610v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10610v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Magnetically actuated capsule robots promise minimally invasive diagnosis and therapy in the gastrointestinal (GI) tract, but existing systems largely neglect control of capsule pitch, a degree of freedom critical for contact-rich interaction with inclined gastric walls. This paper presents a nonlinear, model-based framework for magnetic pitch control of an ingestible capsule robot actuated by a four-coil electromagnetic array. Angle-dependent magnetic forces and torques acting on embedded permanent magnets are characterized using three-dimensional finite-element simulations and embedded as lookup tables in a control-oriented rigid-body pitching model with rolling contact and actuator dynamics. A constrained model predictive controller (MPC) is designed to regulate pitch while respecting hardware-imposed current and slew-rate limits. Experiments on a compliant stomach-inspired surface demonstrate robust pitch reorientation from both horizontal and upright configurations, achieving about three to five times faster settling and reduced oscillatory motion than on-off control. Furthermore, an extended Kalman filter (EKF) fusing inertial sensing with intermittent visual measurements enables stable closed-loop control when the camera update rate is reduced from 30 Hz to 1 Hz, emulating clinically realistic imaging constraints. These results establish finite-element-informed MPC with sensor fusion as a scalable strategy for pitch regulation, controlled docking, and future multi-degree-of-freedom capsule locomotion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>磁驱动胶囊机器人磁偏航控制的非线性FEA基MPC和EKF多传感器融合</div>
<div class="mono" style="margin-top:8px">磁驱动胶囊机器人在消化道（GI）中实现微创诊断和治疗具有潜力，但现有系统大多忽视了胶囊偏航控制，这是与倾斜胃壁进行接触丰富交互的关键自由度。本文提出了一种基于非线性模型的磁偏航控制框架，用于由四线圈电磁阵列驱动的可吞咽胶囊机器人。通过三维有限元仿真表征嵌入式永磁体上的角度依赖磁力和力矩，并将其嵌入到包含滚动接触和执行器动力学的控制导向刚体偏航模型中。设计了一个受约束的模型预测控制器（MPC）来调节偏航并遵守硬件限制的电流和加速度率限制。实验表明，该系统在柔软的胃模拟表面上能够从水平和直立配置中实现稳健的偏航重定位，比开关控制快3到5倍，并且具有更少的振荡运动。此外，通过惯性传感与间歇性视觉测量的扩展卡尔曼滤波器（EKF）融合，当相机更新率从30 Hz降低到1 Hz时，仍能实现稳定的闭环控制，模拟临床现实的成像约束。这些结果确立了基于有限元的MPC与传感器融合作为偏航调节、可控对接和未来多自由度胶囊运动的可扩展策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the control of pitch angle for a magnetically actuated capsule robot in the gastrointestinal tract. It introduces a nonlinear model-based framework using finite-element analysis for magnetic pitch control, incorporating a constrained model predictive controller and extended Kalman filter for multisensory fusion. Experiments show that this approach enables robust pitch reorientation and faster settling times compared to on-off control, and maintains stable closed-loop control even with reduced camera update rates.</div>
<div class="mono" style="margin-top:8px">本文研究了磁驱动胶囊机器人在消化道中控制俯仰角的问题。提出了一种基于有限元分析的非线性模型框架，结合了约束模型预测控制和扩展卡尔曼滤波器进行多传感器融合。实验结果显示，该方法能够实现快速稳定的俯仰角重新定向，并且即使在降低相机更新率的情况下也能保持闭环控制的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision</div>
<div class="meta-line">Authors: Hanbit Oh, Masaki Murooka, Tomohiro Motoda, Ryoichi Nakajo, Yukiyasu Domae</div>
<div class="meta-line">First: 2025-09-11T23:10:56+00:00 · Latest: 2026-02-11T07:12:19+00:00</div>
<div class="meta-line">Comments: 21 pages, 10 figures, Advanced Robotics accepted 2026.02.03</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09893v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.09893v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/sart-il">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning is a promising paradigm for training robot agents; however, standard approaches typically require substantial data acquisition -- via numerous demonstrations or random exploration -- to ensure reliable performance. Although exploration reduces human effort, it lacks safety guarantees and often results in frequent collisions -- particularly in clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual environmental resets and imposing additional human burden. This study proposes Self-Augmented Robot Trajectory (SART), a framework that enables policy learning from a single human demonstration, while safely expanding the dataset through autonomous augmentation. SART consists of two stages: (1) human teaching only once, where a single demonstration is provided and precision boundaries -- represented as spheres around key waypoints -- are annotated, followed by one environment reset; (2) robot self-augmentation, where the robot generates diverse, collision-free trajectories within these boundaries and reconnects to the original demonstration. This design improves the data collection efficiency by minimizing human effort while ensuring safety. Extensive evaluations in simulation and real-world manipulation tasks show that SART achieves substantially higher success rates than policies trained solely on human-collected demonstrations. Video results available at https://sites.google.com/view/sart-il .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自我增强机器人轨迹：通过安全自我增强与演示者标注的精确性扩展现有人类演示</div>
<div class="mono" style="margin-top:8px">模仿学习是一种有前景的机器人代理训练范式；然而，标准方法通常需要大量数据采集——通过多次演示或随机探索——以确保可靠性能。尽管探索减少了人力，但缺乏安全性保证，经常导致频繁碰撞——特别是在空间受限的任务（如孔中插针）中——从而需要手动重置环境并增加额外的人力负担。本研究提出了一种自我增强机器人轨迹（SART）框架，该框架能够仅从一次人类演示中学习策略，并通过自主增强安全地扩展数据集。SART 包含两个阶段：（1）人类教学仅一次，提供一次演示并标注精度边界（以关键航点周围的球体表示），随后重置一次环境；（2）机器人自我增强，机器人在这些边界内生成多样且无碰撞的轨迹，并重新连接到原始演示。此设计通过最小化人力来提高数据采集效率，同时确保安全性。在模拟和实际操作任务中的广泛评估表明，SART 在仅使用人类收集的演示训练的策略中实现了显著更高的成功率。视频结果可在 https://sites.google.com/view/sart-il 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces Self-Augmented Robot Trajectory (SART), a framework for imitation learning that uses a single human demonstration and autonomous augmentation to generate diverse, collision-free trajectories. SART reduces the need for extensive data collection and manual resets, improving data efficiency and safety. Experimental results demonstrate that SART outperforms policies trained solely on human-collected demonstrations in both simulation and real-world manipulation tasks, achieving higher success rates. Video results are available at https://sites.google.com/view/sart-il .</div>
<div class="mono" style="margin-top:8px">本研究提出了一种名为Self-Augmented Robot Trajectory (SART) 的框架，利用单次人类演示和自主扩充生成多样且无碰撞的轨迹。SART 减少了大量数据收集和手动重置的需求，提高了数据收集效率和安全性。实验结果显示，SART 在仿真和真实世界操作任务中的表现优于仅基于人类收集数据训练的策略，成功率更高。视频结果可查看 https://sites.google.com/view/sart-il .</div>
</details>
</div>
<div class="card">
<div class="title">Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping</div>
<div class="meta-line">Authors: Dwip Dalal, Gautam Vashishtha, Utkarsh Mishra, Jeonghwan Kim, Madhav Kanda, Hyeonjeong Ha, Svetlana Lazebnik, Heng Ji, Unnat Jain</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-10T17:57:06+00:00 · Latest: 2026-02-11T06:32:04+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09741v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09741v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) often miss small details and spatial relations in cluttered scenes, leading to errors in fine-grained perceptual grounding. We introduce AttWarp, a lightweight method that allocates more resolution to query-relevant content while compressing less informative areas, all while preserving global context. At test time, the approach uses an MLLM&#x27;s cross-modal attention to perform rectilinear warping of the input image, reallocating spatial resolution toward regions the model deems important, without changing model weights or architecture. This attention-guided warping preserves all original image information but redistributes it non-uniformly, so small objects and subtle relationships become easier for the same model to read while the global layout remains intact. Across five benchmarks (TextVQA, GQA, DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and InstructBLIP), AttWarp consistently improves accuracy, strengthens compositional reasoning, and reduces hallucinations, outperforming four competitive baselines that manipulate raw images at test time. Together, these results show that attention-guided warping prioritizes information relevant to the query while preserving context, and that the same MLLMs perform better when given such warped inputs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>建设性失真：通过注意力引导的图像扭曲改进MLLM</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）往往在杂乱场景中遗漏小细节和空间关系，导致细粒度感知定位错误。我们引入了AttWarp，这是一种轻量级方法，它将更多分辨率分配给与查询相关的内容，同时压缩不那么信息丰富的区域，同时保持全局上下文。在测试时，该方法使用MLLM的跨模态注意力对输入图像进行线性扭曲，重新分配空间分辨率到模型认为重要的区域，而不改变模型权重或架构。这种注意力引导的扭曲保留了所有原始图像信息，但重新分配得不均匀，因此小对象和微妙的关系变得更容易让同一模型读取，而全局布局保持不变。在五个基准（TextVQA、GQA、DocVQA、POPE、MMMU）和四个MLLM（LLaVA、Qwen-VL、InternVL、InstructBLIP）上，AttWarp 一致地提高了准确性，增强了组合推理，并减少了幻觉，优于四个在测试时操作原始图像的竞争基线。这些结果共同表明，注意力引导的扭曲优先考虑与查询相关的信息，同时保留上下文，而且在给定此类扭曲输入时，相同的MLLM表现更好。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of MLLMs missing small details and spatial relations in cluttered scenes, proposing AttWarp, a method that uses cross-modal attention to perform rectilinear warping on input images, reallocating resolution to query-relevant content. This method improves accuracy, strengthens compositional reasoning, and reduces hallucinations across five benchmarks and four MLLMs, outperforming four competitive baselines that manipulate raw images at test time.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决多模态大型语言模型在处理复杂场景时容易忽略小细节和空间关系的问题，来提升其感知定位能力。AttWarp 方法通过注意力引导的图像变形，将更多分辨率分配给与查询相关的内容，同时压缩不相关信息区域，而不改变模型架构或权重。该方法在五个基准和四种多模态大型语言模型中提高了准确率，增强了组合推理能力，并减少了幻觉现象，优于四种在测试时操作原始图像的竞争基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots</div>
<div class="meta-line">Authors: Chongxi Meng, Da Zhao, Yifei Zhao, Minghao Zeng, Yanmin Zhou, Zhipeng Wang, Bin He</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-11T06:18:04+00:00 · Latest: 2026-02-11T06:18:04+00:00</div>
<div class="meta-line">Comments: Accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10561v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10561v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a closed-loop automation framework for heterogeneous modular robots, covering the full pipeline from morphological construction to adaptive control. In this framework, a mobile manipulator handles heterogeneous functional modules including structural, joint, and wheeled modules to dynamically assemble diverse robot configurations and provide them with immediate locomotion capability. To address the state-space explosion in large-scale heterogeneous reconfiguration, we propose a hierarchical planner: the high-level planner uses a bidirectional heuristic search with type-penalty terms to generate module-handling sequences, while the low level planner employs A* search to compute optimal execution trajectories. This design effectively decouples discrete configuration planning from continuous motion execution. For adaptive motion generation of unknown assembled configurations, we introduce a GPU accelerated Annealing-Variance Model Predictive Path Integral (MPPI) controller. By incorporating a multi stage variance annealing strategy to balance global exploration and local convergence, the controller enables configuration-agnostic, real-time motion control. Large scale simulations show that the type-penalty term is critical for planning robustness in heterogeneous scenarios. Moreover, the greedy heuristic produces plans with lower physical execution costs than the Hungarian heuristic. The proposed annealing-variance MPPI significantly outperforms standard MPPI in both velocity tracking accuracy and control frequency, achieving real time control at 50 Hz. The framework validates the full-cycle process, including module assembly, robot merging and splitting, and dynamic motion generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>形态发生装配与自适应控制在异构模块化机器人中的应用</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于异构模块化机器人的闭环自动化框架，涵盖了从形态构建到自适应控制的完整流程。在此框架中，移动执行器处理包括结构、关节和轮式模块在内的异构功能模块，以动态组装多种机器人配置并提供即时移动能力。为了解决大规模异构重构中的状态空间爆炸问题，我们提出了一种分层规划器：高层规划器使用双向启发式搜索和类型惩罚项生成模块处理序列，而低层规划器采用A*搜索计算最优执行轨迹。此设计有效地将离散配置规划与连续运动执行解耦。对于未知组装配置的自适应运动生成，我们引入了一种基于GPU加速的退火方差模型预测路径积分（MPPI）控制器。通过引入多阶段方差退火策略来平衡全局探索和局部收敛，控制器实现了配置无关的实时运动控制。大规模仿真实验表明，类型惩罚项对于异构场景中的规划鲁棒性至关重要。此外，贪婪启发式算法生成的计划在物理执行成本上低于匈牙利启发式算法。所提出的退火方差MPPI在速度跟踪精度和控制频率方面显著优于标准MPPI，实现了50 Hz的实时控制。该框架验证了从模块组装、机器人合并和分裂到动态运动生成的完整过程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a closed-loop automation framework for heterogeneous modular robots, which includes morphological construction and adaptive control. The framework uses a hierarchical planner with a bidirectional heuristic search and A* search for discrete configuration planning and continuous motion execution, respectively. An Annealing-Variance Model Predictive Path Integral (MPPI) controller is proposed for adaptive motion generation, which outperforms standard MPPI in velocity tracking accuracy and control frequency. The type-penalty term is crucial for planning robustness, and the greedy heuristic produces lower physical execution costs compared to the Hungarian heuristic.</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于异构模块化机器人的闭环自动化框架，涵盖了形态构建和自适应控制。该框架采用分层规划者，结合双向启发式搜索和A*搜索分别进行离散配置规划和连续运动执行。还提出了一种带有全局探索和局部收敛平衡策略的GPU加速Annealing-Variance MPPI控制器，能够实现配置无关的实时运动控制。结果表明，类型惩罚项对于异构场景下的稳健规划至关重要，所提出的控制器在速度跟踪精度和控制频率方面优于标准MPPI，实现了50 Hz的实时控制。</div>
</details>
</div>
<div class="card">
<div class="title">LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer</div>
<div class="meta-line">Authors: Lihan Zha, Asher J. Hancock, Mingtong Zhang, Tenny Yin, Yixuan Huang, Dhruv Shah, Allen Z. Ren, Anirudha Majumdar</div>
<div class="meta-line">First: 2026-02-11T06:09:11+00:00 · Latest: 2026-02-11T06:09:11+00:00</div>
<div class="meta-line">Comments: Project website: https://lap-vla.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10556v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10556v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lap-vla.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model&#x27;s input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAP：语言-动作预训练使零样本跨体态迁移成为可能</div>
<div class="mono" style="margin-top:8px">机器人领域的一个长期目标是能够零样本部署在新机器人体态上的通用策略，无需针对每个体态进行适应。尽管进行了大规模的多体态预训练，现有的视觉-语言-动作模型（VLAs）仍然紧密耦合于其训练体态，并且通常需要昂贵的微调。我们引入了语言-动作预训练（LAP），这是一种简单的配方，直接将低级机器人动作表示为自然语言，使动作监督与预训练的视觉-语言模型的输入-输出分布相匹配。LAP 不需要学习分词器，不需要昂贵的注释，也不需要针对特定体态的架构设计。基于 LAP，我们提出了 LAP-3B，据我们所知，这是第一个在无需任何体态特定微调的情况下实现显著零样本迁移至未见过的机器人体态的 VLAs。在多个新型机器人和操作任务上，LAP-3B 达到了超过 50% 的平均零样本成功率，比最强的先前 VLAs 提供了大约 2 倍的改进。我们还展示了 LAP 能够实现高效的适应和有利的扩展，并在共享的语言-动作格式中统一了动作预测和 VQA，通过协同训练进一步获得收益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a generalist policy for robots that can be deployed zero-shot on new embodiments without per-embodiment adaptation. The key method is Language-Action Pre-training (LAP), which represents low-level robot actions in natural language, aligning action supervision with the pre-trained vision-language model&#x27;s input-output distribution. This approach requires no learned tokenizer, costly annotation, or embodiment-specific architectural design. LAP-3B, based on LAP, achieves substantial zero-shot transfer to previously unseen robots, with over 50% average zero-shot success across multiple manipulation tasks, doubling the success rate of previous models. Additionally, LAP enables efficient adaptation and favorable scaling, unifying action prediction and VQA in a shared language-action format for further gains through co-training.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种通用的机器人策略，能够在无需针对特定机器人进行调整的情况下，零样本部署到新的机器人身体上。Language-Action Pre-training (LAP) 方法将低级机器人动作直接表示为自然语言，使动作监督与预训练的视觉-语言模型的输入输出分布对齐。基于LAP的LAP-3B实现了在未见过的机器人身体上进行零样本转移，无需特定身体的微调，平均零样本成功率超过50%，相比最强的先前视觉-语言-行动模型，性能提高了约2倍。</div>
</details>
</div>
<div class="card">
<div class="title">RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments</div>
<div class="meta-line">Authors: Dharmendra Sharma, Archit Sharma, John Rebeiro, Vaibhav Kesharwani, Peeyush Thakur, Narendra Kumar Dhar, Laxmidhar Behera</div>
<div class="meta-line">First: 2026-02-10T17:37:35+00:00 · Latest: 2026-02-11T05:36:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10015v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10015v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboSubtaskNet：在实际环境中的类人到机器人技能转移的时序子任务分割</div>
<div class="mono" style="margin-top:8px">在长的未剪辑视频中，准确地定位和分类细粒度的子任务片段对于安全的人机协作至关重要。与通用活动识别不同，协作操作需要可以直接由机器人执行的子任务标签。我们提出了RoboSubtaskNet，这是一种多阶段的人机子任务分割框架，结合了注意力增强的I3D特征（RGB加上光流）与修改后的MS-TCN，采用斐波那契扩张计划来捕捉更好的短期过渡，如接近-拾取-放置。该网络通过复合目标训练，包括交叉熵和时间正则化（截断均方误差和一个过渡感知项），以减少过度分割并鼓励有效的子任务进展。为了弥合视觉基准与控制之间的差距，我们引入了RoboSubtask数据集，该数据集包含医疗保健和工业演示，并在子任务级别进行了注释，旨在确定性地映射到操作器原语。实验上，RoboSubtaskNet在GTEA和我们的RoboSubtask基准（边界敏感和序列指标）上优于MS-TCN和MS-TCN++，在长时序的Breakfast基准上保持竞争力。具体而言，RoboSubtaskNet在GTEA上达到F1@50=79.5%，编辑距离=88.6%，准确率=78.9%；在Breakfast上达到F1@50=30.4%，编辑距离=52.0%，准确率=53.5%；在RoboSubtask上达到F1@50=94.2%，编辑距离=95.6%，准确率=92.2%。我们进一步在7自由度Kinova Gen3操作器上验证了完整的感知到执行管道，在物理试验中实现了可靠的端到端行为（总体任务成功率约为91.25%）。这些结果展示了从子任务级别视频理解到实际环境中部署的机器人操作的实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboSubtaskNet is a multi-stage framework for segmenting fine-grained sub-tasks in long videos, crucial for safe human-robot collaboration. It uses attention-enhanced I3D features combined with a modified MS-TCN for better short-horizon transitions. The network is trained with a composite objective to reduce over-segmentation and encourage valid sub-task progressions. RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and RoboSubtask benchmarks, achieving high F1, Edit, and Acc scores. It also demonstrates reliable end-to-end behavior on a 7-DoF Kinova Gen3 manipulator in physical trials.</div>
<div class="mono" style="margin-top:8px">研究旨在开发一种框架，用于在长视频中分割细粒度子任务，以实现安全的人机协作。RoboSubtaskNet 使用多阶段方法，结合注意力增强的 I3D 特征和修改后的 MS-TCN 来分类子任务。网络通过复合目标进行训练，以减少过度分割并鼓励有效的子任务进展。实验表明，RoboSubtaskNet 在 GTEA 和 RoboSubtask 基准测试中优于 MS-TCN 和 MS-TCN++，并在 7-DoF Kinova Gen3 操作器的物理试验中实现了可靠的端到端行为。</div>
</details>
</div>
<div class="card">
<div class="title">Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Shihao Dong, Yeke Chen, Zeren Luo, Jiahui Zhang, Bowen Xu, Jinghan Lin, Yimin Han, Ji Ma, Zhiyou Yu, Yudong Zhao, Peng Lu</div>
<div class="meta-line">First: 2026-02-11T04:28:04+00:00 · Latest: 2026-02-11T04:28:04+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10514v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>共跳：基于多智能体强化学习的四足机器人协同跳跃</div>
<div class="mono" style="margin-top:8px">虽然单智能体腿式运动取得了显著进展，但个体机器人仍然受到物理执行限制的基本约束。为超越这些限制，我们引入了共跳，这是一种两个四足机器人同步执行远超单个机器人能力的跳跃任务。我们在此去中心化的设置下处理这项任务的高冲击接触动力学，实现了无需显式通信或预设运动基元的同步。我们的框架利用了增强的多智能体近端策略优化（MAPPO），并结合了渐进式课程策略，有效地克服了机械耦合系统中固有的稀疏奖励探索挑战。我们在仿真中展示了稳健的性能，并成功转移到物理硬件，执行多方向跳跃至高达1.5米高的平台。具体而言，其中一个机器人实现了脚端抬高1.1米，这比单独的四足机器人跳跃高度0.45米提高了144%，展示了卓越的垂直性能。值得注意的是，这种精确的协调仅通过本体感受反馈实现，为受限环境下的无通信协作运动奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the capabilities of quadrupedal robots by enabling cooperative jumping, which surpasses the limitations of individual robots. The study employs Multi-Agent Proximal Policy Optimization (MAPPO) with a progressive curriculum strategy to address the sparse-reward challenges in mechanically coupled systems. Key findings include robust simulation and physical hardware performance, with one robot achieving a 1.1 m foot-end elevation, a 144% improvement over solo jumps, and demonstrating precise coordination through proprioceptive feedback alone.</div>
<div class="mono" style="margin-top:8px">研究引入了Co-jump，一种让两台四足机器人同步执行超出单个机器人能力范围的跳跃任务。研究使用了带有渐进式课程策略的Multi-Agent Proximal Policy Optimization (MAPPO)，以解决机械耦合系统中的稀疏奖励探索挑战。实验结果表明，在模拟和物理硬件中表现出稳健性能，其中一个机器人实现了比单个机器人跳跃高度提高144%的垂直跳跃，达到1.1米，这证明了在没有显式通信或预设运动模式的情况下，可以实现精确的协调跳跃。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0413.html">20260211_0413</a>
<a href="archive/20260210_1402.html">20260210_1402</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0339.html">20260208_0339</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0337.html">20260202_0337</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
